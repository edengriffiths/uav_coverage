========== uav-v12 ==========
Seed: 1086557178
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v12_1
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.94e+03 |
| time/              |           |
|    fps             | 286       |
|    iterations      | 1         |
|    time_elapsed    | 57        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-2376.13 +/- 2394.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.38e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007830555 |
|    clip_fraction        | 0.071       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.00211     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 4.04e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.08e+03 |
| time/              |           |
|    fps             | 240       |
|    iterations      | 2         |
|    time_elapsed    | 136       |
|    total_timesteps | 32768     |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.01e+03  |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 3          |
|    time_elapsed         | 202        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.00843692 |
|    clip_fraction        | 0.0694     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.81      |
|    explained_variance   | -9.89e-06  |
|    learning_rate        | 0.0003     |
|    loss                 | 1.52e+04   |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00928   |
|    value_loss           | 2.93e+04   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-823.48 +/- 278.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -823        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008677963 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -9.18e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47e+04    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 2.65e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.27e+03 |
| time/              |           |
|    fps             | 232       |
|    iterations      | 4         |
|    time_elapsed    | 281       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-597.58 +/- 379.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -598        |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.009322232 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.33e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000224   |
|    value_loss           | 9.14e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.33e+03 |
| time/              |           |
|    fps             | 227       |
|    iterations      | 5         |
|    time_elapsed    | 360       |
|    total_timesteps | 81920     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -2.33e+03    |
| time/                   |              |
|    fps                  | 230          |
|    iterations           | 6            |
|    time_elapsed         | 426          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0138118025 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.6e+03      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 1.06e+04     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=-1317.30 +/- 448.65
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.32e+03    |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0066264225 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.000538     |
|    learning_rate        | 0.0003       |
|    loss                 | 844          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00666     |
|    value_loss           | 3.76e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    fps             | 226       |
|    iterations      | 7         |
|    time_elapsed    | 505       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1437.32 +/- 480.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.44e+03    |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0057533546 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00728     |
|    value_loss           | 2.06e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.41e+03 |
| time/              |           |
|    fps             | 224       |
|    iterations      | 8         |
|    time_elapsed    | 584       |
|    total_timesteps | 131072    |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.02e+03    |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 9            |
|    time_elapsed         | 651          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0071394444 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.9e+03      |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 6.05e+03     |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-1317.70 +/- 239.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.010353833 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 168         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -748     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 10       |
|    time_elapsed    | 730      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-1077.68 +/- 239.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.012662636 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.7        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 169         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -624     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 11       |
|    time_elapsed    | 809      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -348        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 12          |
|    time_elapsed         | 875         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.012967849 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.19        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-1656.65 +/- 2155.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.66e+03   |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.013734544 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.7        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -332     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 13       |
|    time_elapsed    | 954      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-233.85 +/- 472.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -234        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.011218881 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 215         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -366     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 14       |
|    time_elapsed    | 1033     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -340        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 15          |
|    time_elapsed         | 1100        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.010825993 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 240         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=3.07 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.013800697 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.0412      |
|    learning_rate        | 0.0003      |
|    loss                 | 60.7        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 161         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 16       |
|    time_elapsed    | 1179     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=3.40 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.013478445 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 138         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 17       |
|    time_elapsed    | 1258     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -309        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 18          |
|    time_elapsed         | 1325        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.017736368 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 85.8        |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=4.57 +/- 2.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.57        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.010005236 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | -0.814      |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 430         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 19       |
|    time_elapsed    | 1404     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=8.34 +/- 10.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.34        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.015489642 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.9        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 112         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 20       |
|    time_elapsed    | 1483     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -252        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 21          |
|    time_elapsed         | 1550        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.017864898 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=-3452.53 +/- 6913.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.45e+03   |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.019658808 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.2        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 91.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -303     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 22       |
|    time_elapsed    | 1629     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-115.17 +/- 238.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.010718331 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.0456      |
|    learning_rate        | 0.0003      |
|    loss                 | 998         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 1.42e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -449     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 23       |
|    time_elapsed    | 1708     |
|    total_timesteps | 376832   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -640         |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 24           |
|    time_elapsed         | 1775         |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0107593285 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.577        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.92e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.0039      |
|    value_loss           | 4.42e+03     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=3.41 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.008546347 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 5.43e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -808     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 25       |
|    time_elapsed    | 1854     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=3.64 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.64        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.019123802 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.4        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 369         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -690     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 26       |
|    time_elapsed    | 1933     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -416        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 27          |
|    time_elapsed         | 2000        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.011926293 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 929         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 836         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=16.67 +/- 25.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 16.7        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.009776463 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.3        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 808         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -258     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 28       |
|    time_elapsed    | 2079     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=4.09 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.09        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.013530455 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.3         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 1.25e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -289     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 29       |
|    time_elapsed    | 2158     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -213        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 30          |
|    time_elapsed         | 2224        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.020495888 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.14        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 727         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=122.04 +/- 229.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 122        |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.01858138 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.69      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.4       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.00909   |
|    value_loss           | 28.3       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 31       |
|    time_elapsed    | 2303     |
|    total_timesteps | 507904   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -156         |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 32           |
|    time_elapsed         | 2370         |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0072466666 |
|    clip_fraction        | 0.0641       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+03     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00714     |
|    value_loss           | 1.61e+03     |
------------------------------------------
Eval num_timesteps=525000, episode_reward=21.05 +/- 15.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 21         |
| time/                   |            |
|    total_timesteps      | 525000     |
| train/                  |            |
|    approx_kl            | 0.01775274 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 49.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 33       |
|    time_elapsed    | 2449     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=76.83 +/- 147.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 76.8       |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.01517266 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.68      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 652        |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.00563   |
|    value_loss           | 649        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 34       |
|    time_elapsed    | 2528     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 35          |
|    time_elapsed         | 2595        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.012477949 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.2        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 285         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=9.00 +/- 8.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9           |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.020250568 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00909    |
|    value_loss           | 34          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 36       |
|    time_elapsed    | 2674     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=6.77 +/- 2.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.77        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.011434952 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 160         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 341         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 37       |
|    time_elapsed    | 2753     |
|    total_timesteps | 606208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -93.9      |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 38         |
|    time_elapsed         | 2819       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.02559317 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.66      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.6       |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00125   |
|    value_loss           | 154        |
----------------------------------------
Eval num_timesteps=625000, episode_reward=5.84 +/- 2.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.84       |
| time/                   |            |
|    total_timesteps      | 625000     |
| train/                  |            |
|    approx_kl            | 0.01694959 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.67      |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.77       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.00629   |
|    value_loss           | 275        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 39       |
|    time_elapsed    | 2898     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=44.32 +/- 76.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 44.3        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.021290466 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0003      |
|    loss                 | 89          |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.000906    |
|    value_loss           | 338         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 40       |
|    time_elapsed    | 2978     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -179        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 41          |
|    time_elapsed         | 3044        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.018663185 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 706         |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.000657    |
|    value_loss           | 740         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=27.07 +/- 34.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 27.1        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.016338792 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 258         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 42       |
|    time_elapsed    | 3123     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=25.16 +/- 24.37
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 25.2      |
| time/                   |           |
|    total_timesteps      | 700000    |
| train/                  |           |
|    approx_kl            | 0.0177887 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.68     |
|    explained_variance   | 0.825     |
|    learning_rate        | 0.0003    |
|    loss                 | 23.6      |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.00272  |
|    value_loss           | 441       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 43       |
|    time_elapsed    | 3202     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -361        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 44          |
|    time_elapsed         | 3269        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.007034651 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=4.40 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.017954772 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.21        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 114         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 45       |
|    time_elapsed    | 3348     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=9.96 +/- 6.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.96        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.011988245 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.921       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 813         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 46       |
|    time_elapsed    | 3427     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 47          |
|    time_elapsed         | 3494        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.015656903 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 99          |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=31.74 +/- 47.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 31.7        |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.024057934 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 138         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.2    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 48       |
|    time_elapsed    | 3573     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=14.63 +/- 10.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.018037185 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.2        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 49       |
|    time_elapsed    | 3652     |
|    total_timesteps | 802816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -120      |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 50        |
|    time_elapsed         | 3719      |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.0123106 |
|    clip_fraction        | 0.133     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.66     |
|    explained_variance   | 0.65      |
|    learning_rate        | 0.0003    |
|    loss                 | 367       |
|    n_updates            | 490       |
|    policy_gradient_loss | -0.00427  |
|    value_loss           | 377       |
---------------------------------------
Eval num_timesteps=825000, episode_reward=-24.06 +/- 273.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -24.1       |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.022003898 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 47.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 51       |
|    time_elapsed    | 3798     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=28.16 +/- 43.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 28.2        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.016554682 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 185         |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 365         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 52       |
|    time_elapsed    | 3877     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 53          |
|    time_elapsed         | 3944        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.017738547 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.202       |
|    learning_rate        | 0.0003      |
|    loss                 | 60          |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 89.6        |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=6.16 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.16        |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.018602666 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.7        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 19          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -95.1    |
| time/              |          |
|    fps             | 219      |
|    iterations      | 54       |
|    time_elapsed    | 4023     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=126.39 +/- 236.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 126         |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.012395827 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.7        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 260         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 55       |
|    time_elapsed    | 4102     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -42.8       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 56          |
|    time_elapsed         | 4168        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.018222237 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 45.2        |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=-1590.93 +/- 3201.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.59e+03   |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.016214605 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.3        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 65.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 57       |
|    time_elapsed    | 4247     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=13.17 +/- 6.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 13.2        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.014086042 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 254         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 58       |
|    time_elapsed    | 4326     |
|    total_timesteps | 950272   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 59         |
|    time_elapsed         | 4393       |
|    total_timesteps      | 966656     |
| train/                  |            |
|    approx_kl            | 0.01311942 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 50.6       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.00569   |
|    value_loss           | 361        |
----------------------------------------
Eval num_timesteps=975000, episode_reward=15.45 +/- 8.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.019365104 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.0025      |
|    value_loss           | 258         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 60       |
|    time_elapsed    | 4472     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 61          |
|    time_elapsed         | 4538        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.019523874 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 23.7        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=6.97 +/- 3.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.97        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.007726335 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 417         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 732         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 62       |
|    time_elapsed    | 4616     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=7.35 +/- 2.57
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 7.35         |
| time/                   |              |
|    total_timesteps      | 1025000      |
| train/                  |              |
|    approx_kl            | 0.0116845565 |
|    clip_fraction        | 0.139        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.57        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.1         |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00521     |
|    value_loss           | 201          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 63       |
|    time_elapsed    | 4693     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -198        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 64          |
|    time_elapsed         | 4759        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.016202085 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.8        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 387         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=37.00 +/- 45.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 37          |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.018071383 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 31.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 65       |
|    time_elapsed    | 4837     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=67.12 +/- 76.57
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 67.1         |
| time/                   |              |
|    total_timesteps      | 1075000      |
| train/                  |              |
|    approx_kl            | 0.0056653405 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.62        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.0003       |
|    loss                 | 94.4         |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00339     |
|    value_loss           | 481          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 66       |
|    time_elapsed    | 4914     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -84.9       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 67          |
|    time_elapsed         | 4980        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.024089072 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.36        |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 29.6        |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=7.89 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.89        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.010441305 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.29        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 36.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.3    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 68       |
|    time_elapsed    | 5058     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=15.11 +/- 7.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 15.1       |
| time/                   |            |
|    total_timesteps      | 1125000    |
| train/                  |            |
|    approx_kl            | 0.01650975 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.47       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.00255   |
|    value_loss           | 60         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 9.85     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 69       |
|    time_elapsed    | 5136     |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 3.35       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 70         |
|    time_elapsed         | 5201       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.02124995 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 6          |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.003     |
|    value_loss           | 23.3       |
----------------------------------------
Eval num_timesteps=1150000, episode_reward=8.77 +/- 5.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.77        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.021710956 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00696    |
|    value_loss           | 53.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 71       |
|    time_elapsed    | 5279     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=109.59 +/- 175.94
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 110        |
| time/                   |            |
|    total_timesteps      | 1175000    |
| train/                  |            |
|    approx_kl            | 0.01745998 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.00658   |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -98.4    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 72       |
|    time_elapsed    | 5356     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -125        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 73          |
|    time_elapsed         | 5422        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.008062418 |
|    clip_fraction        | 0.0817      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.00212     |
|    learning_rate        | 0.0003      |
|    loss                 | 96.9        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 453         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=44.28 +/- 15.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 44.3        |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.012220867 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.9        |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00956    |
|    value_loss           | 202         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 74       |
|    time_elapsed    | 5500     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=40.03 +/- 21.63
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 40        |
| time/                   |           |
|    total_timesteps      | 1225000   |
| train/                  |           |
|    approx_kl            | 0.0182542 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6      |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.69      |
|    n_updates            | 740       |
|    policy_gradient_loss | -0.00552  |
|    value_loss           | 25.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 75       |
|    time_elapsed    | 5577     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -209        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 76          |
|    time_elapsed         | 5643        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.010190643 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 577         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=48.69 +/- 67.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 48.7        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.010351764 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0003      |
|    loss                 | 577         |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 77       |
|    time_elapsed    | 5721     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=27.05 +/- 8.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 27.1        |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.009251952 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 197         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 602         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 78       |
|    time_elapsed    | 5798     |
|    total_timesteps | 1277952  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -165         |
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 79           |
|    time_elapsed         | 5864         |
|    total_timesteps      | 1294336      |
| train/                  |              |
|    approx_kl            | 0.0149550475 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.48        |
|    explained_variance   | 0.815        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.4         |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00588     |
|    value_loss           | 311          |
------------------------------------------
Eval num_timesteps=1300000, episode_reward=16.22 +/- 15.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.019299267 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.1        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 82.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 80       |
|    time_elapsed    | 5942     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=68.85 +/- 35.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 68.8        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.017588291 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.19        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.46        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.3    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 81       |
|    time_elapsed    | 6019     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.7       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 82          |
|    time_elapsed         | 6085        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.018553516 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | -0.0609     |
|    learning_rate        | 0.0003      |
|    loss                 | 53.3        |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=21.80 +/- 18.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 21.8        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.015333766 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.56        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 369         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 83       |
|    time_elapsed    | 6163     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=109.72 +/- 155.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 110         |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.008755049 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 621         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 84       |
|    time_elapsed    | 6240     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -150        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 85          |
|    time_elapsed         | 6306        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.024631653 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 219         |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=14.37 +/- 10.00
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 14.4       |
| time/                   |            |
|    total_timesteps      | 1400000    |
| train/                  |            |
|    approx_kl            | 0.01512251 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.2       |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.00776   |
|    value_loss           | 91.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 86       |
|    time_elapsed    | 6384     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=41.49 +/- 32.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 41.5       |
| time/                   |            |
|    total_timesteps      | 1425000    |
| train/                  |            |
|    approx_kl            | 0.01741495 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.4       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.00764   |
|    value_loss           | 55.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 87       |
|    time_elapsed    | 6462     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 9.74        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 88          |
|    time_elapsed         | 6527        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.014327159 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.7        |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 60.4        |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=37.45 +/- 18.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 37.4        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.018110715 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.86        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 60          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 89       |
|    time_elapsed    | 6605     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.43        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 90          |
|    time_elapsed         | 6670        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.016410857 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.42        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=-13.11 +/- 112.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -13.1       |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.014386458 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.4        |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.2    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 91       |
|    time_elapsed    | 6748     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=94.03 +/- 87.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 94          |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.011766506 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 207         |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 349         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93.1    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 92       |
|    time_elapsed    | 6826     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 93          |
|    time_elapsed         | 6891        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.012206692 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.8        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 243         |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=24.86 +/- 22.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 24.9        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.021067215 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.8        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.5    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 94       |
|    time_elapsed    | 6969     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=34.83 +/- 21.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 34.8        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.017709576 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.1        |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.1    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 95       |
|    time_elapsed    | 7047     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44.9       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 96          |
|    time_elapsed         | 7113        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.014097988 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.97        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=-2111.02 +/- 4257.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -2.11e+03  |
| time/                   |            |
|    total_timesteps      | 1575000    |
| train/                  |            |
|    approx_kl            | 0.01694075 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 183        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00725   |
|    value_loss           | 181        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.5    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 97       |
|    time_elapsed    | 7190     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=20.81 +/- 23.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 20.8        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.019587986 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.2         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 94.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.5    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 98       |
|    time_elapsed    | 7268     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -29.3      |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 99         |
|    time_elapsed         | 7333       |
|    total_timesteps      | 1622016    |
| train/                  |            |
|    approx_kl            | 0.01842527 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 61.6       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.00861   |
|    value_loss           | 91.6       |
----------------------------------------
Eval num_timesteps=1625000, episode_reward=42.42 +/- 18.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 42.4        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.013209932 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 457         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00651    |
|    value_loss           | 201         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.9    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 100      |
|    time_elapsed    | 7411     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=54.91 +/- 50.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 54.9        |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.010890145 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | 70          |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 469         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.1    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 101      |
|    time_elapsed    | 7489     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -74.5       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 102         |
|    time_elapsed         | 7554        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.018771777 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.6        |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=100.01 +/- 101.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 100         |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.017521191 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.8        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00913    |
|    value_loss           | 63          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 103      |
|    time_elapsed    | 7632     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=69.35 +/- 63.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 69.4        |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.016454183 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.56        |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 67.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 104      |
|    time_elapsed    | 7710     |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 105        |
|    time_elapsed         | 7776       |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 0.01809622 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.07       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0078    |
|    value_loss           | 130        |
----------------------------------------
Eval num_timesteps=1725000, episode_reward=66.52 +/- 62.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 66.5        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.015516572 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.17        |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 94.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 106      |
|    time_elapsed    | 7854     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=40.77 +/- 12.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 40.8        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.014720585 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.5        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.6    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 107      |
|    time_elapsed    | 7932     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.9       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 108         |
|    time_elapsed         | 7997        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.009687254 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 282         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 323         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=105.24 +/- 175.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 105         |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.019610414 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 37.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.7    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 109      |
|    time_elapsed    | 8075     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=65.96 +/- 51.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 66          |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.016975936 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 31.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 110      |
|    time_elapsed    | 8153     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 51.9        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 111         |
|    time_elapsed         | 8219        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.019226074 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 85          |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=34.31 +/- 34.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 34.3        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.018903986 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 23.2        |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 19.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 50.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 112      |
|    time_elapsed    | 8297     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=43.72 +/- 30.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 43.7        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.019031927 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.47        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 72.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 54.2     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 113      |
|    time_elapsed    | 8375     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 24.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 114         |
|    time_elapsed         | 8440        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.015831526 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.7        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 65          |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=28.56 +/- 5.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 28.6        |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.015805334 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 58.1        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 247         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 115      |
|    time_elapsed    | 8518     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=179.06 +/- 278.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 179        |
| time/                   |            |
|    total_timesteps      | 1900000    |
| train/                  |            |
|    approx_kl            | 0.01938103 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.09       |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.00712   |
|    value_loss           | 33.7       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.9      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 116      |
|    time_elapsed    | 8596     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 40.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 117         |
|    time_elapsed         | 8661        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.014510559 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.2        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 158         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=83.38 +/- 83.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 83.4        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.016969573 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.83        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 33.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 40.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 118      |
|    time_elapsed    | 8739     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 68.6        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 119         |
|    time_elapsed         | 8805        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.015841458 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.75        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=28.50 +/- 23.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 28.5        |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.016828384 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.3        |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00816    |
|    value_loss           | 30.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 120      |
|    time_elapsed    | 8883     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=66.18 +/- 55.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 66.2        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.014142428 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.7        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 93.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 121      |
|    time_elapsed    | 8961     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 60.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 122         |
|    time_elapsed         | 9027        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.020356432 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.46        |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 26.4        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=60.44 +/- 29.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 60.4        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.012607886 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.8        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00672    |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 45.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 123      |
|    time_elapsed    | 9104     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=178.85 +/- 253.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.016248714 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.1        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 88.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 124      |
|    time_elapsed    | 9182     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 14.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 125         |
|    time_elapsed         | 9248        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.015870038 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 92.8        |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=19.05 +/- 9.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.019845359 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.5        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 126      |
|    time_elapsed    | 9326     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=29.50 +/- 20.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 29.5        |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.010843728 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.5        |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 398         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.18    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 127      |
|    time_elapsed    | 9404     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 128         |
|    time_elapsed         | 9469        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.017445493 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.24        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 38.9        |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=-1152.47 +/- 2331.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.15e+03   |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.016157951 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.4        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 25.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 48.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 129      |
|    time_elapsed    | 9547     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=106.05 +/- 149.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 106         |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.016734775 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.5        |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 99.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 130      |
|    time_elapsed    | 9625     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98.9        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 131         |
|    time_elapsed         | 9690        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.016475072 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.23        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 37.8        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=163.24 +/- 126.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 163        |
| time/                   |            |
|    total_timesteps      | 2150000    |
| train/                  |            |
|    approx_kl            | 0.01895675 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.37       |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.00735   |
|    value_loss           | 24.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 79.3     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 132      |
|    time_elapsed    | 9768     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=34.61 +/- 31.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 34.6        |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.017622855 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.6        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 73.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 53.8     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 133      |
|    time_elapsed    | 9846     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 46.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 134         |
|    time_elapsed         | 9912        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.019162338 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.2        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 99.5        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=75.25 +/- 54.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 75.2        |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.018494334 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 31.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 49.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 135      |
|    time_elapsed    | 9990     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=49.31 +/- 28.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 49.3       |
| time/                   |            |
|    total_timesteps      | 2225000    |
| train/                  |            |
|    approx_kl            | 0.02017042 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.00987   |
|    value_loss           | 5.5        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 43.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 136      |
|    time_elapsed    | 10068    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 52          |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 137         |
|    time_elapsed         | 10134       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.012324406 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.9        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=40.30 +/- 35.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 40.3        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.016889896 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 81.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 46.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 138      |
|    time_elapsed    | 10212    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=-1064.42 +/- 2389.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.06e+03   |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.020673204 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.5        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 66          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 139      |
|    time_elapsed    | 10289    |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 49.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 140         |
|    time_elapsed         | 10355       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.018678997 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | 186         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 177         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=74.16 +/- 95.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 74.2        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.017777666 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.2        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 53.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 47       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 141      |
|    time_elapsed    | 10433    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=52.57 +/- 63.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 52.6        |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.023874713 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.7        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 62.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 44.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 142      |
|    time_elapsed    | 10511    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 143         |
|    time_elapsed         | 10577       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.018127648 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.39        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 83.7        |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=243.97 +/- 273.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 244         |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.018215943 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00888    |
|    value_loss           | 26.9        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 73.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 144      |
|    time_elapsed    | 10655    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=111.49 +/- 50.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 111        |
| time/                   |            |
|    total_timesteps      | 2375000    |
| train/                  |            |
|    approx_kl            | 0.02071944 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.00875   |
|    value_loss           | 45.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 145      |
|    time_elapsed    | 10733    |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 61.9        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 146         |
|    time_elapsed         | 10798       |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.016056757 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 93          |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=159.12 +/- 205.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 159         |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.017902128 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 43.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 147      |
|    time_elapsed    | 10876    |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 57.8        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 148         |
|    time_elapsed         | 10942       |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.020482257 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.8         |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=156.56 +/- 161.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.014662867 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 163         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 68.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 149      |
|    time_elapsed    | 11020    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=175.06 +/- 106.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.015408438 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00863    |
|    value_loss           | 76.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 74.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 150      |
|    time_elapsed    | 11098    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 85.7        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 151         |
|    time_elapsed         | 11163       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.021514246 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 56.8        |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=226.25 +/- 200.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.016389297 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 38.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 152      |
|    time_elapsed    | 11241    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=88.91 +/- 59.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 88.9        |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.017196832 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 153      |
|    time_elapsed    | 11319    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 94.5        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 154         |
|    time_elapsed         | 11384       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.019500138 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 32.1        |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=129.65 +/- 89.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.016163476 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 92.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 155      |
|    time_elapsed    | 11462    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=68.61 +/- 82.35
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 68.6     |
| time/                   |          |
|    total_timesteps      | 2550000  |
| train/                  |          |
|    approx_kl            | 0.01812  |
|    clip_fraction        | 0.229    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.92    |
|    explained_variance   | 0.873    |
|    learning_rate        | 0.0003   |
|    loss                 | 10.9     |
|    n_updates            | 1550     |
|    policy_gradient_loss | -0.00632 |
|    value_loss           | 44.1     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 156      |
|    time_elapsed    | 11540    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 117         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 157         |
|    time_elapsed         | 11605       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.021921976 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.65        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 65.1        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=104.38 +/- 63.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.019536471 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 10.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 158      |
|    time_elapsed    | 11683    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=92.60 +/- 101.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 92.6        |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.021057568 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.44        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 34.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 159      |
|    time_elapsed    | 11761    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 125         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 160         |
|    time_elapsed         | 11827       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.016906349 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.77        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 52.5        |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=157.50 +/- 142.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.018733412 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 45.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 99.3     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 161      |
|    time_elapsed    | 11905    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=401.95 +/- 146.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 402         |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.014387641 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 135         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 38.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 162      |
|    time_elapsed    | 11983    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 163         |
|    time_elapsed         | 12048       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.015478103 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | -0.0155     |
|    learning_rate        | 0.0003      |
|    loss                 | 35          |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 285         |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=264.66 +/- 182.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 265         |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.021466773 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 164      |
|    time_elapsed    | 12126    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=92.41 +/- 79.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 92.4       |
| time/                   |            |
|    total_timesteps      | 2700000    |
| train/                  |            |
|    approx_kl            | 0.01999549 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 21         |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.00964   |
|    value_loss           | 40.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 165      |
|    time_elapsed    | 12205    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 61.8        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 166         |
|    time_elapsed         | 12270       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.019272989 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.97        |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 100         |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=97.17 +/- 40.80
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 97.2         |
| time/                   |              |
|    total_timesteps      | 2725000      |
| train/                  |              |
|    approx_kl            | 0.0151886195 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.83        |
|    explained_variance   | 0.751        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.1         |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00997     |
|    value_loss           | 158          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 88       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 167      |
|    time_elapsed    | 12349    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=217.84 +/- 173.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.016981699 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.5        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 84.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 38.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 168      |
|    time_elapsed    | 12427    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 18          |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 169         |
|    time_elapsed         | 12492       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.012837198 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.3        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 276         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=85.67 +/- 49.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 85.7        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.017415546 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00901    |
|    value_loss           | 263         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 63.2     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 170      |
|    time_elapsed    | 12570    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=200.29 +/- 124.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.019536192 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.67        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 31.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 171      |
|    time_elapsed    | 12648    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 149         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 172         |
|    time_elapsed         | 12714       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.017124113 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.2        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 56.9        |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=183.12 +/- 148.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.017758079 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.22        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 42.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 173      |
|    time_elapsed    | 12792    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=200.04 +/- 186.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.022834342 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.81        |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 155         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 174      |
|    time_elapsed    | 12870    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 88.3        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 175         |
|    time_elapsed         | 12936       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.016847223 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.4        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 64          |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=205.38 +/- 150.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.018765755 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 18          |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00304    |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 40.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 176      |
|    time_elapsed    | 13014    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 19          |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 177         |
|    time_elapsed         | 13079       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.015758067 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 121         |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=112.99 +/- 115.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.018954106 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | 303         |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 315         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.82    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 178      |
|    time_elapsed    | 13157    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=274.77 +/- 52.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.014689175 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.6        |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 43.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 179      |
|    time_elapsed    | 13235    |
|    total_timesteps | 2932736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 103        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 180        |
|    time_elapsed         | 13301      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.02088226 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.44      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.14       |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 14.2       |
----------------------------------------
Eval num_timesteps=2950000, episode_reward=227.05 +/- 142.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.017573288 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.8         |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 46.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 181      |
|    time_elapsed    | 13378    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=217.44 +/- 263.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.020240277 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 73          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 84.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 182      |
|    time_elapsed    | 13456    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 77.7        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 183         |
|    time_elapsed         | 13522       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.016567621 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 75.8        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=152.00 +/- 94.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.020347359 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.88        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 22.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 184      |
|    time_elapsed    | 13600    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=248.15 +/- 249.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 248        |
| time/                   |            |
|    total_timesteps      | 3025000    |
| train/                  |            |
|    approx_kl            | 0.01762316 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.95      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 49.4       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.00765   |
|    value_loss           | 75.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 185      |
|    time_elapsed    | 13678    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 193         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 186         |
|    time_elapsed         | 13743       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.017949801 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.3        |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=189.12 +/- 202.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.022598755 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 187      |
|    time_elapsed    | 13821    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=164.47 +/- 176.96
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 164          |
| time/                   |              |
|    total_timesteps      | 3075000      |
| train/                  |              |
|    approx_kl            | 0.0073200683 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0003       |
|    loss                 | 72.8         |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00768     |
|    value_loss           | 1.14e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 67.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 188      |
|    time_elapsed    | 13900    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 24.6        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 189         |
|    time_elapsed         | 13965       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.019902585 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 78.2        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=219.53 +/- 105.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 220        |
| time/                   |            |
|    total_timesteps      | 3100000    |
| train/                  |            |
|    approx_kl            | 0.02043067 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.22      |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.6       |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 194        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.78    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 190      |
|    time_elapsed    | 14043    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=289.95 +/- 214.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.022845425 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.2        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 84.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 191      |
|    time_elapsed    | 14121    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 77.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 192         |
|    time_elapsed         | 14187       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.020472646 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.5        |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.00116     |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=264.70 +/- 189.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 265         |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.014944876 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.7        |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.00946    |
|    value_loss           | 195         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 68.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 193      |
|    time_elapsed    | 14265    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=192.59 +/- 129.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.018543594 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 127         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 73.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 194      |
|    time_elapsed    | 14342    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98          |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 195         |
|    time_elapsed         | 14408       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.016501695 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=-2162.62 +/- 4763.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -2.16e+03  |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.01903952 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.86      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 52.3       |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0077    |
|    value_loss           | 119        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 196      |
|    time_elapsed    | 14486    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=143.35 +/- 96.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.020547796 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.922       |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 30.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 94.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 197      |
|    time_elapsed    | 14564    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 116         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 198         |
|    time_elapsed         | 14630       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.015099727 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.97       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 359         |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=185.20 +/- 194.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.018776953 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.9         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00765    |
|    value_loss           | 89.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 199      |
|    time_elapsed    | 14708    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=-2087.05 +/- 4783.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.09e+03   |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.021742696 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.18       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.96        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00892    |
|    value_loss           | 41          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 200      |
|    time_elapsed    | 14786    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 165         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 201         |
|    time_elapsed         | 14851       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.019106291 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.6        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00862    |
|    value_loss           | 30.1        |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=-2088.51 +/- 4656.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.09e+03   |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.018316418 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 31.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 202      |
|    time_elapsed    | 14929    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=201.25 +/- 165.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.023097191 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 203      |
|    time_elapsed    | 15007    |
|    total_timesteps | 3325952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 98.1       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 204        |
|    time_elapsed         | 15073      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.01884934 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.19      |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0003     |
|    loss                 | 413        |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.00772   |
|    value_loss           | 120        |
----------------------------------------
Eval num_timesteps=3350000, episode_reward=294.02 +/- 56.93
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 3350000    |
| train/                  |            |
|    approx_kl            | 0.01832612 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.33      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.2       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.00226   |
|    value_loss           | 99.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 205      |
|    time_elapsed    | 15151    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=128.07 +/- 230.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 128        |
| time/                   |            |
|    total_timesteps      | 3375000    |
| train/                  |            |
|    approx_kl            | 0.02250615 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.95      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.89       |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.00985   |
|    value_loss           | 56.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 93.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 206      |
|    time_elapsed    | 15229    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 207         |
|    time_elapsed         | 15294       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.018246464 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0003      |
|    loss                 | 271         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=166.74 +/- 155.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.023255821 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35        |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 73.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 208      |
|    time_elapsed    | 15372    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 209         |
|    time_elapsed         | 15438       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.019566834 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=317.25 +/- 191.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.019888539 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.92       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 192         |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 210      |
|    time_elapsed    | 15516    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=157.89 +/- 105.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.024693124 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.2        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 75.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 211      |
|    time_elapsed    | 15594    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 212         |
|    time_elapsed         | 15659       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.023287816 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.32        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 13.3        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=117.17 +/- 115.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 117         |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.017230952 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.2        |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 187         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 213      |
|    time_elapsed    | 15737    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=98.37 +/- 73.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 98.4        |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.020917322 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 61.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 214      |
|    time_elapsed    | 15815    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 124         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 215         |
|    time_elapsed         | 15880       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.019938366 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=164.09 +/- 163.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.016719755 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 483         |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 177         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 216      |
|    time_elapsed    | 15959    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=103.44 +/- 78.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 103         |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.021122713 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 50.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 217      |
|    time_elapsed    | 16037    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 124         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 218         |
|    time_elapsed         | 16102       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.015553351 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.64        |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 210         |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=-198.81 +/- 526.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -199        |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.020647675 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.6        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00778    |
|    value_loss           | 72.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 219      |
|    time_elapsed    | 16180    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=109.35 +/- 72.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.023288505 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.9        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 220      |
|    time_elapsed    | 16258    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 104         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 221         |
|    time_elapsed         | 16324       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.018910788 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | 488         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 475         |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=178.20 +/- 185.50
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 178        |
| time/                   |            |
|    total_timesteps      | 3625000    |
| train/                  |            |
|    approx_kl            | 0.02258134 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.98      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 50.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 222      |
|    time_elapsed    | 16402    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=135.64 +/- 121.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 136         |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.019500123 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.48        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 46.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 223      |
|    time_elapsed    | 16479    |
|    total_timesteps | 3653632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 164        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 224        |
|    time_elapsed         | 16545      |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.02122464 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.99      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.9       |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 32.9       |
----------------------------------------
Eval num_timesteps=3675000, episode_reward=80.49 +/- 58.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 80.5        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.020944946 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 237         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 225      |
|    time_elapsed    | 16623    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=91.17 +/- 86.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 91.2        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.025314623 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.25        |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 67          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 226      |
|    time_elapsed    | 16701    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 131         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 227         |
|    time_elapsed         | 16766       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.022533895 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.0186      |
|    learning_rate        | 0.0003      |
|    loss                 | 169         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 398         |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=140.03 +/- 94.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 140         |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.023232842 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.71        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 27.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 228      |
|    time_elapsed    | 16844    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=45.05 +/- 115.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 45.1        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.018077943 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.3        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 96.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 229      |
|    time_elapsed    | 16922    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 230         |
|    time_elapsed         | 16988       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.024586396 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 7.81        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=142.99 +/- 84.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.020919655 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 27.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 231      |
|    time_elapsed    | 17066    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=201.05 +/- 206.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.020611838 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.95        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 49.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 232      |
|    time_elapsed    | 17144    |
|    total_timesteps | 3801088  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 233        |
|    time_elapsed         | 17210      |
|    total_timesteps      | 3817472    |
| train/                  |            |
|    approx_kl            | 0.01933179 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.43      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.8       |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.00745   |
|    value_loss           | 224        |
----------------------------------------
Eval num_timesteps=3825000, episode_reward=169.24 +/- 218.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.022814155 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.7        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 266         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 92.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 234      |
|    time_elapsed    | 17287    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=79.65 +/- 44.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 79.7        |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.017096512 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.21        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 584         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 235      |
|    time_elapsed    | 17365    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 236         |
|    time_elapsed         | 17431       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.019679379 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.000986   |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=103.24 +/- 76.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 103        |
| time/                   |            |
|    total_timesteps      | 3875000    |
| train/                  |            |
|    approx_kl            | 0.02248442 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.4       |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.54       |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.00912   |
|    value_loss           | 101        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 237      |
|    time_elapsed    | 17509    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 152         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 238         |
|    time_elapsed         | 17574       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.021009408 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.06        |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 25          |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=246.69 +/- 106.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.015047124 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.7        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 732         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 239      |
|    time_elapsed    | 17652    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=164.71 +/- 172.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.026626859 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.8        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00929    |
|    value_loss           | 20.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 240      |
|    time_elapsed    | 17730    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 137         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 241         |
|    time_elapsed         | 17796       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.020582344 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 37.4        |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=281.84 +/- 179.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.015553795 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.32        |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 410         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 242      |
|    time_elapsed    | 17874    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=174.76 +/- 107.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.021536889 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 243      |
|    time_elapsed    | 17952    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 134         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 244         |
|    time_elapsed         | 18018       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.013224382 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 130         |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=246.41 +/- 212.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.017872868 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 48.9        |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 87          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 245      |
|    time_elapsed    | 18096    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=244.66 +/- 71.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.024131127 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.57        |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 214         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 246      |
|    time_elapsed    | 18174    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 164         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 247         |
|    time_elapsed         | 18239       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.023045184 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=179.26 +/- 42.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.015668387 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00969    |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 248      |
|    time_elapsed    | 18317    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=112.61 +/- 56.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.015387381 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.6        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 212         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 249      |
|    time_elapsed    | 18395    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 250         |
|    time_elapsed         | 18461       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.022780769 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.92        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 31.3        |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=176.20 +/- 116.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.014441412 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | 481         |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00975    |
|    value_loss           | 503         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 251      |
|    time_elapsed    | 18539    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=202.56 +/- 168.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.024913937 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.14        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 35.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 252      |
|    time_elapsed    | 18617    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 253         |
|    time_elapsed         | 18682       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.014797902 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.5        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=231.64 +/- 158.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.021240529 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.5         |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 29.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 254      |
|    time_elapsed    | 18760    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=104.09 +/- 55.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.015423097 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 585         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 239         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 255      |
|    time_elapsed    | 18838    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 171         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 256         |
|    time_elapsed         | 18903       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.017304126 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00905    |
|    value_loss           | 114         |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=259.07 +/- 239.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.021156523 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.19        |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 332         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 257      |
|    time_elapsed    | 18981    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=69.96 +/- 114.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 70          |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.019190628 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.01        |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 258      |
|    time_elapsed    | 19059    |
|    total_timesteps | 4227072  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 194       |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 259       |
|    time_elapsed         | 19125     |
|    total_timesteps      | 4243456   |
| train/                  |           |
|    approx_kl            | 0.0190748 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.47     |
|    explained_variance   | 0.885     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.58      |
|    n_updates            | 2580      |
|    policy_gradient_loss | -0.0124   |
|    value_loss           | 33.7      |
---------------------------------------
Eval num_timesteps=4250000, episode_reward=246.87 +/- 177.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.017033886 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.45        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 13.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 260      |
|    time_elapsed    | 19203    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=169.36 +/- 72.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.016995024 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 297         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 261      |
|    time_elapsed    | 19280    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 179         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 262         |
|    time_elapsed         | 19346       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.016538348 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.97        |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 54.1        |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=226.52 +/- 151.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.018130343 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00958    |
|    value_loss           | 57.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 263      |
|    time_elapsed    | 19424    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=197.64 +/- 140.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.018539503 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.2        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 449         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 97.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 264      |
|    time_elapsed    | 19502    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 76.7        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 265         |
|    time_elapsed         | 19567       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.024540043 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.9        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 245         |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=143.99 +/- 83.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 144         |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.019193234 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.32        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 31.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 98.8     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 266      |
|    time_elapsed    | 19645    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 159         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 267         |
|    time_elapsed         | 19711       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.017663285 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 96.5        |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=143.81 +/- 21.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 144         |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.019962791 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.32        |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 86.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 268      |
|    time_elapsed    | 19789    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=202.84 +/- 205.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.023773348 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.34        |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 19.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 269      |
|    time_elapsed    | 19867    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 203         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 270         |
|    time_elapsed         | 19932       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.018523538 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.555       |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 8.34        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=284.65 +/- 248.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.014331808 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.87        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 84          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 271      |
|    time_elapsed    | 20010    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=217.10 +/- 253.28
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 217        |
| time/                   |            |
|    total_timesteps      | 4450000    |
| train/                  |            |
|    approx_kl            | 0.23533957 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.96      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | 34.3       |
|    n_updates            | 2710       |
|    policy_gradient_loss | 0.00621    |
|    value_loss           | 44.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 272      |
|    time_elapsed    | 20088    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.9       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 273         |
|    time_elapsed         | 20154       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.019303039 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.56        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 217         |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=290.00 +/- 152.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.010771872 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 547         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.4    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 274      |
|    time_elapsed    | 20232    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=236.50 +/- 209.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.020520827 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.72        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 128         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.9    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 275      |
|    time_elapsed    | 20310    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 276         |
|    time_elapsed         | 20375       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.018189918 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.0072     |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=220.17 +/- 175.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.013215825 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 547         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 82.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 277      |
|    time_elapsed    | 20453    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=205.05 +/- 157.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.026994448 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 72.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 278      |
|    time_elapsed    | 20531    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 104         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 279         |
|    time_elapsed         | 20596       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.023064721 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.4        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=291.47 +/- 233.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.020266326 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.96        |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 32.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 280      |
|    time_elapsed    | 20674    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=214.48 +/- 160.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.017808255 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 56.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 202      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 281      |
|    time_elapsed    | 20752    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 220         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 282         |
|    time_elapsed         | 20818       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.022850245 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.02        |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 70.4        |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=169.62 +/- 40.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.014081754 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.7        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 283      |
|    time_elapsed    | 20896    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=241.93 +/- 104.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.015412572 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 860         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 612         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 284      |
|    time_elapsed    | 20974    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 140         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 285         |
|    time_elapsed         | 21039       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.019929258 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 83.2        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=-1036.19 +/- 2584.69
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.04e+03    |
| time/                   |              |
|    total_timesteps      | 4675000      |
| train/                  |              |
|    approx_kl            | 0.0142145725 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.672        |
|    learning_rate        | 0.0003       |
|    loss                 | 196          |
|    n_updates            | 2850         |
|    policy_gradient_loss | 0.00309      |
|    value_loss           | 244          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 286      |
|    time_elapsed    | 21118    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=369.34 +/- 184.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.020893402 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.4        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00834    |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 287      |
|    time_elapsed    | 21196    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 121         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 288         |
|    time_elapsed         | 21261       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.022145849 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 209         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 58.6        |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=173.99 +/- 79.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.012812692 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 33          |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00926    |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 289      |
|    time_elapsed    | 21339    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=203.28 +/- 176.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.020219868 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 751         |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 436         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 290      |
|    time_elapsed    | 21417    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 89.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 291         |
|    time_elapsed         | 21483       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.019331507 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.72        |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 272         |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=131.69 +/- 196.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 132         |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.013598143 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 186         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00251    |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 292      |
|    time_elapsed    | 21561    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=350.21 +/- 199.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 350         |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.025019368 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.06        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00992    |
|    value_loss           | 19.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 293      |
|    time_elapsed    | 21639    |
|    total_timesteps | 4800512  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 158        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 294        |
|    time_elapsed         | 21705      |
|    total_timesteps      | 4816896    |
| train/                  |            |
|    approx_kl            | 0.01929273 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.33      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.4       |
|    n_updates            | 2930       |
|    policy_gradient_loss | -0.00906   |
|    value_loss           | 101        |
----------------------------------------
Eval num_timesteps=4825000, episode_reward=213.68 +/- 201.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.017527528 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | 759         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 753         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 295      |
|    time_elapsed    | 21782    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 160         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 296         |
|    time_elapsed         | 21848       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.025501383 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.38        |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 27.6        |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=133.79 +/- 22.94
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 134       |
| time/                   |           |
|    total_timesteps      | 4850000   |
| train/                  |           |
|    approx_kl            | 0.0182333 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.32     |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.8      |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.0102   |
|    value_loss           | 39.5      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 297      |
|    time_elapsed    | 21926    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=422.37 +/- 197.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 422         |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.015998736 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.01        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00672    |
|    value_loss           | 167         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 298      |
|    time_elapsed    | 22003    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 209         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 299         |
|    time_elapsed         | 22069       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.015742555 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 90.7        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=267.18 +/- 217.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.022193715 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 68          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 300      |
|    time_elapsed    | 22147    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=123.35 +/- 95.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 123         |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.018854288 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 301      |
|    time_elapsed    | 22225    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 302         |
|    time_elapsed         | 22291       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.020146202 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.4        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 32.2        |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=151.87 +/- 66.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.019295726 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.3        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 303      |
|    time_elapsed    | 22369    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=202.63 +/- 127.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.020502346 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 138         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 304      |
|    time_elapsed    | 22447    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 132         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 305         |
|    time_elapsed         | 22512       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.018717661 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.99        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=-1049.99 +/- 2376.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.05e+03   |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.016582709 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 82          |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 52.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 306      |
|    time_elapsed    | 22590    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=391.23 +/- 127.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 391         |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.018339464 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 345         |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.000179   |
|    value_loss           | 513         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 43.2     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 307      |
|    time_elapsed    | 22668    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 48.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 308         |
|    time_elapsed         | 22734       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.016890634 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 233         |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=178.60 +/- 72.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.015440887 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00769    |
|    value_loss           | 192         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 309      |
|    time_elapsed    | 22812    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=151.69 +/- 33.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.019817354 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 401         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 310      |
|    time_elapsed    | 22890    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 122         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 311         |
|    time_elapsed         | 22956       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.019573111 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.4        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=98.09 +/- 49.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 98.1        |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.026653918 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 23.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 312      |
|    time_elapsed    | 23034    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=150.62 +/- 237.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.024251994 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 41.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 313      |
|    time_elapsed    | 23112    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 143         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 314         |
|    time_elapsed         | 23178       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.017211953 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 193         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=236.32 +/- 254.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.016300492 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00969    |
|    value_loss           | 446         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 68.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 315      |
|    time_elapsed    | 23255    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=274.18 +/- 200.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.021434383 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.2        |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 99.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 71.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 316      |
|    time_elapsed    | 23334    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 73.3        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 317         |
|    time_elapsed         | 23399       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.013376109 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 22.4        |
|    n_updates            | 3160        |
|    policy_gradient_loss | 0.00431     |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=275.19 +/- 56.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 275        |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.02858818 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.48      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.4        |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.0076    |
|    value_loss           | 83.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 318      |
|    time_elapsed    | 23477    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=157.97 +/- 121.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.019695772 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 86.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 319      |
|    time_elapsed    | 23555    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 320         |
|    time_elapsed         | 23621       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.025171388 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.91        |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 69          |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=212.82 +/- 142.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.019719437 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 19.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 321      |
|    time_elapsed    | 23699    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=117.26 +/- 64.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 117         |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.028627533 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 22.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 322      |
|    time_elapsed    | 23777    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 323         |
|    time_elapsed         | 23842       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.020418406 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.605       |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=85.02 +/- 135.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 85          |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.016101914 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.54        |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 76.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 324      |
|    time_elapsed    | 23920    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 168         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 325         |
|    time_elapsed         | 23987       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.029718893 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.85        |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 10.4        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=321.95 +/- 254.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.021114707 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.1         |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 33.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 326      |
|    time_elapsed    | 24065    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=123.74 +/- 39.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 124         |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.022216164 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 365         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 327      |
|    time_elapsed    | 24143    |
|    total_timesteps | 5357568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 142        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 328        |
|    time_elapsed         | 24208      |
|    total_timesteps      | 5373952    |
| train/                  |            |
|    approx_kl            | 0.02527079 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.3       |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 3270       |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 96         |
----------------------------------------
Eval num_timesteps=5375000, episode_reward=369.78 +/- 251.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 370         |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.021182656 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.62        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 29.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 329      |
|    time_elapsed    | 24286    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=122.71 +/- 58.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 123         |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.016975986 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.1        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 235         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 330      |
|    time_elapsed    | 24364    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 331         |
|    time_elapsed         | 24430       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.026549984 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.51        |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 13.1        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=142.34 +/- 89.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 142         |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.019509243 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.9        |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 545         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 332      |
|    time_elapsed    | 24508    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=351.35 +/- 304.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 351         |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.015689848 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 333      |
|    time_elapsed    | 24586    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 131         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 334         |
|    time_elapsed         | 24652       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.024636533 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 83.1        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=229.76 +/- 183.82
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 230          |
| time/                   |              |
|    total_timesteps      | 5475000      |
| train/                  |              |
|    approx_kl            | 0.0116972895 |
|    clip_fraction        | 0.0993       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.613        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.18         |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00232     |
|    value_loss           | 368          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 335      |
|    time_elapsed    | 24730    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=243.51 +/- 112.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 244         |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.021837702 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 336      |
|    time_elapsed    | 24807    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 337         |
|    time_elapsed         | 24873       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.026339144 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.5        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=289.40 +/- 184.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.026956651 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 71.3        |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 63.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 76.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 338      |
|    time_elapsed    | 24951    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=184.48 +/- 148.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.021927658 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 921         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 339      |
|    time_elapsed    | 25030    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 115         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 340         |
|    time_elapsed         | 25095       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.012859742 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 804         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=218.26 +/- 103.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 218        |
| time/                   |            |
|    total_timesteps      | 5575000    |
| train/                  |            |
|    approx_kl            | 0.02366326 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.17      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.839      |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.00994   |
|    value_loss           | 6.34       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 341      |
|    time_elapsed    | 25173    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=202.74 +/- 155.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.026834706 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 6.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 69.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 342      |
|    time_elapsed    | 25251    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 91.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 343         |
|    time_elapsed         | 25317       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.007230608 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | -0.24       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=169.34 +/- 69.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.022348233 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 6.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 45.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 344      |
|    time_elapsed    | 25395    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=340.38 +/- 246.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 340         |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.018802658 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.7        |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 250         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 44.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 345      |
|    time_elapsed    | 25473    |
|    total_timesteps | 5652480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 30.4       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 346        |
|    time_elapsed         | 25538      |
|    total_timesteps      | 5668864    |
| train/                  |            |
|    approx_kl            | 0.01628578 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.402      |
|    learning_rate        | 0.0003     |
|    loss                 | 168        |
|    n_updates            | 3450       |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 235        |
----------------------------------------
Eval num_timesteps=5675000, episode_reward=127.30 +/- 88.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 127         |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.017616764 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 308         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.412   |
| time/              |          |
|    fps             | 221      |
|    iterations      | 347      |
|    time_elapsed    | 25616    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=200.55 +/- 132.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.022262484 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 73          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 24.2     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 348      |
|    time_elapsed    | 25694    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 96          |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 349         |
|    time_elapsed         | 25760       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.020435527 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00856    |
|    value_loss           | 20.2        |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=121.08 +/- 92.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 121        |
| time/                   |            |
|    total_timesteps      | 5725000    |
| train/                  |            |
|    approx_kl            | 0.01641939 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.33      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 3490       |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 129        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 350      |
|    time_elapsed    | 25837    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=113.73 +/- 126.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 114         |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.018133035 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.1        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 234         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 351      |
|    time_elapsed    | 25915    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 109         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 352         |
|    time_elapsed         | 25981       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.017774396 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.6        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 846         |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=80.57 +/- 36.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 80.6        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.025853764 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 341         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 353      |
|    time_elapsed    | 26059    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98.8        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 354         |
|    time_elapsed         | 26124       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.013455568 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 547         |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=159.74 +/- 198.41
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 160       |
| time/                   |           |
|    total_timesteps      | 5800000   |
| train/                  |           |
|    approx_kl            | 0.0213244 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.45     |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0003    |
|    loss                 | 37.4      |
|    n_updates            | 3540      |
|    policy_gradient_loss | -0.0108   |
|    value_loss           | 46.4      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 355      |
|    time_elapsed    | 26202    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=-47.20 +/- 213.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -47.2       |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.018498842 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 356      |
|    time_elapsed    | 26280    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 138         |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 357         |
|    time_elapsed         | 26346       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.018772908 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.8        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 27          |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=186.97 +/- 174.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.023239851 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 9.19        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 81.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 358      |
|    time_elapsed    | 26424    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=268.13 +/- 190.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 268         |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.022694038 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 61.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 359      |
|    time_elapsed    | 26501    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 189         |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 360         |
|    time_elapsed         | 26567       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.015153226 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.84        |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 65.3        |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=142.75 +/- 162.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.012716068 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.76        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 74.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 361      |
|    time_elapsed    | 26645    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=73.75 +/- 59.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 73.7        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.015236901 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 219         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 362      |
|    time_elapsed    | 26723    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 363         |
|    time_elapsed         | 26788       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.018423455 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.0382      |
|    learning_rate        | 0.0003      |
|    loss                 | 185         |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 484         |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=104.92 +/- 66.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 105         |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.025067441 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.37        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 4.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 364      |
|    time_elapsed    | 26866    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=163.55 +/- 99.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.020473357 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 7.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 365      |
|    time_elapsed    | 26944    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 366         |
|    time_elapsed         | 27010       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.014487205 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 519         |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 305         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=138.52 +/- 127.92
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 139       |
| time/                   |           |
|    total_timesteps      | 6000000   |
| train/                  |           |
|    approx_kl            | 0.0180258 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.24     |
|    explained_variance   | 0.945     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.61      |
|    n_updates            | 3660      |
|    policy_gradient_loss | -0.0126   |
|    value_loss           | 8.39      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 367      |
|    time_elapsed    | 27088    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=264.06 +/- 294.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.011375213 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.8        |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 1.08e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 98.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 368      |
|    time_elapsed    | 27166    |
|    total_timesteps | 6029312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 46.9      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 369       |
|    time_elapsed         | 27231     |
|    total_timesteps      | 6045696   |
| train/                  |           |
|    approx_kl            | 0.0249408 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.21     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 3680      |
|    policy_gradient_loss | -0.0146   |
|    value_loss           | 25.2      |
---------------------------------------
Eval num_timesteps=6050000, episode_reward=182.67 +/- 316.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.005176411 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 664         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 598         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 57.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 370      |
|    time_elapsed    | 27309    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=91.87 +/- 56.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 91.9        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.021470657 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 33.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 371      |
|    time_elapsed    | 27387    |
|    total_timesteps | 6078464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 94         |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 372        |
|    time_elapsed         | 27453      |
|    total_timesteps      | 6094848    |
| train/                  |            |
|    approx_kl            | 0.01927913 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.06      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 3710       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 178        |
----------------------------------------
Eval num_timesteps=6100000, episode_reward=171.69 +/- 195.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.015776942 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 373      |
|    time_elapsed    | 27531    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=329.62 +/- 173.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 330        |
| time/                   |            |
|    total_timesteps      | 6125000    |
| train/                  |            |
|    approx_kl            | 0.02277941 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.05      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.95       |
|    n_updates            | 3730       |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 5.8        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 374      |
|    time_elapsed    | 27609    |
|    total_timesteps | 6127616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 375        |
|    time_elapsed         | 27674      |
|    total_timesteps      | 6144000    |
| train/                  |            |
|    approx_kl            | 0.01147499 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.95      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 352        |
|    n_updates            | 3740       |
|    policy_gradient_loss | -0.00658   |
|    value_loss           | 471        |
----------------------------------------
Eval num_timesteps=6150000, episode_reward=57.47 +/- 39.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 57.5        |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.017898384 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 46.8        |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 99.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 376      |
|    time_elapsed    | 27752    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=220.83 +/- 150.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.018427663 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.25        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 83.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 377      |
|    time_elapsed    | 27830    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.89       |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 378         |
|    time_elapsed         | 27896       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.017037801 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.4        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 74.3        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=48.77 +/- 28.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 48.8        |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.013888665 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | -0.102      |
|    learning_rate        | 0.0003      |
|    loss                 | 120         |
|    n_updates            | 3780        |
|    policy_gradient_loss | 0.00523     |
|    value_loss           | 783         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.22    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 379      |
|    time_elapsed    | 27974    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=-1046.59 +/- 2411.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.05e+03   |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.013986495 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 215         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 380      |
|    time_elapsed    | 28052    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 28.2        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 381         |
|    time_elapsed         | 28117       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.019203302 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 43          |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=206.09 +/- 164.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.019396182 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 382      |
|    time_elapsed    | 28195    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=104.39 +/- 64.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.015000192 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.94        |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 383      |
|    time_elapsed    | 28273    |
|    total_timesteps | 6275072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 197        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 384        |
|    time_elapsed         | 28339      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.01924248 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.98      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.79       |
|    n_updates            | 3830       |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 19.1       |
----------------------------------------
Eval num_timesteps=6300000, episode_reward=-2089.54 +/- 4744.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.09e+03   |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.016901212 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.75        |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 261         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 385      |
|    time_elapsed    | 28416    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 386         |
|    time_elapsed         | 28482       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.020812936 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.7        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.0049     |
|    value_loss           | 350         |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=125.43 +/- 95.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 125         |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.019455176 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.75        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 37.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 222      |
|    iterations      | 387      |
|    time_elapsed    | 28560    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=347.35 +/- 218.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 347         |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.022558972 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.849       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 8.25        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 388      |
|    time_elapsed    | 28638    |
|    total_timesteps | 6356992  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 56.3         |
| time/                   |              |
|    fps                  | 222          |
|    iterations           | 389          |
|    time_elapsed         | 28703        |
|    total_timesteps      | 6373376      |
| train/                  |              |
|    approx_kl            | 0.0123132905 |
|    clip_fraction        | 0.0947       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.26        |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.08e+03     |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=6375000, episode_reward=134.26 +/- 101.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.016377527 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 1.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34       |
| time/              |          |
|    fps             | 222      |
|    iterations      | 390      |
|    time_elapsed    | 28781    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=100.37 +/- 69.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 100        |
| time/                   |            |
|    total_timesteps      | 6400000    |
| train/                  |            |
|    approx_kl            | 0.01829201 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.97       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.00863   |
|    value_loss           | 117        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 391      |
|    time_elapsed    | 28860    |
|    total_timesteps | 6406144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 222          |
|    iterations           | 392          |
|    time_elapsed         | 28926        |
|    total_timesteps      | 6422528      |
| train/                  |              |
|    approx_kl            | 0.0155378245 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0003       |
|    loss                 | 28.5         |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.00941     |
|    value_loss           | 71.8         |
------------------------------------------
Eval num_timesteps=6425000, episode_reward=62.63 +/- 41.01
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 62.6         |
| time/                   |              |
|    total_timesteps      | 6425000      |
| train/                  |              |
|    approx_kl            | 0.0139201945 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.0003       |
|    loss                 | 742          |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 1.32e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.4    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 393      |
|    time_elapsed    | 29004    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=216.36 +/- 177.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.015050568 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 423         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.6    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 394      |
|    time_elapsed    | 29082    |
|    total_timesteps | 6455296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 395        |
|    time_elapsed         | 29148      |
|    total_timesteps      | 6471680    |
| train/                  |            |
|    approx_kl            | 0.01576804 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.33      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 171        |
----------------------------------------
Eval num_timesteps=6475000, episode_reward=161.74 +/- 105.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.014949859 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.26    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 396      |
|    time_elapsed    | 29226    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=255.59 +/- 214.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 256        |
| time/                   |            |
|    total_timesteps      | 6500000    |
| train/                  |            |
|    approx_kl            | 0.01997089 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.95      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 481        |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 285        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 57       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 397      |
|    time_elapsed    | 29304    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98.9        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 398         |
|    time_elapsed         | 29370       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.021857211 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 64          |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=101.88 +/- 72.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 102         |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.014754897 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 41.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 399      |
|    time_elapsed    | 29448    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=120.74 +/- 29.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 121         |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.018808624 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.3        |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 400      |
|    time_elapsed    | 29526    |
|    total_timesteps | 6553600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 147        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 401        |
|    time_elapsed         | 29592      |
|    total_timesteps      | 6569984    |
| train/                  |            |
|    approx_kl            | 0.01492136 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.67       |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 69.5       |
----------------------------------------
Eval num_timesteps=6575000, episode_reward=111.54 +/- 94.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 112         |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.018709827 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.21        |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 23.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 83.8     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 402      |
|    time_elapsed    | 29670    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=102.37 +/- 40.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 102         |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.010181715 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 968         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 974         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89.3     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 403      |
|    time_elapsed    | 29749    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 78.5        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 404         |
|    time_elapsed         | 29816       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.019773338 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.41        |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00901    |
|    value_loss           | 70.5        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=226.40 +/- 320.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.019113477 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 79.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 405      |
|    time_elapsed    | 29895    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=176.27 +/- 153.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.022686739 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.16        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 14          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 406      |
|    time_elapsed    | 29975    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 145         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 407         |
|    time_elapsed         | 30042       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.018042408 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.6        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 85.5        |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=235.13 +/- 217.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.020951044 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.64        |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 42.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 408      |
|    time_elapsed    | 30121    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=114.11 +/- 85.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 114         |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.018169222 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 33          |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 466         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 409      |
|    time_elapsed    | 30201    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 126         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 410         |
|    time_elapsed         | 30268       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.019182796 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 2           |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=161.68 +/- 96.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.019832954 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00872    |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 411      |
|    time_elapsed    | 30347    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=108.79 +/- 60.63
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 109        |
| time/                   |            |
|    total_timesteps      | 6750000    |
| train/                  |            |
|    approx_kl            | 0.01385525 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.5       |
|    n_updates            | 4110       |
|    policy_gradient_loss | -0.00873   |
|    value_loss           | 96.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 412      |
|    time_elapsed    | 30426    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 211         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 413         |
|    time_elapsed         | 30493       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.018284772 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 45.5        |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=158.48 +/- 137.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.010882499 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.4        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 414      |
|    time_elapsed    | 30573    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 415         |
|    time_elapsed         | 30639       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.015847638 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 306         |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 658         |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=369.46 +/- 219.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.013705196 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.145       |
|    learning_rate        | 0.0003      |
|    loss                 | 753         |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 1.62e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.4    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 416      |
|    time_elapsed    | 30719    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=147.71 +/- 68.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 148         |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.026092336 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.36        |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 59.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 417      |
|    time_elapsed    | 30798    |
|    total_timesteps | 6832128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 418        |
|    time_elapsed         | 30865      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.00890925 |
|    clip_fraction        | 0.0683     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 520        |
|    n_updates            | 4170       |
|    policy_gradient_loss | -0.00209   |
|    value_loss           | 798        |
----------------------------------------
Eval num_timesteps=6850000, episode_reward=241.10 +/- 153.89
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 6850000      |
| train/                  |              |
|    approx_kl            | 0.0131917875 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.04         |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.00541     |
|    value_loss           | 336          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 73.4     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 419      |
|    time_elapsed    | 30944    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=148.85 +/- 75.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.018431373 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.59        |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00876    |
|    value_loss           | 91.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 67.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 420      |
|    time_elapsed    | 31023    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.5        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 421         |
|    time_elapsed         | 31090       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.014420744 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.7        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=305.98 +/- 162.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.016717833 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.4        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 260         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.8    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 422      |
|    time_elapsed    | 31169    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=134.81 +/- 70.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.019122899 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 89.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37.4    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 423      |
|    time_elapsed    | 31249    |
|    total_timesteps | 6930432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -41.3      |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 424        |
|    time_elapsed         | 31316      |
|    total_timesteps      | 6946816    |
| train/                  |            |
|    approx_kl            | 0.02058651 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.36      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.8       |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 69.4       |
----------------------------------------
Eval num_timesteps=6950000, episode_reward=212.46 +/- 110.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.010669757 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 89          |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 158         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 425      |
|    time_elapsed    | 31396    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=227.77 +/- 229.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.016077174 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.13        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00738    |
|    value_loss           | 33.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 9.29     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 426      |
|    time_elapsed    | 31476    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 36.6        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 427         |
|    time_elapsed         | 31542       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.011188491 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 839         |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=-3073.49 +/- 6663.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.07e+03   |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.016111355 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.98        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 51.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 428      |
|    time_elapsed    | 31621    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=311.03 +/- 151.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.015235988 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | 316         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 401         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 429      |
|    time_elapsed    | 31700    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 82.1        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 430         |
|    time_elapsed         | 31766       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.018481988 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.23        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=191.86 +/- 107.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.016808866 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 132         |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 57.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 431      |
|    time_elapsed    | 31844    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=101.54 +/- 30.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 102         |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.022396535 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00953    |
|    value_loss           | 74.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 432      |
|    time_elapsed    | 31923    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 433         |
|    time_elapsed         | 31989       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.018651662 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.1        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=127.86 +/- 122.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 7100000      |
| train/                  |              |
|    approx_kl            | 0.0102817165 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.87         |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.00884     |
|    value_loss           | 424          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.91     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 434      |
|    time_elapsed    | 32067    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=187.65 +/- 214.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.016705928 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.26        |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 43.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.4    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 435      |
|    time_elapsed    | 32146    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41.5       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 436         |
|    time_elapsed         | 32212       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.011348677 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 368         |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=200.21 +/- 106.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.015649464 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 17.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 437      |
|    time_elapsed    | 32290    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=87.33 +/- 58.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 87.3        |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.020387571 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 32.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 438      |
|    time_elapsed    | 32368    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 42.1        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 439         |
|    time_elapsed         | 32434       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.015006697 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.0525      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05e+03    |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 609         |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=183.00 +/- 124.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.013510638 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.2        |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 440      |
|    time_elapsed    | 32513    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=121.23 +/- 68.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 121         |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.019713202 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.8        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 53.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -82      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 441      |
|    time_elapsed    | 32592    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 59.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 442         |
|    time_elapsed         | 32657       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.009539201 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 324         |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 799         |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=179.71 +/- 118.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.017897855 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.7        |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 51.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 443      |
|    time_elapsed    | 32735    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.61       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 444         |
|    time_elapsed         | 32801       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.013679998 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.9        |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 570         |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=71.31 +/- 36.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 71.3        |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.016227692 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 105         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 445      |
|    time_elapsed    | 32879    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=134.66 +/- 122.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.019005686 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.37        |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00945    |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 87.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 446      |
|    time_elapsed    | 32957    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 140         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 447         |
|    time_elapsed         | 33023       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.020616435 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.05        |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 34.4        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=277.98 +/- 209.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.012177452 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.5        |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 191         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 448      |
|    time_elapsed    | 33101    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=82.39 +/- 94.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 82.4        |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.014620494 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 75.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 449      |
|    time_elapsed    | 33180    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 117         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 450         |
|    time_elapsed         | 33246       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.022147309 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.04        |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 17.9        |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=-393.26 +/- 989.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.014873969 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.4        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 99.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 95.6     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 451      |
|    time_elapsed    | 33324    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=156.98 +/- 128.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.015956076 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.9        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 687         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 36.7     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 452      |
|    time_elapsed    | 33402    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 65.7        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 453         |
|    time_elapsed         | 33467       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.016705163 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.9        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 165         |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=166.10 +/- 158.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 166         |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.016579606 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.84        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 37.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 51.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 454      |
|    time_elapsed    | 33545    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=181.21 +/- 150.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.018999707 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 112         |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 29.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 455      |
|    time_elapsed    | 33623    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 134         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 456         |
|    time_elapsed         | 33689       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.017793339 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.78        |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 86.2        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=130.42 +/- 122.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.016893338 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 457      |
|    time_elapsed    | 33767    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=232.33 +/- 125.70
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 232       |
| time/                   |           |
|    total_timesteps      | 7500000   |
| train/                  |           |
|    approx_kl            | 0.0180563 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.38     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 28.8      |
|    n_updates            | 4570      |
|    policy_gradient_loss | -0.00947  |
|    value_loss           | 21.5      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 86.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 458      |
|    time_elapsed    | 33845    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 93.4        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 459         |
|    time_elapsed         | 33911       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.014029687 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=-203.14 +/- 750.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -203        |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.020457175 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.08        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 35.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 460      |
|    time_elapsed    | 33989    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=108.65 +/- 122.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.018054342 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.779       |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 49          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 461      |
|    time_elapsed    | 34067    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 168         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 462         |
|    time_elapsed         | 34133       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.016641498 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.37        |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 30.2        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=129.77 +/- 97.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.023893692 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 16.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 463      |
|    time_elapsed    | 34211    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=115.51 +/- 120.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 116         |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.016446393 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 30.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 464      |
|    time_elapsed    | 34288    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 169         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 465         |
|    time_elapsed         | 34354       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.020359602 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.75        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 23.8        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=150.12 +/- 87.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.015616073 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.47        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 466      |
|    time_elapsed    | 34433    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=233.80 +/- 238.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.020012334 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.03        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 33.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 467      |
|    time_elapsed    | 34513    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 95.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 468         |
|    time_elapsed         | 34580       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.017057696 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 16.8        |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=-55.48 +/- 241.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -55.5       |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.009996379 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 318         |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 512         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 469      |
|    time_elapsed    | 34661    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=279.42 +/- 125.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.019347126 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.4        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 75.5     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 470      |
|    time_elapsed    | 34744    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 76.6        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 471         |
|    time_elapsed         | 34813       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.021065667 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 2           |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 26.4        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=82.59 +/- 42.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 82.6        |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.021316322 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 33.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 472      |
|    time_elapsed    | 34896    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 156         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 473         |
|    time_elapsed         | 34966       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.016389282 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.8        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 13.1        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=265.12 +/- 245.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 265         |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.017131865 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 586         |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 90.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 474      |
|    time_elapsed    | 35048    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=67.49 +/- 53.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 67.5        |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.023428744 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.97        |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 507         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 78.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 475      |
|    time_elapsed    | 35131    |
|    total_timesteps | 7782400  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 87.9       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 476        |
|    time_elapsed         | 35201      |
|    total_timesteps      | 7798784    |
| train/                  |            |
|    approx_kl            | 0.01836046 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.18      |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.07       |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 38.4       |
----------------------------------------
Eval num_timesteps=7800000, episode_reward=198.10 +/- 117.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.017727004 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.72        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 17.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 477      |
|    time_elapsed    | 35284    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=103.36 +/- 46.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 103         |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.013960319 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.72        |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 44.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 478      |
|    time_elapsed    | 35366    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 479         |
|    time_elapsed         | 35435       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.018080998 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 11.2        |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=107.00 +/- 69.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 107        |
| time/                   |            |
|    total_timesteps      | 7850000    |
| train/                  |            |
|    approx_kl            | 0.02024475 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.33      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.55       |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 50.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 480      |
|    time_elapsed    | 35517    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=90.15 +/- 49.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 90.2        |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.024050297 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.59        |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.00897    |
|    value_loss           | 98.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 96       |
| time/              |          |
|    fps             | 221      |
|    iterations      | 481      |
|    time_elapsed    | 35601    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 83.2        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 482         |
|    time_elapsed         | 35670       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.019672994 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.8         |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.000354   |
|    value_loss           | 485         |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=278.72 +/- 202.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.020241942 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 30.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 77.1     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 483      |
|    time_elapsed    | 35753    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=290.89 +/- 239.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.019729894 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.0003      |
|    loss                 | 275         |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 484      |
|    time_elapsed    | 35836    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 115         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 485         |
|    time_elapsed         | 35905       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.021541763 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.52        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 532         |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=142.97 +/- 152.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.017624496 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 486      |
|    time_elapsed    | 35988    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=244.19 +/- 179.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 244         |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.019660857 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00886    |
|    value_loss           | 45.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 487      |
|    time_elapsed    | 36070    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 172         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 488         |
|    time_elapsed         | 36139       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.016970316 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.6        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=250.64 +/- 116.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 251         |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.020334408 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.8        |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 61.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 489      |
|    time_elapsed    | 36222    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=150.62 +/- 337.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.022083525 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.65        |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 21.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 490      |
|    time_elapsed    | 36304    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 194         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 491         |
|    time_elapsed         | 36373       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.017735474 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.565       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.63        |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 263         |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=177.15 +/- 107.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.020299375 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 4.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 492      |
|    time_elapsed    | 36457    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=127.77 +/- 102.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 128         |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.019980097 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 124         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 493      |
|    time_elapsed    | 36540    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 24.8        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 494         |
|    time_elapsed         | 36610       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.011514088 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.103      |
|    learning_rate        | 0.0003      |
|    loss                 | 5           |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=109.02 +/- 99.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.022522848 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 364         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.79     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 495      |
|    time_elapsed    | 36693    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=297.90 +/- 212.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.026423391 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 7.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 496      |
|    time_elapsed    | 36777    |
|    total_timesteps | 8126464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 0.321        |
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 497          |
|    time_elapsed         | 36846        |
|    total_timesteps      | 8142848      |
| train/                  |              |
|    approx_kl            | 0.0051758485 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.765        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.4         |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.0027      |
|    value_loss           | 462          |
------------------------------------------
Eval num_timesteps=8150000, episode_reward=397.49 +/- 296.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.018089727 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 33.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.93     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 498      |
|    time_elapsed    | 36930    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=172.12 +/- 91.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 172        |
| time/                   |            |
|    total_timesteps      | 8175000    |
| train/                  |            |
|    approx_kl            | 0.01825395 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.00905   |
|    value_loss           | 11.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 499      |
|    time_elapsed    | 37013    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 187         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 500         |
|    time_elapsed         | 37082       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.017505025 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 89.7        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=206.52 +/- 107.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.015131644 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.4        |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 85.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 501      |
|    time_elapsed    | 37165    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 502         |
|    time_elapsed         | 37235       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.014747751 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.2        |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 207         |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=103.48 +/- 65.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 103         |
| time/                   |             |
|    total_timesteps      | 8225000     |
| train/                  |             |
|    approx_kl            | 0.015600262 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.9        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 252         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 69       |
| time/              |          |
|    fps             | 220      |
|    iterations      | 503      |
|    time_elapsed    | 37318    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=192.50 +/- 161.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.018152878 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 474         |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 1.29e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 504      |
|    time_elapsed    | 37402    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -81.9       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 505         |
|    time_elapsed         | 37472       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.019239184 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 976         |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 695         |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=227.38 +/- 154.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.014967906 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 517         |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 615         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 506      |
|    time_elapsed    | 37555    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=138.38 +/- 71.17
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 138        |
| time/                   |            |
|    total_timesteps      | 8300000    |
| train/                  |            |
|    approx_kl            | 0.01879615 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.39      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.21       |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 28.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 507      |
|    time_elapsed    | 37639    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 61.7        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 508         |
|    time_elapsed         | 37708       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.017294288 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 465         |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=247.66 +/- 123.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 248        |
| time/                   |            |
|    total_timesteps      | 8325000    |
| train/                  |            |
|    approx_kl            | 0.02676569 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.2       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.73       |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 27.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 509      |
|    time_elapsed    | 37790    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=149.98 +/- 89.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.015314763 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 630         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 510      |
|    time_elapsed    | 37872    |
|    total_timesteps | 8355840  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 49.8       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 511        |
|    time_elapsed         | 37942      |
|    total_timesteps      | 8372224    |
| train/                  |            |
|    approx_kl            | 0.01193002 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 826        |
|    n_updates            | 5100       |
|    policy_gradient_loss | -0.00224   |
|    value_loss           | 1.44e+03   |
----------------------------------------
Eval num_timesteps=8375000, episode_reward=171.32 +/- 148.54
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 171        |
| time/                   |            |
|    total_timesteps      | 8375000    |
| train/                  |            |
|    approx_kl            | 0.01918711 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.38      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.76       |
|    n_updates            | 5110       |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 32.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 50       |
| time/              |          |
|    fps             | 220      |
|    iterations      | 512      |
|    time_elapsed    | 38025    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=220.18 +/- 216.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.013470809 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.63        |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 157         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 513      |
|    time_elapsed    | 38109    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 181         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 514         |
|    time_elapsed         | 38178       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.022656724 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.04        |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 19.1        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=206.75 +/- 106.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.015647147 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0003      |
|    loss                 | 286         |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 344         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 515      |
|    time_elapsed    | 38261    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=275.29 +/- 178.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.014126902 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 99.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 516      |
|    time_elapsed    | 38343    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 152         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 517         |
|    time_elapsed         | 38412       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.014390854 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 171         |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 248         |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=163.54 +/- 66.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 164        |
| time/                   |            |
|    total_timesteps      | 8475000    |
| train/                  |            |
|    approx_kl            | 0.01601985 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 305        |
|    n_updates            | 5170       |
|    policy_gradient_loss | -0.00734   |
|    value_loss           | 208        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 518      |
|    time_elapsed    | 38495    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=188.93 +/- 136.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.016346842 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 165         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 519      |
|    time_elapsed    | 38577    |
|    total_timesteps | 8503296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 16.1         |
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 520          |
|    time_elapsed         | 38646        |
|    total_timesteps      | 8519680      |
| train/                  |              |
|    approx_kl            | 0.0072451187 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.27        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.5         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 898          |
------------------------------------------
Eval num_timesteps=8525000, episode_reward=112.49 +/- 272.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 112         |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.018128667 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.93        |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 44.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 21.2     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 521      |
|    time_elapsed    | 38727    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=91.16 +/- 49.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 91.2        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.016598132 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.52        |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 13.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34.3     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 522      |
|    time_elapsed    | 38809    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 523         |
|    time_elapsed         | 38878       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.013157702 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00771    |
|    value_loss           | 225         |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=227.24 +/- 93.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.016580068 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 98.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 524      |
|    time_elapsed    | 38959    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=120.55 +/- 111.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 121         |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.017475348 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 74.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 525      |
|    time_elapsed    | 39041    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 144         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 526         |
|    time_elapsed         | 39111       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.016011091 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 430         |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=184.78 +/- 101.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.014917077 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 68.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 202      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 527      |
|    time_elapsed    | 39193    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=-37.47 +/- 284.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -37.5       |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.020600874 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.05        |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 29          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 528      |
|    time_elapsed    | 39275    |
|    total_timesteps | 8650752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 147         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 529         |
|    time_elapsed         | 39344       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.010705807 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.47        |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 569         |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=167.16 +/- 95.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.015475904 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.8        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00815    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 530      |
|    time_elapsed    | 39426    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.1       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 531         |
|    time_elapsed         | 39494       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.017489968 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=171.80 +/- 189.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.015940223 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 164         |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 1.55e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 8.24     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 532      |
|    time_elapsed    | 39576    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=213.49 +/- 235.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.016143639 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23e+03    |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 890         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.2    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 533      |
|    time_elapsed    | 39657    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.4       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 534         |
|    time_elapsed         | 39725       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.014985133 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.7        |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 277         |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=198.18 +/- 211.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.013427018 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.46        |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 73.9     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 535      |
|    time_elapsed    | 39807    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=-592.08 +/- 1211.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -592        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.017182913 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 161         |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 536      |
|    time_elapsed    | 39888    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 72.7        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 537         |
|    time_elapsed         | 39957       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.020899467 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 519         |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=110.63 +/- 153.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 111         |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.013499831 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 2.09e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 538      |
|    time_elapsed    | 40038    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=2.02 +/- 2.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.015619783 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.5        |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 281         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -98.1    |
| time/              |          |
|    fps             | 220      |
|    iterations      | 539      |
|    time_elapsed    | 40120    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.2       |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 540         |
|    time_elapsed         | 40188       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.018796325 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.47        |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 43.7        |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=39.43 +/- 48.17
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 39.4       |
| time/                   |            |
|    total_timesteps      | 8850000    |
| train/                  |            |
|    approx_kl            | 0.01472238 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.94      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.4       |
|    n_updates            | 5400       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 192        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 541      |
|    time_elapsed    | 40269    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=-698.81 +/- 1144.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -699       |
| time/                   |            |
|    total_timesteps      | 8875000    |
| train/                  |            |
|    approx_kl            | 0.01957843 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.82      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 22.5       |
|    n_updates            | 5410       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 73.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 542      |
|    time_elapsed    | 40351    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 34          |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 543         |
|    time_elapsed         | 40419       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.020310588 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.75        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 25.4        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=-45.94 +/- 177.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -45.9       |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.010174508 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 551         |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 719         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 59.8     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 544      |
|    time_elapsed    | 40501    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=-85.76 +/- 257.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -85.8       |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.016302846 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 36.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 41       |
| time/              |          |
|    fps             | 220      |
|    iterations      | 545      |
|    time_elapsed    | 40582    |
|    total_timesteps | 8929280  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 34         |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 546        |
|    time_elapsed         | 40651      |
|    total_timesteps      | 8945664    |
| train/                  |            |
|    approx_kl            | 0.02040409 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.12      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.18       |
|    n_updates            | 5450       |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 37.5       |
----------------------------------------
Eval num_timesteps=8950000, episode_reward=109.44 +/- 105.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.018705186 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 44.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 547      |
|    time_elapsed    | 40732    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=119.71 +/- 121.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 120         |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.016204486 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.63        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 35.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 548      |
|    time_elapsed    | 40813    |
|    total_timesteps | 8978432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 92.2       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 549        |
|    time_elapsed         | 40882      |
|    total_timesteps      | 8994816    |
| train/                  |            |
|    approx_kl            | 0.02107133 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.84      |
|    explained_variance   | 0.395      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.52       |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 246        |
----------------------------------------
Eval num_timesteps=9000000, episode_reward=4.23 +/- 1.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.23        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.018000191 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.67        |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 39.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 54.6     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 550      |
|    time_elapsed    | 40964    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=74.82 +/- 88.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 74.8       |
| time/                   |            |
|    total_timesteps      | 9025000    |
| train/                  |            |
|    approx_kl            | 0.01701007 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.08      |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.3       |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 175        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 45.7     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 551      |
|    time_elapsed    | 41045    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 97.3        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 552         |
|    time_elapsed         | 41114       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.020628914 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.13        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 68          |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=212.06 +/- 116.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 212        |
| time/                   |            |
|    total_timesteps      | 9050000    |
| train/                  |            |
|    approx_kl            | 0.01143297 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.28      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.7       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.00942   |
|    value_loss           | 150        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 553      |
|    time_elapsed    | 41195    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=163.68 +/- 130.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.018649146 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.85        |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 59.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 554      |
|    time_elapsed    | 41276    |
|    total_timesteps | 9076736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 153        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 555        |
|    time_elapsed         | 41345      |
|    total_timesteps      | 9093120    |
| train/                  |            |
|    approx_kl            | 0.02107222 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.3       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.5       |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 11.3       |
----------------------------------------
Eval num_timesteps=9100000, episode_reward=137.97 +/- 264.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 138         |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.013083553 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 161         |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 214         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 556      |
|    time_elapsed    | 41426    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=-727.17 +/- 1632.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -727        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.015465608 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 70.4        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 557      |
|    time_elapsed    | 41508    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 129         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 558         |
|    time_elapsed         | 41576       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.017665815 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 106         |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=-17.39 +/- 397.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -17.4       |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.017622333 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 58          |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 559      |
|    time_elapsed    | 41657    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=128.29 +/- 82.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 128         |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.018754357 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 75.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 560      |
|    time_elapsed    | 41738    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 122         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 561         |
|    time_elapsed         | 41807       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.020668497 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.3        |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 37.3        |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=180.50 +/- 208.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.014345522 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.9        |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.1        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 255         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 86       |
| time/              |          |
|    fps             | 219      |
|    iterations      | 562      |
|    time_elapsed    | 41889    |
|    total_timesteps | 9207808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 77.4       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 563        |
|    time_elapsed         | 41957      |
|    total_timesteps      | 9224192    |
| train/                  |            |
|    approx_kl            | 0.01762557 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.96      |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.31       |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 75.9       |
----------------------------------------
Eval num_timesteps=9225000, episode_reward=284.54 +/- 220.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.016950306 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.9        |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00963    |
|    value_loss           | 52.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 65.2     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 564      |
|    time_elapsed    | 42039    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=190.49 +/- 169.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.019991597 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 45          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89.4     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 565      |
|    time_elapsed    | 42120    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 96.8        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 566         |
|    time_elapsed         | 42189       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.019126164 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.04       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.67        |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 38.3        |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=135.15 +/- 141.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.019221548 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77        |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 26.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 567      |
|    time_elapsed    | 42271    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=84.86 +/- 96.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 84.9        |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.015490022 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 90.2     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 568      |
|    time_elapsed    | 42352    |
|    total_timesteps | 9306112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 58.9       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 569        |
|    time_elapsed         | 42420      |
|    total_timesteps      | 9322496    |
| train/                  |            |
|    approx_kl            | 0.01614168 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.29      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 171        |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.0084    |
|    value_loss           | 111        |
----------------------------------------
Eval num_timesteps=9325000, episode_reward=53.32 +/- 34.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 53.3        |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.018625885 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 95.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 60.5     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 570      |
|    time_elapsed    | 42502    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=-54.47 +/- 276.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -54.5       |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.017021462 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 45          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89.1     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 571      |
|    time_elapsed    | 42584    |
|    total_timesteps | 9355264  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 572        |
|    time_elapsed         | 42652      |
|    total_timesteps      | 9371648    |
| train/                  |            |
|    approx_kl            | 0.01575733 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.7       |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 5710       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 51         |
----------------------------------------
Eval num_timesteps=9375000, episode_reward=213.13 +/- 205.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.016355854 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.97        |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 86.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 573      |
|    time_elapsed    | 42734    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=128.18 +/- 210.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 128        |
| time/                   |            |
|    total_timesteps      | 9400000    |
| train/                  |            |
|    approx_kl            | 0.01685075 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.42      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.33       |
|    n_updates            | 5730       |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 15.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 574      |
|    time_elapsed    | 42815    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 152         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 575         |
|    time_elapsed         | 42884       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.020261887 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.46        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 51.5        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=170.04 +/- 126.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.015993673 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 3.1         |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 27.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 576      |
|    time_elapsed    | 42966    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=185.88 +/- 122.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.014273479 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.51        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 37.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 577      |
|    time_elapsed    | 43048    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 166         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 578         |
|    time_elapsed         | 43117       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.021696394 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 9.55        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=262.37 +/- 110.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.018157039 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.5         |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 32.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 579      |
|    time_elapsed    | 43199    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=80.09 +/- 61.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 80.1        |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.013740392 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 580      |
|    time_elapsed    | 43282    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 176         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 581         |
|    time_elapsed         | 43352       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.023658285 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=267.65 +/- 164.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 268         |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.021243546 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.06        |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 16.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 582      |
|    time_elapsed    | 43435    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=261.67 +/- 123.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 262        |
| time/                   |            |
|    total_timesteps      | 9550000    |
| train/                  |            |
|    approx_kl            | 0.01891886 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.2        |
|    n_updates            | 5820       |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 26.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 583      |
|    time_elapsed    | 43518    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 584         |
|    time_elapsed         | 43588       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.025382169 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 19.2        |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=336.97 +/- 219.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 337         |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.012703224 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 68.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 585      |
|    time_elapsed    | 43671    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=56.68 +/- 212.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 56.7        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.020392084 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.3        |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 586      |
|    time_elapsed    | 43753    |
|    total_timesteps | 9601024  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 180        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 587        |
|    time_elapsed         | 43823      |
|    total_timesteps      | 9617408    |
| train/                  |            |
|    approx_kl            | 0.01798341 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.65      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 44         |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 79.5       |
----------------------------------------
Eval num_timesteps=9625000, episode_reward=189.71 +/- 127.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.020450048 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.9         |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 28.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 588      |
|    time_elapsed    | 43906    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=122.17 +/- 81.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 122         |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.024780443 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.71        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 71.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 589      |
|    time_elapsed    | 43989    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.3       |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 590         |
|    time_elapsed         | 44059       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.006024878 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.0595      |
|    learning_rate        | 0.0003      |
|    loss                 | 50.9        |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=256.94 +/- 386.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 257         |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.017657299 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 267         |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.1    |
| time/              |          |
|    fps             | 219      |
|    iterations      | 591      |
|    time_elapsed    | 44142    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -71.5       |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 592         |
|    time_elapsed         | 44212       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.017353816 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.4        |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 48.3        |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=319.67 +/- 221.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.020484574 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 66.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 593      |
|    time_elapsed    | 44295    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=95.11 +/- 53.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 95.1       |
| time/                   |            |
|    total_timesteps      | 9725000    |
| train/                  |            |
|    approx_kl            | 0.01913625 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.63      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 11.3       |
|    n_updates            | 5930       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 59.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 594      |
|    time_elapsed    | 44377    |
|    total_timesteps | 9732096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 190        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 595        |
|    time_elapsed         | 44446      |
|    total_timesteps      | 9748480    |
| train/                  |            |
|    approx_kl            | 0.01659472 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.15      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.7       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.00789   |
|    value_loss           | 28.5       |
----------------------------------------
Eval num_timesteps=9750000, episode_reward=145.04 +/- 64.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.016169025 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.74        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 86.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 596      |
|    time_elapsed    | 44531    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=259.18 +/- 184.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.017042192 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 88          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 597      |
|    time_elapsed    | 44615    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 598         |
|    time_elapsed         | 44686       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.018119905 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 12.6        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=189.67 +/- 205.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.013407345 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00909    |
|    value_loss           | 408         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 599      |
|    time_elapsed    | 44771    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=189.82 +/- 149.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.017331887 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.1        |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 342         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 600      |
|    time_elapsed    | 44856    |
|    total_timesteps | 9830400  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 601        |
|    time_elapsed         | 44927      |
|    total_timesteps      | 9846784    |
| train/                  |            |
|    approx_kl            | 0.01979161 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.11      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.32       |
|    n_updates            | 6000       |
|    policy_gradient_loss | -0.00934   |
|    value_loss           | 30.1       |
----------------------------------------
Eval num_timesteps=9850000, episode_reward=83.38 +/- 93.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 83.4        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.016693478 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.3         |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 29.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 602      |
|    time_elapsed    | 45011    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=74.05 +/- 63.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 74.1        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.017977716 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.12        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 42.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 603      |
|    time_elapsed    | 45095    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 174         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 604         |
|    time_elapsed         | 45166       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.018827833 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 85.4        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=69.01 +/- 197.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 69          |
| time/                   |             |
|    total_timesteps      | 9900000     |
| train/                  |             |
|    approx_kl            | 0.019727048 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 124         |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 39.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 605      |
|    time_elapsed    | 45251    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=91.19 +/- 65.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 91.2        |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.014577389 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.3        |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 88.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 606      |
|    time_elapsed    | 45335    |
|    total_timesteps | 9928704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 144        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 607        |
|    time_elapsed         | 45406      |
|    total_timesteps      | 9945088    |
| train/                  |            |
|    approx_kl            | 0.01506817 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.64      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.8       |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 73.6       |
----------------------------------------
Eval num_timesteps=9950000, episode_reward=33.68 +/- 340.45
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 33.7       |
| time/                   |            |
|    total_timesteps      | 9950000    |
| train/                  |            |
|    approx_kl            | 0.02215607 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.73      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.19       |
|    n_updates            | 6070       |
|    policy_gradient_loss | -0.00925   |
|    value_loss           | 14.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 608      |
|    time_elapsed    | 45490    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=74.29 +/- 130.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 74.3        |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.018475823 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 28.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 609      |
|    time_elapsed    | 45574    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 198         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 610         |
|    time_elapsed         | 45644       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.019297563 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.39        |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 26.4        |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=51.00 +/- 176.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 51          |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.020457748 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.52        |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 5.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 611      |
|    time_elapsed    | 45728    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v12_1
