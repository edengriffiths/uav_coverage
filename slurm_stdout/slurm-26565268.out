========== uav-v3 ==========
Seed: 1356452581
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v3_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.48e+03 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 1         |
|    time_elapsed    | 102       |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-2857.63 +/- 2119.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.86e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008220692 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | -0.000208   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.05e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 4.71e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.07e+03 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 2         |
|    time_elapsed    | 255       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.14e+03   |
| time/                   |             |
|    fps                  | 128         |
|    iterations           | 3           |
|    time_elapsed         | 383         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009651388 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -4.17e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.93e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 1.51e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-2251.26 +/- 2207.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.25e+03   |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.026640596 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -1.25e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.88e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 1.29e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.68e+03 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 4         |
|    time_elapsed    | 536       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-957.62 +/- 293.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -958        |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.016348891 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 6.41e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 1.53e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.62e+03 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 5         |
|    time_elapsed    | 689       |
|    total_timesteps | 81920     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -2.16e+03    |
| time/                   |              |
|    fps                  | 120          |
|    iterations           | 6            |
|    time_elapsed         | 817          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0063932547 |
|    clip_fraction        | 0.0857       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86e+03     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 7.45e+03     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=-1317.52 +/- 448.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.007130923 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 309         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 2.28e+03    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 7         |
|    time_elapsed    | 970       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1197.58 +/- 657.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.006182302 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | 386         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 1.11e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -997     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 8        |
|    time_elapsed    | 1122     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -791        |
| time/                   |             |
|    fps                  | 117         |
|    iterations           | 9           |
|    time_elapsed         | 1250        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008345225 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.5        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 514         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-1077.02 +/- 448.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.010674989 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.1        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 378         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -703     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 10       |
|    time_elapsed    | 1403     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-837.38 +/- 611.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -837        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.008957233 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -650     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 11       |
|    time_elapsed    | 1557     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -589        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 12          |
|    time_elapsed         | 1685        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.017352577 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 99.9        |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-117.62 +/- 239.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.013895825 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.3        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 146         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -578     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 13       |
|    time_elapsed    | 1839     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-117.51 +/- 239.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.009685534 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 501         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -443     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 14       |
|    time_elapsed    | 1992     |
|    total_timesteps | 229376   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -419         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 15           |
|    time_elapsed         | 2120         |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0132053625 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0003       |
|    loss                 | 14.8         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 141          |
------------------------------------------
Eval num_timesteps=250000, episode_reward=2.54 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.011795227 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.5        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 231         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -562     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 16       |
|    time_elapsed    | 2273     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-237.77 +/- 479.97
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -238         |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0072601163 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0003       |
|    loss                 | 888          |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00609     |
|    value_loss           | 1.45e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 17       |
|    time_elapsed    | 2426     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -514        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 18          |
|    time_elapsed         | 2554        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.010627872 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.9        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 717         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=2.59 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.016055884 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 101         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 19       |
|    time_elapsed    | 2706     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=2.45 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.017001359 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -388     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 20       |
|    time_elapsed    | 2858     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -402        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 21          |
|    time_elapsed         | 2986        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.011448519 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.55e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00606    |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=2.62 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.62         |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0108101815 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.91e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00512     |
|    value_loss           | 2.99e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 22       |
|    time_elapsed    | 3138     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=2.68 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.019945875 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 67          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 23       |
|    time_elapsed    | 3291     |
|    total_timesteps | 376832   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -400         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 24           |
|    time_elapsed         | 3418         |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0105291065 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.08e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00923     |
|    value_loss           | 1.25e+03     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=3.52 +/- 0.81
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.52         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0123121105 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.0003       |
|    loss                 | 106          |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 471          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -317     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 25       |
|    time_elapsed    | 3570     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=3.23 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.23        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.014270892 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 327         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -316     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 26       |
|    time_elapsed    | 3722     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 27          |
|    time_elapsed         | 3850        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.014221478 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | -0.0248     |
|    learning_rate        | 0.0003      |
|    loss                 | 31.2        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 294         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=2.93 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.019427545 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0076     |
|    value_loss           | 61.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 28       |
|    time_elapsed    | 4004     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=2.99 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.99        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.008481216 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 1.22e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -388     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 29       |
|    time_elapsed    | 4156     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -483        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 30          |
|    time_elapsed         | 4285        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.009276353 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76e+03    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 2.62e+03    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=3.12 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.12        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.017338652 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 258         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 31       |
|    time_elapsed    | 4438     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -312        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 32          |
|    time_elapsed         | 4565        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.023304751 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.26        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=3.00 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.010197533 |
|    clip_fraction        | 0.0985      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0003      |
|    loss                 | 289         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 372         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 33       |
|    time_elapsed    | 4717     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=4.23 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.23        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.030909443 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.1        |
|    n_updates            | 330         |
|    policy_gradient_loss | 0.0044      |
|    value_loss           | 482         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 34       |
|    time_elapsed    | 4870     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -268        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 35          |
|    time_elapsed         | 4997        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.013092807 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.8        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 252         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=3.52 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.52        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.008241216 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.5        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 419         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 36       |
|    time_elapsed    | 5149     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-1174.25 +/- 2355.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.17e+03   |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.018401463 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.1        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 37       |
|    time_elapsed    | 5301     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 38          |
|    time_elapsed         | 5429        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.020113539 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.93        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 13.4        |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=5.93 +/- 1.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.93        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.016756177 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.1        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 49.5        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 39       |
|    time_elapsed    | 5581     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=3.81 +/- 1.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.81        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.008131939 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.3        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 667         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 40       |
|    time_elapsed    | 5733     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -179        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 41          |
|    time_elapsed         | 5860        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.016313264 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.39        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 96.4        |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=4.51 +/- 2.62
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 4.51      |
| time/                   |           |
|    total_timesteps      | 675000    |
| train/                  |           |
|    approx_kl            | 0.0200403 |
|    clip_fraction        | 0.319     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6      |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 88.7      |
|    n_updates            | 410       |
|    policy_gradient_loss | -0.0078   |
|    value_loss           | 13        |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -217     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 42       |
|    time_elapsed    | 6013     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=4.97 +/- 1.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.97        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.008269554 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 558         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -219     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 43       |
|    time_elapsed    | 6165     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 44          |
|    time_elapsed         | 6290        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.010782631 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.3        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 887         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=4.50 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.5         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.017142119 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 80.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -236     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 45       |
|    time_elapsed    | 6439     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=5.58 +/- 3.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.58        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.014867952 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 62          |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 228         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 46       |
|    time_elapsed    | 6588     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -185        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 47          |
|    time_elapsed         | 6713        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.041521147 |
|    clip_fraction        | 0.455       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 460         |
|    policy_gradient_loss | 0.00964     |
|    value_loss           | 245         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=6.45 +/- 2.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.45        |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.008698735 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 419         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 48       |
|    time_elapsed    | 6861     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=6.60 +/- 4.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.6         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.013863032 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 21.2        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 225         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 49       |
|    time_elapsed    | 7009     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -109        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 50          |
|    time_elapsed         | 7134        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.020270146 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.3        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 58.6        |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=4.25 +/- 1.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.25       |
| time/                   |            |
|    total_timesteps      | 825000     |
| train/                  |            |
|    approx_kl            | 0.02206564 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.61       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00311   |
|    value_loss           | 22.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 51       |
|    time_elapsed    | 7282     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=6.13 +/- 3.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.13        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.017405052 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.2        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 52       |
|    time_elapsed    | 7430     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -99.7       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 53          |
|    time_elapsed         | 7554        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.022047251 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 520         |
|    policy_gradient_loss | 0.00065     |
|    value_loss           | 251         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=5.30 +/- 2.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.3         |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.017701643 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.99        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 49.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 54       |
|    time_elapsed    | 7703     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=5.67 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.67        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.017734982 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 70.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 55       |
|    time_elapsed    | 7851     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -106        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 56          |
|    time_elapsed         | 7976        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.006846059 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | -0.0534     |
|    learning_rate        | 0.0003      |
|    loss                 | 52.6        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 293         |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=3.39 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.39        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.019826803 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.9        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 11.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 57       |
|    time_elapsed    | 8125     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=4.60 +/- 2.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.014207297 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 47.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 58       |
|    time_elapsed    | 8273     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -127        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 59          |
|    time_elapsed         | 8397        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.019881692 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 25.2        |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=2.70 +/- 0.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0074318415 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.619        |
|    learning_rate        | 0.0003       |
|    loss                 | 285          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00414     |
|    value_loss           | 644          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 60       |
|    time_elapsed    | 8547     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -232        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 61          |
|    time_elapsed         | 8672        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.018707069 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.9         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=2.83 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.035058327 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.5        |
|    n_updates            | 610         |
|    policy_gradient_loss | 0.00292     |
|    value_loss           | 371         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 62       |
|    time_elapsed    | 8820     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=2.82 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.014738491 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 281         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -247     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 63       |
|    time_elapsed    | 8969     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -225        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 64          |
|    time_elapsed         | 9094        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.026370278 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.00279     |
|    value_loss           | 271         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=3.69 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.69        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.017867662 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 50.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -259     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 65       |
|    time_elapsed    | 9243     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=3.15 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.15        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.012716224 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 852         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000427   |
|    value_loss           | 826         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 66       |
|    time_elapsed    | 9392     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -507        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 67          |
|    time_elapsed         | 9517        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.012299884 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=3.39 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.39         |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0068268343 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.41        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.0003       |
|    loss                 | 912          |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00709     |
|    value_loss           | 1.31e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 68       |
|    time_elapsed    | 9666     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=2.92 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.92        |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.014887869 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -326     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 69       |
|    time_elapsed    | 9815     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -246        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 70          |
|    time_elapsed         | 9939        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.017119242 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.987       |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 40.9        |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=3.93 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.93        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.015900094 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.38        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 71       |
|    time_elapsed    | 10087    |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=3.07 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.019224368 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 45.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 72       |
|    time_elapsed    | 10235    |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 73          |
|    time_elapsed         | 10360       |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.016742036 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.584       |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=3.63 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.63        |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.004210137 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 879         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 74       |
|    time_elapsed    | 10508    |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=3.42 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.42        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.017047416 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.12        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 39.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 75       |
|    time_elapsed    | 10657    |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -172        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 76          |
|    time_elapsed         | 10781       |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.011046742 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.2        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00634    |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=5.22 +/- 4.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.22        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.019153524 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.507       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00039    |
|    value_loss           | 47.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 77       |
|    time_elapsed    | 10929    |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=3.48 +/- 0.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.48       |
| time/                   |            |
|    total_timesteps      | 1275000    |
| train/                  |            |
|    approx_kl            | 0.01825203 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.46       |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.00927   |
|    value_loss           | 11.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 78       |
|    time_elapsed    | 11077    |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -136        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 79          |
|    time_elapsed         | 11202       |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.015900366 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.56        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 169         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=3.83 +/- 0.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.83         |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0094198845 |
|    clip_fraction        | 0.0971       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.35        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0003       |
|    loss                 | 41.2         |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 653          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 80       |
|    time_elapsed    | 11350    |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=3.74 +/- 0.66
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 3.74     |
| time/                   |          |
|    total_timesteps      | 1325000  |
| train/                  |          |
|    approx_kl            | 0.019299 |
|    clip_fraction        | 0.295    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.32    |
|    explained_variance   | 0.825    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.801    |
|    n_updates            | 800      |
|    policy_gradient_loss | -0.00885 |
|    value_loss           | 8.63     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 81       |
|    time_elapsed    | 11498    |
|    total_timesteps | 1327104  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -149         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 82           |
|    time_elapsed         | 11622        |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0077651944 |
|    clip_fraction        | 0.0661       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.35        |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0003       |
|    loss                 | 159          |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00738     |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=4.04 +/- 1.52
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 4.04      |
| time/                   |           |
|    total_timesteps      | 1350000   |
| train/                  |           |
|    approx_kl            | 0.0245252 |
|    clip_fraction        | 0.332     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.31     |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.3      |
|    n_updates            | 820       |
|    policy_gradient_loss | 0.00146   |
|    value_loss           | 177       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 83       |
|    time_elapsed    | 11771    |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=4.13 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.13        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.020978048 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.71        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 21.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 84       |
|    time_elapsed    | 11919    |
|    total_timesteps | 1376256  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -84.4      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 85         |
|    time_elapsed         | 12044      |
|    total_timesteps      | 1392640    |
| train/                  |            |
|    approx_kl            | 0.02258284 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.5        |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.00269   |
|    value_loss           | 245        |
----------------------------------------
Eval num_timesteps=1400000, episode_reward=4.95 +/- 2.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.95        |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.015305974 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.55        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 23.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 86       |
|    time_elapsed    | 12193    |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=3.45 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.45        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.016910125 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 10.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 87       |
|    time_elapsed    | 12342    |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 88          |
|    time_elapsed         | 12466       |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.023029193 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.304       |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 2.95        |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=4.02 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.012562367 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 11.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 89       |
|    time_elapsed    | 12615    |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 90          |
|    time_elapsed         | 12739       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.019091677 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.167       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 6.71        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=4.99 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.99        |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.027320817 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.404       |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.00429     |
|    value_loss           | 5.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 91       |
|    time_elapsed    | 12887    |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=5.41 +/- 1.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.41       |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.02983198 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.541      |
|    n_updates            | 910        |
|    policy_gradient_loss | -3.56e-05  |
|    value_loss           | 5.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.81    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 92       |
|    time_elapsed    | 13036    |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.07       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 93          |
|    time_elapsed         | 13161       |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.018790517 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=4.31 +/- 2.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.31        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.018825172 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.218       |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 9.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 94       |
|    time_elapsed    | 13309    |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=5.10 +/- 4.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.018236527 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.000748    |
|    value_loss           | 23.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 95       |
|    time_elapsed    | 13457    |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -252        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 96          |
|    time_elapsed         | 13582       |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.013203869 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.92        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 99.9        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=3.59 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.59        |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.008197557 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 491         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 97       |
|    time_elapsed    | 13730    |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=4.65 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.65        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.019220797 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.605       |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.000143   |
|    value_loss           | 41.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -244     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 98       |
|    time_elapsed    | 13879    |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -97.6       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 99          |
|    time_elapsed         | 14003       |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.019187752 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 65.7        |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=4.18 +/- 1.45
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.18       |
| time/                   |            |
|    total_timesteps      | 1625000    |
| train/                  |            |
|    approx_kl            | 0.01955345 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.00611   |
|    value_loss           | 10.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 100      |
|    time_elapsed    | 14153    |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=3.97 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.97        |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.019549947 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.000348   |
|    value_loss           | 363         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 101      |
|    time_elapsed    | 14302    |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -242        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 102         |
|    time_elapsed         | 14426       |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.016640995 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 41.2        |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00135     |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=3.41 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.016710222 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 390         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -214     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 103      |
|    time_elapsed    | 14575    |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=3.48 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.023093985 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 19.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 104      |
|    time_elapsed    | 14725    |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -112        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 105         |
|    time_elapsed         | 14850       |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.018446308 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 44.2        |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=3.53 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.53        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.025463313 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.7        |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.000306    |
|    value_loss           | 26.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 106      |
|    time_elapsed    | 14998    |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=3.08 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.08        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.033428445 |
|    clip_fraction        | 0.375       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.37        |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 46.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 107      |
|    time_elapsed    | 15147    |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 108         |
|    time_elapsed         | 15271       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.015450272 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=3.40 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.021165587 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.262       |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.000758   |
|    value_loss           | 3.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 109      |
|    time_elapsed    | 15420    |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=3.07 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.019352468 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.132       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 110      |
|    time_elapsed    | 15568    |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -34.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 111         |
|    time_elapsed         | 15692       |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.020201312 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 73          |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=3.49 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.49        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.027384423 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.36        |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 39.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -97.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 112      |
|    time_elapsed    | 15841    |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=4.60 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.6        |
| time/                   |            |
|    total_timesteps      | 1850000    |
| train/                  |            |
|    approx_kl            | 0.01193109 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 447        |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.00494   |
|    value_loss           | 390        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 113      |
|    time_elapsed    | 15989    |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -119        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 114         |
|    time_elapsed         | 16114       |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.013095001 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.5        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=-1175.25 +/- 2360.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.015884452 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00594    |
|    value_loss           | 141         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 115      |
|    time_elapsed    | 16262    |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=4.86 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.86        |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.015491634 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 413         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 116      |
|    time_elapsed    | 16410    |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41.1       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 117         |
|    time_elapsed         | 16535       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.017732257 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.000923   |
|    value_loss           | 10.4        |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=3.45 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.45        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.017488396 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.778       |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 11.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 118      |
|    time_elapsed    | 16683    |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.2       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 119         |
|    time_elapsed         | 16808       |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.018189617 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 36.2        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.000659   |
|    value_loss           | 36.8        |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=5.12 +/- 3.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.12        |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.017691977 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.878       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 6.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 120      |
|    time_elapsed    | 16957    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=3.98 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.98        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.019668926 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 280         |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.000744   |
|    value_loss           | 60.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 121      |
|    time_elapsed    | 17105    |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 122         |
|    time_elapsed         | 17230       |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.015515136 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 37.5        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=2.60 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.022225615 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.22        |
|    n_updates            | 1220        |
|    policy_gradient_loss | 0.00127     |
|    value_loss           | 73.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 123      |
|    time_elapsed    | 17378    |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=3.29 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.29        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.019494206 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.7        |
|    n_updates            | 1230        |
|    policy_gradient_loss | 0.000433    |
|    value_loss           | 97.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 124      |
|    time_elapsed    | 17527    |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.8       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 125         |
|    time_elapsed         | 17651       |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.018390171 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43        |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.0008      |
|    value_loss           | 16.3        |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=3.97 +/- 1.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.97       |
| time/                   |            |
|    total_timesteps      | 2050000    |
| train/                  |            |
|    approx_kl            | 0.01681619 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.9        |
|    n_updates            | 1250       |
|    policy_gradient_loss | 0.0017     |
|    value_loss           | 14.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 126      |
|    time_elapsed    | 17800    |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=4.59 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.59        |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.020301038 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.000622   |
|    value_loss           | 54.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 127      |
|    time_elapsed    | 17948    |
|    total_timesteps | 2080768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -16.2      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 128        |
|    time_elapsed         | 18072      |
|    total_timesteps      | 2097152    |
| train/                  |            |
|    approx_kl            | 0.02081693 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.00389   |
|    value_loss           | 3.61       |
----------------------------------------
Eval num_timesteps=2100000, episode_reward=-188.96 +/- 386.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -189        |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.016894482 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0463      |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.125       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 129      |
|    time_elapsed    | 18221    |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=5.90 +/- 2.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.9         |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.016025502 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.618       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 6.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.76    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 130      |
|    time_elapsed    | 18370    |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.02       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 131         |
|    time_elapsed         | 18494       |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.015278053 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.81        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 17          |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=5.27 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.27        |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.018486949 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 0.525       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.23    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 132      |
|    time_elapsed    | 18643    |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=6.63 +/- 1.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.63        |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.018969469 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.99        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 26.3        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 133      |
|    time_elapsed    | 18791    |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -32.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 134         |
|    time_elapsed         | 18915       |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.022056343 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.00395     |
|    value_loss           | 48.7        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=5.95 +/- 1.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.95        |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.011960547 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.5        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 94.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -52.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 135      |
|    time_elapsed    | 19064    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=6.17 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.17        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.025162883 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.3        |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 105         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 136      |
|    time_elapsed    | 19212    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -148        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 137         |
|    time_elapsed         | 19336       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.023118457 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.68        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 13.5        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=6.52 +/- 2.61
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.52         |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0039274036 |
|    clip_fraction        | 0.00739      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.0003       |
|    loss                 | 387          |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 521          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 138      |
|    time_elapsed    | 19484    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=5.79 +/- 1.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.79        |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.010275161 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 158         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 139      |
|    time_elapsed    | 19633    |
|    total_timesteps | 2277376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 140        |
|    time_elapsed         | 19758      |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.02061069 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.00299   |
|    value_loss           | 9.77       |
----------------------------------------
Eval num_timesteps=2300000, episode_reward=5.25 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.25        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.020110335 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.67        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 68.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 141      |
|    time_elapsed    | 19906    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=5.04 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.04        |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.006072605 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 324         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 515         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -97.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 142      |
|    time_elapsed    | 20055    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -180        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 143         |
|    time_elapsed         | 20179       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.016444173 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.34        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 12.9        |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=3.90 +/- 0.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.9          |
| time/                   |              |
|    total_timesteps      | 2350000      |
| train/                  |              |
|    approx_kl            | 0.0065387664 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00619     |
|    value_loss           | 381          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 144      |
|    time_elapsed    | 20327    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=6.67 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.67        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.019923618 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.271       |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.00342     |
|    value_loss           | 7.28        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 145      |
|    time_elapsed    | 20476    |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -95.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 146         |
|    time_elapsed         | 20600       |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.017153218 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0498      |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 2.33        |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=5.12 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.12        |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.016114501 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 62.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 147      |
|    time_elapsed    | 20749    |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 148         |
|    time_elapsed         | 20873       |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.009314951 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.65        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 31          |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=4.60 +/- 0.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.021920834 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.84        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 2.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 149      |
|    time_elapsed    | 21021    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=4.78 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.78        |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.015760042 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.909       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 150      |
|    time_elapsed    | 21170    |
|    total_timesteps | 2457600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -102         |
| time/                   |              |
|    fps                  | 116          |
|    iterations           | 151          |
|    time_elapsed         | 21294        |
|    total_timesteps      | 2473984      |
| train/                  |              |
|    approx_kl            | 0.0028309405 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.7         |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 581          |
------------------------------------------
Eval num_timesteps=2475000, episode_reward=5.54 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.54        |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.025628563 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 25.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 152      |
|    time_elapsed    | 21443    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=4.35 +/- 1.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.35       |
| time/                   |            |
|    total_timesteps      | 2500000    |
| train/                  |            |
|    approx_kl            | 0.01878965 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.704      |
|    n_updates            | 1520       |
|    policy_gradient_loss | 0.000158   |
|    value_loss           | 13.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 153      |
|    time_elapsed    | 21592    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.89       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 154         |
|    time_elapsed         | 21717       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.017024357 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.215       |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.000994    |
|    value_loss           | 0.984       |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=7.35 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.35        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.018525507 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0393      |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 0.354       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.42    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 155      |
|    time_elapsed    | 21865    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=4.49 +/- 0.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.49       |
| time/                   |            |
|    total_timesteps      | 2550000    |
| train/                  |            |
|    approx_kl            | 0.01967008 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.86       |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.00218   |
|    value_loss           | 3.91       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.236   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 156      |
|    time_elapsed    | 22013    |
|    total_timesteps | 2555904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -0.987     |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 157        |
|    time_elapsed         | 22138      |
|    total_timesteps      | 2572288    |
| train/                  |            |
|    approx_kl            | 0.01870994 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00853   |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.00572   |
|    value_loss           | 0.271      |
----------------------------------------
Eval num_timesteps=2575000, episode_reward=6.22 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.22        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.012985782 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.573       |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 31.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.57    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 158      |
|    time_elapsed    | 22286    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=4.99 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.99       |
| time/                   |            |
|    total_timesteps      | 2600000    |
| train/                  |            |
|    approx_kl            | 0.01444193 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.00497   |
|    value_loss           | 9.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.88    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 159      |
|    time_elapsed    | 22434    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -33.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 160         |
|    time_elapsed         | 22559       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.023397997 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.000538   |
|    value_loss           | 34.2        |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=7.57 +/- 3.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.57        |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.013263149 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.65        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 229         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 161      |
|    time_elapsed    | 22707    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=9.13 +/- 3.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 9.13       |
| time/                   |            |
|    total_timesteps      | 2650000    |
| train/                  |            |
|    approx_kl            | 0.01993836 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.8       |
|    n_updates            | 1610       |
|    policy_gradient_loss | 0.00168    |
|    value_loss           | 70.8       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 162      |
|    time_elapsed    | 22855    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -91.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 163         |
|    time_elapsed         | 22980       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.021049159 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 157         |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.00248     |
|    value_loss           | 72.4        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=6.27 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.27        |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.015171386 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.8        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 234         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 164      |
|    time_elapsed    | 23128    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=8.56 +/- 3.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.56        |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.019777521 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 224         |
|    n_updates            | 1640        |
|    policy_gradient_loss | 0.000727    |
|    value_loss           | 229         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 165      |
|    time_elapsed    | 23276    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 166         |
|    time_elapsed         | 23401       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.018229485 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.394       |
|    n_updates            | 1650        |
|    policy_gradient_loss | 5.47e-05    |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=6.13 +/- 1.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.13        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.009467019 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 219         |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 167      |
|    time_elapsed    | 23549    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=6.88 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.88        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.017280757 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.4        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 62.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 168      |
|    time_elapsed    | 23699    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 169         |
|    time_elapsed         | 23823       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.019286714 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.934       |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 4.99        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=6.74 +/- 1.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.74        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.015027606 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.819       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 3.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 170      |
|    time_elapsed    | 23971    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=8.04 +/- 1.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.04        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.017023955 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.364       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 16.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 171      |
|    time_elapsed    | 24120    |
|    total_timesteps | 2801664  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -15.6      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 172        |
|    time_elapsed         | 24244      |
|    total_timesteps      | 2818048    |
| train/                  |            |
|    approx_kl            | 0.01235643 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 61         |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.00773   |
|    value_loss           | 55.2       |
----------------------------------------
Eval num_timesteps=2825000, episode_reward=7.88 +/- 3.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.88        |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.017458523 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 0.443       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 173      |
|    time_elapsed    | 24392    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=9.38 +/- 3.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.38        |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.018068803 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.71        |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.000908    |
|    value_loss           | 140         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 174      |
|    time_elapsed    | 24541    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 175         |
|    time_elapsed         | 24665       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.014369337 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 120         |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 196         |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=5.86 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.86        |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.020491382 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.16        |
|    n_updates            | 1750        |
|    policy_gradient_loss | 0.00229     |
|    value_loss           | 17          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -223     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 176      |
|    time_elapsed    | 24813    |
|    total_timesteps | 2883584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -148       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 177        |
|    time_elapsed         | 24938      |
|    total_timesteps      | 2899968    |
| train/                  |            |
|    approx_kl            | 0.01849642 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.19       |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.000213  |
|    value_loss           | 265        |
----------------------------------------
Eval num_timesteps=2900000, episode_reward=6.46 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.46        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.014843536 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.281       |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 178      |
|    time_elapsed    | 25086    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=5.67 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.67        |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.016950458 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | -0.00305    |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 508         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 179      |
|    time_elapsed    | 25234    |
|    total_timesteps | 2932736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -100       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 180        |
|    time_elapsed         | 25359      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.01570748 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.1       |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0069    |
|    value_loss           | 24.1       |
----------------------------------------
Eval num_timesteps=2950000, episode_reward=4.86 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.86        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.010850447 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 181      |
|    time_elapsed    | 25507    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=9.31 +/- 3.53
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 9.31      |
| time/                   |           |
|    total_timesteps      | 2975000   |
| train/                  |           |
|    approx_kl            | 0.0197604 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.54     |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.02      |
|    n_updates            | 1810      |
|    policy_gradient_loss | -0.000947 |
|    value_loss           | 48.3      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 182      |
|    time_elapsed    | 25656    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 183         |
|    time_elapsed         | 25780       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.010871417 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00275    |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=5.43 +/- 2.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.43        |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.024716381 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.00382     |
|    value_loss           | 208         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 184      |
|    time_elapsed    | 25928    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=6.02 +/- 4.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.02        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.017324906 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.31        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 8.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 185      |
|    time_elapsed    | 26077    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -139        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 186         |
|    time_elapsed         | 26201       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.016021738 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.000601   |
|    value_loss           | 344         |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=5.55 +/- 1.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.55        |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.011379005 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.1        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 80.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 187      |
|    time_elapsed    | 26349    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=-2307.97 +/- 4629.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.31e+03   |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.019058626 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 1870        |
|    policy_gradient_loss | 0.00429     |
|    value_loss           | 79.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 188      |
|    time_elapsed    | 26498    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 189         |
|    time_elapsed         | 26622       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.044616602 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.7        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 112         |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=6.20 +/- 1.72
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 6.2       |
| time/                   |           |
|    total_timesteps      | 3100000   |
| train/                  |           |
|    approx_kl            | 0.0159096 |
|    clip_fraction        | 0.169     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.44     |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.28      |
|    n_updates            | 1890      |
|    policy_gradient_loss | -0.00489  |
|    value_loss           | 20.1      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -99.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 190      |
|    time_elapsed    | 26770    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=6.26 +/- 1.91
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.26         |
| time/                   |              |
|    total_timesteps      | 3125000      |
| train/                  |              |
|    approx_kl            | 0.0069655585 |
|    clip_fraction        | 0.0524       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.3         |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 316          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -98.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 191      |
|    time_elapsed    | 26919    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -106        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 192         |
|    time_elapsed         | 27043       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.015157222 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 97          |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=5.18 +/- 1.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.18       |
| time/                   |            |
|    total_timesteps      | 3150000    |
| train/                  |            |
|    approx_kl            | 0.01485846 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.45      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.61       |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.00248   |
|    value_loss           | 190        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 193      |
|    time_elapsed    | 27192    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=7.80 +/- 4.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.013600086 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.29        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 40.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -82.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 194      |
|    time_elapsed    | 27340    |
|    total_timesteps | 3178496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -67.8      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 195        |
|    time_elapsed         | 27465      |
|    total_timesteps      | 3194880    |
| train/                  |            |
|    approx_kl            | 0.03405536 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.43       |
|    n_updates            | 1940       |
|    policy_gradient_loss | 0.00353    |
|    value_loss           | 188        |
----------------------------------------
Eval num_timesteps=3200000, episode_reward=5.36 +/- 1.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.36       |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.01551251 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.003     |
|    value_loss           | 19         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 196      |
|    time_elapsed    | 27614    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=8.12 +/- 3.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.12        |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.015460245 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 4.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.83    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 197      |
|    time_elapsed    | 27763    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 198         |
|    time_elapsed         | 27887       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.022214696 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 1970        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 55.8        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=6.52 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.52        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.014882036 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.4         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 83.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 199      |
|    time_elapsed    | 28036    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=7.19 +/- 2.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.19        |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.026977122 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 27.9        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 37.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 200      |
|    time_elapsed    | 28185    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 201         |
|    time_elapsed         | 28310       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.020416994 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.42        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 211         |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=7.52 +/- 4.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.52        |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.020276226 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.5        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -1.54e-05   |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 202      |
|    time_elapsed    | 28458    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=4.79 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.79        |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.017032363 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 2.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 203      |
|    time_elapsed    | 28607    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 204         |
|    time_elapsed         | 28731       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.012722932 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 22.3        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=7.53 +/- 3.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.53       |
| time/                   |            |
|    total_timesteps      | 3350000    |
| train/                  |            |
|    approx_kl            | 0.01874911 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.076      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.00639   |
|    value_loss           | 1.25       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 205      |
|    time_elapsed    | 28879    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=7.01 +/- 4.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.01       |
| time/                   |            |
|    total_timesteps      | 3375000    |
| train/                  |            |
|    approx_kl            | 0.00884093 |
|    clip_fraction        | 0.068      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 170        |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.000597  |
|    value_loss           | 281        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 206      |
|    time_elapsed    | 29028    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -112        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 207         |
|    time_elapsed         | 29153       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.013764697 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.94        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 9.38        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=9.19 +/- 3.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.19        |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.020243859 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.7        |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.000547    |
|    value_loss           | 141         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 208      |
|    time_elapsed    | 29301    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -137        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 209         |
|    time_elapsed         | 29426       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.016203709 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.34        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 45.3        |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=7.58 +/- 3.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.58       |
| time/                   |            |
|    total_timesteps      | 3425000    |
| train/                  |            |
|    approx_kl            | 0.00499115 |
|    clip_fraction        | 0.018      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.5       |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.00473   |
|    value_loss           | 426        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 210      |
|    time_elapsed    | 29574    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=8.98 +/- 2.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.98        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.017634314 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.432       |
|    n_updates            | 2100        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 55.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 211      |
|    time_elapsed    | 29723    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 212         |
|    time_elapsed         | 29847       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.020896189 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.02        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 12.7        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=5.78 +/- 1.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.78        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.014433458 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.03        |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 213      |
|    time_elapsed    | 29996    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=5.07 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.07        |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.014735252 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 214      |
|    time_elapsed    | 30144    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 215         |
|    time_elapsed         | 30268       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.019810366 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 5.97        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=4.48 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.48        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.012586556 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 216      |
|    time_elapsed    | 30417    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=5.91 +/- 1.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.91        |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.014537293 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 53.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 217      |
|    time_elapsed    | 30566    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 218         |
|    time_elapsed         | 30690       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.017135926 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.15        |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 2.21        |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=5.06 +/- 1.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.06        |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.016768675 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0569      |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 0.367       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 219      |
|    time_elapsed    | 30839    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=6.55 +/- 4.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.55        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.018239647 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 1.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.0574  |
| time/              |          |
|    fps             | 116      |
|    iterations      | 220      |
|    time_elapsed    | 30988    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.57       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 221         |
|    time_elapsed         | 31112       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.014016147 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 5.35        |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=7.80 +/- 3.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.020883396 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.000714   |
|    value_loss           | 69.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 222      |
|    time_elapsed    | 31260    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=6.75 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.75        |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.017748017 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.14        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 43.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 223      |
|    time_elapsed    | 31410    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 224         |
|    time_elapsed         | 31535       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.015659463 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=6.71 +/- 2.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.71       |
| time/                   |            |
|    total_timesteps      | 3675000    |
| train/                  |            |
|    approx_kl            | 0.01842748 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.00294   |
|    value_loss           | 4.89       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 225      |
|    time_elapsed    | 31684    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=5.59 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.59        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.018147875 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0828      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 25.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 226      |
|    time_elapsed    | 31832    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.6        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 227         |
|    time_elapsed         | 31957       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.019176444 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0807      |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 1.23        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=4.94 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.94        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.013008172 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0689      |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 9.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.00261  |
| time/              |          |
|    fps             | 116      |
|    iterations      | 228      |
|    time_elapsed    | 32105    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=7.48 +/- 2.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.48        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.022288408 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 4.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.0431  |
| time/              |          |
|    fps             | 116      |
|    iterations      | 229      |
|    time_elapsed    | 32254    |
|    total_timesteps | 3751936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.56      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 230        |
|    time_elapsed         | 32378      |
|    total_timesteps      | 3768320    |
| train/                  |            |
|    approx_kl            | 0.01712311 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0668     |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.00492   |
|    value_loss           | 0.963      |
----------------------------------------
Eval num_timesteps=3775000, episode_reward=8.44 +/- 2.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.44        |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.018069264 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 16.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 231      |
|    time_elapsed    | 32527    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=5.96 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.96        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.015569047 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 337         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 232      |
|    time_elapsed    | 32675    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -42.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 233         |
|    time_elapsed         | 32800       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.018455125 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 50          |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=5.90 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.9         |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.017329251 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 1.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 234      |
|    time_elapsed    | 32948    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=7.21 +/- 1.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.21       |
| time/                   |            |
|    total_timesteps      | 3850000    |
| train/                  |            |
|    approx_kl            | 0.02246648 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 57.8       |
|    n_updates            | 2340       |
|    policy_gradient_loss | 0.00189    |
|    value_loss           | 66.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 235      |
|    time_elapsed    | 33096    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 236         |
|    time_elapsed         | 33221       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.012515096 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 44.9        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=7.33 +/- 3.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.33       |
| time/                   |            |
|    total_timesteps      | 3875000    |
| train/                  |            |
|    approx_kl            | 0.01665714 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.263      |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.00493   |
|    value_loss           | 11.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 237      |
|    time_elapsed    | 33369    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 238         |
|    time_elapsed         | 33494       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.018060423 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.677       |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 17.9        |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=6.10 +/- 3.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.1         |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.013658911 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.404       |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 6.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 239      |
|    time_elapsed    | 33642    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=7.10 +/- 2.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.1         |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.030398639 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.00412     |
|    value_loss           | 102         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 240      |
|    time_elapsed    | 33791    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -136        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 241         |
|    time_elapsed         | 33915       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.022997802 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 114         |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=6.24 +/- 3.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.24        |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.021296676 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.58        |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.00344     |
|    value_loss           | 294         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 242      |
|    time_elapsed    | 34063    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=7.31 +/- 1.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.31        |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.008014291 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 243      |
|    time_elapsed    | 34212    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -135        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 244         |
|    time_elapsed         | 34336       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.017639782 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.353       |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00156    |
|    value_loss           | 1.23        |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=9.49 +/- 4.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.49        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.010969214 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 28.4        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 245      |
|    time_elapsed    | 34485    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=6.51 +/- 3.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.51        |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.014578199 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 0.765       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.67    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 246      |
|    time_elapsed    | 34633    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.45       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 247         |
|    time_elapsed         | 34758       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.014194181 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 10.1        |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=9.58 +/- 5.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.58        |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.013468547 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.6        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 12.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 248      |
|    time_elapsed    | 34907    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=6.53 +/- 4.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.53        |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.022551518 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 458         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 249      |
|    time_elapsed    | 35055    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -46         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 250         |
|    time_elapsed         | 35179       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.016442388 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 2.66        |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=5.03 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.03        |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.018554106 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.82        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 7.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.4     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 251      |
|    time_elapsed    | 35328    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=5.42 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.42        |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.015914004 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 1.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.66    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 252      |
|    time_elapsed    | 35477    |
|    total_timesteps | 4128768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.81      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 253        |
|    time_elapsed         | 35601      |
|    total_timesteps      | 4145152    |
| train/                  |            |
|    approx_kl            | 0.01585836 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0043    |
|    value_loss           | 6.57       |
----------------------------------------
Eval num_timesteps=4150000, episode_reward=6.57 +/- 3.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.57        |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.018489141 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00939    |
|    value_loss           | 0.638       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 254      |
|    time_elapsed    | 35750    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=11.73 +/- 13.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 11.7        |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.009475559 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 28.4        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 255      |
|    time_elapsed    | 35898    |
|    total_timesteps | 4177920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.7      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 256        |
|    time_elapsed         | 36023      |
|    total_timesteps      | 4194304    |
| train/                  |            |
|    approx_kl            | 0.01644582 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0812     |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.153      |
----------------------------------------
Eval num_timesteps=4200000, episode_reward=7.17 +/- 2.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.17        |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.014726719 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.16        |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 6.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 257      |
|    time_elapsed    | 36171    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=6.15 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.15        |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.009172697 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.785       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 258      |
|    time_elapsed    | 36320    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.9       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 259         |
|    time_elapsed         | 36444       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.017230975 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.284       |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 4.64        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=6.21 +/- 2.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.21        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.018656582 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.534       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 260      |
|    time_elapsed    | 36593    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=6.41 +/- 2.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.41        |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.013679177 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0467      |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 0.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 261      |
|    time_elapsed    | 36741    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.91       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 262         |
|    time_elapsed         | 36866       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.018129338 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0971      |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 0.454       |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=5.44 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.44        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.007385271 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.689       |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 10          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.11    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 263      |
|    time_elapsed    | 37014    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=4.40 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.026914215 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 2630        |
|    policy_gradient_loss | 0.00196     |
|    value_loss           | 63.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 264      |
|    time_elapsed    | 37163    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.8       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 265         |
|    time_elapsed         | 37287       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.020450529 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 257         |
|    n_updates            | 2640        |
|    policy_gradient_loss | 0.00296     |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=7.80 +/- 4.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.028136557 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.88        |
|    n_updates            | 2650        |
|    policy_gradient_loss | 0.0057      |
|    value_loss           | 77.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 266      |
|    time_elapsed    | 37435    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.7       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 267         |
|    time_elapsed         | 37560       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.018784449 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 0.304       |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=5.33 +/- 1.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.33       |
| time/                   |            |
|    total_timesteps      | 4375000    |
| train/                  |            |
|    approx_kl            | 0.01757291 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00178    |
|    n_updates            | 2670       |
|    policy_gradient_loss | -0.00609   |
|    value_loss           | 1.04       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 268      |
|    time_elapsed    | 37709    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=4.97 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.97        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.014109112 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0359      |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 2.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.51     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 269      |
|    time_elapsed    | 37857    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.03       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 270         |
|    time_elapsed         | 37982       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.015841443 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0754      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 0.697       |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=5.17 +/- 1.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.17        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.023338897 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00338    |
|    n_updates            | 2700        |
|    policy_gradient_loss | 0.000118    |
|    value_loss           | 84.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.19    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 271      |
|    time_elapsed    | 38130    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=5.42 +/- 1.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.42        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.020650342 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00982     |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 0.912       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.7     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 272      |
|    time_elapsed    | 38279    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.84       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 273         |
|    time_elapsed         | 38403       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.017046735 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 0.445       |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=6.57 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.57        |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.018987315 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.52        |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 1.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.05    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 274      |
|    time_elapsed    | 38552    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=5.02 +/- 1.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.02        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.011445754 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 44.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 275      |
|    time_elapsed    | 38701    |
|    total_timesteps | 4505600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -24.5      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 276        |
|    time_elapsed         | 38825      |
|    total_timesteps      | 4521984    |
| train/                  |            |
|    approx_kl            | 0.01611375 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.0127     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.000391  |
|    value_loss           | 122        |
----------------------------------------
Eval num_timesteps=4525000, episode_reward=6.59 +/- 1.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.59       |
| time/                   |            |
|    total_timesteps      | 4525000    |
| train/                  |            |
|    approx_kl            | 0.01761438 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.614      |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.00654   |
|    value_loss           | 17.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 277      |
|    time_elapsed    | 38974    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=5.83 +/- 1.04
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.83       |
| time/                   |            |
|    total_timesteps      | 4550000    |
| train/                  |            |
|    approx_kl            | 0.01831679 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.49       |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.00467   |
|    value_loss           | 21.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 278      |
|    time_elapsed    | 39123    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -95.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 279         |
|    time_elapsed         | 39248       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.013802367 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.1         |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 161         |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=7.63 +/- 2.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.63        |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.018235741 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.059       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 2790        |
|    policy_gradient_loss | 0.0049      |
|    value_loss           | 450         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 280      |
|    time_elapsed    | 39396    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=8.86 +/- 4.87
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 8.86         |
| time/                   |              |
|    total_timesteps      | 4600000      |
| train/                  |              |
|    approx_kl            | 0.0149693005 |
|    clip_fraction        | 0.194        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.41        |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.0003       |
|    loss                 | 39.9         |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00453     |
|    value_loss           | 275          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 281      |
|    time_elapsed    | 39545    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -134        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 282         |
|    time_elapsed         | 39669       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.017731443 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.571       |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 68.7        |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=5.18 +/- 0.78
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 5.18      |
| time/                   |           |
|    total_timesteps      | 4625000   |
| train/                  |           |
|    approx_kl            | 0.0148027 |
|    clip_fraction        | 0.183     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.41     |
|    explained_variance   | 0.749     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.26      |
|    n_updates            | 2820      |
|    policy_gradient_loss | -0.00838  |
|    value_loss           | 81.9      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 283      |
|    time_elapsed    | 39817    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=7.47 +/- 2.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.47        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.018489486 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.339       |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 4.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.73    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 284      |
|    time_elapsed    | 39967    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.28       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 285         |
|    time_elapsed         | 40091       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.014553241 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.84        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 8.43        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=3.96 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.016969826 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.625       |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 2.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 286      |
|    time_elapsed    | 40240    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=6.73 +/- 2.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.73        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.013495291 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.308       |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.000981   |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 287      |
|    time_elapsed    | 40389    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -29.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 288         |
|    time_elapsed         | 40514       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.015543696 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.86        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 62.2        |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=7.13 +/- 4.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.13        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.016443985 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.36        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 85          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 289      |
|    time_elapsed    | 40662    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=5.69 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.69        |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.016797509 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00511    |
|    value_loss           | 1.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 290      |
|    time_elapsed    | 40811    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.29       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 291         |
|    time_elapsed         | 40936       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.015599212 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0173      |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.402       |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=4.07 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.07        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.016824242 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.106       |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 0.967       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.98    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 292      |
|    time_elapsed    | 41085    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=5.03 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.03        |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.010997457 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 21.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.61    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 293      |
|    time_elapsed    | 41234    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 294         |
|    time_elapsed         | 41359       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.017125098 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 2.7         |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=5.57 +/- 2.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.57        |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.014921198 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 295      |
|    time_elapsed    | 41507    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 296         |
|    time_elapsed         | 41632       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.017334472 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.196       |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=3.00 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3            |
| time/                   |              |
|    total_timesteps      | 4850000      |
| train/                  |              |
|    approx_kl            | 0.0152326245 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.41        |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.68         |
|    n_updates            | 2960         |
|    policy_gradient_loss | -0.00492     |
|    value_loss           | 14.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 297      |
|    time_elapsed    | 41781    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=3.85 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.85        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.020037862 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.41        |
|    n_updates            | 2970        |
|    policy_gradient_loss | 0.000928    |
|    value_loss           | 47.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 298      |
|    time_elapsed    | 41930    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 299         |
|    time_elapsed         | 42054       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.016021246 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 26.1        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=6.09 +/- 2.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.09        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.029556904 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.471       |
|    n_updates            | 2990        |
|    policy_gradient_loss | 2.04e-05    |
|    value_loss           | 60.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 300      |
|    time_elapsed    | 42203    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=4.99 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.99        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.020835988 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00834    |
|    value_loss           | 0.295       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 301      |
|    time_elapsed    | 42353    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 302         |
|    time_elapsed         | 42477       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.018821515 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 1           |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=5.25 +/- 2.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.25        |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.016398124 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.868       |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.000275   |
|    value_loss           | 24.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.75    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 303      |
|    time_elapsed    | 42626    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=6.72 +/- 1.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.72        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.017163355 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.633       |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 10.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 304      |
|    time_elapsed    | 42775    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.7       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 305         |
|    time_elapsed         | 42899       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.019194255 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 55          |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=7.71 +/- 3.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.71       |
| time/                   |            |
|    total_timesteps      | 5000000    |
| train/                  |            |
|    approx_kl            | 0.01842915 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.93       |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.0016    |
|    value_loss           | 11.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 306      |
|    time_elapsed    | 43048    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=5.80 +/- 2.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.018368468 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.15        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 25.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -97.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 307      |
|    time_elapsed    | 43197    |
|    total_timesteps | 5029888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -82.2      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 308        |
|    time_elapsed         | 43322      |
|    total_timesteps      | 5046272    |
| train/                  |            |
|    approx_kl            | 0.01309743 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.32      |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.8       |
|    n_updates            | 3070       |
|    policy_gradient_loss | -0.00398   |
|    value_loss           | 179        |
----------------------------------------
Eval num_timesteps=5050000, episode_reward=5.38 +/- 2.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.38       |
| time/                   |            |
|    total_timesteps      | 5050000    |
| train/                  |            |
|    approx_kl            | 0.01927689 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0806     |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.00255   |
|    value_loss           | 1.91       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 309      |
|    time_elapsed    | 43471    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=4.69 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.69        |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.015943188 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.318       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 3.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 310      |
|    time_elapsed    | 43620    |
|    total_timesteps | 5079040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -20.2        |
| time/                   |              |
|    fps                  | 116          |
|    iterations           | 311          |
|    time_elapsed         | 43745        |
|    total_timesteps      | 5095424      |
| train/                  |              |
|    approx_kl            | 0.0111609325 |
|    clip_fraction        | 0.0901       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.57         |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00496     |
|    value_loss           | 56.8         |
------------------------------------------
Eval num_timesteps=5100000, episode_reward=5.10 +/- 1.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.015211733 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 3110        |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 312      |
|    time_elapsed    | 43893    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=3.94 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.94        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.037147246 |
|    clip_fraction        | 0.397       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.084       |
|    n_updates            | 3120        |
|    policy_gradient_loss | 0.00517     |
|    value_loss           | 96.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 313      |
|    time_elapsed    | 44042    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 314         |
|    time_elapsed         | 44167       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.015227623 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.137       |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 1.68        |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=3.04 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.04        |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.013335829 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.934       |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 3.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.75    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 315      |
|    time_elapsed    | 44316    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=3.29 +/- 0.72
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.29       |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.02303551 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.045      |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.000271  |
|    value_loss           | 0.634      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.28     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 316      |
|    time_elapsed    | 44464    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 317         |
|    time_elapsed         | 44589       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.012665361 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.206       |
|    n_updates            | 3160        |
|    policy_gradient_loss | 9.64e-05    |
|    value_loss           | 34.2        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=3.17 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.17       |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.01948344 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.32      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.959      |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.00168   |
|    value_loss           | 46.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 318      |
|    time_elapsed    | 44737    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=4.54 +/- 1.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.54        |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.012223274 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.02        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 92          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 319      |
|    time_elapsed    | 44885    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -79.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 320         |
|    time_elapsed         | 45010       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.021079106 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.08        |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.000466   |
|    value_loss           | 165         |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=5.23 +/- 2.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.23        |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.018613975 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 145         |
|    n_updates            | 3200        |
|    policy_gradient_loss | 0.00225     |
|    value_loss           | 380         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -71.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 321      |
|    time_elapsed    | 45159    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=4.30 +/- 1.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.017138813 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 7.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 322      |
|    time_elapsed    | 45307    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -58.1       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 323         |
|    time_elapsed         | 45432       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.014778003 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 75.9        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=2.67 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67        |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.018074904 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.308       |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.654       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 324      |
|    time_elapsed    | 45580    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 325         |
|    time_elapsed         | 45706       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.015415877 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.607       |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 18.8        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=2.49 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.016126007 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.242       |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 11.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.01    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 326      |
|    time_elapsed    | 45855    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2.68 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.016620249 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.634       |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 2.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 327      |
|    time_elapsed    | 46004    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 328         |
|    time_elapsed         | 46129       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.018890722 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.76        |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=2.84 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.84        |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.017972417 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.478       |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 22.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 329      |
|    time_elapsed    | 46278    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=2.44 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.015002351 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 7.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 330      |
|    time_elapsed    | 46427    |
|    total_timesteps | 5406720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -20.9      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 331        |
|    time_elapsed         | 46551      |
|    total_timesteps      | 5423104    |
| train/                  |            |
|    approx_kl            | 0.01929945 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.00527   |
|    value_loss           | 250        |
----------------------------------------
Eval num_timesteps=5425000, episode_reward=2.63 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63        |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.017107408 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 1.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 332      |
|    time_elapsed    | 46700    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2.38 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.019734764 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.9        |
|    n_updates            | 3320        |
|    policy_gradient_loss | 0.00366     |
|    value_loss           | 220         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 333      |
|    time_elapsed    | 46849    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -63.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 334         |
|    time_elapsed         | 46974       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.018009927 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 21.9        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=2.85 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.85        |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.019505568 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.26        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 3           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 335      |
|    time_elapsed    | 47124    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=2.53 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.020096358 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 42.1        |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.000791   |
|    value_loss           | 17.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.63    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 336      |
|    time_elapsed    | 47272    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 337         |
|    time_elapsed         | 47397       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.017932625 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.11        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 19.2        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=2.63 +/- 0.50
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.63      |
| time/                   |           |
|    total_timesteps      | 5525000   |
| train/                  |           |
|    approx_kl            | 0.0290834 |
|    clip_fraction        | 0.305     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.24     |
|    explained_variance   | 0.319     |
|    learning_rate        | 0.0003    |
|    loss                 | 369       |
|    n_updates            | 3370      |
|    policy_gradient_loss | 0.00456   |
|    value_loss           | 653       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -52.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 338      |
|    time_elapsed    | 47545    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=2.36 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.019283997 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 2.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 339      |
|    time_elapsed    | 47695    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -50.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 340         |
|    time_elapsed         | 47820       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.022874264 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0397      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 2.01        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=2.47 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.47        |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.016282938 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.000857   |
|    value_loss           | 37          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 341      |
|    time_elapsed    | 47969    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=2.34 +/- 0.23
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.34       |
| time/                   |            |
|    total_timesteps      | 5600000    |
| train/                  |            |
|    approx_kl            | 0.01869582 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.469      |
|    n_updates            | 3410       |
|    policy_gradient_loss | -0.00454   |
|    value_loss           | 5.72       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 342      |
|    time_elapsed    | 48118    |
|    total_timesteps | 5603328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -10.6      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 343        |
|    time_elapsed         | 48242      |
|    total_timesteps      | 5619712    |
| train/                  |            |
|    approx_kl            | 0.01855375 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.32      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.007     |
|    value_loss           | 1.36       |
----------------------------------------
Eval num_timesteps=5625000, episode_reward=2.78 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 5625000      |
| train/                  |              |
|    approx_kl            | 0.0134725645 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.253        |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 3.26         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.7     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 344      |
|    time_elapsed    | 48391    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=2.66 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.019921478 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0415      |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.41    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 345      |
|    time_elapsed    | 48540    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.28       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 346         |
|    time_elapsed         | 48664       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.018409919 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0933      |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.2         |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=2.43 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.015831955 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.0885      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0635      |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.25    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 347      |
|    time_elapsed    | 48813    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=2.42 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.015370449 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 1.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.07    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 348      |
|    time_elapsed    | 48962    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.7       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 349         |
|    time_elapsed         | 49087       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.015858661 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 40.2        |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=2.70 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 5725000      |
| train/                  |              |
|    approx_kl            | 0.0150959445 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52         |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 141          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 350      |
|    time_elapsed    | 49235    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2.54 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.013990172 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.58        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 3.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 351      |
|    time_elapsed    | 49384    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 352         |
|    time_elapsed         | 49509       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.020181872 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 3510        |
|    policy_gradient_loss | 0.00293     |
|    value_loss           | 44.5        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=2.57 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.014892746 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 10.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 353      |
|    time_elapsed    | 49658    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 354         |
|    time_elapsed         | 49784       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.019362144 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.05        |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 0.548       |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=3.09 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.012537947 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 0.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.44    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 355      |
|    time_elapsed    | 49933    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=2.97 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.016244322 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.000146   |
|    value_loss           | 41.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.85    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 356      |
|    time_elapsed    | 50082    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.66       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 357         |
|    time_elapsed         | 50207       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.021261944 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.74        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 29.4        |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=2.53 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.016272593 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 8.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 358      |
|    time_elapsed    | 50356    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=2.60 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.016610421 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.238       |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 2.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.74    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 359      |
|    time_elapsed    | 50504    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -62         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 360         |
|    time_elapsed         | 50629       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.018045388 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.6        |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 8.65        |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=2.71 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.71        |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.016974453 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 3600        |
|    policy_gradient_loss | 0.00336     |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 361      |
|    time_elapsed    | 50778    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=2.58 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.019450823 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0627      |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.000313   |
|    value_loss           | 0.182       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 362      |
|    time_elapsed    | 50927    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -65.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 363         |
|    time_elapsed         | 51052       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.014743893 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.694       |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 21          |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=2.32 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.014694431 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.56        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 72.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.08    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 364      |
|    time_elapsed    | 51201    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=3.33 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.33        |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.016453184 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0986      |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 0.905       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.18    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 365      |
|    time_elapsed    | 51350    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.28       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 366         |
|    time_elapsed         | 51475       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.018205984 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 4.96        |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=2.88 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.013150471 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.21        |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.00618    |
|    value_loss           | 9.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.71    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 367      |
|    time_elapsed    | 51623    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=2.73 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.73        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.016003728 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0653      |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 5.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.35    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 368      |
|    time_elapsed    | 51772    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.11       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 369         |
|    time_elapsed         | 51897       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.017036818 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 24.7        |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=4.60 +/- 3.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.016101766 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.21        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 5.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.37    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 370      |
|    time_elapsed    | 52045    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=3.57 +/- 1.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.57        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.021190464 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0767      |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 3.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 371      |
|    time_elapsed    | 52194    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.4       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 372         |
|    time_elapsed         | 52319       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.012881595 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.778       |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 25.7        |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=2.99 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.99        |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.015645847 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.2        |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 67.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 373      |
|    time_elapsed    | 52468    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=3.18 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.18        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.017034926 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | -0.071      |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 762         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 374      |
|    time_elapsed    | 52616    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -155        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 375         |
|    time_elapsed         | 52741       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.020421155 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0003      |
|    loss                 | 212         |
|    n_updates            | 3740        |
|    policy_gradient_loss | 0.00162     |
|    value_loss           | 328         |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=2.86 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.012123812 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | 43.2        |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 323         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 376      |
|    time_elapsed    | 52890    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=3.34 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.34       |
| time/                   |            |
|    total_timesteps      | 6175000    |
| train/                  |            |
|    approx_kl            | 0.01641088 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 324        |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.00392   |
|    value_loss           | 356        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 377      |
|    time_elapsed    | 53038    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -131        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 378         |
|    time_elapsed         | 53163       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.016996577 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.9        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 128         |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=2.63 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63        |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.019841623 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 77.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 379      |
|    time_elapsed    | 53312    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2.82 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.022437071 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 23.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 380      |
|    time_elapsed    | 53461    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.3       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 381         |
|    time_elapsed         | 53586       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.018244833 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 15.8        |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=2.93 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.015815606 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.86        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 53.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 382      |
|    time_elapsed    | 53734    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=3.20 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.2         |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.018189596 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00769    |
|    value_loss           | 0.918       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 383      |
|    time_elapsed    | 53883    |
|    total_timesteps | 6275072  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -10.5        |
| time/                   |              |
|    fps                  | 116          |
|    iterations           | 384          |
|    time_elapsed         | 54008        |
|    total_timesteps      | 6291456      |
| train/                  |              |
|    approx_kl            | 0.0136303445 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.4          |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00718     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=6300000, episode_reward=3.34 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.014479328 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.42        |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 12          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.15    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 385      |
|    time_elapsed    | 54158    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2          |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 386         |
|    time_elapsed         | 54283       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.015502811 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.316       |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 1.54        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=2.74 +/- 0.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.74       |
| time/                   |            |
|    total_timesteps      | 6325000    |
| train/                  |            |
|    approx_kl            | 0.01593259 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.277      |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.00557   |
|    value_loss           | 0.771      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.15    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 387      |
|    time_elapsed    | 54432    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=3.17 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.015391528 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.228       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 2.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1        |
| time/              |          |
|    fps             | 116      |
|    iterations      | 388      |
|    time_elapsed    | 54581    |
|    total_timesteps | 6356992  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.35       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 389        |
|    time_elapsed         | 54706      |
|    total_timesteps      | 6373376    |
| train/                  |            |
|    approx_kl            | 0.01657525 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0946     |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0076    |
|    value_loss           | 1.19       |
----------------------------------------
Eval num_timesteps=6375000, episode_reward=3.12 +/- 0.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.12       |
| time/                   |            |
|    total_timesteps      | 6375000    |
| train/                  |            |
|    approx_kl            | 0.01569812 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0715     |
|    n_updates            | 3890       |
|    policy_gradient_loss | -0.00659   |
|    value_loss           | 1.56       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.501    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 390      |
|    time_elapsed    | 54856    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=2.93 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.019631572 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.67    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 391      |
|    time_elapsed    | 55005    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.01       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 392         |
|    time_elapsed         | 55131       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.019796375 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.515       |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 21.4        |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=2.87 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.87        |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.015673613 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00559    |
|    value_loss           | 2.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.1     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 393      |
|    time_elapsed    | 55280    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=3.17 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.015390123 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.304       |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 1.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.79    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 394      |
|    time_elapsed    | 55429    |
|    total_timesteps | 6455296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 0.774      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 395        |
|    time_elapsed         | 55553      |
|    total_timesteps      | 6471680    |
| train/                  |            |
|    approx_kl            | 0.01827221 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.269      |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00709   |
|    value_loss           | 0.557      |
----------------------------------------
Eval num_timesteps=6475000, episode_reward=3.29 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.29        |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.014032489 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 1.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.649    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 396      |
|    time_elapsed    | 55702    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2.43 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.016228866 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.705       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 15.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.44    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 397      |
|    time_elapsed    | 55851    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.309       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 398         |
|    time_elapsed         | 55976       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.018394189 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.69        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 2.02        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=2.80 +/- 0.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.8        |
| time/                   |            |
|    total_timesteps      | 6525000    |
| train/                  |            |
|    approx_kl            | 0.01597429 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.223      |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.00266   |
|    value_loss           | 1.92       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.85    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 399      |
|    time_elapsed    | 56125    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=3.05 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.017421218 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.665       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00784    |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.528   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 400      |
|    time_elapsed    | 56274    |
|    total_timesteps | 6553600  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -6.7      |
| time/                   |           |
|    fps                  | 116       |
|    iterations           | 401       |
|    time_elapsed         | 56399     |
|    total_timesteps      | 6569984   |
| train/                  |           |
|    approx_kl            | 0.0165211 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.06     |
|    explained_variance   | 0.939     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.35      |
|    n_updates            | 4000      |
|    policy_gradient_loss | -0.00581  |
|    value_loss           | 2.5       |
---------------------------------------
Eval num_timesteps=6575000, episode_reward=3.38 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.38        |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.016536018 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61        |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 36.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.81    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 402      |
|    time_elapsed    | 56548    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=3.29 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.29        |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.020603117 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 0.987       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.46    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 403      |
|    time_elapsed    | 56698    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.1       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 404         |
|    time_elapsed         | 56823       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.014303124 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.303       |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 2.37        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=3.32 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.32        |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.017988589 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 405      |
|    time_elapsed    | 56972    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=3.12 +/- 0.28
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 3.12     |
| time/                   |          |
|    total_timesteps      | 6650000  |
| train/                  |          |
|    approx_kl            | 0.018608 |
|    clip_fraction        | 0.24     |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.08    |
|    explained_variance   | 0.718    |
|    learning_rate        | 0.0003   |
|    loss                 | 9.6      |
|    n_updates            | 4050     |
|    policy_gradient_loss | -0.00157 |
|    value_loss           | 64.7     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 406      |
|    time_elapsed    | 57121    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -145        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 407         |
|    time_elapsed         | 57246       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.008596936 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.54        |
|    n_updates            | 4060        |
|    policy_gradient_loss | 1.5e-05     |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=2.58 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.021301977 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 207         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 408      |
|    time_elapsed    | 57394    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=2.82 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.012794603 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.459       |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 256         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 409      |
|    time_elapsed    | 57543    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -37         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 410         |
|    time_elapsed         | 57668       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.025218828 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0866      |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 46          |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=2.52 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.018080615 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 1.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 411      |
|    time_elapsed    | 57817    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2.75 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.013579169 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 2.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.24    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 412      |
|    time_elapsed    | 57966    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -3.41      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 413        |
|    time_elapsed         | 58091      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.01685633 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.885      |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00452   |
|    value_loss           | 11.7       |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=2.56 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.020672817 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.93    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 414      |
|    time_elapsed    | 58240    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.85       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 415         |
|    time_elapsed         | 58365       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.018321209 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.949       |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 0.833       |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=2.80 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.016070386 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 4.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.27    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 416      |
|    time_elapsed    | 58514    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=2.64 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.020067886 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.458       |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00851    |
|    value_loss           | 0.665       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.09    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 417      |
|    time_elapsed    | 58664    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.44       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 418         |
|    time_elapsed         | 58789       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.013486154 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.105       |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 17.8        |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=3.48 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.017900024 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 2.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.53    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 419      |
|    time_elapsed    | 58938    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=3.61 +/- 1.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.61        |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.016368505 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0699      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 0.801       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.36    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 420      |
|    time_elapsed    | 59087    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.97        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 421         |
|    time_elapsed         | 59212       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.014556745 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 2.19        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=3.55 +/- 1.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.55       |
| time/                   |            |
|    total_timesteps      | 6900000    |
| train/                  |            |
|    approx_kl            | 0.01764358 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.32      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.0061    |
|    value_loss           | 0.516      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 422      |
|    time_elapsed    | 59360    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=4.40 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.017455835 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 3.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.12    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 423      |
|    time_elapsed    | 59509    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.82       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 424         |
|    time_elapsed         | 59634       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.016501796 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=7.14 +/- 4.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.14        |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.018006783 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.362       |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 1.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.53    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 425      |
|    time_elapsed    | 59784    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=3.80 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.015852867 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 1.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.413   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 426      |
|    time_elapsed    | 59933    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.68       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 427         |
|    time_elapsed         | 60059       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.022063132 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.856       |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.000763   |
|    value_loss           | 3.46        |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=3.71 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.71        |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.026511088 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.0861      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.52        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 85.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 428      |
|    time_elapsed    | 60208    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=2.99 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.99       |
| time/                   |            |
|    total_timesteps      | 7025000    |
| train/                  |            |
|    approx_kl            | 0.02096945 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.1       |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0762     |
|    n_updates            | 4280       |
|    policy_gradient_loss | -0.00609   |
|    value_loss           | 49         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 429      |
|    time_elapsed    | 60357    |
|    total_timesteps | 7028736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11.7      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 430        |
|    time_elapsed         | 60482      |
|    total_timesteps      | 7045120    |
| train/                  |            |
|    approx_kl            | 0.01568147 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.00344   |
|    value_loss           | 4.22       |
----------------------------------------
Eval num_timesteps=7050000, episode_reward=3.91 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.91        |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.018609874 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 20.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.74    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 431      |
|    time_elapsed    | 60631    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=3.53 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.53        |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.016385585 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.15        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 11.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.66    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 432      |
|    time_elapsed    | 60779    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.42       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 433         |
|    time_elapsed         | 60904       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.018958583 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.201       |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 0.661       |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=3.27 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.27        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.011439256 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.004      |
|    value_loss           | 32.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.24    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 434      |
|    time_elapsed    | 61052    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=2.86 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.017518874 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 10.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 435      |
|    time_elapsed    | 61201    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.5       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 436         |
|    time_elapsed         | 61326       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.022459999 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.495       |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 16.9        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=3.57 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.57        |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.019516038 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 437      |
|    time_elapsed    | 61475    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=3.03 +/- 0.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.03       |
| time/                   |            |
|    total_timesteps      | 7175000    |
| train/                  |            |
|    approx_kl            | 0.02110073 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 203        |
|    n_updates            | 4370       |
|    policy_gradient_loss | -0.00634   |
|    value_loss           | 39.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.88    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 438      |
|    time_elapsed    | 61624    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.47       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 439         |
|    time_elapsed         | 61748       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.018878017 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.41        |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00412    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=3.85 +/- 0.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.85       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.02283679 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.00516   |
|    value_loss           | 1.79       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.449   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 440      |
|    time_elapsed    | 61897    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=3.96 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.013617238 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.18        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 3.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.85    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 441      |
|    time_elapsed    | 62046    |
|    total_timesteps | 7225344  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -0.503     |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 442        |
|    time_elapsed         | 62170      |
|    total_timesteps      | 7241728    |
| train/                  |            |
|    approx_kl            | 0.01830559 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 4410       |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.733      |
----------------------------------------
Eval num_timesteps=7250000, episode_reward=2.86 +/- 0.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.86       |
| time/                   |            |
|    total_timesteps      | 7250000    |
| train/                  |            |
|    approx_kl            | 0.01766647 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0757     |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.00969   |
|    value_loss           | 0.823      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.657   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 443      |
|    time_elapsed    | 62319    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.584      |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 444         |
|    time_elapsed         | 62444       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.018766634 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00922    |
|    value_loss           | 0.58        |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=4.37 +/- 3.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.37        |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.019204836 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 2.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.845   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 445      |
|    time_elapsed    | 62593    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=2.72 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.015698506 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.287       |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 1.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.653   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 446      |
|    time_elapsed    | 62743    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.01       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 447         |
|    time_elapsed         | 62868       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.021189505 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0642      |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 1.27        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=4.21 +/- 1.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.21       |
| time/                   |            |
|    total_timesteps      | 7325000    |
| train/                  |            |
|    approx_kl            | 0.01973223 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.072      |
|    n_updates            | 4470       |
|    policy_gradient_loss | -0.00571   |
|    value_loss           | 1.68       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.499    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 448      |
|    time_elapsed    | 63017    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=4.28 +/- 1.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.28        |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.018319808 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.204       |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 3.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 449      |
|    time_elapsed    | 63166    |
|    total_timesteps | 7356416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 3.38       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 450        |
|    time_elapsed         | 63292      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.01950107 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.364      |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.00754   |
|    value_loss           | 0.542      |
----------------------------------------
Eval num_timesteps=7375000, episode_reward=3.48 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.017692238 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 7.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.88     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 451      |
|    time_elapsed    | 63441    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=4.53 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.53        |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.016744688 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0926      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 3.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 452      |
|    time_elapsed    | 63589    |
|    total_timesteps | 7405568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -61.7      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 453        |
|    time_elapsed         | 63714      |
|    total_timesteps      | 7421952    |
| train/                  |            |
|    approx_kl            | 0.01806138 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.00674   |
|    value_loss           | 13         |
----------------------------------------
Eval num_timesteps=7425000, episode_reward=3.59 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.59        |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.010668209 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.0125      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.72        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 524         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 454      |
|    time_elapsed    | 63863    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=3.62 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.62        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.018453877 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 120         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 455      |
|    time_elapsed    | 64012    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -75         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 456         |
|    time_elapsed         | 64136       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.019164033 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.826       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0066     |
|    value_loss           | 19.8        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=-581.64 +/- 1181.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -582        |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.017507246 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.245       |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 6.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 457      |
|    time_elapsed    | 64285    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=3.50 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.016926028 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0931      |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 3.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.56    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 458      |
|    time_elapsed    | 64434    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.69       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 459         |
|    time_elapsed         | 64559       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.018780056 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.181       |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 1.47        |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=4.41 +/- 1.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.41        |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.018444482 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.7        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 20.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 460      |
|    time_elapsed    | 64709    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=3.28 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.28        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.013441203 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.61        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.000711   |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 461      |
|    time_elapsed    | 64858    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -168        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 462         |
|    time_elapsed         | 64983       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.018260475 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.393       |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 11.6        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=4.11 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.11        |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.015643928 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.277       |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 14.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 463      |
|    time_elapsed    | 65132    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=4.77 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.77        |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.012147755 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 46          |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 174         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 464      |
|    time_elapsed    | 65280    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -21         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 465         |
|    time_elapsed         | 65405       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.019593727 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.3        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 8.45        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=3.73 +/- 0.56
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.73       |
| time/                   |            |
|    total_timesteps      | 7625000    |
| train/                  |            |
|    approx_kl            | 0.01698551 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 4650       |
|    policy_gradient_loss | -0.00495   |
|    value_loss           | 1.52       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 466      |
|    time_elapsed    | 65554    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=6.01 +/- 2.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.01        |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.014244504 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 9.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3       |
| time/              |          |
|    fps             | 116      |
|    iterations      | 467      |
|    time_elapsed    | 65703    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.0261     |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 468         |
|    time_elapsed         | 65828       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.020370798 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.43        |
|    n_updates            | 4670        |
|    policy_gradient_loss | 0.000182    |
|    value_loss           | 5.93        |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=5.02 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.02        |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.017773062 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.61        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 1.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.21    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 469      |
|    time_elapsed    | 65977    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=4.09 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.09        |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.017455392 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.93        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 22.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.17    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 470      |
|    time_elapsed    | 66126    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.953      |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 471         |
|    time_elapsed         | 66251       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.018597841 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.436       |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00075    |
|    value_loss           | 0.681       |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=3.82 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.82        |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.017310394 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.124       |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 1.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.422   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 472      |
|    time_elapsed    | 66400    |
|    total_timesteps | 7733248  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.26       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 473        |
|    time_elapsed         | 66527      |
|    total_timesteps      | 7749632    |
| train/                  |            |
|    approx_kl            | 0.01716942 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.2        |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.00916   |
|    value_loss           | 0.949      |
----------------------------------------
Eval num_timesteps=7750000, episode_reward=5.19 +/- 2.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.19        |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.015398283 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.355       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00364    |
|    value_loss           | 1.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.95    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 474      |
|    time_elapsed    | 66677    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=4.57 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.57        |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.008138632 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 347         |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 94.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.87    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 475      |
|    time_elapsed    | 66826    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.9       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 476         |
|    time_elapsed         | 66951       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.017350484 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.57        |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 3.63        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=6.46 +/- 3.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.46        |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.013454903 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.07        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 8.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.06    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 477      |
|    time_elapsed    | 67100    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=5.77 +/- 2.12
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.77       |
| time/                   |            |
|    total_timesteps      | 7825000    |
| train/                  |            |
|    approx_kl            | 0.02020806 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.3        |
|    n_updates            | 4770       |
|    policy_gradient_loss | -0.00114   |
|    value_loss           | 2.56       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 478      |
|    time_elapsed    | 67249    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.9       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 479         |
|    time_elapsed         | 67373       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.015167121 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 24.2        |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 91.2        |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=4.30 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.024135105 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.1        |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 63.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 480      |
|    time_elapsed    | 67522    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=4.98 +/- 2.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.98        |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.017373323 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.192       |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.00559    |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 481      |
|    time_elapsed    | 67671    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.9       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 482         |
|    time_elapsed         | 67796       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.016474914 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.434       |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 3.9         |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=5.91 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.91        |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.015163546 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.11        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 13.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 483      |
|    time_elapsed    | 67946    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=4.37 +/- 1.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.37       |
| time/                   |            |
|    total_timesteps      | 7925000    |
| train/                  |            |
|    approx_kl            | 0.02067021 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.233      |
|    n_updates            | 4830       |
|    policy_gradient_loss | -0.00881   |
|    value_loss           | 6.52       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.07    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 484      |
|    time_elapsed    | 68095    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.98       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 485         |
|    time_elapsed         | 68221       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.019785471 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 2.27        |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=4.39 +/- 0.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.39       |
| time/                   |            |
|    total_timesteps      | 7950000    |
| train/                  |            |
|    approx_kl            | 0.01356748 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 4850       |
|    policy_gradient_loss | -0.00486   |
|    value_loss           | 2.25       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.29    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 486      |
|    time_elapsed    | 68370    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=5.24 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.24        |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.020175073 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.208       |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 0.797       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.84    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 487      |
|    time_elapsed    | 68519    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.72       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 488         |
|    time_elapsed         | 68644       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.016048124 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.403       |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 2.19        |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=5.42 +/- 2.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.42        |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.016983703 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.582       |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00695    |
|    value_loss           | 13.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.59    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 489      |
|    time_elapsed    | 68794    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=5.53 +/- 1.47
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.53       |
| time/                   |            |
|    total_timesteps      | 8025000    |
| train/                  |            |
|    approx_kl            | 0.01583244 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.914      |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.00767   |
|    value_loss           | 4.57       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.84    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 490      |
|    time_elapsed    | 68943    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.98       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 491         |
|    time_elapsed         | 69068       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.015294272 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.701       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 17.9        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=6.79 +/- 2.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.79        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.018375698 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0874      |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 2.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 492      |
|    time_elapsed    | 69217    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=6.16 +/- 2.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.16        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.011728713 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.73        |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00502    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 493      |
|    time_elapsed    | 69365    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.21       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 494         |
|    time_elapsed         | 69490       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.018105319 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.433       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 2.23        |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=3.54 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.54        |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.019677643 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.25        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 11.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.05    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 495      |
|    time_elapsed    | 69638    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=5.71 +/- 2.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.71        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.016872996 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.999       |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 6.42        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.8     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 496      |
|    time_elapsed    | 69787    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.21       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 497         |
|    time_elapsed         | 69912       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.020467304 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.06        |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 33.2        |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=5.86 +/- 2.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.86        |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.017541839 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.87        |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 6.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 498      |
|    time_elapsed    | 70060    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=4.25 +/- 1.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.25        |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.015112357 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.17        |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 30.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 499      |
|    time_elapsed    | 70209    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.19       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 500         |
|    time_elapsed         | 70334       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.015971191 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.4         |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 1.13        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=4.41 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.41        |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.016681805 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 1.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 501      |
|    time_elapsed    | 70483    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.961      |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 502         |
|    time_elapsed         | 70607       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.018889211 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 0.664       |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=-1175.92 +/- 2360.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 8225000     |
| train/                  |             |
|    approx_kl            | 0.016806437 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 9.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.13    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 503      |
|    time_elapsed    | 70756    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=-583.23 +/- 1179.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -583        |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.017504945 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0617      |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 0.596       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.15    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 504      |
|    time_elapsed    | 70905    |
|    total_timesteps | 8257536  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.07      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 505        |
|    time_elapsed         | 71030      |
|    total_timesteps      | 8273920    |
| train/                  |            |
|    approx_kl            | 0.01671875 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.065      |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.00901   |
|    value_loss           | 2.73       |
----------------------------------------
Eval num_timesteps=8275000, episode_reward=6.64 +/- 3.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.64        |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.018445592 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 0.518       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.4      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 506      |
|    time_elapsed    | 71179    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=3.43 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.017746981 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 11          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 507      |
|    time_elapsed    | 71327    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.308       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 508         |
|    time_elapsed         | 71452       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.018386345 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.000506   |
|    value_loss           | 7.11        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=2.99 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.99         |
| time/                   |              |
|    total_timesteps      | 8325000      |
| train/                  |              |
|    approx_kl            | 0.0146079995 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.297        |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00326     |
|    value_loss           | 21           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.75    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 509      |
|    time_elapsed    | 71601    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=3.67 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.67        |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.017849332 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 186         |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 510      |
|    time_elapsed    | 71750    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.7       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 511         |
|    time_elapsed         | 71875       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.017507166 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.348       |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 8.78        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=6.46 +/- 3.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.46         |
| time/                   |              |
|    total_timesteps      | 8375000      |
| train/                  |              |
|    approx_kl            | 0.0144877955 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.166        |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.00499     |
|    value_loss           | 6.87         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.75    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 512      |
|    time_elapsed    | 72025    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=6.43 +/- 3.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.43        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.016553715 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 0.894       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.96    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 513      |
|    time_elapsed    | 72175    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.79       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 514         |
|    time_elapsed         | 72300       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.013776278 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 40.3        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=5.35 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.35        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.015499715 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.296       |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 2.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 515      |
|    time_elapsed    | 72449    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=-2355.79 +/- 2881.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.36e+03   |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.012700216 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.88        |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 25.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 516      |
|    time_elapsed    | 72598    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.54       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 517         |
|    time_elapsed         | 72723       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.017097179 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 2.18        |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=-1176.50 +/- 2359.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.015046527 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.13        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 11.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.24    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 518      |
|    time_elapsed    | 72872    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=6.02 +/- 3.69
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.02       |
| time/                   |            |
|    total_timesteps      | 8500000    |
| train/                  |            |
|    approx_kl            | 0.01721044 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.05       |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.00777   |
|    value_loss           | 2.67       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 519      |
|    time_elapsed    | 73020    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 520         |
|    time_elapsed         | 73145       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.013827829 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=4.14 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.14        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.016373051 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 18          |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 2.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -55.8    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 521      |
|    time_elapsed    | 73293    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=5.42 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.42        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.016509723 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.084       |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00672    |
|    value_loss           | 2.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 522      |
|    time_elapsed    | 73442    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.17       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 523         |
|    time_elapsed         | 73567       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.016345467 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00782    |
|    value_loss           | 4.38        |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=3.51 +/- 0.86
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.51       |
| time/                   |            |
|    total_timesteps      | 8575000    |
| train/                  |            |
|    approx_kl            | 0.01772596 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.179      |
|    n_updates            | 5230       |
|    policy_gradient_loss | -0.00759   |
|    value_loss           | 0.741      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.07    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 524      |
|    time_elapsed    | 73716    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=5.37 +/- 1.05
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.37       |
| time/                   |            |
|    total_timesteps      | 8600000    |
| train/                  |            |
|    approx_kl            | 0.01926053 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.107      |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.00768   |
|    value_loss           | 0.325      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.212   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 525      |
|    time_elapsed    | 73865    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 526         |
|    time_elapsed         | 73989       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.011501154 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.504       |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00195    |
|    value_loss           | 7.12        |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=4.42 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.42        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.015802067 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.94        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00412    |
|    value_loss           | 239         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 527      |
|    time_elapsed    | 74138    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=9.27 +/- 4.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 9.27       |
| time/                   |            |
|    total_timesteps      | 8650000    |
| train/                  |            |
|    approx_kl            | 0.01677638 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.9       |
|    n_updates            | 5270       |
|    policy_gradient_loss | -0.00118   |
|    value_loss           | 13         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.9    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 528      |
|    time_elapsed    | 74287    |
|    total_timesteps | 8650752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 529         |
|    time_elapsed         | 74414       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.017227076 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 62.3        |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=6.36 +/- 3.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.36        |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.016267233 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.324       |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 37.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 530      |
|    time_elapsed    | 74564    |
|    total_timesteps | 8683520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -14.7      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 531        |
|    time_elapsed         | 74688      |
|    total_timesteps      | 8699904    |
| train/                  |            |
|    approx_kl            | 0.01457719 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.459      |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.00569   |
|    value_loss           | 2.68       |
----------------------------------------
Eval num_timesteps=8700000, episode_reward=5.00 +/- 1.12
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 5         |
| time/                   |           |
|    total_timesteps      | 8700000   |
| train/                  |           |
|    approx_kl            | 0.0166534 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.04     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.349     |
|    n_updates            | 5310      |
|    policy_gradient_loss | -0.00703  |
|    value_loss           | 1.87      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.17    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 532      |
|    time_elapsed    | 74837    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=-90.28 +/- 194.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -90.3       |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.016236886 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.669       |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 3.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.72    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 533      |
|    time_elapsed    | 74986    |
|    total_timesteps | 8732672  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.01      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 534        |
|    time_elapsed         | 75110      |
|    total_timesteps      | 8749056    |
| train/                  |            |
|    approx_kl            | 0.01766451 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.303      |
|    n_updates            | 5330       |
|    policy_gradient_loss | -0.00523   |
|    value_loss           | 5.05       |
----------------------------------------
Eval num_timesteps=8750000, episode_reward=3.75 +/- 0.80
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.75         |
| time/                   |              |
|    total_timesteps      | 8750000      |
| train/                  |              |
|    approx_kl            | 0.0154830925 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.224        |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 4.19         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.36    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 535      |
|    time_elapsed    | 75259    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=4.04 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.04        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.017778516 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 3.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.87    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 536      |
|    time_elapsed    | 75408    |
|    total_timesteps | 8781824  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.14      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 537        |
|    time_elapsed         | 75533      |
|    total_timesteps      | 8798208    |
| train/                  |            |
|    approx_kl            | 0.01880547 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.945      |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.00818   |
|    value_loss           | 1.46       |
----------------------------------------
Eval num_timesteps=8800000, episode_reward=4.22 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.22        |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.015566939 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 2.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.8     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 538      |
|    time_elapsed    | 75682    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=3.32 +/- 0.51
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.32       |
| time/                   |            |
|    total_timesteps      | 8825000    |
| train/                  |            |
|    approx_kl            | 0.01882397 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.911      |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.00503   |
|    value_loss           | 5.47       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.75    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 539      |
|    time_elapsed    | 75831    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.35       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 540         |
|    time_elapsed         | 75956       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.019455127 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 16.9        |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=4.29 +/- 1.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.29        |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.015849266 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.228       |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 7.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 541      |
|    time_elapsed    | 76105    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=4.32 +/- 2.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.32        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.022473631 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 34          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 542      |
|    time_elapsed    | 76254    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.19       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 543         |
|    time_elapsed         | 76378       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.018145222 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.178       |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 0.432       |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=4.85 +/- 1.95
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.85       |
| time/                   |            |
|    total_timesteps      | 8900000    |
| train/                  |            |
|    approx_kl            | 0.01732134 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0669     |
|    n_updates            | 5430       |
|    policy_gradient_loss | -0.00872   |
|    value_loss           | 0.274      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.84    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 544      |
|    time_elapsed    | 76527    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=4.08 +/- 0.40
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.08       |
| time/                   |            |
|    total_timesteps      | 8925000    |
| train/                  |            |
|    approx_kl            | 0.01755632 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.09       |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.00819   |
|    value_loss           | 3.02       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.00649 |
| time/              |          |
|    fps             | 116      |
|    iterations      | 545      |
|    time_elapsed    | 76676    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.28       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 546         |
|    time_elapsed         | 76801       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.015609731 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 4.24        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=7.14 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.14        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.016495084 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.95        |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0076     |
|    value_loss           | 14.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 547      |
|    time_elapsed    | 76950    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=4.87 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.87        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.020508017 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.7        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 26.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 548      |
|    time_elapsed    | 77098    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.9        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 549         |
|    time_elapsed         | 77223       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.020216037 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0757      |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 0.417       |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=6.57 +/- 5.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.57        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.015267698 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 1.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 550      |
|    time_elapsed    | 77372    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=3.50 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.011252044 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 176         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.1    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 551      |
|    time_elapsed    | 77520    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -88.2       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 552         |
|    time_elapsed         | 77645       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.020515118 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.538       |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00304    |
|    value_loss           | 96.8        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=3.96 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.011716286 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.2        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.000914   |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 553      |
|    time_elapsed    | 77794    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=3.66 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.66        |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.019643325 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 5530        |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 4.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.4    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 554      |
|    time_elapsed    | 77943    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.14       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 555         |
|    time_elapsed         | 78067       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.017360575 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.00385    |
|    value_loss           | 21.3        |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=3.21 +/- 1.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.21        |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.024470842 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 5550        |
|    policy_gradient_loss | 0.000118    |
|    value_loss           | 1.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.205   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 556      |
|    time_elapsed    | 78216    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=3.31 +/- 1.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.31        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.018284416 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 1.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 557      |
|    time_elapsed    | 78366    |
|    total_timesteps | 9125888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.08         |
| time/                   |              |
|    fps                  | 116          |
|    iterations           | 558          |
|    time_elapsed         | 78490        |
|    total_timesteps      | 9142272      |
| train/                  |              |
|    approx_kl            | 0.0139533635 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.38         |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00793     |
|    value_loss           | 9.56         |
------------------------------------------
Eval num_timesteps=9150000, episode_reward=3.78 +/- 1.53
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.78         |
| time/                   |              |
|    total_timesteps      | 9150000      |
| train/                  |              |
|    approx_kl            | 0.0119234165 |
|    clip_fraction        | 0.0891       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.63         |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 6.18         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 559      |
|    time_elapsed    | 78639    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=3.88 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.88        |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.019193448 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0644      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 2.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.18     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 560      |
|    time_elapsed    | 78788    |
|    total_timesteps | 9175040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.89       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 561        |
|    time_elapsed         | 78913      |
|    total_timesteps      | 9191424    |
| train/                  |            |
|    approx_kl            | 0.01742239 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0425     |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.000387  |
|    value_loss           | 13.6       |
----------------------------------------
Eval num_timesteps=9200000, episode_reward=4.94 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.94        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.019010454 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 5610        |
|    policy_gradient_loss | 0.000501    |
|    value_loss           | 6.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 562      |
|    time_elapsed    | 79063    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.756      |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 563         |
|    time_elapsed         | 79188       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.015970502 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.279       |
|    n_updates            | 5620        |
|    policy_gradient_loss | 0.000475    |
|    value_loss           | 1.21        |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=3.55 +/- 0.93
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.55       |
| time/                   |            |
|    total_timesteps      | 9225000    |
| train/                  |            |
|    approx_kl            | 0.01574747 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.703      |
|    n_updates            | 5630       |
|    policy_gradient_loss | -0.00418   |
|    value_loss           | 17.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.72    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 564      |
|    time_elapsed    | 79336    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=3.51 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.51        |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.018341443 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.365       |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 3.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.18    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 565      |
|    time_elapsed    | 79485    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.65       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 566         |
|    time_elapsed         | 79610       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.015437828 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 5.73        |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=4.54 +/- 1.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.54        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.015642514 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.168       |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 5           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.24    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 567      |
|    time_elapsed    | 79759    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=5.60 +/- 2.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.6         |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.017488787 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.5        |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 28.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.97    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 568      |
|    time_elapsed    | 79908    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.22       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 569         |
|    time_elapsed         | 80033       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.018739406 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 7.76        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=3.84 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.84        |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.018461835 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 0.465       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.26     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 570      |
|    time_elapsed    | 80181    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=4.75 +/- 1.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.75        |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.014387051 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 1.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 571      |
|    time_elapsed    | 80330    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.19        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 572         |
|    time_elapsed         | 80454       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.016381083 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.288       |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 1.56        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=5.87 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.87        |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.016929138 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 7.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.0197   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 573      |
|    time_elapsed    | 80603    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=4.35 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.35        |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.018118594 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 2.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.12    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 574      |
|    time_elapsed    | 80752    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.52       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 575         |
|    time_elapsed         | 80877       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.017133974 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.5         |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 7.09        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=5.63 +/- 1.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.63        |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.013333935 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 24.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 576      |
|    time_elapsed    | 81026    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=5.30 +/- 2.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.3         |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.019146409 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.185       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 577      |
|    time_elapsed    | 81175    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17         |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 578         |
|    time_elapsed         | 81300       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.015892563 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.449       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 2.92        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=4.63 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.63        |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.017520245 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.547       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.94    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 579      |
|    time_elapsed    | 81448    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=5.92 +/- 1.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.92        |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.014137055 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.387       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 1.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 580      |
|    time_elapsed    | 81597    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.1       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 581         |
|    time_elapsed         | 81722       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.018537786 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 60.2        |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=4.25 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.25        |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.017992727 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.263       |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 119         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 582      |
|    time_elapsed    | 81872    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=5.77 +/- 2.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.77        |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.018411707 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 0.335       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 583      |
|    time_elapsed    | 82021    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.41       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 584         |
|    time_elapsed         | 82146       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.015382055 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.16        |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 4.45        |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=5.02 +/- 1.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.02        |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.016880888 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 0.249       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.799   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 585      |
|    time_elapsed    | 82295    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=3.79 +/- 1.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.79        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.013985271 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.533       |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 1.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.0395  |
| time/              |          |
|    fps             | 116      |
|    iterations      | 586      |
|    time_elapsed    | 82444    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.333       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 587         |
|    time_elapsed         | 82569       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.016792148 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 3.19        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=4.28 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.28        |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.016979331 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.244       |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 4.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.671    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 588      |
|    time_elapsed    | 82717    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=4.27 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.27        |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.013993535 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.224       |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 2.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.572    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 589      |
|    time_elapsed    | 82866    |
|    total_timesteps | 9650176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.26       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 590        |
|    time_elapsed         | 82991      |
|    total_timesteps      | 9666560    |
| train/                  |            |
|    approx_kl            | 0.01771836 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.14       |
|    n_updates            | 5890       |
|    policy_gradient_loss | -0.00387   |
|    value_loss           | 0.411      |
----------------------------------------
Eval num_timesteps=9675000, episode_reward=3.78 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.78        |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.017230779 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 1.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 591      |
|    time_elapsed    | 83139    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.07        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 592         |
|    time_elapsed         | 83264       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.016357906 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0837      |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00901    |
|    value_loss           | 0.353       |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=4.24 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.24        |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.014881799 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0552      |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.869       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.962    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 593      |
|    time_elapsed    | 83413    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=4.35 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.35        |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.016488561 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 1.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.949    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 594      |
|    time_elapsed    | 83561    |
|    total_timesteps | 9732096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.1       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 595        |
|    time_elapsed         | 83686      |
|    total_timesteps      | 9748480    |
| train/                  |            |
|    approx_kl            | 0.01981036 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.00784   |
|    value_loss           | 0.743      |
----------------------------------------
Eval num_timesteps=9750000, episode_reward=3.87 +/- 0.61
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.87       |
| time/                   |            |
|    total_timesteps      | 9750000    |
| train/                  |            |
|    approx_kl            | 0.01966343 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.1       |
|    n_updates            | 5950       |
|    policy_gradient_loss | -0.00354   |
|    value_loss           | 16.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.03    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 596      |
|    time_elapsed    | 83834    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=4.86 +/- 2.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.86        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.016802376 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.9        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 4.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.6    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 597      |
|    time_elapsed    | 83983    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -60.7       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 598         |
|    time_elapsed         | 84108       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.015442133 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.0715      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.398       |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 236         |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=4.80 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.8         |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.019640349 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0265      |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.5    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 599      |
|    time_elapsed    | 84257    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=4.82 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.82        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.007113074 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 27.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.7    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 600      |
|    time_elapsed    | 84405    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.95       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 601         |
|    time_elapsed         | 84530       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.020213157 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.319       |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 22.5        |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=4.57 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.57        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.018334476 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.03        |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 2.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.73    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 602      |
|    time_elapsed    | 84679    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=-115.12 +/- 238.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.017510712 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 32.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.724   |
| time/              |          |
|    fps             | 116      |
|    iterations      | 603      |
|    time_elapsed    | 84827    |
|    total_timesteps | 9879552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.01      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 604        |
|    time_elapsed         | 84952      |
|    total_timesteps      | 9895936    |
| train/                  |            |
|    approx_kl            | 0.02018099 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.572      |
|    n_updates            | 6030       |
|    policy_gradient_loss | -0.00713   |
|    value_loss           | 1.97       |
----------------------------------------
Eval num_timesteps=9900000, episode_reward=4.43 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.43        |
| time/                   |             |
|    total_timesteps      | 9900000     |
| train/                  |             |
|    approx_kl            | 0.016854122 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 5.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 605      |
|    time_elapsed    | 85100    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=4.87 +/- 1.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.87       |
| time/                   |            |
|    total_timesteps      | 9925000    |
| train/                  |            |
|    approx_kl            | 0.02059945 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 6050       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 2.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.15    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 606      |
|    time_elapsed    | 85249    |
|    total_timesteps | 9928704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.56      |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 607        |
|    time_elapsed         | 85375      |
|    total_timesteps      | 9945088    |
| train/                  |            |
|    approx_kl            | 0.01878453 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.363      |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.00831   |
|    value_loss           | 4.3        |
----------------------------------------
Eval num_timesteps=9950000, episode_reward=5.29 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.29        |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.016209915 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 3.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.32    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 608      |
|    time_elapsed    | 85524    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=5.79 +/- 2.23
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.79       |
| time/                   |            |
|    total_timesteps      | 9975000    |
| train/                  |            |
|    approx_kl            | 0.01657487 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.28      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.567      |
|    n_updates            | 6080       |
|    policy_gradient_loss | -0.00457   |
|    value_loss           | 4.34       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.24    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 609      |
|    time_elapsed    | 85673    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.29       |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 610         |
|    time_elapsed         | 85799       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.020819025 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0375      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 0.687       |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=4.60 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.013809945 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.528       |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 68.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.16    |
| time/              |          |
|    fps             | 116      |
|    iterations      | 611      |
|    time_elapsed    | 85948    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v3_2
