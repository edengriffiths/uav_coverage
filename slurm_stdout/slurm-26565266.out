========== uav-v1 ==========
Seed: 3547315831
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v1_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.24e+03 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 1         |
|    time_elapsed    | 105       |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-1315.79 +/- 235.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008066768 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.000448    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00905    |
|    value_loss           | 4.63e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.34e+03 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 2         |
|    time_elapsed    | 260       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.54e+03   |
| time/                   |             |
|    fps                  | 125         |
|    iterations           | 3           |
|    time_elapsed         | 391         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008182775 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -1.54e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 2.38e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-1895.42 +/- 2628.52
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.9e+03     |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0133722965 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | -7.15e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 8.04e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00877     |
|    value_loss           | 1.64e+04     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.14e+03 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 4         |
|    time_elapsed    | 546       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-1198.14 +/- 380.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.016248534 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09e+04    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 1.71e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.86e+03 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 5         |
|    time_elapsed    | 702       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.56e+03   |
| time/                   |             |
|    fps                  | 118         |
|    iterations           | 6           |
|    time_elapsed         | 832         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.010729541 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | -7.15e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.99e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-877.75 +/- 269.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -878       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.01336623 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.8       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 2.49e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00598   |
|    value_loss           | 4.82e+03   |
----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.53e+03 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 7         |
|    time_elapsed    | 988       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-959.10 +/- 293.91
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -959         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0027983002 |
|    clip_fraction        | 0.00111      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 2.91e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.86e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 1.53e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.4e+03 |
| time/              |          |
|    fps             | 114      |
|    iterations      | 8        |
|    time_elapsed    | 1144     |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -2.28e+03    |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 9            |
|    time_elapsed         | 1274         |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0067904457 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.05e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00604     |
|    value_loss           | 6.28e+03     |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-1319.09 +/- 699.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.007956677 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 1e+03       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00909    |
|    value_loss           | 4.4e+03     |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 10        |
|    time_elapsed    | 1429      |
|    total_timesteps | 163840    |
----------------------------------
Eval num_timesteps=175000, episode_reward=-838.98 +/- 480.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -839        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.009799223 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.6        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 380         |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.09e+03 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 11        |
|    time_elapsed    | 1585      |
|    total_timesteps | 180224    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -720        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 12          |
|    time_elapsed         | 1715        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.012033138 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.8        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-959.13 +/- 479.95
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -959         |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0077690603 |
|    clip_fraction        | 0.0696       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.75        |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0003       |
|    loss                 | 90.9         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00941     |
|    value_loss           | 353          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 13       |
|    time_elapsed    | 1871     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-959.18 +/- 479.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -959        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.010674292 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 50.7        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 206         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -493     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 14       |
|    time_elapsed    | 2026     |
|    total_timesteps | 229376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -445       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 15         |
|    time_elapsed         | 2156       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.01494894 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.75      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 94.9       |
----------------------------------------
Eval num_timesteps=250000, episode_reward=-239.30 +/- 293.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -239        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.014341477 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.7        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 133         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -368     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 16       |
|    time_elapsed    | 2312     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-119.03 +/- 239.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.011999896 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.3        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 173         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 17       |
|    time_elapsed    | 2467     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -387        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 18          |
|    time_elapsed         | 2597        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.013986895 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 27          |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-119.08 +/- 240.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.009679037 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | 252         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 1.3e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -603     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 19       |
|    time_elapsed    | 2752     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-117.23 +/- 236.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.008053989 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.1        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 3.03e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -945     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 20       |
|    time_elapsed    | 2907     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -929        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 21          |
|    time_elapsed         | 3038        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.007594686 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 4.7e+03     |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=0.91 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.908       |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.015474398 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 71.7        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 92          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -687     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 22       |
|    time_elapsed    | 3193     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=2.27 +/- 2.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.27        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.014531152 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 98          |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 134         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -464     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 23       |
|    time_elapsed    | 3348     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 24          |
|    time_elapsed         | 3479        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.015438296 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.7        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1.02 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.02        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.010158962 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.0903      |
|    learning_rate        | 0.0003      |
|    loss                 | 45.9        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 477         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 25       |
|    time_elapsed    | 3635     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=3.35 +/- 3.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.35        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.012007982 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.6        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 241         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -299     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 26       |
|    time_elapsed    | 3790     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -469        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 27          |
|    time_elapsed         | 3921        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.010218201 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 341         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=17.00 +/- 19.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.008796599 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00771    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 28       |
|    time_elapsed    | 4077     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.32 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.017207576 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 29       |
|    time_elapsed    | 4233     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -371        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 30          |
|    time_elapsed         | 4363        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.018245209 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.429       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.2        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=3.43 +/- 4.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.007810369 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 910         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 1.13e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 31       |
|    time_elapsed    | 4519     |
|    total_timesteps | 507904   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -178      |
| time/                   |           |
|    fps                  | 112       |
|    iterations           | 32        |
|    time_elapsed         | 4650      |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0179327 |
|    clip_fraction        | 0.26      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.67     |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.0003    |
|    loss                 | 62.2      |
|    n_updates            | 310       |
|    policy_gradient_loss | -0.0108   |
|    value_loss           | 45.2      |
---------------------------------------
Eval num_timesteps=525000, episode_reward=0.97 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.967       |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.013761234 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00773    |
|    value_loss           | 331         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -210     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 33       |
|    time_elapsed    | 4805     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2.17 +/- 2.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.014216227 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 194         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 260         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 34       |
|    time_elapsed    | 4961     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -106        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 35          |
|    time_elapsed         | 5091        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.016109504 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.83        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 36.4        |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=34.57 +/- 27.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 34.6        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.020207457 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.6        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 70          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -300     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 36       |
|    time_elapsed    | 5247     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=6.49 +/- 10.84
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.49         |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0068061952 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.61         |
|    learning_rate        | 0.0003       |
|    loss                 | 767          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 1.41e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -448     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 37       |
|    time_elapsed    | 5403     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -457        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 38          |
|    time_elapsed         | 5534        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.017362371 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 860         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=4.08 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.08        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.019399568 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 87          |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 153         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -365     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 39       |
|    time_elapsed    | 5690     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1.34 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.34       |
| time/                   |            |
|    total_timesteps      | 650000     |
| train/                  |            |
|    approx_kl            | 0.01942626 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.9       |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00851   |
|    value_loss           | 32.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -297     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 40       |
|    time_elapsed    | 5846     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 41          |
|    time_elapsed         | 5976        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.011184655 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.5        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=8.12 +/- 7.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.12        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.018588765 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.66        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 41.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 42       |
|    time_elapsed    | 6131     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-76.41 +/- 127.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -76.4       |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.014737523 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 337         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 458         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 43       |
|    time_elapsed    | 6286     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -305        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 44          |
|    time_elapsed         | 6414        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.012269874 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.6        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 863         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=2.80 +/- 3.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.007916395 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00945    |
|    value_loss           | 2.3e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 45       |
|    time_elapsed    | 6564     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=6.83 +/- 5.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.83        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.016002081 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -217     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 46       |
|    time_elapsed    | 6715     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -199        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 47          |
|    time_elapsed         | 6842        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.017859459 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=20.77 +/- 32.48
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 20.8      |
| time/                   |           |
|    total_timesteps      | 775000    |
| train/                  |           |
|    approx_kl            | 0.0187382 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.62     |
|    explained_variance   | 0.761     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.98      |
|    n_updates            | 470       |
|    policy_gradient_loss | -0.00955  |
|    value_loss           | 61.9      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42.1    |
| time/              |          |
|    fps             | 112      |
|    iterations      | 48       |
|    time_elapsed    | 6994     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=9.43 +/- 16.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.43        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.012509724 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00958    |
|    value_loss           | 203         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 49       |
|    time_elapsed    | 7145     |
|    total_timesteps | 802816   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -154         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 50           |
|    time_elapsed         | 7272         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0072626583 |
|    clip_fraction        | 0.0629       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.62        |
|    explained_variance   | 0.697        |
|    learning_rate        | 0.0003       |
|    loss                 | 537          |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00612     |
|    value_loss           | 609          |
------------------------------------------
Eval num_timesteps=825000, episode_reward=-574.68 +/- 1180.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -575       |
| time/                   |            |
|    total_timesteps      | 825000     |
| train/                  |            |
|    approx_kl            | 0.01869307 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.58      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00994   |
|    value_loss           | 39.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 51       |
|    time_elapsed    | 7424     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=25.85 +/- 33.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 25.8        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.014544048 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 80.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 52       |
|    time_elapsed    | 7575     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -31.1       |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 53          |
|    time_elapsed         | 7702        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.012370124 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 84          |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 205         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=125.54 +/- 244.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 126         |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.017193083 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.37        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 206         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85.4    |
| time/              |          |
|    fps             | 112      |
|    iterations      | 54       |
|    time_elapsed    | 7854     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=53.88 +/- 75.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 53.9        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.009098554 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | 498         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 475         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64      |
| time/              |          |
|    fps             | 112      |
|    iterations      | 55       |
|    time_elapsed    | 8006     |
|    total_timesteps | 901120   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -47.6      |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 56         |
|    time_elapsed         | 8132       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.01866731 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 25         |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00985   |
|    value_loss           | 24.4       |
----------------------------------------
Eval num_timesteps=925000, episode_reward=37.62 +/- 38.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 37.6        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.019562028 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.22        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 38.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 57       |
|    time_elapsed    | 8283     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=96.01 +/- 98.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 96          |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.006067924 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26e+03    |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 778         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.6    |
| time/              |          |
|    fps             | 112      |
|    iterations      | 58       |
|    time_elapsed    | 8434     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -109        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 59          |
|    time_elapsed         | 8561        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.014845777 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 63.3        |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=47.29 +/- 68.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 47.3        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.014857508 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 487         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 282         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.9    |
| time/              |          |
|    fps             | 112      |
|    iterations      | 60       |
|    time_elapsed    | 8712     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 61         |
|    time_elapsed         | 8839       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.01671266 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.58      |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0003     |
|    loss                 | 114        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.00622   |
|    value_loss           | 658        |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=6.08 +/- 8.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.08        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.019110866 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.1        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 62       |
|    time_elapsed    | 8990     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=15.54 +/- 27.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.017949639 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.6        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 220         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 63       |
|    time_elapsed    | 9141     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -86.8       |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 64          |
|    time_elapsed         | 9268        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.017347444 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.67        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 36.7        |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=81.45 +/- 85.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 81.4        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.010873748 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 181         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 65       |
|    time_elapsed    | 9419     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=2.53 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.015549943 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 86.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.81    |
| time/              |          |
|    fps             | 112      |
|    iterations      | 66       |
|    time_elapsed    | 9570     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 22.6        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 67          |
|    time_elapsed         | 9696        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.018395863 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.16        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 50.9        |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=-51.22 +/- 109.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -51.2       |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.020301495 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 87          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 68       |
|    time_elapsed    | 9848     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=134.01 +/- 139.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.013624949 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.5        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 156         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.9    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 69       |
|    time_elapsed    | 10000    |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 70          |
|    time_elapsed         | 10127       |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.015415614 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 447         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 343         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=73.39 +/- 113.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 73.4        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.008688128 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 167         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 957         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 71       |
|    time_elapsed    | 10279    |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=15.81 +/- 17.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.019699289 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.2         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 63          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 72       |
|    time_elapsed    | 10430    |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -110        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 73          |
|    time_elapsed         | 10556       |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.017095178 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.65        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 67.9        |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=1.75 +/- 1.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.016003288 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.76        |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 81.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 74       |
|    time_elapsed    | 10707    |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=33.15 +/- 26.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 33.2        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.020455036 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.9        |
|    n_updates            | 740         |
|    policy_gradient_loss | 0.000993    |
|    value_loss           | 89.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 75       |
|    time_elapsed    | 10859    |
|    total_timesteps | 1228800  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -20.9      |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 76         |
|    time_elapsed         | 10987      |
|    total_timesteps      | 1245184    |
| train/                  |            |
|    approx_kl            | 0.01541939 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 49.9       |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0041    |
|    value_loss           | 236        |
----------------------------------------
Eval num_timesteps=1250000, episode_reward=19.83 +/- 23.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 19.8        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.018291794 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.8        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 69.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 77       |
|    time_elapsed    | 11138    |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=22.17 +/- 21.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 22.2       |
| time/                   |            |
|    total_timesteps      | 1275000    |
| train/                  |            |
|    approx_kl            | 0.01799442 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.57      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.78       |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.00661   |
|    value_loss           | 38         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 78       |
|    time_elapsed    | 11290    |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -33.2       |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 79          |
|    time_elapsed         | 11418       |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.019778293 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.99        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=4.21 +/- 3.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.21        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.008061562 |
|    clip_fraction        | 0.0658      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 97.1        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 334         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 80       |
|    time_elapsed    | 11570    |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=35.65 +/- 38.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 35.6        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.022224143 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 102         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.7    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 81       |
|    time_elapsed    | 11721    |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -112        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 82          |
|    time_elapsed         | 11848       |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.012987636 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.6        |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=37.88 +/- 45.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 37.9        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.009438372 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 225         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.8    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 83       |
|    time_elapsed    | 11999    |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=21.22 +/- 38.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 21.2       |
| time/                   |            |
|    total_timesteps      | 1375000    |
| train/                  |            |
|    approx_kl            | 0.01964348 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.57      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.00568   |
|    value_loss           | 21         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 84       |
|    time_elapsed    | 12150    |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 27.8        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 85          |
|    time_elapsed         | 12276       |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.013963052 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=175.04 +/- 187.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.016694346 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.46        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 134         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 26.3     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 86       |
|    time_elapsed    | 12428    |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=85.29 +/- 75.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 85.3        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.014699706 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 70.6        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.48     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 87       |
|    time_elapsed    | 12579    |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.8       |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 88          |
|    time_elapsed         | 12707       |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.017810386 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | 33.1        |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 205         |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=44.43 +/- 24.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 44.4        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.015908476 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.8        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 243         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -76.9    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 89       |
|    time_elapsed    | 12859    |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -70.6       |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 90          |
|    time_elapsed         | 12986       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.017294936 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.6        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 318         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=176.73 +/- 177.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.019905202 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 95.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.9    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 91       |
|    time_elapsed    | 13137    |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=89.97 +/- 66.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 90          |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.017209306 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.72        |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 68.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 66.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 92       |
|    time_elapsed    | 13288    |
|    total_timesteps | 1507328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 61.1       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 93         |
|    time_elapsed         | 13415      |
|    total_timesteps      | 1523712    |
| train/                  |            |
|    approx_kl            | 0.01895749 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.9        |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00506   |
|    value_loss           | 43.5       |
----------------------------------------
Eval num_timesteps=1525000, episode_reward=190.62 +/- 220.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 1525000      |
| train/                  |              |
|    approx_kl            | 0.0113726165 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.0003       |
|    loss                 | 78.5         |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00835     |
|    value_loss           | 184          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 47.4     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 94       |
|    time_elapsed    | 13566    |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=98.79 +/- 117.46
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 98.8       |
| time/                   |            |
|    total_timesteps      | 1550000    |
| train/                  |            |
|    approx_kl            | 0.01482994 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 198        |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 207        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -69.5    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 95       |
|    time_elapsed    | 13716    |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -71.3       |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 96          |
|    time_elapsed         | 13843       |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.019191178 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.2        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.000423   |
|    value_loss           | 303         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=179.99 +/- 146.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.021023136 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.84        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 33.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 97       |
|    time_elapsed    | 13994    |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=64.29 +/- 56.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 64.3        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.010998648 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.1        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.6    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 98       |
|    time_elapsed    | 14145    |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 37          |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 99          |
|    time_elapsed         | 14271       |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.013189509 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.4        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 346         |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=236.86 +/- 220.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.017585644 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 52.9        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 100      |
|    time_elapsed    | 14422    |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=217.88 +/- 209.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.014601666 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 249         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34.3     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 101      |
|    time_elapsed    | 14573    |
|    total_timesteps | 1654784  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -20.9      |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 102        |
|    time_elapsed         | 14700      |
|    total_timesteps      | 1671168    |
| train/                  |            |
|    approx_kl            | 0.01141561 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0003     |
|    loss                 | 135        |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0098    |
|    value_loss           | 528        |
----------------------------------------
Eval num_timesteps=1675000, episode_reward=223.83 +/- 184.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 224        |
| time/                   |            |
|    total_timesteps      | 1675000    |
| train/                  |            |
|    approx_kl            | 0.01533983 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.00964   |
|    value_loss           | 160        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.4    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 103      |
|    time_elapsed    | 14851    |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=113.32 +/- 121.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.021086082 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.000985   |
|    value_loss           | 218         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42.7    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 104      |
|    time_elapsed    | 15002    |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 105         |
|    time_elapsed         | 15130       |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.018436939 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.8        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 112         |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=112.73 +/- 72.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.017442234 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 495         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 223         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 106      |
|    time_elapsed    | 15282    |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=180.77 +/- 124.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.019747764 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 249         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.19     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 107      |
|    time_elapsed    | 15434    |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 24.2        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 108         |
|    time_elapsed         | 15561       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.016770061 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.6        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=60.73 +/- 104.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 60.7       |
| time/                   |            |
|    total_timesteps      | 1775000    |
| train/                  |            |
|    approx_kl            | 0.01756281 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.2       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00638   |
|    value_loss           | 28.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 109      |
|    time_elapsed    | 15712    |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=-988.11 +/- 2206.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -988         |
| time/                   |              |
|    total_timesteps      | 1800000      |
| train/                  |              |
|    approx_kl            | 0.0155900605 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.2         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00709     |
|    value_loss           | 221          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 35.4     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 110      |
|    time_elapsed    | 15864    |
|    total_timesteps | 1802240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 70.9         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 111          |
|    time_elapsed         | 15990        |
|    total_timesteps      | 1818624      |
| train/                  |              |
|    approx_kl            | 0.0132135935 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.0003       |
|    loss                 | 122          |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.0063      |
|    value_loss           | 246          |
------------------------------------------
Eval num_timesteps=1825000, episode_reward=109.62 +/- 79.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 110        |
| time/                   |            |
|    total_timesteps      | 1825000    |
| train/                  |            |
|    approx_kl            | 0.01716515 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.86       |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.00692   |
|    value_loss           | 59.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 92.2     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 112      |
|    time_elapsed    | 16142    |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=136.11 +/- 136.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 136        |
| time/                   |            |
|    total_timesteps      | 1850000    |
| train/                  |            |
|    approx_kl            | 0.01916872 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 28.1       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.00992   |
|    value_loss           | 38.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 92.7     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 113      |
|    time_elapsed    | 16294    |
|    total_timesteps | 1851392  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 95.5       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 114        |
|    time_elapsed         | 16421      |
|    total_timesteps      | 1867776    |
| train/                  |            |
|    approx_kl            | 0.01783156 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.75       |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 36.5       |
----------------------------------------
Eval num_timesteps=1875000, episode_reward=162.34 +/- 222.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.018480044 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.7        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00909    |
|    value_loss           | 52.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 91.4     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 115      |
|    time_elapsed    | 16573    |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=55.67 +/- 44.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 55.7        |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.017529657 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.3        |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 79.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.74    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 116      |
|    time_elapsed    | 16725    |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.315      |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 117         |
|    time_elapsed         | 16852       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.011471812 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.206       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 275         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=134.07 +/- 84.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.018297877 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 33.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.34    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 118      |
|    time_elapsed    | 17004    |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 13.5        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 119         |
|    time_elapsed         | 17131       |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.019481432 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 258         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=118.28 +/- 69.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 118         |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.020036256 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 53          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 120      |
|    time_elapsed    | 17283    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=139.42 +/- 107.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.018042874 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.778       |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 43.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 27.1     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 121      |
|    time_elapsed    | 17435    |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 42.5        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 122         |
|    time_elapsed         | 17562       |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.007375094 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 508         |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=143.54 +/- 35.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 144        |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.02056606 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.11       |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.00875   |
|    value_loss           | 37         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 52.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 123      |
|    time_elapsed    | 17714    |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=215.81 +/- 138.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.017701302 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.22        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 30.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 124      |
|    time_elapsed    | 17866    |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98.6        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 125         |
|    time_elapsed         | 17993       |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.011403448 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=279.00 +/- 164.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.010761567 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.1        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 220         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 126      |
|    time_elapsed    | 18145    |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=124.52 +/- 68.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 125         |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.009881736 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00919    |
|    value_loss           | 433         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 127      |
|    time_elapsed    | 18296    |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -141        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 128         |
|    time_elapsed         | 18422       |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.018970829 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 58.9        |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=209.47 +/- 104.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.012488889 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.0823      |
|    learning_rate        | 0.0003      |
|    loss                 | 70.1        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 364         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.7    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 129      |
|    time_elapsed    | 18574    |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=-1108.05 +/- 2478.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.11e+03   |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.015420594 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 80          |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 46.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 130      |
|    time_elapsed    | 18726    |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 29.1        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 131         |
|    time_elapsed         | 18853       |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.018154703 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.67        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 95.9        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=172.50 +/- 109.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 173        |
| time/                   |            |
|    total_timesteps      | 2150000    |
| train/                  |            |
|    approx_kl            | 0.01779243 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 15.6       |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.00782   |
|    value_loss           | 139        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 132      |
|    time_elapsed    | 19005    |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=108.28 +/- 103.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 108         |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.008394841 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 149         |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 769         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.1    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 133      |
|    time_elapsed    | 19157    |
|    total_timesteps | 2179072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 134        |
|    time_elapsed         | 19284      |
|    total_timesteps      | 2195456    |
| train/                  |            |
|    approx_kl            | 0.01401917 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 81.6       |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 387        |
----------------------------------------
Eval num_timesteps=2200000, episode_reward=218.59 +/- 76.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.016477238 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 75          |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 207         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.8    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 135      |
|    time_elapsed    | 19436    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=201.03 +/- 138.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.015541288 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 127         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 56.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 136      |
|    time_elapsed    | 19587    |
|    total_timesteps | 2228224  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 90.6       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 137        |
|    time_elapsed         | 19714      |
|    total_timesteps      | 2244608    |
| train/                  |            |
|    approx_kl            | 0.01798967 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.68       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 83.5       |
----------------------------------------
Eval num_timesteps=2250000, episode_reward=226.74 +/- 102.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.017696615 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 96.6        |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 74.1     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 138      |
|    time_elapsed    | 19865    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=150.92 +/- 50.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.014103238 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 224         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 57.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 139      |
|    time_elapsed    | 20016    |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 46.8        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 140         |
|    time_elapsed         | 20143       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.013479283 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00946    |
|    value_loss           | 283         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=252.37 +/- 192.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 252         |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.017674126 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.2        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 93          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.5     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 141      |
|    time_elapsed    | 20295    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=181.45 +/- 114.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.016269073 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.9        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 92.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 142      |
|    time_elapsed    | 20446    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 105         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 143         |
|    time_elapsed         | 20574       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.016582886 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.2        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 117         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=177.22 +/- 87.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.019297568 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.2        |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 75.9     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 144      |
|    time_elapsed    | 20725    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=227.81 +/- 142.86
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 2375000      |
| train/                  |              |
|    approx_kl            | 0.0128663145 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00795     |
|    value_loss           | 287          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 65.8     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 145      |
|    time_elapsed    | 20876    |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 69.5        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 146         |
|    time_elapsed         | 21003       |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.017798653 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=129.29 +/- 88.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.018061437 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.2        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.5     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 147      |
|    time_elapsed    | 21154    |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 148         |
|    time_elapsed         | 21281       |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.016583078 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.94        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 31.2        |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=162.54 +/- 129.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.012542119 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.5        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 149      |
|    time_elapsed    | 21432    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=150.57 +/- 67.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.017177671 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 129         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 150      |
|    time_elapsed    | 21583    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 174         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 151         |
|    time_elapsed         | 21710       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.018174374 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.8        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 53.9        |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=121.87 +/- 41.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 122         |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.017163403 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.3        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 41.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 152      |
|    time_elapsed    | 21862    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=204.39 +/- 70.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.014777238 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.4        |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 251         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 91.6     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 153      |
|    time_elapsed    | 22014    |
|    total_timesteps | 2506752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 47.6       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 154        |
|    time_elapsed         | 22141      |
|    total_timesteps      | 2523136    |
| train/                  |            |
|    approx_kl            | 0.02082615 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 86.4       |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 339        |
----------------------------------------
Eval num_timesteps=2525000, episode_reward=-2082.92 +/- 4631.20
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -2.08e+03 |
| time/                   |           |
|    total_timesteps      | 2525000   |
| train/                  |           |
|    approx_kl            | 0.0169929 |
|    clip_fraction        | 0.227     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.43     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 179       |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.00948  |
|    value_loss           | 208       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 51       |
| time/              |          |
|    fps             | 113      |
|    iterations      | 155      |
|    time_elapsed    | 22293    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=159.56 +/- 33.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 160         |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.018673476 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.9        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 63.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 156      |
|    time_elapsed    | 22445    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 157         |
|    time_elapsed         | 22572       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.014169346 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 421         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=182.40 +/- 109.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.019292232 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.48        |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 52.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 158      |
|    time_elapsed    | 22724    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=231.94 +/- 57.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 232        |
| time/                   |            |
|    total_timesteps      | 2600000    |
| train/                  |            |
|    approx_kl            | 0.00982061 |
|    clip_fraction        | 0.0969     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0003     |
|    loss                 | 49.6       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0062    |
|    value_loss           | 465        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -69.1    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 159      |
|    time_elapsed    | 22876    |
|    total_timesteps | 2605056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 3.84       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 160        |
|    time_elapsed         | 23003      |
|    total_timesteps      | 2621440    |
| train/                  |            |
|    approx_kl            | 0.01605518 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 97.9       |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 162        |
----------------------------------------
Eval num_timesteps=2625000, episode_reward=184.02 +/- 25.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.016969621 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.4        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 316         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 6.27     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 161      |
|    time_elapsed    | 23155    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=222.97 +/- 41.91
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 223        |
| time/                   |            |
|    total_timesteps      | 2650000    |
| train/                  |            |
|    approx_kl            | 0.02037291 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.73       |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0054    |
|    value_loss           | 22         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.2     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 162      |
|    time_elapsed    | 23307    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 91.6        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 163         |
|    time_elapsed         | 23434       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.016681626 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 57.5        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=226.04 +/- 90.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.019135468 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 94.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 164      |
|    time_elapsed    | 23586    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=153.10 +/- 62.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.015368409 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 195         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 165      |
|    time_elapsed    | 23737    |
|    total_timesteps | 2703360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 151        |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 166        |
|    time_elapsed         | 23864      |
|    total_timesteps      | 2719744    |
| train/                  |            |
|    approx_kl            | 0.01595334 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.7       |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.00787   |
|    value_loss           | 48.2       |
----------------------------------------
Eval num_timesteps=2725000, episode_reward=-912.74 +/- 2291.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -913        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.018049207 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00868    |
|    value_loss           | 30.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 167      |
|    time_elapsed    | 24015    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=-628.27 +/- 2038.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -628        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.019299414 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.55        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 44.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 168      |
|    time_elapsed    | 24167    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 169         |
|    time_elapsed         | 24295       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.017031025 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.2        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=258.60 +/- 106.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.018030256 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.14        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 35.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 170      |
|    time_elapsed    | 24446    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=293.09 +/- 127.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.020631116 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.32        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 46.8        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 171      |
|    time_elapsed    | 24598    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 171         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 172         |
|    time_elapsed         | 24726       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.017771317 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.9        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 75.8        |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=224.91 +/- 70.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 225         |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.016875122 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.5        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 173      |
|    time_elapsed    | 24877    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=290.32 +/- 140.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.020286312 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0003      |
|    loss                 | 144         |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 166         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 174      |
|    time_elapsed    | 25029    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 134         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 175         |
|    time_elapsed         | 25156       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.018877592 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.9        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=206.75 +/- 94.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 207        |
| time/                   |            |
|    total_timesteps      | 2875000    |
| train/                  |            |
|    approx_kl            | 0.01975165 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.4        |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 57.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.2     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 176      |
|    time_elapsed    | 25308    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 119         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 177         |
|    time_elapsed         | 25435       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.015880937 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.4        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=266.65 +/- 51.78
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 267        |
| time/                   |            |
|    total_timesteps      | 2900000    |
| train/                  |            |
|    approx_kl            | 0.01944692 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.3        |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.00964   |
|    value_loss           | 72         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 178      |
|    time_elapsed    | 25587    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=191.47 +/- 79.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.019464433 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 73          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 179      |
|    time_elapsed    | 25738    |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 179         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 180         |
|    time_elapsed         | 25865       |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.022543663 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.9        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 83.9        |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=-315.72 +/- 1152.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -316        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.013312015 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.6        |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 181         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 181      |
|    time_elapsed    | 26015    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=226.06 +/- 95.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.019355373 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 356         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 182      |
|    time_elapsed    | 26167    |
|    total_timesteps | 2981888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 196        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 183        |
|    time_elapsed         | 26294      |
|    total_timesteps      | 2998272    |
| train/                  |            |
|    approx_kl            | 0.01863417 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.18       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 24.1       |
----------------------------------------
Eval num_timesteps=3000000, episode_reward=241.34 +/- 124.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.016714921 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 48.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 184      |
|    time_elapsed    | 26445    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=339.75 +/- 137.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 340         |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.014652696 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 180         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 185      |
|    time_elapsed    | 26597    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 160         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 186         |
|    time_elapsed         | 26725       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.019383628 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35        |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 18          |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=213.75 +/- 51.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.013970206 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00609    |
|    value_loss           | 153         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 187      |
|    time_elapsed    | 26877    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=386.01 +/- 191.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 386        |
| time/                   |            |
|    total_timesteps      | 3075000    |
| train/                  |            |
|    approx_kl            | 0.01718163 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0003     |
|    loss                 | 176        |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.00847   |
|    value_loss           | 57.8       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 188      |
|    time_elapsed    | 27028    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 192         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 189         |
|    time_elapsed         | 27155       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.020159472 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.7        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 49.6        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=-1910.31 +/- 4222.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.91e+03   |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.020184401 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 67.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 190      |
|    time_elapsed    | 27307    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=154.95 +/- 53.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 155         |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.019879807 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.74        |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.00382     |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 191      |
|    time_elapsed    | 27458    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 192         |
|    time_elapsed         | 27584       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.021075666 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.31        |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 7.37        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=259.09 +/- 84.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.011471618 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 294         |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 458         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 193      |
|    time_elapsed    | 27735    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=272.90 +/- 98.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.018600136 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1930        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 155         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 9.73     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 194      |
|    time_elapsed    | 27886    |
|    total_timesteps | 3178496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.72      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 195        |
|    time_elapsed         | 28012      |
|    total_timesteps      | 3194880    |
| train/                  |            |
|    approx_kl            | 0.02067798 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.00102   |
|    value_loss           | 153        |
----------------------------------------
Eval num_timesteps=3200000, episode_reward=337.19 +/- 108.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 337         |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.018300299 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.5        |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 46          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 196      |
|    time_elapsed    | 28163    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=301.28 +/- 77.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.010682817 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.3        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 357         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 197      |
|    time_elapsed    | 28314    |
|    total_timesteps | 3227648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 112        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 198        |
|    time_elapsed         | 28440      |
|    total_timesteps      | 3244032    |
| train/                  |            |
|    approx_kl            | 0.01966286 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.1       |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.00803   |
|    value_loss           | 52.5       |
----------------------------------------
Eval num_timesteps=3250000, episode_reward=284.17 +/- 100.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.019900627 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.1        |
|    n_updates            | 1980        |
|    policy_gradient_loss | -2.86e-06   |
|    value_loss           | 199         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 46.1     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 199      |
|    time_elapsed    | 28591    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=264.43 +/- 125.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.018988773 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.69        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 93.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 200      |
|    time_elapsed    | 28743    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 201         |
|    time_elapsed         | 28870       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.013944551 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 41.5        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=293.05 +/- 134.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.021047039 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 80.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 202      |
|    time_elapsed    | 29022    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=171.60 +/- 57.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.017004892 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 13.9        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00851    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 203      |
|    time_elapsed    | 29172    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 186         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 204         |
|    time_elapsed         | 29299       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.022858772 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.4         |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 34.2        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=271.77 +/- 119.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 272        |
| time/                   |            |
|    total_timesteps      | 3350000    |
| train/                  |            |
|    approx_kl            | 0.01909909 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.89       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.00957   |
|    value_loss           | 40.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 205      |
|    time_elapsed    | 29450    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=235.04 +/- 110.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.016539816 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 206      |
|    time_elapsed    | 29601    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 55.2        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 207         |
|    time_elapsed         | 29727       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.024305198 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.9        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 157         |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=224.52 +/- 87.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 225         |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.011456442 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 280         |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 315         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 36.3     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 208      |
|    time_elapsed    | 29878    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 34.8        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 209         |
|    time_elapsed         | 30004       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.017461114 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 155         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 155         |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=295.39 +/- 29.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.021802966 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.7        |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00052    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 83.8     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 210      |
|    time_elapsed    | 30155    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=220.04 +/- 93.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.020657249 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 77.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 211      |
|    time_elapsed    | 30306    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 212         |
|    time_elapsed         | 30432       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.016991906 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 30.7        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=88.84 +/- 415.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 88.8        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.016726207 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 52          |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 213      |
|    time_elapsed    | 30583    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=80.82 +/- 322.85
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 80.8       |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.02457938 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.73       |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.00405   |
|    value_loss           | 104        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 214      |
|    time_elapsed    | 30734    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 143         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 215         |
|    time_elapsed         | 30860       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.018449161 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 71.9        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=188.49 +/- 98.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.018279515 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 2           |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 44.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 216      |
|    time_elapsed    | 31011    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=192.39 +/- 30.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.018874545 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.72        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 27.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 216      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 217      |
|    time_elapsed    | 31163    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 225         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 218         |
|    time_elapsed         | 31290       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.017352354 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.5        |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 35.7        |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=218.60 +/- 36.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.018412817 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.87        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 23.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 219      |
|    time_elapsed    | 31442    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=254.71 +/- 146.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.018003592 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 45.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 220      |
|    time_elapsed    | 31593    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 207         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 221         |
|    time_elapsed         | 31719       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.019100804 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 78          |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=242.56 +/- 138.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.019257855 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 50.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 222      |
|    time_elapsed    | 31870    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=-2950.99 +/- 6409.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.95e+03   |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.020273954 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.9        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 58          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 223      |
|    time_elapsed    | 32022    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 149         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 224         |
|    time_elapsed         | 32149       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.013163734 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.35        |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=298.31 +/- 161.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.025516722 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 2240        |
|    policy_gradient_loss | 0.00166     |
|    value_loss           | 203         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 225      |
|    time_elapsed    | 32300    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=361.81 +/- 127.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 362         |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.020035623 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 53.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 226      |
|    time_elapsed    | 32452    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 165         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 227         |
|    time_elapsed         | 32579       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.022275712 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.000995   |
|    value_loss           | 47.5        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=215.67 +/- 110.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.021308899 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 158         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 228      |
|    time_elapsed    | 32730    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=232.02 +/- 39.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.020675637 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.8        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 18.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 229      |
|    time_elapsed    | 32881    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 233         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 230         |
|    time_elapsed         | 33007       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.019346684 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 23.7        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=260.07 +/- 105.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.014848172 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.05        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 231      |
|    time_elapsed    | 33158    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=278.02 +/- 74.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.019903684 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 63.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 232      |
|    time_elapsed    | 33308    |
|    total_timesteps | 3801088  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 113          |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 233          |
|    time_elapsed         | 33435        |
|    total_timesteps      | 3817472      |
| train/                  |              |
|    approx_kl            | 0.0092493445 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.72         |
|    learning_rate        | 0.0003       |
|    loss                 | 93.1         |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00448     |
|    value_loss           | 581          |
------------------------------------------
Eval num_timesteps=3825000, episode_reward=390.42 +/- 186.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 390         |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.020987535 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.18        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 32.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 234      |
|    time_elapsed    | 33586    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=256.17 +/- 113.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 256        |
| time/                   |            |
|    total_timesteps      | 3850000    |
| train/                  |            |
|    approx_kl            | 0.01765988 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.4       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.00622   |
|    value_loss           | 35.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 235      |
|    time_elapsed    | 33736    |
|    total_timesteps | 3850240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 183          |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 236          |
|    time_elapsed         | 33863        |
|    total_timesteps      | 3866624      |
| train/                  |              |
|    approx_kl            | 0.0147362985 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.658        |
|    learning_rate        | 0.0003       |
|    loss                 | 54.9         |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 264          |
------------------------------------------
Eval num_timesteps=3875000, episode_reward=256.99 +/- 105.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 257         |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.018936599 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.67        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 50.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 237      |
|    time_elapsed    | 34014    |
|    total_timesteps | 3883008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 176        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 238        |
|    time_elapsed         | 34140      |
|    total_timesteps      | 3899392    |
| train/                  |            |
|    approx_kl            | 0.01825006 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.1       |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.00795   |
|    value_loss           | 37.1       |
----------------------------------------
Eval num_timesteps=3900000, episode_reward=232.28 +/- 44.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.010776384 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 132         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 374         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 239      |
|    time_elapsed    | 34291    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=270.80 +/- 113.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.018744212 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.57        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 34.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 240      |
|    time_elapsed    | 34443    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 136         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 241         |
|    time_elapsed         | 34569       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.011991467 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 164         |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=218.10 +/- 86.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.021297865 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00945    |
|    value_loss           | 9.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 242      |
|    time_elapsed    | 34720    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=232.99 +/- 88.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.016099192 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.9        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 31.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 243      |
|    time_elapsed    | 34871    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 119         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 244         |
|    time_elapsed         | 34998       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.017067375 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 246         |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=287.86 +/- 80.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.018318668 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0003      |
|    loss                 | 161         |
|    n_updates            | 2440        |
|    policy_gradient_loss | 0.00692     |
|    value_loss           | 199         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 78.7     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 245      |
|    time_elapsed    | 35149    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=-54.80 +/- 683.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -54.8       |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.022211714 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 155         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 90.3     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 246      |
|    time_elapsed    | 35302    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 247         |
|    time_elapsed         | 35429       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.021504324 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.854       |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 6.47        |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=316.25 +/- 146.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.015343333 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 50.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 248      |
|    time_elapsed    | 35580    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=262.23 +/- 114.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.020852162 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.5        |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 157         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 249      |
|    time_elapsed    | 35731    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 135         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 250         |
|    time_elapsed         | 35858       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.013523383 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.5        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0066     |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=334.33 +/- 116.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.018755205 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.1        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 102         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 251      |
|    time_elapsed    | 36009    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=287.92 +/- 191.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.022462282 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 87.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 252      |
|    time_elapsed    | 36161    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 253         |
|    time_elapsed         | 36288       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.015998658 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 31.3        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=323.76 +/- 130.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 324         |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.020076524 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 157         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 254      |
|    time_elapsed    | 36440    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=260.46 +/- 131.45
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 260        |
| time/                   |            |
|    total_timesteps      | 4175000    |
| train/                  |            |
|    approx_kl            | 0.02067592 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.82      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.21       |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0055    |
|    value_loss           | 58.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 255      |
|    time_elapsed    | 36591    |
|    total_timesteps | 4177920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 140        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 256        |
|    time_elapsed         | 36718      |
|    total_timesteps      | 4194304    |
| train/                  |            |
|    approx_kl            | 0.01715924 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.87      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 81.3       |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.00257   |
|    value_loss           | 149        |
----------------------------------------
Eval num_timesteps=4200000, episode_reward=-877.09 +/- 2306.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -877        |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.021732405 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0003      |
|    loss                 | 252         |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 347         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 257      |
|    time_elapsed    | 36870    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=-1762.61 +/- 4169.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.76e+03   |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.018453253 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.98        |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 258      |
|    time_elapsed    | 37022    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 148         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 259         |
|    time_elapsed         | 37148       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.019260366 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 176         |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.000394   |
|    value_loss           | 374         |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=-742.38 +/- 2200.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -742        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.021669067 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.5        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 260      |
|    time_elapsed    | 37299    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=222.42 +/- 216.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.013395998 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 260         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 261      |
|    time_elapsed    | 37450    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 137         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 262         |
|    time_elapsed         | 37576       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.014062191 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 381         |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=297.97 +/- 83.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.022009984 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.7        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00866    |
|    value_loss           | 150         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 263      |
|    time_elapsed    | 37728    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=-360.88 +/- 1146.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -361        |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.020940475 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 2630        |
|    policy_gradient_loss | 0.000955    |
|    value_loss           | 392         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 114      |
|    iterations      | 264      |
|    time_elapsed    | 37879    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 97.2        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 265         |
|    time_elapsed         | 38006       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.018650807 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 33.2        |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=276.40 +/- 61.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 276         |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.018379088 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.97        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 67.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 266      |
|    time_elapsed    | 38158    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 179         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 267         |
|    time_elapsed         | 38285       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.016897708 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 192         |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=279.47 +/- 47.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.020501005 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.76        |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 268      |
|    time_elapsed    | 38436    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=119.03 +/- 46.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 119         |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.019663982 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89        |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 35.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 269      |
|    time_elapsed    | 38587    |
|    total_timesteps | 4407296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 222        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 270        |
|    time_elapsed         | 38713      |
|    total_timesteps      | 4423680    |
| train/                  |            |
|    approx_kl            | 0.01737772 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.78      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.89       |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 50.2       |
----------------------------------------
Eval num_timesteps=4425000, episode_reward=203.70 +/- 121.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.017703436 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.3        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 47.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 271      |
|    time_elapsed    | 38864    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=284.50 +/- 192.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.020281449 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.582       |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 272      |
|    time_elapsed    | 39015    |
|    total_timesteps | 4456448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 253        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 273        |
|    time_elapsed         | 39142      |
|    total_timesteps      | 4472832    |
| train/                  |            |
|    approx_kl            | 0.01795487 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.5       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.00697   |
|    value_loss           | 35.8       |
----------------------------------------
Eval num_timesteps=4475000, episode_reward=258.31 +/- 105.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.020659786 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00695    |
|    value_loss           | 51.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 274      |
|    time_elapsed    | 39292    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=237.90 +/- 35.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.020421814 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.71        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 275      |
|    time_elapsed    | 39443    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 167         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 276         |
|    time_elapsed         | 39570       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.023882087 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.26        |
|    n_updates            | 2750        |
|    policy_gradient_loss | 0.00322     |
|    value_loss           | 63.8        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=268.14 +/- 114.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 268         |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.017995562 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 277      |
|    time_elapsed    | 39720    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=199.84 +/- 41.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.020515252 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 39.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 220      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 278      |
|    time_elapsed    | 39871    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 279         |
|    time_elapsed         | 39997       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.015356388 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.68        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 60.3        |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=272.96 +/- 58.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.018844899 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 89          |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 278         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 280      |
|    time_elapsed    | 40148    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=-941.54 +/- 2380.04
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -942       |
| time/                   |            |
|    total_timesteps      | 4600000    |
| train/                  |            |
|    approx_kl            | 0.02161371 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.66      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.00508   |
|    value_loss           | 134        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 281      |
|    time_elapsed    | 40300    |
|    total_timesteps | 4603904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 282        |
|    time_elapsed         | 40427      |
|    total_timesteps      | 4620288    |
| train/                  |            |
|    approx_kl            | 0.01934997 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.51      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.0083    |
|    value_loss           | 8.18       |
----------------------------------------
Eval num_timesteps=4625000, episode_reward=-855.38 +/- 2186.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -855        |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.013863027 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.3        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 168         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 283      |
|    time_elapsed    | 40577    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=250.42 +/- 103.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.012211995 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | 305         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 443         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 284      |
|    time_elapsed    | 40728    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 128         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 285         |
|    time_elapsed         | 40855       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.017351624 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 72.7        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=258.70 +/- 126.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.022220097 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.5        |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 61.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 286      |
|    time_elapsed    | 41006    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=168.68 +/- 110.00
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 4700000      |
| train/                  |              |
|    approx_kl            | 0.0155333225 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.0003       |
|    loss                 | 506          |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00955     |
|    value_loss           | 202          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 287      |
|    time_elapsed    | 41157    |
|    total_timesteps | 4702208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 83.7       |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 288        |
|    time_elapsed         | 41284      |
|    total_timesteps      | 4718592    |
| train/                  |            |
|    approx_kl            | 0.01876268 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03e+03   |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.00915   |
|    value_loss           | 420        |
----------------------------------------
Eval num_timesteps=4725000, episode_reward=334.03 +/- 100.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.020464068 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.9        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 526         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 83.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 289      |
|    time_elapsed    | 41436    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=342.87 +/- 167.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 343         |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.022141004 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 290      |
|    time_elapsed    | 41587    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 264         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 291         |
|    time_elapsed         | 41714       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.023242252 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.1         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00975    |
|    value_loss           | 27.9        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=221.74 +/- 123.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.021352222 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.2        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 31.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 278      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 292      |
|    time_elapsed    | 41866    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=364.77 +/- 100.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.021007743 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.99        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 44.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 293      |
|    time_elapsed    | 42018    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 266         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 294         |
|    time_elapsed         | 42145       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.024058852 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.1        |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 35.8        |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=265.81 +/- 100.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 266        |
| time/                   |            |
|    total_timesteps      | 4825000    |
| train/                  |            |
|    approx_kl            | 0.02245652 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.92       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.00982   |
|    value_loss           | 39.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 248      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 295      |
|    time_elapsed    | 42296    |
|    total_timesteps | 4833280  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 193        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 296        |
|    time_elapsed         | 42422      |
|    total_timesteps      | 4849664    |
| train/                  |            |
|    approx_kl            | 0.01721164 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.61      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 2950       |
|    policy_gradient_loss | -0.00349   |
|    value_loss           | 251        |
----------------------------------------
Eval num_timesteps=4850000, episode_reward=274.41 +/- 148.51
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 274        |
| time/                   |            |
|    total_timesteps      | 4850000    |
| train/                  |            |
|    approx_kl            | 0.02088899 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.3       |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.02       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 100        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 297      |
|    time_elapsed    | 42573    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=253.47 +/- 68.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 253        |
| time/                   |            |
|    total_timesteps      | 4875000    |
| train/                  |            |
|    approx_kl            | 0.02292481 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 22.7       |
|    n_updates            | 2970       |
|    policy_gradient_loss | -0.0089    |
|    value_loss           | 75         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 70.1     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 298      |
|    time_elapsed    | 42724    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 113         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 299         |
|    time_elapsed         | 42850       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.014506322 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0003      |
|    loss                 | 216         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 607         |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=240.59 +/- 63.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.026646383 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.76        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 75.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 300      |
|    time_elapsed    | 43001    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=443.56 +/- 166.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 444         |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.015760608 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 80.6        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 356         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 46.8     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 301      |
|    time_elapsed    | 43152    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 302         |
|    time_elapsed         | 43279       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.016579885 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.4        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 399         |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=266.46 +/- 116.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 266         |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.024780408 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.4        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 43       |
| time/              |          |
|    fps             | 114      |
|    iterations      | 303      |
|    time_elapsed    | 43430    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=329.39 +/- 147.51
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 329        |
| time/                   |            |
|    total_timesteps      | 4975000    |
| train/                  |            |
|    approx_kl            | 0.02478602 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.23      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 3030       |
|    policy_gradient_loss | -0.00521   |
|    value_loss           | 106        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 92.1     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 304      |
|    time_elapsed    | 43581    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 148         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 305         |
|    time_elapsed         | 43708       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.021461157 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=317.08 +/- 49.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.019680992 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.59        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 29.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 306      |
|    time_elapsed    | 43859    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=260.54 +/- 37.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.010936515 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 329         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 307      |
|    time_elapsed    | 44009    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 116         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 308         |
|    time_elapsed         | 44136       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.016462747 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 649         |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=244.62 +/- 100.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.018949352 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 88       |
| time/              |          |
|    fps             | 114      |
|    iterations      | 309      |
|    time_elapsed    | 44286    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=333.56 +/- 89.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.019887779 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 453         |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 308         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 310      |
|    time_elapsed    | 44438    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 166         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 311         |
|    time_elapsed         | 44565       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.020297091 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 206         |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 397         |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=335.46 +/- 64.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 335        |
| time/                   |            |
|    total_timesteps      | 5100000    |
| train/                  |            |
|    approx_kl            | 0.01792755 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.0003     |
|    loss                 | 34.7       |
|    n_updates            | 3110       |
|    policy_gradient_loss | -0.00516   |
|    value_loss           | 378        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 312      |
|    time_elapsed    | 44717    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=398.33 +/- 91.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 398         |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.024182115 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 3120        |
|    policy_gradient_loss | 0.00011     |
|    value_loss           | 52.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 313      |
|    time_elapsed    | 44868    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 172         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 314         |
|    time_elapsed         | 44995       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.017482886 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 223         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=353.84 +/- 133.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.021501437 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.05       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.15        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 27.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 315      |
|    time_elapsed    | 45146    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=259.66 +/- 70.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 260        |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.03098244 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.17      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.00794   |
|    value_loss           | 520        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 316      |
|    time_elapsed    | 45297    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 119         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 317         |
|    time_elapsed         | 45423       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.023905633 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 21.9        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=300.36 +/- 129.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.012493673 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.6        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.000407   |
|    value_loss           | 400         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 52.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 318      |
|    time_elapsed    | 45574    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=219.29 +/- 98.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.018009644 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 598         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 319      |
|    time_elapsed    | 45724    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 320         |
|    time_elapsed         | 45851       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.025363538 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.5         |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 28.9        |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=-2558.02 +/- 5628.44
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -2.56e+03  |
| time/                   |            |
|    total_timesteps      | 5250000    |
| train/                  |            |
|    approx_kl            | 0.01683738 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.18      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.97       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.00887   |
|    value_loss           | 102        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 321      |
|    time_elapsed    | 46001    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=323.81 +/- 108.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 324         |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.021332504 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.2        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 60.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 322      |
|    time_elapsed    | 46152    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 109         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 323         |
|    time_elapsed         | 46279       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.018779956 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.7        |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 202         |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=241.84 +/- 48.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.012112087 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 707         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 76.6     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 324      |
|    time_elapsed    | 46430    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 37.8        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 325         |
|    time_elapsed         | 46557       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.014595555 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 553         |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 696         |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=299.62 +/- 133.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.022946466 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.16        |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 71.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 71.6     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 326      |
|    time_elapsed    | 46708    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=255.28 +/- 41.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.025527706 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 16.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 196      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 327      |
|    time_elapsed    | 46858    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 221         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 328         |
|    time_elapsed         | 46985       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.018456008 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 81.3        |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=330.45 +/- 107.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.016993798 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 212         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 329      |
|    time_elapsed    | 47137    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=-267.17 +/- 1091.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -267        |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.010212539 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 379         |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 682         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 330      |
|    time_elapsed    | 47289    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 162         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 331         |
|    time_elapsed         | 47416       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.018950356 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.8         |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 72.7        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=392.84 +/- 117.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 393         |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.018786132 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 21          |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 89.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 332      |
|    time_elapsed    | 47567    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=293.05 +/- 76.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.019433178 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 10.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 261      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 333      |
|    time_elapsed    | 47718    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 278         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 334         |
|    time_elapsed         | 47845       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.020855114 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 42.9        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=288.23 +/- 140.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 288        |
| time/                   |            |
|    total_timesteps      | 5475000    |
| train/                  |            |
|    approx_kl            | 0.02292214 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.14      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 3340       |
|    policy_gradient_loss | -0.00962   |
|    value_loss           | 15.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 335      |
|    time_elapsed    | 47995    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=244.94 +/- 175.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 245        |
| time/                   |            |
|    total_timesteps      | 5500000    |
| train/                  |            |
|    approx_kl            | 0.02141068 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.97      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.29       |
|    n_updates            | 3350       |
|    policy_gradient_loss | -0.00518   |
|    value_loss           | 33.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 336      |
|    time_elapsed    | 48146    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 298         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 337         |
|    time_elapsed         | 48273       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.021886796 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.5         |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 11.5        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=310.79 +/- 96.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.018965665 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 27.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 260      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 338      |
|    time_elapsed    | 48423    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=245.35 +/- 66.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 245        |
| time/                   |            |
|    total_timesteps      | 5550000    |
| train/                  |            |
|    approx_kl            | 0.01537133 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.96      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 91.5       |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0045    |
|    value_loss           | 239        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 339      |
|    time_elapsed    | 48574    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 129         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 340         |
|    time_elapsed         | 48701       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.022617212 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 418         |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=320.51 +/- 86.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.022213042 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 3400        |
|    policy_gradient_loss | 0.000642    |
|    value_loss           | 259         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 341      |
|    time_elapsed    | 48852    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=262.29 +/- 132.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.026171817 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 8.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 342      |
|    time_elapsed    | 49002    |
|    total_timesteps | 5603328  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 114       |
| time/                   |           |
|    fps                  | 114       |
|    iterations           | 343       |
|    time_elapsed         | 49129     |
|    total_timesteps      | 5619712   |
| train/                  |           |
|    approx_kl            | 0.0182289 |
|    clip_fraction        | 0.176     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.26     |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 3420      |
|    policy_gradient_loss | -0.00832  |
|    value_loss           | 28.1      |
---------------------------------------
Eval num_timesteps=5625000, episode_reward=358.47 +/- 53.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 358        |
| time/                   |            |
|    total_timesteps      | 5625000    |
| train/                  |            |
|    approx_kl            | 0.03213588 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.42      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 845        |
|    n_updates            | 3430       |
|    policy_gradient_loss | 0.000949   |
|    value_loss           | 606        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 344      |
|    time_elapsed    | 49279    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=392.36 +/- 87.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 392         |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.022426166 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 3440        |
|    policy_gradient_loss | 0.00578     |
|    value_loss           | 191         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 345      |
|    time_elapsed    | 49430    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 164         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 346         |
|    time_elapsed         | 49556       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.022579214 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 571         |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=342.27 +/- 69.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.021321304 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 20.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 347      |
|    time_elapsed    | 49707    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=289.95 +/- 30.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.015351243 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.7        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 188         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 348      |
|    time_elapsed    | 49857    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 224         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 349         |
|    time_elapsed         | 49984       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.017050305 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 573         |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 435         |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=299.96 +/- 82.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.022917103 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89        |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 25.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 350      |
|    time_elapsed    | 50134    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=258.25 +/- 91.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 258          |
| time/                   |              |
|    total_timesteps      | 5750000      |
| train/                  |              |
|    approx_kl            | 0.0143551845 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.01        |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.0003       |
|    loss                 | 234          |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.00459     |
|    value_loss           | 465          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 227      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 351      |
|    time_elapsed    | 50285    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 241         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 352         |
|    time_elapsed         | 50411       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.018972326 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.07       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.8        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=444.30 +/- 113.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 444        |
| time/                   |            |
|    total_timesteps      | 5775000    |
| train/                  |            |
|    approx_kl            | 0.02145661 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.15      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.1       |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.00696   |
|    value_loss           | 16.4       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 353      |
|    time_elapsed    | 50563    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 285         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 354         |
|    time_elapsed         | 50690       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.018932655 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 82.6        |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 52.9        |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=423.05 +/- 198.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 423        |
| time/                   |            |
|    total_timesteps      | 5800000    |
| train/                  |            |
|    approx_kl            | 0.02105861 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.24      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.31       |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 28         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 355      |
|    time_elapsed    | 50841    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=257.77 +/- 154.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.013558351 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.91       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 447         |
|    n_updates            | 3550        |
|    policy_gradient_loss | 0.000836    |
|    value_loss           | 1.53e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 356      |
|    time_elapsed    | 50992    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 120         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 357         |
|    time_elapsed         | 51118       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.022780992 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 51.2        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=380.98 +/- 154.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 381         |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.021140298 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.04        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 193         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 27.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 358      |
|    time_elapsed    | 51269    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=284.79 +/- 137.17
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 285        |
| time/                   |            |
|    total_timesteps      | 5875000    |
| train/                  |            |
|    approx_kl            | 0.01808004 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.91      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 22.8       |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.00428   |
|    value_loss           | 625        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 359      |
|    time_elapsed    | 51421    |
|    total_timesteps | 5881856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -121         |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 360          |
|    time_elapsed         | 51548        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0102579715 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.15        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.0003       |
|    loss                 | 391          |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 1.52e+03     |
------------------------------------------
Eval num_timesteps=5900000, episode_reward=305.36 +/- 113.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 305        |
| time/                   |            |
|    total_timesteps      | 5900000    |
| train/                  |            |
|    approx_kl            | 0.01688219 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.79      |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.6       |
|    n_updates            | 3600       |
|    policy_gradient_loss | 0.00195    |
|    value_loss           | 266        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.2    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 361      |
|    time_elapsed    | 51699    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=232.32 +/- 183.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.011092688 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.1        |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 705         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 362      |
|    time_elapsed    | 51850    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.35       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 363         |
|    time_elapsed         | 51976       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.015179554 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.97       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 196         |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 303         |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=309.58 +/- 96.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.011052607 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.1        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 568         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 54.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 364      |
|    time_elapsed    | 52127    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=415.52 +/- 134.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 416        |
| time/                   |            |
|    total_timesteps      | 5975000    |
| train/                  |            |
|    approx_kl            | 0.02331631 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.73      |
|    explained_variance   | 0.473      |
|    learning_rate        | 0.0003     |
|    loss                 | 53.7       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.00767   |
|    value_loss           | 689        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 84.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 365      |
|    time_elapsed    | 52278    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 366         |
|    time_elapsed         | 52406       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.013618993 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.27        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 370         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=205.51 +/- 95.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.020200113 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 367      |
|    time_elapsed    | 52557    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=364.32 +/- 141.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 364         |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.014761349 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 205         |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 316         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 368      |
|    time_elapsed    | 52707    |
|    total_timesteps | 6029312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 172          |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 369          |
|    time_elapsed         | 52834        |
|    total_timesteps      | 6045696      |
| train/                  |              |
|    approx_kl            | 0.0152575085 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0003       |
|    loss                 | 52.9         |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.00943     |
|    value_loss           | 151          |
------------------------------------------
Eval num_timesteps=6050000, episode_reward=294.95 +/- 159.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.015960146 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 370      |
|    time_elapsed    | 52984    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=203.18 +/- 145.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.017910495 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 31.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 371      |
|    time_elapsed    | 53136    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 255         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 372         |
|    time_elapsed         | 53263       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.017101798 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.12       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.18        |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00787    |
|    value_loss           | 49.7        |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=255.65 +/- 96.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.016293943 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.6        |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00933    |
|    value_loss           | 80.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 373      |
|    time_elapsed    | 53414    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=246.12 +/- 171.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.017909307 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.91       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 29.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 248      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 374      |
|    time_elapsed    | 53566    |
|    total_timesteps | 6127616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 256        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 375        |
|    time_elapsed         | 53693      |
|    total_timesteps      | 6144000    |
| train/                  |            |
|    approx_kl            | 0.01613985 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.56      |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 3740       |
|    policy_gradient_loss | -0.00652   |
|    value_loss           | 503        |
----------------------------------------
Eval num_timesteps=6150000, episode_reward=197.72 +/- 75.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 6150000    |
| train/                  |            |
|    approx_kl            | 0.02038169 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.59      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 3750       |
|    policy_gradient_loss | -0.00767   |
|    value_loss           | 16.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 376      |
|    time_elapsed    | 53845    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=252.70 +/- 143.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.015794368 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 87.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 377      |
|    time_elapsed    | 53996    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 264         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 378         |
|    time_elapsed         | 54123       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.012400909 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.9        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 155         |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=213.49 +/- 137.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.020629428 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.32        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 11.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 379      |
|    time_elapsed    | 54274    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=310.81 +/- 120.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.011651448 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 118         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 380      |
|    time_elapsed    | 54425    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 235         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 381         |
|    time_elapsed         | 54551       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.021163233 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.9        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 74.6        |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=255.32 +/- 55.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.014597919 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.6        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 129         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 382      |
|    time_elapsed    | 54703    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=-1708.35 +/- 3969.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.71e+03   |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.020097557 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.73        |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 35.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 255      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 383      |
|    time_elapsed    | 54855    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 277         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 384         |
|    time_elapsed         | 54981       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.019757655 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.05        |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 16.8        |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=93.21 +/- 110.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 93.2       |
| time/                   |            |
|    total_timesteps      | 6300000    |
| train/                  |            |
|    approx_kl            | 0.01712155 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.37      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 22.3       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 55.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 385      |
|    time_elapsed    | 55132    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 253         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 386         |
|    time_elapsed         | 55259       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.017801663 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.03        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00956    |
|    value_loss           | 31.1        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=-2729.84 +/- 4543.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.73e+03   |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.009998591 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 859         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 387      |
|    time_elapsed    | 55411    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=344.18 +/- 107.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 344         |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.021746997 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.7        |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00847    |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 388      |
|    time_elapsed    | 55563    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 217         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 389         |
|    time_elapsed         | 55690       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.020990202 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 36.9        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=293.62 +/- 103.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.017292839 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0072     |
|    value_loss           | 93.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 390      |
|    time_elapsed    | 55841    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=-4494.67 +/- 5796.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -4.49e+03   |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.014856046 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 92          |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 231      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 391      |
|    time_elapsed    | 55993    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 392         |
|    time_elapsed         | 56120       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.014241656 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 158         |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=253.40 +/- 86.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.026093164 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.5        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00417    |
|    value_loss           | 258         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 393      |
|    time_elapsed    | 56272    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=-2165.60 +/- 4777.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.17e+03   |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.024058543 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.92       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.99        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 40.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 394      |
|    time_elapsed    | 56423    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 253         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 395         |
|    time_elapsed         | 56550       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.016446196 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0003      |
|    loss                 | 270         |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 657         |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=309.22 +/- 35.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 309         |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.020036267 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.56        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 7.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 94.8     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 396      |
|    time_elapsed    | 56701    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=187.64 +/- 69.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.009808872 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.3        |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 772         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 93.5     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 397      |
|    time_elapsed    | 56852    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 398         |
|    time_elapsed         | 56979       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.016120333 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 39.9        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=431.10 +/- 110.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 431         |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.017052744 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.9        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 399      |
|    time_elapsed    | 57130    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=362.96 +/- 115.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 363         |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.021153536 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.1         |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 80.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 270      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 400      |
|    time_elapsed    | 57282    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 280         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 401         |
|    time_elapsed         | 57409       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.016892336 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.9        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 34.8        |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=393.21 +/- 167.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 393        |
| time/                   |            |
|    total_timesteps      | 6575000    |
| train/                  |            |
|    approx_kl            | 0.01948499 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.9       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 4010       |
|    policy_gradient_loss | -0.00712   |
|    value_loss           | 22.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 402      |
|    time_elapsed    | 57561    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=362.72 +/- 192.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 363         |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.011002455 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00637    |
|    value_loss           | 280         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 403      |
|    time_elapsed    | 57712    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98.6        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 404         |
|    time_elapsed         | 57839       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.021552738 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 225         |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=306.99 +/- 124.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.015101231 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.9        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 344         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 67.3     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 405      |
|    time_elapsed    | 57991    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=279.61 +/- 214.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 280         |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.016683692 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.6        |
|    n_updates            | 4050        |
|    policy_gradient_loss | 0.00174     |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 96.2     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 406      |
|    time_elapsed    | 58142    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 193         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 407         |
|    time_elapsed         | 58270       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.014882187 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.6        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 77.9        |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=310.25 +/- 201.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.015588755 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.8        |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 408      |
|    time_elapsed    | 58421    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=299.80 +/- 121.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.021770438 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.9         |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 23.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 409      |
|    time_elapsed    | 58571    |
|    total_timesteps | 6701056  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 178       |
| time/                   |           |
|    fps                  | 114       |
|    iterations           | 410       |
|    time_elapsed         | 58698     |
|    total_timesteps      | 6717440   |
| train/                  |           |
|    approx_kl            | 0.0134032 |
|    clip_fraction        | 0.0953    |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.25     |
|    explained_variance   | 0.572     |
|    learning_rate        | 0.0003    |
|    loss                 | 493       |
|    n_updates            | 4090      |
|    policy_gradient_loss | -0.0147   |
|    value_loss           | 484       |
---------------------------------------
Eval num_timesteps=6725000, episode_reward=312.43 +/- 102.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.019476417 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.9        |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 554         |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 477         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 411      |
|    time_elapsed    | 58851    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=284.53 +/- 94.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.026910687 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.99        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 12.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 412      |
|    time_elapsed    | 59002    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 228        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 413        |
|    time_elapsed         | 59129      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.01200171 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.39      |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 466        |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00663   |
|    value_loss           | 407        |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=314.24 +/- 115.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 314         |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.029851086 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.9        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 17.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 263      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 414      |
|    time_elapsed    | 59281    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 235         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 415         |
|    time_elapsed         | 59407       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.017899146 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.14        |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00943    |
|    value_loss           | 11.8        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=278.57 +/- 101.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.018166412 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.4        |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 58.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 416      |
|    time_elapsed    | 59558    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=230.20 +/- 66.86
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 230        |
| time/                   |            |
|    total_timesteps      | 6825000    |
| train/                  |            |
|    approx_kl            | 0.01714563 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.36      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.54       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.00863   |
|    value_loss           | 20.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 280      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 417      |
|    time_elapsed    | 59709    |
|    total_timesteps | 6832128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 193        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 418        |
|    time_elapsed         | 59835      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.01697933 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.16      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 4170       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 142        |
----------------------------------------
Eval num_timesteps=6850000, episode_reward=313.61 +/- 144.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 314         |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.016713137 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.0699      |
|    learning_rate        | 0.0003      |
|    loss                 | 77.8        |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 732         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 419      |
|    time_elapsed    | 59986    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=259.39 +/- 117.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.026325254 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.969       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 8.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 420      |
|    time_elapsed    | 60136    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 421         |
|    time_elapsed         | 60263       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.023198813 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.04        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 14.8        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=285.31 +/- 104.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.009445498 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 550         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 422      |
|    time_elapsed    | 60413    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=245.38 +/- 62.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.024068274 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 26.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 273      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 423      |
|    time_elapsed    | 60564    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 296         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 424         |
|    time_elapsed         | 60690       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.019543357 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 95.8        |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=229.06 +/- 78.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 229        |
| time/                   |            |
|    total_timesteps      | 6950000    |
| train/                  |            |
|    approx_kl            | 0.02383779 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.56      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.7        |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.00989   |
|    value_loss           | 83.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 425      |
|    time_elapsed    | 60841    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=293.33 +/- 128.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.011140362 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 82.5        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 43.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 426      |
|    time_elapsed    | 60992    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 328         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 427         |
|    time_elapsed         | 61118       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.026697634 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 36.7        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00897    |
|    value_loss           | 53.5        |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=357.83 +/- 52.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.014205823 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 7           |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 55.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 428      |
|    time_elapsed    | 61269    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=273.22 +/- 66.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.020128576 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.39        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 64.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 429      |
|    time_elapsed    | 61420    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 115         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 430         |
|    time_elapsed         | 61546       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.015675876 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 669         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 706         |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=170.85 +/- 55.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.016402267 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 55.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 96       |
| time/              |          |
|    fps             | 114      |
|    iterations      | 431      |
|    time_elapsed    | 61697    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=288.16 +/- 143.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.024193147 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 432      |
|    time_elapsed    | 61847    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 242         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 433         |
|    time_elapsed         | 61974       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.014812786 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 365         |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=234.46 +/- 126.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.019196387 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | 576         |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 434      |
|    time_elapsed    | 62124    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=231.57 +/- 92.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 232        |
| time/                   |            |
|    total_timesteps      | 7125000    |
| train/                  |            |
|    approx_kl            | 0.01629505 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.59      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 28.8       |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0032    |
|    value_loss           | 412        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 435      |
|    time_elapsed    | 62275    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 82.4        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 436         |
|    time_elapsed         | 62402       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.022111015 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.29        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 111         |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=244.33 +/- 116.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 244         |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.017049279 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.94        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 437      |
|    time_elapsed    | 62554    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=214.66 +/- 58.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.025953975 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 42.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 438      |
|    time_elapsed    | 62704    |
|    total_timesteps | 7176192  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 164        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 439        |
|    time_elapsed         | 62831      |
|    total_timesteps      | 7192576    |
| train/                  |            |
|    approx_kl            | 0.01722286 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.31      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.57       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.00744   |
|    value_loss           | 37.4       |
----------------------------------------
Eval num_timesteps=7200000, episode_reward=-490.76 +/- 1462.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -491        |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.016798321 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 455         |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 406         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 440      |
|    time_elapsed    | 62982    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=321.63 +/- 132.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.022504456 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.86        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 24.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 441      |
|    time_elapsed    | 63133    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 274         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 442         |
|    time_elapsed         | 63260       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.018542293 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.7         |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 28.1        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=213.47 +/- 94.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.020024907 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 12.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 443      |
|    time_elapsed    | 63410    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 318         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 444         |
|    time_elapsed         | 63537       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.018053027 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.17       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 34.6        |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=102.63 +/- 93.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 103         |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.022819765 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.56        |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00872    |
|    value_loss           | 13.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 445      |
|    time_elapsed    | 63687    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=175.40 +/- 204.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.016826315 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.76        |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00886    |
|    value_loss           | 70.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 446      |
|    time_elapsed    | 63838    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 346         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 447         |
|    time_elapsed         | 63965       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.019731905 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 97          |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=-599.89 +/- 1301.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -600        |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.021190587 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 10.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 448      |
|    time_elapsed    | 64117    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=147.94 +/- 144.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 148         |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.016058732 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 38.6        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 68.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 449      |
|    time_elapsed    | 64268    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 75          |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 450         |
|    time_elapsed         | 64394       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.023198728 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 253         |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 226         |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=182.65 +/- 100.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.014662773 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.2        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 747         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 451      |
|    time_elapsed    | 64545    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=227.86 +/- 110.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.024128644 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.3        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 1.44e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.56    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 452      |
|    time_elapsed    | 64696    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 50.5        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 453         |
|    time_elapsed         | 64822       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.021653868 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 94.2        |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=296.94 +/- 141.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.020737242 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.3        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 81          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 454      |
|    time_elapsed    | 64973    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=337.81 +/- 139.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 338          |
| time/                   |              |
|    total_timesteps      | 7450000      |
| train/                  |              |
|    approx_kl            | 0.0139843505 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.45        |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.0003       |
|    loss                 | 158          |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 202          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 455      |
|    time_elapsed    | 65125    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 275         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 456         |
|    time_elapsed         | 65252       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.021132426 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.9         |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 162         |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=230.70 +/- 85.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.020737091 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.07        |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 32.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 457      |
|    time_elapsed    | 65403    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=-273.11 +/- 1186.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.015992794 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 6.44        |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 94.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 458      |
|    time_elapsed    | 65554    |
|    total_timesteps | 7503872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 300        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 459        |
|    time_elapsed         | 65681      |
|    total_timesteps      | 7520256    |
| train/                  |            |
|    approx_kl            | 0.01562736 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.6       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.00968   |
|    value_loss           | 135        |
----------------------------------------
Eval num_timesteps=7525000, episode_reward=352.67 +/- 181.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 353         |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.017676137 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.07        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 39.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 283      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 460      |
|    time_elapsed    | 65832    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=296.76 +/- 112.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.016791914 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 153         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 461      |
|    time_elapsed    | 65983    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 282         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 462         |
|    time_elapsed         | 66110       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.018671883 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.61        |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 25.6        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=229.53 +/- 134.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 230         |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.012767879 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.6        |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 97.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 463      |
|    time_elapsed    | 66261    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=46.30 +/- 384.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 46.3        |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.018005775 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.2        |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 369         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 464      |
|    time_elapsed    | 66412    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 465         |
|    time_elapsed         | 66541       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.020839157 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.1         |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 16.1        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=264.34 +/- 133.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.017949047 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 49.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 466      |
|    time_elapsed    | 66691    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=221.07 +/- 93.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.016494885 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 44.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 467      |
|    time_elapsed    | 66842    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 281         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 468         |
|    time_elapsed         | 66969       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.018850705 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=405.99 +/- 178.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 406         |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.017083894 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 51          |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 381         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 469      |
|    time_elapsed    | 67120    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=330.78 +/- 150.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.022160951 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 76.2        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 37.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 470      |
|    time_elapsed    | 67272    |
|    total_timesteps | 7700480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 326        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 471        |
|    time_elapsed         | 67399      |
|    total_timesteps      | 7716864    |
| train/                  |            |
|    approx_kl            | 0.02069456 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.68      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 21.8       |
----------------------------------------
Eval num_timesteps=7725000, episode_reward=300.11 +/- 145.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.018030511 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.103       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.15        |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 546         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 472      |
|    time_elapsed    | 67551    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 141         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 473         |
|    time_elapsed         | 67678       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.019253477 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.7        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 958         |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=455.63 +/- 231.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 456         |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.021596927 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 325         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 474      |
|    time_elapsed    | 67830    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=290.84 +/- 80.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.020161916 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 76          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 475      |
|    time_elapsed    | 67980    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 241         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 476         |
|    time_elapsed         | 68107       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.018700637 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.06        |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 29.8        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=255.35 +/- 57.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 255          |
| time/                   |              |
|    total_timesteps      | 7800000      |
| train/                  |              |
|    approx_kl            | 0.0139182955 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.544        |
|    learning_rate        | 0.0003       |
|    loss                 | 25.1         |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 158          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 477      |
|    time_elapsed    | 68258    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=340.40 +/- 91.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 340         |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.018585298 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 24.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 259      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 478      |
|    time_elapsed    | 68409    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 237         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 479         |
|    time_elapsed         | 68536       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.013307835 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.2        |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00738    |
|    value_loss           | 305         |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=342.29 +/- 259.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.020812072 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.01        |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 47.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 480      |
|    time_elapsed    | 68687    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=283.81 +/- 42.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.023035359 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.4        |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 63.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 283      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 481      |
|    time_elapsed    | 68838    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 320         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 482         |
|    time_elapsed         | 68966       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.019330028 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 168         |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=390.84 +/- 80.05
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 391        |
| time/                   |            |
|    total_timesteps      | 7900000    |
| train/                  |            |
|    approx_kl            | 0.01992397 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.2       |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 32.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 483      |
|    time_elapsed    | 69118    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=383.36 +/- 212.80
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 383        |
| time/                   |            |
|    total_timesteps      | 7925000    |
| train/                  |            |
|    approx_kl            | 0.01794766 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.52      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 242        |
|    n_updates            | 4830       |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 707        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 484      |
|    time_elapsed    | 69270    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 144         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 485         |
|    time_elapsed         | 69397       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.015792094 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 239         |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=323.15 +/- 98.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 323         |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.019816805 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.03        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 8.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 486      |
|    time_elapsed    | 69549    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=306.96 +/- 68.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.017848738 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.53        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 487      |
|    time_elapsed    | 69701    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 158         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 488         |
|    time_elapsed         | 69828       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.016247388 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 321         |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=364.85 +/- 101.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.023159865 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 20.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 489      |
|    time_elapsed    | 69980    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=351.02 +/- 45.90
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 351       |
| time/                   |           |
|    total_timesteps      | 8025000   |
| train/                  |           |
|    approx_kl            | 0.0234082 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.37     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.28      |
|    n_updates            | 4890      |
|    policy_gradient_loss | -0.00887  |
|    value_loss           | 23.7      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 490      |
|    time_elapsed    | 70132    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 336         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 491         |
|    time_elapsed         | 70259       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.016486883 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.865       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=447.76 +/- 121.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 448         |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.017628264 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 25.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 492      |
|    time_elapsed    | 70410    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=366.01 +/- 83.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 366         |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.013949318 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00834    |
|    value_loss           | 144         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 493      |
|    time_elapsed    | 70561    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 149         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 494         |
|    time_elapsed         | 70687       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.019171849 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61        |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 15.7        |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=225.05 +/- 86.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 225         |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.014029103 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.6        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 360         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 495      |
|    time_elapsed    | 70838    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=365.26 +/- 164.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 365        |
| time/                   |            |
|    total_timesteps      | 8125000    |
| train/                  |            |
|    approx_kl            | 0.02463384 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.75      |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.18       |
|    n_updates            | 4950       |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 146        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 496      |
|    time_elapsed    | 70990    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 265         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 497         |
|    time_elapsed         | 71117       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.013936502 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 211         |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=289.42 +/- 52.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.023973763 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.18        |
|    n_updates            | 4970        |
|    policy_gradient_loss | 0.005       |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 498      |
|    time_elapsed    | 71268    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=300.88 +/- 128.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.014844995 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 490         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 499      |
|    time_elapsed    | 71420    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 239         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 500         |
|    time_elapsed         | 71547       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.019546669 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.32        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 38.7        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=267.98 +/- 130.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 268         |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.016451683 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.0097     |
|    value_loss           | 736         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 501      |
|    time_elapsed    | 71698    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 286         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 502         |
|    time_elapsed         | 71825       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.025384173 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 87.9        |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=458.75 +/- 117.00
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 459        |
| time/                   |            |
|    total_timesteps      | 8225000    |
| train/                  |            |
|    approx_kl            | 0.01940098 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.17      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.47       |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.00888   |
|    value_loss           | 89.7       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 217      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 503      |
|    time_elapsed    | 71975    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=357.54 +/- 191.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 358        |
| time/                   |            |
|    total_timesteps      | 8250000    |
| train/                  |            |
|    approx_kl            | 0.01245448 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.02      |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 30         |
|    n_updates            | 5030       |
|    policy_gradient_loss | -0.00161   |
|    value_loss           | 365        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 9.3      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 504      |
|    time_elapsed    | 72126    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 41.4        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 505         |
|    time_elapsed         | 72252       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.008550997 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0003      |
|    loss                 | 697         |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=459.11 +/- 82.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 459         |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.016902182 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 24.1        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 506      |
|    time_elapsed    | 72403    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=120.77 +/- 187.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 121        |
| time/                   |            |
|    total_timesteps      | 8300000    |
| train/                  |            |
|    approx_kl            | 0.01803586 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.41      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 76.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 507      |
|    time_elapsed    | 72555    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 362         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 508         |
|    time_elapsed         | 72682       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.020943746 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 14.8        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=391.09 +/- 129.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 391         |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.017415766 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 134         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 509      |
|    time_elapsed    | 72833    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=350.32 +/- 204.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 350         |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.022332903 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.35        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 73.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 510      |
|    time_elapsed    | 72985    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 278         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 511         |
|    time_elapsed         | 73111       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.018149108 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.799       |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 77.7        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=435.25 +/- 154.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 435         |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.015660122 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0003      |
|    loss                 | 428         |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 998         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 512      |
|    time_elapsed    | 73262    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=-3110.33 +/- 6957.92
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -3.11e+03  |
| time/                   |            |
|    total_timesteps      | 8400000    |
| train/                  |            |
|    approx_kl            | 0.01974994 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.06       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.00734   |
|    value_loss           | 25.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 93.9     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 513      |
|    time_elapsed    | 73413    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 98          |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 514         |
|    time_elapsed         | 73540       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.007976278 |
|    clip_fraction        | 0.0592      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01e+03    |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=287.53 +/- 84.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.021485396 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.78        |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 515      |
|    time_elapsed    | 73692    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=393.24 +/- 83.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 393         |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.024215294 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 189         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 516      |
|    time_elapsed    | 73843    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 300         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 517         |
|    time_elapsed         | 73970       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.011370722 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 874         |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=428.46 +/- 181.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 428         |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.021435112 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 588         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 518      |
|    time_elapsed    | 74122    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=382.61 +/- 208.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 383         |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.014986005 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.01        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 76.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 519      |
|    time_elapsed    | 74274    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 237         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 520         |
|    time_elapsed         | 74403       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.012221131 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | 489         |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=421.48 +/- 213.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.017520396 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 17.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 521      |
|    time_elapsed    | 74554    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=306.64 +/- 99.72
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 307        |
| time/                   |            |
|    total_timesteps      | 8550000    |
| train/                  |            |
|    approx_kl            | 0.01750366 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.23       |
|    n_updates            | 5210       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 31         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 522      |
|    time_elapsed    | 74705    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 334         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 523         |
|    time_elapsed         | 74832       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.021082098 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 71          |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 78.2        |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=344.67 +/- 116.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 345         |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.019134503 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 524      |
|    time_elapsed    | 74983    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=371.35 +/- 183.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.017636057 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.2        |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 291         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 525      |
|    time_elapsed    | 75134    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 526         |
|    time_elapsed         | 75260       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.024889817 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.59        |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 61.9        |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=403.41 +/- 118.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 403         |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.019478258 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.8         |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 59.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 527      |
|    time_elapsed    | 75411    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=407.53 +/- 87.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 408         |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.018809991 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.69        |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 23.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 528      |
|    time_elapsed    | 75562    |
|    total_timesteps | 8650752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 319         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 529         |
|    time_elapsed         | 75688       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.018571699 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 312         |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=416.27 +/- 110.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.023808401 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.52        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00909    |
|    value_loss           | 24.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 530      |
|    time_elapsed    | 75839    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 531         |
|    time_elapsed         | 75966       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.016904768 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 295         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00866    |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=334.47 +/- 39.48
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 334       |
| time/                   |           |
|    total_timesteps      | 8700000   |
| train/                  |           |
|    approx_kl            | 0.0161556 |
|    clip_fraction        | 0.149     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.35     |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.0003    |
|    loss                 | 25.6      |
|    n_updates            | 5310      |
|    policy_gradient_loss | -0.00851  |
|    value_loss           | 173       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 532      |
|    time_elapsed    | 76117    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=393.61 +/- 215.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 394         |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.017529905 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 21          |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 81          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 262      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 533      |
|    time_elapsed    | 76268    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 279         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 534         |
|    time_elapsed         | 76395       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.014094156 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.2        |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 221         |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=407.92 +/- 140.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 408         |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.021672146 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.1         |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 80.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 535      |
|    time_elapsed    | 76547    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=455.33 +/- 144.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 455        |
| time/                   |            |
|    total_timesteps      | 8775000    |
| train/                  |            |
|    approx_kl            | 0.01647954 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.3       |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.91       |
|    n_updates            | 5350       |
|    policy_gradient_loss | -0.00624   |
|    value_loss           | 86.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 536      |
|    time_elapsed    | 76698    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 286         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 537         |
|    time_elapsed         | 76825       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.020313658 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=413.48 +/- 231.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 413         |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.014881432 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.7        |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 538      |
|    time_elapsed    | 76977    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=440.82 +/- 138.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 441         |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.017325625 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 270      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 539      |
|    time_elapsed    | 77129    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 200         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 540         |
|    time_elapsed         | 77256       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.022329539 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.54        |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 15.3        |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=341.36 +/- 130.40
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 341        |
| time/                   |            |
|    total_timesteps      | 8850000    |
| train/                  |            |
|    approx_kl            | 0.01751712 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.91      |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.2       |
|    n_updates            | 5400       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 338        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 541      |
|    time_elapsed    | 77407    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=335.39 +/- 81.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 335         |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.017314734 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.3        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 64.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 542      |
|    time_elapsed    | 77559    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 207         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 543         |
|    time_elapsed         | 77686       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.018745746 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.93        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00933    |
|    value_loss           | 21.5        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=427.55 +/- 103.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 428         |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.009632837 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | 951         |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 1.43e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 544      |
|    time_elapsed    | 77837    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=306.96 +/- 93.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.012904648 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 390         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 545      |
|    time_elapsed    | 77989    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 546         |
|    time_elapsed         | 78116       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.016338581 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.8        |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 279         |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=452.79 +/- 195.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 453         |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.016433967 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 5460        |
|    policy_gradient_loss | 0.00275     |
|    value_loss           | 514         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 547      |
|    time_elapsed    | 78268    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=304.87 +/- 74.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 305          |
| time/                   |              |
|    total_timesteps      | 8975000      |
| train/                  |              |
|    approx_kl            | 0.0075599286 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.0003       |
|    loss                 | 126          |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 265          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 548      |
|    time_elapsed    | 78419    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 549         |
|    time_elapsed         | 78545       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.017387133 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 32          |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 128         |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=323.67 +/- 141.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 324         |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.020567149 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 60.9        |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 22.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 550      |
|    time_elapsed    | 78697    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=-2878.47 +/- 6613.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.88e+03   |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.019445017 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 5.68        |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 92.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 551      |
|    time_elapsed    | 78847    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 260         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 552         |
|    time_elapsed         | 78974       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.016828254 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.6        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 78.5        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=422.33 +/- 72.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 422         |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.011320416 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 639         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 87.5     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 553      |
|    time_elapsed    | 79125    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=350.13 +/- 53.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 350        |
| time/                   |            |
|    total_timesteps      | 9075000    |
| train/                  |            |
|    approx_kl            | 0.01475933 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.13      |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | 280        |
|    n_updates            | 5530       |
|    policy_gradient_loss | -0.0247    |
|    value_loss           | 302        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.5     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 554      |
|    time_elapsed    | 79277    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 37          |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 555         |
|    time_elapsed         | 79404       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.018884884 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 254         |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=-3040.58 +/- 6804.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.04e+03   |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.021781042 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 258         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 556      |
|    time_elapsed    | 79555    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=368.98 +/- 131.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.019609638 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.1        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 114         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 279      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 557      |
|    time_elapsed    | 79706    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 291         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 558         |
|    time_elapsed         | 79832       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.019555077 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.9        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=265.92 +/- 119.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 266        |
| time/                   |            |
|    total_timesteps      | 9150000    |
| train/                  |            |
|    approx_kl            | 0.01768665 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.2       |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.00997   |
|    value_loss           | 39.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 559      |
|    time_elapsed    | 79983    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=388.45 +/- 109.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 388         |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.021704856 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 14.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 560      |
|    time_elapsed    | 80134    |
|    total_timesteps | 9175040  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 348       |
| time/                   |           |
|    fps                  | 114       |
|    iterations           | 561       |
|    time_elapsed         | 80260     |
|    total_timesteps      | 9191424   |
| train/                  |           |
|    approx_kl            | 0.0135964 |
|    clip_fraction        | 0.153     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.73     |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0003    |
|    loss                 | 20.8      |
|    n_updates            | 5600      |
|    policy_gradient_loss | -0.00928  |
|    value_loss           | 82.6      |
---------------------------------------
Eval num_timesteps=9200000, episode_reward=-3147.98 +/- 7080.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -3.15e+03  |
| time/                   |            |
|    total_timesteps      | 9200000    |
| train/                  |            |
|    approx_kl            | 0.01608428 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.53      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 62.3       |
|    n_updates            | 5610       |
|    policy_gradient_loss | -0.00137   |
|    value_loss           | 308        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 562      |
|    time_elapsed    | 80412    |
|    total_timesteps | 9207808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 261        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 563        |
|    time_elapsed         | 80539      |
|    total_timesteps      | 9224192    |
| train/                  |            |
|    approx_kl            | 0.02039035 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.17      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.42       |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 36.1       |
----------------------------------------
Eval num_timesteps=9225000, episode_reward=320.18 +/- 190.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.019934224 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.2         |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 564      |
|    time_elapsed    | 80691    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=351.87 +/- 153.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 352         |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.018980261 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.22        |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 26.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 565      |
|    time_elapsed    | 80842    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 366         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 566         |
|    time_elapsed         | 80969       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.013076175 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.4        |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 67.2        |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=292.97 +/- 190.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.017233191 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 9.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 567      |
|    time_elapsed    | 81120    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=322.30 +/- 147.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.015181458 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.26        |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 97.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 568      |
|    time_elapsed    | 81272    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 327         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 569         |
|    time_elapsed         | 81399       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.025757357 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.706       |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 13.3        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=484.42 +/- 204.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 484         |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.015654355 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.5        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 130         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 273      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 570      |
|    time_elapsed    | 81550    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=329.95 +/- 206.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.015989967 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.6        |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 96.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 571      |
|    time_elapsed    | 81701    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 290         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 572         |
|    time_elapsed         | 81828       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.017252546 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.18        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 42.2        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=309.66 +/- 176.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.017000802 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 19.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 573      |
|    time_elapsed    | 81980    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=403.15 +/- 132.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 403         |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.026975809 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 77.2        |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 56.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 574      |
|    time_elapsed    | 82131    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 306         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 575         |
|    time_elapsed         | 82258       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.014156812 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.2        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 59.2        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=386.17 +/- 95.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 386        |
| time/                   |            |
|    total_timesteps      | 9425000    |
| train/                  |            |
|    approx_kl            | 0.01704811 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 26.8       |
|    n_updates            | 5750       |
|    policy_gradient_loss | -0.00755   |
|    value_loss           | 58         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 576      |
|    time_elapsed    | 82409    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=325.09 +/- 141.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 325         |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.015043117 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 213         |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 335         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 577      |
|    time_elapsed    | 82561    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 578         |
|    time_elapsed         | 82688       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.019318143 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.49        |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 39.9        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=384.42 +/- 166.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 384         |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.026084539 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.2        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 42          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 266      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 579      |
|    time_elapsed    | 82839    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=330.57 +/- 92.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.016418314 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00826    |
|    value_loss           | 57          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 580      |
|    time_elapsed    | 82991    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 328         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 581         |
|    time_elapsed         | 83118       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.017019888 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 49.9        |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=395.26 +/- 147.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 395         |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.020366728 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 7           |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 194         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 582      |
|    time_elapsed    | 83269    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=389.17 +/- 141.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.017096449 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.4        |
|    n_updates            | 5820        |
|    policy_gradient_loss | 0.00166     |
|    value_loss           | 52.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 583      |
|    time_elapsed    | 83421    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 250         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 584         |
|    time_elapsed         | 83548       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.021934116 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.6         |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=259.72 +/- 116.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.021222245 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.06        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 95.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 585      |
|    time_elapsed    | 83699    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=276.55 +/- 150.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 277        |
| time/                   |            |
|    total_timesteps      | 9600000    |
| train/                  |            |
|    approx_kl            | 0.02023901 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.23      |
|    explained_variance   | 0.382      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.99       |
|    n_updates            | 5850       |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 531        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 269      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 586      |
|    time_elapsed    | 83851    |
|    total_timesteps | 9601024  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 337        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 587        |
|    time_elapsed         | 83978      |
|    total_timesteps      | 9617408    |
| train/                  |            |
|    approx_kl            | 0.02070649 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.47      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.67       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.00554   |
|    value_loss           | 87.5       |
----------------------------------------
Eval num_timesteps=9625000, episode_reward=-617.36 +/- 2014.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -617        |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.018294107 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 35.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 588      |
|    time_elapsed    | 84129    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=354.16 +/- 165.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.017869547 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 52.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 589      |
|    time_elapsed    | 84281    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 373         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 590         |
|    time_elapsed         | 84408       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.020038214 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.78        |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 27.5        |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=421.20 +/- 150.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.018068198 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 80.5        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 71.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 591      |
|    time_elapsed    | 84559    |
|    total_timesteps | 9682944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 311        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 592        |
|    time_elapsed         | 84687      |
|    total_timesteps      | 9699328    |
| train/                  |            |
|    approx_kl            | 0.01887118 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.74      |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | 43.1       |
|    n_updates            | 5910       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 62         |
----------------------------------------
Eval num_timesteps=9700000, episode_reward=314.51 +/- 171.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.017159289 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.7        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 143         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 593      |
|    time_elapsed    | 84838    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=328.62 +/- 215.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.015727196 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.8        |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 259         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 594      |
|    time_elapsed    | 84990    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 242         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 595         |
|    time_elapsed         | 85117       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.018310409 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.6        |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=389.13 +/- 223.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.018312607 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 596      |
|    time_elapsed    | 85268    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=259.31 +/- 143.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.025841951 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 275      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 597      |
|    time_elapsed    | 85421    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 314         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 598         |
|    time_elapsed         | 85548       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.017195664 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 32.4        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=347.81 +/- 247.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 348         |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.018862635 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.65        |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 15.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 599      |
|    time_elapsed    | 85699    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=366.14 +/- 169.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 366         |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.018436238 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.61        |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 600      |
|    time_elapsed    | 85850    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 289         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 601         |
|    time_elapsed         | 85977       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.015490886 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 730         |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=210.31 +/- 178.08
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 210       |
| time/                   |           |
|    total_timesteps      | 9850000   |
| train/                  |           |
|    approx_kl            | 0.0183939 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.46     |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.0003    |
|    loss                 | 117       |
|    n_updates            | 6010      |
|    policy_gradient_loss | -0.00772  |
|    value_loss           | 147       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 602      |
|    time_elapsed    | 86129    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=371.17 +/- 174.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 371        |
| time/                   |            |
|    total_timesteps      | 9875000    |
| train/                  |            |
|    approx_kl            | 0.02042256 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.86      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 8.92       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 27.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 603      |
|    time_elapsed    | 86281    |
|    total_timesteps | 9879552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 604        |
|    time_elapsed         | 86409      |
|    total_timesteps      | 9895936    |
| train/                  |            |
|    approx_kl            | 0.01827328 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.53      |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.8       |
|    n_updates            | 6030       |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 149        |
----------------------------------------
slurmstepd: error: *** JOB 26565266 ON m3i031 CANCELLED AT 2022-06-28T01:06:58 DUE TO TIME LIMIT ***
