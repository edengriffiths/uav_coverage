========== uav-v9 ==========
Seed: 1235230544
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v9_1
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.69e+03 |
| time/              |           |
|    fps             | 260       |
|    iterations      | 1         |
|    time_elapsed    | 62        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=59.87 +/- 3216.10
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 59.9         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0076556155 |
|    clip_fraction        | 0.0687       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | -4.09e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00824     |
|    value_loss           | 3.13e+04     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.98e+03 |
| time/              |           |
|    fps             | 216       |
|    iterations      | 2         |
|    time_elapsed    | 151       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.06e+03   |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 3           |
|    time_elapsed         | 225         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008821754 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 2.49e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1562.37 +/- 480.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010100779 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.33e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 5.76e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.25e+03 |
| time/              |           |
|    fps             | 208       |
|    iterations      | 4         |
|    time_elapsed    | 314       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=863.04 +/- 1006.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 863         |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.010803839 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 358         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.47e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -770     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 5        |
|    time_elapsed    | 403      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 26.8        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 6           |
|    time_elapsed         | 477         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009131158 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 399         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 954         |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=983.00 +/- 1359.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 983         |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.008387522 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 248         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 444         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 7        |
|    time_elapsed    | 566      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=125000, episode_reward=1442.39 +/- 293.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44e+03    |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.008768735 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 71          |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 318         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 8        |
|    time_elapsed    | 654      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 786         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 9           |
|    time_elapsed         | 728         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008391277 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 231         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 442         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=1922.59 +/- 449.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.92e+03     |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0070220027 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.0003       |
|    loss                 | 179          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 419          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 10       |
|    time_elapsed    | 817      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=1562.67 +/- 293.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.007624012 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 343         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 199      |
|    iterations      | 11       |
|    time_elapsed    | 905      |
|    total_timesteps | 180224   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 758          |
| time/                   |              |
|    fps                  | 200          |
|    iterations           | 12           |
|    time_elapsed         | 979          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0069826487 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.0003       |
|    loss                 | 149          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00701     |
|    value_loss           | 481          |
------------------------------------------
Eval num_timesteps=200000, episode_reward=1922.38 +/- 448.87
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.92e+03     |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0072108707 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.0003       |
|    loss                 | 140          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00723     |
|    value_loss           | 438          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 199      |
|    iterations      | 13       |
|    time_elapsed    | 1068     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=1922.70 +/- 240.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92e+03    |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.008069534 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 445         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 14       |
|    time_elapsed    | 1156     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 643         |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 15          |
|    time_elapsed         | 1230        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.007927476 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 153         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 421         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1562.39 +/- 293.99
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.56e+03     |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0075918874 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.0003       |
|    loss                 | 145          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00893     |
|    value_loss           | 375          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 693      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 16       |
|    time_elapsed    | 1318     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=1342.65 +/- 994.94
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.34e+03     |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0077991867 |
|    clip_fraction        | 0.0564       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 151          |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00952     |
|    value_loss           | 423          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 700      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 17       |
|    time_elapsed    | 1406     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 18          |
|    time_elapsed         | 1480        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.007967638 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 208         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 398         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=2402.43 +/- 379.57
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.4e+03      |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0068919435 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | 205          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00758     |
|    value_loss           | 391          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 19       |
|    time_elapsed    | 1568     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=2402.45 +/- 379.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.4e+03      |
| time/                   |              |
|    total_timesteps      | 325000       |
| train/                  |              |
|    approx_kl            | 0.0073635485 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00755     |
|    value_loss           | 357          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 676      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 20       |
|    time_elapsed    | 1656     |
|    total_timesteps | 327680   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 697          |
| time/                   |              |
|    fps                  | 198          |
|    iterations           | 21           |
|    time_elapsed         | 1730         |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0077589387 |
|    clip_fraction        | 0.0738       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.62        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 136          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.0088      |
|    value_loss           | 425          |
------------------------------------------
Eval num_timesteps=350000, episode_reward=2162.88 +/- 479.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.16e+03     |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0074896836 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.6         |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | 136          |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00675     |
|    value_loss           | 355          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 632      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 22       |
|    time_elapsed    | 1819     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=2162.53 +/- 293.65
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.16e+03     |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0074712816 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.0003       |
|    loss                 | 84.8         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00836     |
|    value_loss           | 395          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 23       |
|    time_elapsed    | 1907     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 780         |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 24          |
|    time_elapsed         | 1981        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.007758636 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 205         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=2522.44 +/- 449.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.007704426 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 203         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 304         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 714      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 25       |
|    time_elapsed    | 2070     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=2522.46 +/- 448.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.008211716 |
|    clip_fraction        | 0.0697      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 167         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 501         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 662      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 26       |
|    time_elapsed    | 2159     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 601         |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 27          |
|    time_elapsed         | 2233        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.008229852 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 168         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00784    |
|    value_loss           | 373         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=1922.35 +/- 449.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92e+03    |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.008265827 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 361         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 28       |
|    time_elapsed    | 2321     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=2402.24 +/- 536.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4e+03     |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.009049004 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00861    |
|    value_loss           | 325         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 29       |
|    time_elapsed    | 2409     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 30          |
|    time_elapsed         | 2483        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.009437583 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 320         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=2402.56 +/- 379.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.4e+03      |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0070948927 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 219          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00646     |
|    value_loss           | 421          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 31       |
|    time_elapsed    | 2572     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 32          |
|    time_elapsed         | 2646        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.007840591 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.7        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 320         |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=2522.74 +/- 448.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.007762263 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 374         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 33       |
|    time_elapsed    | 2734     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2642.64 +/- 293.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.008817847 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.5        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 620         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 34       |
|    time_elapsed    | 2822     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 703         |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 35          |
|    time_elapsed         | 2896        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.008805361 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 327         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=2522.39 +/- 449.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.52e+03   |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.00826434 |
|    clip_fraction        | 0.0774     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.3       |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.00951   |
|    value_loss           | 438        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 623      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 36       |
|    time_elapsed    | 2984     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2642.65 +/- 293.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.008921483 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 254         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 444         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 721      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 37       |
|    time_elapsed    | 3072     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 38          |
|    time_elapsed         | 3146        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.008838319 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 326         |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=2522.77 +/- 240.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.52e+03   |
| time/                   |            |
|    total_timesteps      | 625000     |
| train/                  |            |
|    approx_kl            | 0.00833994 |
|    clip_fraction        | 0.079      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 99.9       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.00809   |
|    value_loss           | 283        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 39       |
|    time_elapsed    | 3234     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2642.57 +/- 294.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.008885034 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 183         |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 322         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 40       |
|    time_elapsed    | 3321     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 991         |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 41          |
|    time_elapsed         | 3395        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.006273718 |
|    clip_fraction        | 0.0483      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 450         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=1942.71 +/- 995.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94e+03    |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.008109974 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00847    |
|    value_loss           | 378         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 42       |
|    time_elapsed    | 3483     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=2642.53 +/- 293.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.009055568 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 243         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 341         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 976      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 43       |
|    time_elapsed    | 3571     |
|    total_timesteps | 704512   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.06e+03     |
| time/                   |              |
|    fps                  | 197          |
|    iterations           | 44           |
|    time_elapsed         | 3644         |
|    total_timesteps      | 720896       |
| train/                  |              |
|    approx_kl            | 0.0079057515 |
|    clip_fraction        | 0.0668       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.0003       |
|    loss                 | 196          |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00753     |
|    value_loss           | 534          |
------------------------------------------
Eval num_timesteps=725000, episode_reward=2762.69 +/- 293.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.009980952 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 321         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 45       |
|    time_elapsed    | 3731     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2522.36 +/- 239.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.008951144 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 393         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 46       |
|    time_elapsed    | 3818     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 47          |
|    time_elapsed         | 3891        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.012991387 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.6        |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=2522.79 +/- 240.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.009140816 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 217         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 48       |
|    time_elapsed    | 3977     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2642.36 +/- 480.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.006671929 |
|    clip_fraction        | 0.0554      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 194         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 405         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 981      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 49       |
|    time_elapsed    | 4063     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.01e+03    |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 50          |
|    time_elapsed         | 4134        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.007882246 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 297         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 411         |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=3002.34 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.008490509 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.5        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 252         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 965      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 51       |
|    time_elapsed    | 4217     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2762.68 +/- 293.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.008891316 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 307         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 198      |
|    iterations      | 52       |
|    time_elapsed    | 4300     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 950         |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 53          |
|    time_elapsed         | 4369        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.009355582 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 307         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=1943.10 +/- 919.05
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.94e+03   |
| time/                   |            |
|    total_timesteps      | 875000     |
| train/                  |            |
|    approx_kl            | 0.00876865 |
|    clip_fraction        | 0.0871     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.7       |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.00592   |
|    value_loss           | 176        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 977      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 54       |
|    time_elapsed    | 4451     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=2882.37 +/- 239.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.007271833 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 305         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 198      |
|    iterations      | 55       |
|    time_elapsed    | 4533     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.16e+03    |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 56          |
|    time_elapsed         | 4602        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.010985872 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 97.8        |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=2762.58 +/- 293.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.007882681 |
|    clip_fraction        | 0.0621      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 520         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 199      |
|    iterations      | 57       |
|    time_elapsed    | 4683     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2882.52 +/- 240.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.008970936 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 218         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 199      |
|    iterations      | 58       |
|    time_elapsed    | 4765     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.13e+03    |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 59          |
|    time_elapsed         | 4832        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.008923033 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 277         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 320         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=2882.38 +/- 240.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.008831399 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 175         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 426         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 200      |
|    iterations      | 60       |
|    time_elapsed    | 4912     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.05e+03   |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 61         |
|    time_elapsed         | 4980       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.00802629 |
|    clip_fraction        | 0.0685     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.00673   |
|    value_loss           | 368        |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=2762.49 +/- 293.75
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.76e+03     |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0071690385 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.0003       |
|    loss                 | 176          |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 446          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 200      |
|    iterations      | 62       |
|    time_elapsed    | 5059     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=2882.70 +/- 239.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.009175975 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 145         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 266         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 200      |
|    iterations      | 63       |
|    time_elapsed    | 5139     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.11e+03    |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 64          |
|    time_elapsed         | 5206        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.009247547 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.1        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 249         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=2762.45 +/- 294.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.76e+03     |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0068642846 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0003       |
|    loss                 | 163          |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00661     |
|    value_loss           | 667          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 201      |
|    iterations      | 65       |
|    time_elapsed    | 5285     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=2422.64 +/- 1159.82
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 1075000      |
| train/                  |              |
|    approx_kl            | 0.0084348135 |
|    clip_fraction        | 0.0779       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 231          |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00913     |
|    value_loss           | 326          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 201      |
|    iterations      | 66       |
|    time_elapsed    | 5364     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 67          |
|    time_elapsed         | 5430        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.009557853 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.4        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 241         |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=2882.42 +/- 240.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.008017034 |
|    clip_fraction        | 0.0579      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 207         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 449         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 202      |
|    iterations      | 68       |
|    time_elapsed    | 5509     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=3002.65 +/- 0.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 1125000    |
| train/                  |            |
|    approx_kl            | 0.00984418 |
|    clip_fraction        | 0.0986     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 82         |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.00622   |
|    value_loss           | 181        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 202      |
|    iterations      | 69       |
|    time_elapsed    | 5588     |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.31e+03   |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 70         |
|    time_elapsed         | 5655       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.01016986 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.99      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 70.1       |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00872   |
|    value_loss           | 224        |
----------------------------------------
Eval num_timesteps=1150000, episode_reward=3002.60 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.012291308 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.3        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 251         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 202      |
|    iterations      | 71       |
|    time_elapsed    | 5734     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=3002.39 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.010574171 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 92          |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 273         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 202      |
|    iterations      | 72       |
|    time_elapsed    | 5812     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.35e+03    |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 73          |
|    time_elapsed         | 5879        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.009263161 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 95          |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 278         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=3002.55 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.009073821 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 148         |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 366         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 203      |
|    iterations      | 74       |
|    time_elapsed    | 5958     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=2302.76 +/- 1123.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.010217901 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 240         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 203      |
|    iterations      | 75       |
|    time_elapsed    | 6036     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.42e+03    |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 76          |
|    time_elapsed         | 6102        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.008479229 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 325         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=3002.64 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.010242883 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 280         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 204      |
|    iterations      | 77       |
|    time_elapsed    | 6181     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=3002.41 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.006920685 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.2        |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 257         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 204      |
|    iterations      | 78       |
|    time_elapsed    | 6260     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.66e+03    |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 79          |
|    time_elapsed         | 6326        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.008251346 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.6        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 159         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=2882.52 +/- 239.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0120441625 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 141          |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00796     |
|    value_loss           | 148          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    fps             | 204      |
|    iterations      | 80       |
|    time_elapsed    | 6404     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=2882.70 +/- 239.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.009759577 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 93          |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 203         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 204      |
|    iterations      | 81       |
|    time_elapsed    | 6481     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 82          |
|    time_elapsed         | 6545        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.007497024 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=2882.77 +/- 239.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.008425029 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 218         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 243         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 205      |
|    iterations      | 83       |
|    time_elapsed    | 6622     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=3002.36 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.009241387 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 44.8        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    fps             | 205      |
|    iterations      | 84       |
|    time_elapsed    | 6699     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.71e+03    |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 85          |
|    time_elapsed         | 6764        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.008107897 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 277         |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=3002.62 +/- 0.65
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1400000      |
| train/                  |              |
|    approx_kl            | 0.0098161865 |
|    clip_fraction        | 0.0891       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 149          |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00636     |
|    value_loss           | 273          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    fps             | 205      |
|    iterations      | 86       |
|    time_elapsed    | 6841     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=2882.55 +/- 240.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 1425000      |
| train/                  |              |
|    approx_kl            | 0.0079890825 |
|    clip_fraction        | 0.07         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.41        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.0003       |
|    loss                 | 194          |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00684     |
|    value_loss           | 429          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.59e+03 |
| time/              |          |
|    fps             | 206      |
|    iterations      | 87       |
|    time_elapsed    | 6917     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 88          |
|    time_elapsed         | 6982        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.007606256 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 151         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 305         |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=3002.63 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.008920796 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 343         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 206      |
|    iterations      | 89       |
|    time_elapsed    | 7059     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 90          |
|    time_elapsed         | 7124        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.008441171 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 299         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=3002.35 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.008732945 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 278         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 207      |
|    iterations      | 91       |
|    time_elapsed    | 7201     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=3002.49 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.010877064 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 44.9        |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 327         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    fps             | 207      |
|    iterations      | 92       |
|    time_elapsed    | 7278     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.64e+03    |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 93          |
|    time_elapsed         | 7342        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.007921882 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00738    |
|    value_loss           | 367         |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=2422.67 +/- 1159.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.008001488 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.7        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 207      |
|    iterations      | 94       |
|    time_elapsed    | 7419     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2443.77 +/- 1117.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 1550000      |
| train/                  |              |
|    approx_kl            | 0.0064761974 |
|    clip_fraction        | 0.0612       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.9         |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 164          |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 299          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    fps             | 207      |
|    iterations      | 95       |
|    time_elapsed    | 7496     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.88e+03    |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 96          |
|    time_elapsed         | 7561        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.009154249 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 251         |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=2558.17 +/- 888.86
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.56e+03   |
| time/                   |            |
|    total_timesteps      | 1575000    |
| train/                  |            |
|    approx_kl            | 0.01147955 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.74      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.4       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00637   |
|    value_loss           | 109        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 97       |
|    time_elapsed    | 7638     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=2977.50 +/- 49.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.98e+03    |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.009570943 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 209         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 266         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 98       |
|    time_elapsed    | 7715     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.09e+03   |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 99         |
|    time_elapsed         | 7780       |
|    total_timesteps      | 1622016    |
| train/                  |            |
|    approx_kl            | 0.00992757 |
|    clip_fraction        | 0.0806     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.76      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.6       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.00739   |
|    value_loss           | 250        |
----------------------------------------
Eval num_timesteps=1625000, episode_reward=3002.31 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.007379159 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.1        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 185         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 100      |
|    time_elapsed    | 7857     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=3002.44 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.008691326 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.6        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 101      |
|    time_elapsed    | 7934     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 102         |
|    time_elapsed         | 7998        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.008654712 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.6        |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 161         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=3002.77 +/- 0.61
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1675000      |
| train/                  |              |
|    approx_kl            | 0.0059507107 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.4         |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0003       |
|    loss                 | 173          |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00452     |
|    value_loss           | 573          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 103      |
|    time_elapsed    | 8076     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=3002.45 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0102630295 |
|    clip_fraction        | 0.0928       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | 50.4         |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00482     |
|    value_loss           | 327          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 104      |
|    time_elapsed    | 8153     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 105         |
|    time_elapsed         | 8217        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.007303601 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 215         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 187         |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=3002.29 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.005327682 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 48.5        |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 202         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 209      |
|    iterations      | 106      |
|    time_elapsed    | 8294     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=3002.67 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.012178909 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.4        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 252         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 209      |
|    iterations      | 107      |
|    time_elapsed    | 8371     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.2e+03     |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 108         |
|    time_elapsed         | 8436        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.019574795 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=3002.50 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 1775000    |
| train/                  |            |
|    approx_kl            | 0.00744088 |
|    clip_fraction        | 0.0745     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 69.6       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00458   |
|    value_loss           | 134        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 209      |
|    iterations      | 109      |
|    time_elapsed    | 8513     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=3002.47 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1800000      |
| train/                  |              |
|    approx_kl            | 0.0058731027 |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 60.5         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00437     |
|    value_loss           | 150          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 209      |
|    iterations      | 110      |
|    time_elapsed    | 8590     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.46e+03    |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 111         |
|    time_elapsed         | 8654        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.008039944 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.9        |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 173         |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=3002.83 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.010183748 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 77.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 112      |
|    time_elapsed    | 8731     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=3002.40 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.006916412 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 115         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 113      |
|    time_elapsed    | 8808     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 114         |
|    time_elapsed         | 8873        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.010348776 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.5        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 86.2        |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=3002.51 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1875000      |
| train/                  |              |
|    approx_kl            | 0.0056020813 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.5         |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 90           |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 105          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 115      |
|    time_elapsed    | 8950     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=3002.50 +/- 0.38
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1900000      |
| train/                  |              |
|    approx_kl            | 0.0075426376 |
|    clip_fraction        | 0.0655       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.52        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.6         |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 148          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 116      |
|    time_elapsed    | 9027     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.4e+03     |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 117         |
|    time_elapsed         | 9091        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.008813514 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.3        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=3002.53 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1925000      |
| train/                  |              |
|    approx_kl            | 0.0068894792 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 21.3         |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00366     |
|    value_loss           | 117          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 118      |
|    time_elapsed    | 9168     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.48e+03    |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 119         |
|    time_elapsed         | 9233        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.009514645 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.1        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 93.6        |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=2422.71 +/- 1159.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 1950000    |
| train/                  |            |
|    approx_kl            | 0.00996048 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.7       |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.00407   |
|    value_loss           | 135        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 120      |
|    time_elapsed    | 9310     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=3002.53 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.013750652 |
|    clip_fraction        | 0.0916      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.56        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 96.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 121      |
|    time_elapsed    | 9387     |
|    total_timesteps | 1982464  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.56e+03  |
| time/                   |           |
|    fps                  | 211       |
|    iterations           | 122       |
|    time_elapsed         | 9452      |
|    total_timesteps      | 1998848   |
| train/                  |           |
|    approx_kl            | 0.0172073 |
|    clip_fraction        | 0.129     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.45     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 9.29      |
|    n_updates            | 1210      |
|    policy_gradient_loss | -0.000761 |
|    value_loss           | 31.8      |
---------------------------------------
Eval num_timesteps=2000000, episode_reward=2423.14 +/- 1159.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.01362906 |
|    clip_fraction        | 0.0941     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.00116   |
|    value_loss           | 49.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 123      |
|    time_elapsed    | 9529     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=3002.81 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2025000      |
| train/                  |              |
|    approx_kl            | 0.0034349049 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.0003       |
|    loss                 | 74.1         |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 366          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 124      |
|    time_elapsed    | 9606     |
|    total_timesteps | 2031616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.07e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 125          |
|    time_elapsed         | 9670         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0077245156 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | 64.8         |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00588     |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=2050000, episode_reward=3002.36 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 0.0064691976 |
|    clip_fraction        | 0.0604       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 345          |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00502     |
|    value_loss           | 257          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 126      |
|    time_elapsed    | 9747     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=3002.35 +/- 0.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0073676305 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.826       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 30.2         |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 40.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 127      |
|    time_elapsed    | 9824     |
|    total_timesteps | 2080768  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.55e+03     |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 128          |
|    time_elapsed         | 9889         |
|    total_timesteps      | 2097152      |
| train/                  |              |
|    approx_kl            | 0.0077197864 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.865       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 86.1         |
------------------------------------------
Eval num_timesteps=2100000, episode_reward=3002.26 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.012152733 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 97.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 212      |
|    iterations      | 129      |
|    time_elapsed    | 9966     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=3002.68 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2125000    |
| train/                  |            |
|    approx_kl            | 0.00841506 |
|    clip_fraction        | 0.0688     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.859     |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.84       |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.00303   |
|    value_loss           | 48.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 130      |
|    time_elapsed    | 10043    |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 131         |
|    time_elapsed         | 10108       |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.008827863 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.9        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=3002.56 +/- 0.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2150000    |
| train/                  |            |
|    approx_kl            | 0.02158169 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.00141   |
|    value_loss           | 7.74       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 132      |
|    time_elapsed    | 10184    |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=3002.73 +/- 0.42
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2175000    |
| train/                  |            |
|    approx_kl            | 0.00903558 |
|    clip_fraction        | 0.0753     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.5       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.000607  |
|    value_loss           | 90.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 133      |
|    time_elapsed    | 10261    |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.57e+03    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 134         |
|    time_elapsed         | 10326       |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.026124561 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.624       |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 4.02        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=3002.39 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.002734299 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 312         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 135      |
|    time_elapsed    | 10403    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=3002.66 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.004159213 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.979      |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 532         |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 574         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 136      |
|    time_elapsed    | 10480    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.89e+03    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 137         |
|    time_elapsed         | 10546       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.018116862 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.4        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=3002.77 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0049531944 |
|    clip_fraction        | 0.0447       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.752       |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 33.2         |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 204          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 138      |
|    time_elapsed    | 10623    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=3002.66 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.008922914 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.3        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 346         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 139      |
|    time_elapsed    | 10700    |
|    total_timesteps | 2277376  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.33e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 140          |
|    time_elapsed         | 10765        |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0043026833 |
|    clip_fraction        | 0.0442       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.633       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.5         |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 88.7         |
------------------------------------------
Eval num_timesteps=2300000, episode_reward=3002.53 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.009339104 |
|    clip_fraction        | 0.0666      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.4        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 141      |
|    time_elapsed    | 10843    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=3002.35 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.007333402 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.5        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 265         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 142      |
|    time_elapsed    | 10920    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 143         |
|    time_elapsed         | 10985       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.005113991 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00458    |
|    value_loss           | 177         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=3002.43 +/- 0.23
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2350000    |
| train/                  |            |
|    approx_kl            | 0.01458741 |
|    clip_fraction        | 0.0776     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.5       |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.000986  |
|    value_loss           | 17.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 144      |
|    time_elapsed    | 11062    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=3002.46 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.010381542 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 2.94        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 145      |
|    time_elapsed    | 11140    |
|    total_timesteps | 2375680  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 146          |
|    time_elapsed         | 11205        |
|    total_timesteps      | 2392064      |
| train/                  |              |
|    approx_kl            | 0.0056131976 |
|    clip_fraction        | 0.0577       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.5          |
|    n_updates            | 1450         |
|    policy_gradient_loss | 0.00162      |
|    value_loss           | 124          |
------------------------------------------
Eval num_timesteps=2400000, episode_reward=1270.84 +/- 3463.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.27e+03   |
| time/                   |            |
|    total_timesteps      | 2400000    |
| train/                  |            |
|    approx_kl            | 0.20257373 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.581      |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0308    |
|    value_loss           | 12         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 213      |
|    iterations      | 147      |
|    time_elapsed    | 11283    |
|    total_timesteps | 2408448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.47e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 148          |
|    time_elapsed         | 11348        |
|    total_timesteps      | 2424832      |
| train/                  |              |
|    approx_kl            | 0.0009532809 |
|    clip_fraction        | 0.00399      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0.576        |
|    learning_rate        | 0.0003       |
|    loss                 | 74.4         |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.000539    |
|    value_loss           | 907          |
------------------------------------------
Eval num_timesteps=2425000, episode_reward=3002.58 +/- 0.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2425000      |
| train/                  |              |
|    approx_kl            | 0.0019492976 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0.647        |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 587          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 149      |
|    time_elapsed    | 11426    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=3002.48 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2450000      |
| train/                  |              |
|    approx_kl            | 0.0013329298 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.375       |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.0003       |
|    loss                 | 235          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 907          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 150      |
|    time_elapsed    | 11503    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 616         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 151         |
|    time_elapsed         | 11568       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.019425105 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 356         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 591         |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=3002.26 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.007153969 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 395         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 152      |
|    time_elapsed    | 11646    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=3002.48 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.020434309 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 45          |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 330         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 153      |
|    time_elapsed    | 11724    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 154         |
|    time_elapsed         | 11789       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.011184884 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00458    |
|    value_loss           | 321         |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=3002.66 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.010193687 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.6        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 155      |
|    time_elapsed    | 11866    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=3002.42 +/- 0.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2550000      |
| train/                  |              |
|    approx_kl            | 0.0066510052 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 12.6         |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.000799    |
|    value_loss           | 13.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 156      |
|    time_elapsed    | 11943    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.89e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 157         |
|    time_elapsed         | 12008       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.008151829 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 15.9        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=2642.43 +/- 294.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.64e+03   |
| time/                   |            |
|    total_timesteps      | 2575000    |
| train/                  |            |
|    approx_kl            | 0.10697422 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.362     |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.282      |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 10.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 158      |
|    time_elapsed    | 12085    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2882.49 +/- 240.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 2600000      |
| train/                  |              |
|    approx_kl            | 0.0030402455 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.682        |
|    learning_rate        | 0.0003       |
|    loss                 | 122          |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 473          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 159      |
|    time_elapsed    | 12162    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.06e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 160         |
|    time_elapsed         | 12230       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.004330796 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 187         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 676         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=2062.51 +/- 1006.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06e+03    |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.023658171 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.2        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 434         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 688      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 161      |
|    time_elapsed    | 12310    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2882.27 +/- 239.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 2650000      |
| train/                  |              |
|    approx_kl            | 0.0052875504 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.826       |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.0003       |
|    loss                 | 131          |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 645          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 162      |
|    time_elapsed    | 12389    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 163         |
|    time_elapsed         | 12456       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.011165257 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 28.1        |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=2882.56 +/- 240.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.010469065 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 550         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 164      |
|    time_elapsed    | 12535    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=3002.35 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.008525056 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 271         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 165      |
|    time_elapsed    | 12614    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.39e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 166         |
|    time_elapsed         | 12681       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.011732135 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.64        |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=2422.66 +/- 1159.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.021287102 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.59        |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 8.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 167      |
|    time_elapsed    | 12760    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=3002.49 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.017805483 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.79        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 8.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 168      |
|    time_elapsed    | 12838    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.81e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 169         |
|    time_elapsed         | 12906       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.020582508 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.6         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 5.57        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=2642.54 +/- 294.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.015922703 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.28        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 23.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 170      |
|    time_elapsed    | 12991    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=3002.22 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2800000    |
| train/                  |            |
|    approx_kl            | 0.02183003 |
|    clip_fraction        | 0.0891     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.484     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.523      |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.00871   |
|    value_loss           | 16.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 171      |
|    time_elapsed    | 13074    |
|    total_timesteps | 2801664  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.67e+03   |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 172        |
|    time_elapsed         | 13143      |
|    total_timesteps      | 2818048    |
| train/                  |            |
|    approx_kl            | 0.02519242 |
|    clip_fraction        | 0.0647     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.97       |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.00541   |
|    value_loss           | 56.3       |
----------------------------------------
Eval num_timesteps=2825000, episode_reward=3002.72 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.026127921 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.376      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.949       |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 58.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 173      |
|    time_elapsed    | 13225    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2422.70 +/- 1159.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 2850000      |
| train/                  |              |
|    approx_kl            | 0.0047731223 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.0003       |
|    loss                 | 51.7         |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 128          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 174      |
|    time_elapsed    | 13306    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.68e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 175         |
|    time_elapsed         | 13374       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.025540378 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.45        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 19.8        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=3002.24 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.020814762 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.9        |
|    n_updates            | 1750        |
|    policy_gradient_loss | -2.85e-05   |
|    value_loss           | 39.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 176      |
|    time_elapsed    | 13455    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 177         |
|    time_elapsed         | 13523       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.004350866 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 55.4        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=3002.72 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.018194778 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 4           |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 73.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 178      |
|    time_elapsed    | 13603    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=3002.50 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2925000      |
| train/                  |              |
|    approx_kl            | 0.0021182373 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 44.5         |
|    n_updates            | 1780         |
|    policy_gradient_loss | 0.00389      |
|    value_loss           | 84           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 179      |
|    time_elapsed    | 13684    |
|    total_timesteps | 2932736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.6e+03      |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 180          |
|    time_elapsed         | 13751        |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0071636178 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.066       |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.342        |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.000305    |
|    value_loss           | 4.97         |
------------------------------------------
Eval num_timesteps=2950000, episode_reward=2422.54 +/- 1159.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.003682749 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 148         |
|    n_updates            | 1800        |
|    policy_gradient_loss | 0.00391     |
|    value_loss           | 453         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 181      |
|    time_elapsed    | 13832    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=3002.90 +/- 0.69
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2975000      |
| train/                  |              |
|    approx_kl            | 0.0066934796 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.36         |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00282     |
|    value_loss           | 10           |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 182      |
|    time_elapsed    | 13913    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 183         |
|    time_elapsed         | 13984       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.008088824 |
|    clip_fraction        | 0.037       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.472       |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=3002.71 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.004487877 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.223      |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 77.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 184      |
|    time_elapsed    | 14068    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=3002.47 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.009295744 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.000665   |
|    value_loss           | 52.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 185      |
|    time_elapsed    | 14150    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.81e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 186         |
|    time_elapsed         | 14219       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.006231506 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.77        |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 16.2        |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=-769.33 +/- 3087.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -769       |
| time/                   |            |
|    total_timesteps      | 3050000    |
| train/                  |            |
|    approx_kl            | 0.20444344 |
|    clip_fraction        | 0.413      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 1.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 187      |
|    time_elapsed    | 14300    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=-49.47 +/- 3808.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -49.5       |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.008878504 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0003      |
|    loss                 | 52          |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 722         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 188      |
|    time_elapsed    | 14381    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -828        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 189         |
|    time_elapsed         | 14449       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.011790978 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 157         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 643         |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=3002.65 +/- 0.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3100000      |
| train/                  |              |
|    approx_kl            | 0.0083115045 |
|    clip_fraction        | 0.073        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.0003       |
|    loss                 | 235          |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.0035      |
|    value_loss           | 543          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.7e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 190      |
|    time_elapsed    | 14530    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=1706.58 +/- 2592.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.022377718 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 202         |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 563         |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.27e+03 |
| time/              |           |
|    fps             | 214       |
|    iterations      | 191       |
|    time_elapsed    | 14610     |
|    total_timesteps | 3129344   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -711        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 192         |
|    time_elapsed         | 14678       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.016588956 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.3        |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 300         |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=1253.54 +/- 3497.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25e+03    |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.016806316 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 614         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 251      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 193      |
|    time_elapsed    | 14759    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=3002.45 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3175000      |
| train/                  |              |
|    approx_kl            | 0.0083483085 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.567       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 443          |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.0156      |
|    value_loss           | 957          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 194      |
|    time_elapsed    | 14839    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.83e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 195         |
|    time_elapsed         | 14907       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.013304261 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.4        |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 377         |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=3002.61 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.006656291 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00926    |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 196      |
|    time_elapsed    | 14991    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=3002.67 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.014633415 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 41.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 197      |
|    time_elapsed    | 15074    |
|    total_timesteps | 3227648  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.71e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 198          |
|    time_elapsed         | 15143        |
|    total_timesteps      | 3244032      |
| train/                  |              |
|    approx_kl            | 0.0071907574 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.34         |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.000307    |
|    value_loss           | 27.2         |
------------------------------------------
Eval num_timesteps=3250000, episode_reward=3002.87 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.003997467 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.166      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.2         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.000501   |
|    value_loss           | 27.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 199      |
|    time_elapsed    | 15225    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=3002.47 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.028293936 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.897       |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 10.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 200      |
|    time_elapsed    | 15306    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.92e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 201         |
|    time_elapsed         | 15374       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.007880377 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.000815   |
|    value_loss           | 32.6        |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=3002.45 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.014108641 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.03        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 17.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 202      |
|    time_elapsed    | 15454    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=3002.46 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3325000      |
| train/                  |              |
|    approx_kl            | 0.0046994532 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.173       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 1.59         |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000522    |
|    value_loss           | 2.96         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 203      |
|    time_elapsed    | 15535    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.91e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 204         |
|    time_elapsed         | 15603       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.009790393 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 5.24        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=3003.02 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.002265843 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51        |
|    n_updates            | 2040        |
|    policy_gradient_loss | 0.000142    |
|    value_loss           | 10.4        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 205      |
|    time_elapsed    | 15683    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=3002.35 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3375000      |
| train/                  |              |
|    approx_kl            | 0.0021055995 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.115        |
|    n_updates            | 2050         |
|    policy_gradient_loss | 6.25e-05     |
|    value_loss           | 11.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 206      |
|    time_elapsed    | 15763    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.78e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 207         |
|    time_elapsed         | 15831       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.056385446 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.64        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 35.5        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=3002.35 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3400000    |
| train/                  |            |
|    approx_kl            | 0.18319373 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.312     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.39       |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 10.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 208      |
|    time_elapsed    | 15911    |
|    total_timesteps | 3407872  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 1.46e+03      |
| time/                   |               |
|    fps                  | 214           |
|    iterations           | 209           |
|    time_elapsed         | 15981         |
|    total_timesteps      | 3424256       |
| train/                  |               |
|    approx_kl            | 0.00078674377 |
|    clip_fraction        | 0.00624       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.18         |
|    explained_variance   | 0.797         |
|    learning_rate        | 0.0003        |
|    loss                 | 126           |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.000727     |
|    value_loss           | 526           |
-------------------------------------------
Eval num_timesteps=3425000, episode_reward=2426.71 +/- 1151.57
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.43e+03     |
| time/                   |              |
|    total_timesteps      | 3425000      |
| train/                  |              |
|    approx_kl            | 0.0012969141 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.848        |
|    learning_rate        | 0.0003       |
|    loss                 | 24.8         |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.000494    |
|    value_loss           | 227          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 210      |
|    time_elapsed    | 16066    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=3002.65 +/- 0.56
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3450000      |
| train/                  |              |
|    approx_kl            | 0.0071922825 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0003       |
|    loss                 | 41.6         |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 240          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 211      |
|    time_elapsed    | 16149    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.24e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 212         |
|    time_elapsed         | 16217       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.018583499 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.5        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=2443.75 +/- 1117.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.44e+03   |
| time/                   |            |
|    total_timesteps      | 3475000    |
| train/                  |            |
|    approx_kl            | 0.02776023 |
|    clip_fraction        | 0.0425     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.4       |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0045    |
|    value_loss           | 68.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 213      |
|    time_elapsed    | 16299    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3002.31 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.008654232 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.1        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 129         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 214      |
|    time_elapsed    | 16380    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.99e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 215         |
|    time_elapsed         | 16448       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.019298088 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.1        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 270         |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=3002.23 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.006063869 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.329      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 35          |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 71.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 216      |
|    time_elapsed    | 16529    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=2422.64 +/- 1159.59
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 3550000      |
| train/                  |              |
|    approx_kl            | 0.0070424937 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.14         |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 51.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 217      |
|    time_elapsed    | 16609    |
|    total_timesteps | 3555328  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.54e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 218          |
|    time_elapsed         | 16677        |
|    total_timesteps      | 3571712      |
| train/                  |              |
|    approx_kl            | 0.0020690302 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.9         |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00038     |
|    value_loss           | 46           |
------------------------------------------
Eval num_timesteps=3575000, episode_reward=3002.63 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.011179353 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.4        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00364    |
|    value_loss           | 81.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 219      |
|    time_elapsed    | 16758    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=3002.43 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3600000      |
| train/                  |              |
|    approx_kl            | 0.0052141547 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.06         |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.000173    |
|    value_loss           | 17.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 220      |
|    time_elapsed    | 16838    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.71e+03   |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 221        |
|    time_elapsed         | 16905      |
|    total_timesteps      | 3620864    |
| train/                  |            |
|    approx_kl            | 0.14376229 |
|    clip_fraction        | 0.084      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.169     |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.76       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.00822   |
|    value_loss           | 2.02       |
----------------------------------------
Eval num_timesteps=3625000, episode_reward=1252.54 +/- 3499.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25e+03    |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.004442297 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 622         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 650      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 222      |
|    time_elapsed    | 16988    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=687.81 +/- 3407.94
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 688          |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0039697792 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.0003       |
|    loss                 | 92           |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 597          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -760     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 223      |
|    time_elapsed    | 17073    |
|    total_timesteps | 3653632  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -874         |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 224          |
|    time_elapsed         | 17142        |
|    total_timesteps      | 3670016      |
| train/                  |              |
|    approx_kl            | 0.0043221917 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.0003       |
|    loss                 | 67.4         |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 398          |
------------------------------------------
Eval num_timesteps=3675000, episode_reward=1706.55 +/- 2591.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.004395867 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.367      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.1        |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -242     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 225      |
|    time_elapsed    | 17224    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=1706.84 +/- 2591.75
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 3700000      |
| train/                  |              |
|    approx_kl            | 0.0054970374 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0003       |
|    loss                 | 41.4         |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.00266     |
|    value_loss           | 286          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 226      |
|    time_elapsed    | 17306    |
|    total_timesteps | 3702784  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 753          |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 227          |
|    time_elapsed         | 17374        |
|    total_timesteps      | 3719168      |
| train/                  |              |
|    approx_kl            | 0.0074299555 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 347          |
------------------------------------------
Eval num_timesteps=3725000, episode_reward=3002.46 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.008465675 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.3        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 228      |
|    time_elapsed    | 17455    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=3002.96 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3750000      |
| train/                  |              |
|    approx_kl            | 0.0050499653 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.308       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 51.8         |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00327     |
|    value_loss           | 256          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 229      |
|    time_elapsed    | 17536    |
|    total_timesteps | 3751936  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.71e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 230          |
|    time_elapsed         | 17604        |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 0.0057494054 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.291       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 77.9         |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 232          |
------------------------------------------
Eval num_timesteps=3775000, episode_reward=3002.44 +/- 0.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3775000      |
| train/                  |              |
|    approx_kl            | 0.0075917686 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 98.5         |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 173          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 231      |
|    time_elapsed    | 17684    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=2423.18 +/- 1159.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.004824576 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 46.2        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.000429   |
|    value_loss           | 71          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 232      |
|    time_elapsed    | 17765    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.38e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 233         |
|    time_elapsed         | 17833       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.004422683 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.2        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 32.1        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=2423.36 +/- 1158.96
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 3825000      |
| train/                  |              |
|    approx_kl            | 0.0016672467 |
|    clip_fraction        | 0.00895      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0689      |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.22         |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 46.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 234      |
|    time_elapsed    | 17913    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=3002.67 +/- 0.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3850000      |
| train/                  |              |
|    approx_kl            | 0.0027390784 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.36         |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.000532    |
|    value_loss           | 16           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 235      |
|    time_elapsed    | 17994    |
|    total_timesteps | 3850240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.9e+03      |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 236          |
|    time_elapsed         | 18067        |
|    total_timesteps      | 3866624      |
| train/                  |              |
|    approx_kl            | 0.0062477486 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0918      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.48         |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 21           |
------------------------------------------
Eval num_timesteps=3875000, episode_reward=3002.32 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3875000      |
| train/                  |              |
|    approx_kl            | 0.0042766747 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.13         |
|    n_updates            | 2360         |
|    policy_gradient_loss | 0.000195     |
|    value_loss           | 7.62         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 237      |
|    time_elapsed    | 18150    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.85e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 238         |
|    time_elapsed         | 18219       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.003855187 |
|    clip_fraction        | 0.00588     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0646     |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.79        |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.000487   |
|    value_loss           | 33.3        |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=2443.09 +/- 1118.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.006783737 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.18        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 239      |
|    time_elapsed    | 18301    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=3002.53 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3925000    |
| train/                  |            |
|    approx_kl            | 0.10887489 |
|    clip_fraction        | 0.0906     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 12.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 240      |
|    time_elapsed    | 18382    |
|    total_timesteps | 3932160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 613          |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 241          |
|    time_elapsed         | 18450        |
|    total_timesteps      | 3948544      |
| train/                  |              |
|    approx_kl            | 0.0015961243 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.0003       |
|    loss                 | 319          |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=3950000, episode_reward=3002.43 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.002745607 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 650         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -578     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 242      |
|    time_elapsed    | 18531    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=3002.40 +/- 0.70
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3975000      |
| train/                  |              |
|    approx_kl            | 0.0020292217 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.292       |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.0003       |
|    loss                 | 365          |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 662          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 213      |
|    iterations      | 243      |
|    time_elapsed    | 18611    |
|    total_timesteps | 3981312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -578         |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 244          |
|    time_elapsed         | 18679        |
|    total_timesteps      | 3997696      |
| train/                  |              |
|    approx_kl            | 0.0016230261 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.25        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.0003       |
|    loss                 | 200          |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 454          |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=3002.67 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.006151045 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.413      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.5        |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 469         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 245      |
|    time_elapsed    | 18759    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=3002.73 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 4025000    |
| train/                  |            |
|    approx_kl            | 0.02020661 |
|    clip_fraction        | 0.0481     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.372     |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 294        |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.00669   |
|    value_loss           | 412        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 570      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 246      |
|    time_elapsed    | 18840    |
|    total_timesteps | 4030464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.22e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 247          |
|    time_elapsed         | 18908        |
|    total_timesteps      | 4046848      |
| train/                  |              |
|    approx_kl            | 0.0053582396 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 42.4         |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00408     |
|    value_loss           | 294          |
------------------------------------------
Eval num_timesteps=4050000, episode_reward=3002.63 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.008083715 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 248      |
|    time_elapsed    | 18988    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=3002.41 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.010213036 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.9        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 54.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 249      |
|    time_elapsed    | 19072    |
|    total_timesteps | 4079616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.75e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 250          |
|    time_elapsed         | 19142        |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0049335007 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.4         |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 23.8         |
------------------------------------------
Eval num_timesteps=4100000, episode_reward=3002.79 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.005528258 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0923     |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 20.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 251      |
|    time_elapsed    | 19225    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=3002.92 +/- 0.81
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4125000      |
| train/                  |              |
|    approx_kl            | 0.0032193307 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.78         |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.000431    |
|    value_loss           | 46.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 252      |
|    time_elapsed    | 19307    |
|    total_timesteps | 4128768  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.84e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 253          |
|    time_elapsed         | 19375        |
|    total_timesteps      | 4145152      |
| train/                  |              |
|    approx_kl            | 0.0019929716 |
|    clip_fraction        | 0.00844      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.78         |
|    n_updates            | 2520         |
|    policy_gradient_loss | 0.000116     |
|    value_loss           | 12.1         |
------------------------------------------
Eval num_timesteps=4150000, episode_reward=3002.17 +/- 0.12
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 4150000       |
| train/                  |               |
|    approx_kl            | 0.00023062127 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0826       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.45          |
|    n_updates            | 2530          |
|    policy_gradient_loss | -0.00027      |
|    value_loss           | 31.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 254      |
|    time_elapsed    | 19456    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2422.71 +/- 1159.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 4175000    |
| train/                  |            |
|    approx_kl            | 0.47173822 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.231     |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 3.72       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 213      |
|    iterations      | 255      |
|    time_elapsed    | 19537    |
|    total_timesteps | 4177920  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.04e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 256          |
|    time_elapsed         | 19605        |
|    total_timesteps      | 4194304      |
| train/                  |              |
|    approx_kl            | 0.0026554498 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.0003       |
|    loss                 | 194          |
|    n_updates            | 2550         |
|    policy_gradient_loss | -7.15e-05    |
|    value_loss           | 763          |
------------------------------------------
Eval num_timesteps=4200000, episode_reward=1822.57 +/- 2359.71
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.82e+03     |
| time/                   |              |
|    total_timesteps      | 4200000      |
| train/                  |              |
|    approx_kl            | 0.0042311307 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.299       |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0003       |
|    loss                 | 520          |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000649    |
|    value_loss           | 1.49e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 257      |
|    time_elapsed    | 19686    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=1706.55 +/- 2592.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.007019213 |
|    clip_fraction        | 0.037       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 693         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -841     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 258      |
|    time_elapsed    | 19766    |
|    total_timesteps | 4227072  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -209         |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 259          |
|    time_elapsed         | 19834        |
|    total_timesteps      | 4243456      |
| train/                  |              |
|    approx_kl            | 0.0039185113 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.288       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.4         |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 327          |
------------------------------------------
Eval num_timesteps=4250000, episode_reward=1126.52 +/- 2561.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.13e+03     |
| time/                   |              |
|    total_timesteps      | 4250000      |
| train/                  |              |
|    approx_kl            | 0.0025442317 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 54.7         |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 446          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 260      |
|    time_elapsed    | 19915    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=410.37 +/- 3174.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 4275000      |
| train/                  |              |
|    approx_kl            | 0.0019846545 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 202          |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 506          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 261      |
|    time_elapsed    | 19995    |
|    total_timesteps | 4276224  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 640          |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 262          |
|    time_elapsed         | 20064        |
|    total_timesteps      | 4292608      |
| train/                  |              |
|    approx_kl            | 0.0056234407 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 144          |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 528          |
------------------------------------------
Eval num_timesteps=4300000, episode_reward=2422.53 +/- 1159.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.003558057 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.9        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -3.36e-05   |
|    value_loss           | 253         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 263      |
|    time_elapsed    | 20149    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=3002.73 +/- 0.18
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4325000      |
| train/                  |              |
|    approx_kl            | 0.0028197658 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.24        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 120          |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 398          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 264      |
|    time_elapsed    | 20231    |
|    total_timesteps | 4325376  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.11e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 265          |
|    time_elapsed         | 20300        |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0043515004 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=4350000, episode_reward=3002.54 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4350000      |
| train/                  |              |
|    approx_kl            | 0.0075874156 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.289       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 43.6         |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.000673    |
|    value_loss           | 266          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 213      |
|    iterations      | 266      |
|    time_elapsed    | 20381    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.36e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 267         |
|    time_elapsed         | 20450       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.002939941 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 392         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=3002.58 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4375000      |
| train/                  |              |
|    approx_kl            | 0.0037405514 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 205          |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 167          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 268      |
|    time_elapsed    | 20530    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=3002.38 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.001835217 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0992     |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 269      |
|    time_elapsed    | 20611    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.35e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 270         |
|    time_elapsed         | 20679       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.004213297 |
|    clip_fraction        | 0.0209      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.5        |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 91          |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=3002.45 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4425000      |
| train/                  |              |
|    approx_kl            | 0.0009656452 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0849      |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.74         |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.000643    |
|    value_loss           | 31.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 271      |
|    time_elapsed    | 20760    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=3002.64 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4450000      |
| train/                  |              |
|    approx_kl            | 0.0071359724 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 363          |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00705     |
|    value_loss           | 1.65e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 272      |
|    time_elapsed    | 20840    |
|    total_timesteps | 4456448  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 600            |
|    ep_rew_mean          | 2.74e+03       |
| time/                   |                |
|    fps                  | 213            |
|    iterations           | 273            |
|    time_elapsed         | 20908          |
|    total_timesteps      | 4472832        |
| train/                  |                |
|    approx_kl            | 0.000100324905 |
|    clip_fraction        | 0.000995       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0271        |
|    explained_variance   | 0.971          |
|    learning_rate        | 0.0003         |
|    loss                 | 23.2           |
|    n_updates            | 2720           |
|    policy_gradient_loss | -5.01e-05      |
|    value_loss           | 25             |
--------------------------------------------
Eval num_timesteps=4475000, episode_reward=3002.97 +/- 0.49
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 4475000       |
| train/                  |               |
|    approx_kl            | 0.00062128424 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0191       |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.0003        |
|    loss                 | 2.38          |
|    n_updates            | 2730          |
|    policy_gradient_loss | -0.000712     |
|    value_loss           | 33.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 274      |
|    time_elapsed    | 20988    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=3002.48 +/- 0.42
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 4500000       |
| train/                  |               |
|    approx_kl            | 8.7795735e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.17          |
|    n_updates            | 2740          |
|    policy_gradient_loss | -8.88e-05     |
|    value_loss           | 23            |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 275      |
|    time_elapsed    | 21068    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.43e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 276         |
|    time_elapsed         | 21139       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.067912236 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0855     |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 4.27        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=3002.46 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.013174122 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 145         |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 753         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 277      |
|    time_elapsed    | 21222    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=2422.75 +/- 1159.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.012718198 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 261         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 278      |
|    time_elapsed    | 21304    |
|    total_timesteps | 4554752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.24e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 279          |
|    time_elapsed         | 21373        |
|    total_timesteps      | 4571136      |
| train/                  |              |
|    approx_kl            | 0.0050364872 |
|    clip_fraction        | 0.0528       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.9         |
|    n_updates            | 2780         |
|    policy_gradient_loss | 0.000105     |
|    value_loss           | 54.7         |
------------------------------------------
Eval num_timesteps=4575000, episode_reward=3002.62 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.019603273 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.9        |
|    n_updates            | 2790        |
|    policy_gradient_loss | 9.04e-05    |
|    value_loss           | 58.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 280      |
|    time_elapsed    | 21455    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=2422.87 +/- 1159.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 4600000    |
| train/                  |            |
|    approx_kl            | 0.06581392 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.529     |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06e+03   |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0244    |
|    value_loss           | 17.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 281      |
|    time_elapsed    | 21536    |
|    total_timesteps | 4603904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.56e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 282          |
|    time_elapsed         | 21604        |
|    total_timesteps      | 4620288      |
| train/                  |              |
|    approx_kl            | 0.0024089373 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.305       |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.0003       |
|    loss                 | 155          |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.000602    |
|    value_loss           | 407          |
------------------------------------------
Eval num_timesteps=4625000, episode_reward=2443.37 +/- 1118.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.002234774 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.1        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 304         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 283      |
|    time_elapsed    | 21685    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=3002.24 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4650000      |
| train/                  |              |
|    approx_kl            | 0.0046704113 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.353       |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0003       |
|    loss                 | 65.5         |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 367          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 973      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 284      |
|    time_elapsed    | 21765    |
|    total_timesteps | 4653056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 943          |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 285          |
|    time_elapsed         | 21833        |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0067766514 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.000214    |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=4675000, episode_reward=2882.56 +/- 239.79
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 4675000      |
| train/                  |              |
|    approx_kl            | 0.0060038795 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 136          |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 289          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 286      |
|    time_elapsed    | 21913    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2423.00 +/- 1158.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.004824794 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 161         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 287      |
|    time_elapsed    | 21994    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.55e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 288         |
|    time_elapsed         | 22061       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.008220745 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.2        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 121         |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=3002.56 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.004922646 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.2        |
|    n_updates            | 2880        |
|    policy_gradient_loss | 0.000358    |
|    value_loss           | 61.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 289      |
|    time_elapsed    | 22141    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=3002.25 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.004140426 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.5        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 213      |
|    iterations      | 290      |
|    time_elapsed    | 22221    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.81e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 291         |
|    time_elapsed         | 22289       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.005847626 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 83.6        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=3002.53 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.003037009 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.8        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 292      |
|    time_elapsed    | 22368    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=3002.26 +/- 0.18
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4800000      |
| train/                  |              |
|    approx_kl            | 0.0038432456 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.432       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 120          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 293      |
|    time_elapsed    | 22449    |
|    total_timesteps | 4800512  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.37e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 294          |
|    time_elapsed         | 22517        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 0.0037238284 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.2         |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=4825000, episode_reward=3002.66 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4825000      |
| train/                  |              |
|    approx_kl            | 0.0049868836 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.884        |
|    n_updates            | 2940         |
|    policy_gradient_loss | 0.00108      |
|    value_loss           | 49.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 295      |
|    time_elapsed    | 22596    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.62e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 296         |
|    time_elapsed         | 22665       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.002076792 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.76        |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.000157   |
|    value_loss           | 68.4        |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=3002.76 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.002460652 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -9.26e-05   |
|    value_loss           | 51.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 297      |
|    time_elapsed    | 22747    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=2422.70 +/- 1159.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.004033989 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 77.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 298      |
|    time_elapsed    | 22828    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 299         |
|    time_elapsed         | 22896       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.016591854 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.99        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 33.4        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=3002.49 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.047804505 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 36.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 300      |
|    time_elapsed    | 22979    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=2422.81 +/- 1159.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.018905515 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.2        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.000749   |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    fps             | 213      |
|    iterations      | 301      |
|    time_elapsed    | 23061    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 302         |
|    time_elapsed         | 23130       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.006904966 |
|    clip_fraction        | 0.0372      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 42.7        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=3002.35 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4950000      |
| train/                  |              |
|    approx_kl            | 0.0058297375 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.262       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 31.7         |
|    n_updates            | 3020         |
|    policy_gradient_loss | 0.0003       |
|    value_loss           | 62.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 303      |
|    time_elapsed    | 23212    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=3002.90 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4975000      |
| train/                  |              |
|    approx_kl            | 0.0043508336 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.23         |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 54.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.24e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 304      |
|    time_elapsed    | 23293    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.46e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 305         |
|    time_elapsed         | 23362       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.004889504 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 183         |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=3002.61 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5000000      |
| train/                  |              |
|    approx_kl            | 0.0040830364 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.92         |
|    n_updates            | 3050         |
|    policy_gradient_loss | 0.00238      |
|    value_loss           | 51.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 306      |
|    time_elapsed    | 23443    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=3002.39 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5025000      |
| train/                  |              |
|    approx_kl            | 0.0036762827 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.05         |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 85           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 307      |
|    time_elapsed    | 23523    |
|    total_timesteps | 5029888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.76e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 308          |
|    time_elapsed         | 23591        |
|    total_timesteps      | 5046272      |
| train/                  |              |
|    approx_kl            | 0.0021763577 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 1.01         |
|    n_updates            | 3070         |
|    policy_gradient_loss | -0.000351    |
|    value_loss           | 5.21         |
------------------------------------------
Eval num_timesteps=5050000, episode_reward=3002.47 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.005876021 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00071    |
|    value_loss           | 34.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 309      |
|    time_elapsed    | 23673    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=3002.21 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.007874111 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0745     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 2.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 310      |
|    time_elapsed    | 23756    |
|    total_timesteps | 5079040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.82e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 311          |
|    time_elapsed         | 23824        |
|    total_timesteps      | 5095424      |
| train/                  |              |
|    approx_kl            | 0.0011286702 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0754      |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.102        |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00041     |
|    value_loss           | 13.8         |
------------------------------------------
Eval num_timesteps=5100000, episode_reward=3002.72 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.008649617 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0543     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.48        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 35.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 312      |
|    time_elapsed    | 23905    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=3002.61 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.016308092 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.316       |
|    n_updates            | 3120        |
|    policy_gradient_loss | 6.86e-05    |
|    value_loss           | 3.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 313      |
|    time_elapsed    | 23987    |
|    total_timesteps | 5128192  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.84e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 314          |
|    time_elapsed         | 24056        |
|    total_timesteps      | 5144576      |
| train/                  |              |
|    approx_kl            | 0.0065057157 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.275        |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.000769    |
|    value_loss           | 7.72         |
------------------------------------------
Eval num_timesteps=5150000, episode_reward=2422.52 +/- 1159.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.007335159 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.82        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.000556   |
|    value_loss           | 8.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 315      |
|    time_elapsed    | 24138    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=3002.42 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.002524681 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0743     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 2.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 316      |
|    time_elapsed    | 24219    |
|    total_timesteps | 5177344  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.7e+03      |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 317          |
|    time_elapsed         | 24288        |
|    total_timesteps      | 5193728      |
| train/                  |              |
|    approx_kl            | 0.0032696081 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.1         |
|    n_updates            | 3160         |
|    policy_gradient_loss | 0.00299      |
|    value_loss           | 86.7         |
------------------------------------------
Eval num_timesteps=5200000, episode_reward=3002.49 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5200000      |
| train/                  |              |
|    approx_kl            | 0.0059728725 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.3         |
|    n_updates            | 3170         |
|    policy_gradient_loss | 0.00418      |
|    value_loss           | 72.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 318      |
|    time_elapsed    | 24370    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=3002.41 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.011916843 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.33        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00588    |
|    value_loss           | 11.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 319      |
|    time_elapsed    | 24451    |
|    total_timesteps | 5226496  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.66e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 320          |
|    time_elapsed         | 24519        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0063116644 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.89         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 54.9         |
------------------------------------------
Eval num_timesteps=5250000, episode_reward=3002.37 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5250000      |
| train/                  |              |
|    approx_kl            | 0.0040337006 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.54         |
|    n_updates            | 3200         |
|    policy_gradient_loss | 0.00249      |
|    value_loss           | 32.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 321      |
|    time_elapsed    | 24600    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=3002.76 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.008633605 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.277      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 87.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 322      |
|    time_elapsed    | 24680    |
|    total_timesteps | 5275648  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.44e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 323          |
|    time_elapsed         | 24747        |
|    total_timesteps      | 5292032      |
| train/                  |              |
|    approx_kl            | 0.0017407566 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.65         |
|    n_updates            | 3220         |
|    policy_gradient_loss | 0.000336     |
|    value_loss           | 35.9         |
------------------------------------------
Eval num_timesteps=5300000, episode_reward=3002.78 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 5300000    |
| train/                  |            |
|    approx_kl            | 0.00893731 |
|    clip_fraction        | 0.0212     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.187     |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 230        |
|    n_updates            | 3230       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 169        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 324      |
|    time_elapsed    | 24827    |
|    total_timesteps | 5308416  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.7e+03      |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 325          |
|    time_elapsed         | 24894        |
|    total_timesteps      | 5324800      |
| train/                  |              |
|    approx_kl            | 0.0023418542 |
|    clip_fraction        | 0.00436      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.18         |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.000618    |
|    value_loss           | 10.7         |
------------------------------------------
Eval num_timesteps=5325000, episode_reward=3002.29 +/- 0.50
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 5325000       |
| train/                  |               |
|    approx_kl            | 0.00070438493 |
|    clip_fraction        | 0.00384       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0233       |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.304         |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.000457     |
|    value_loss           | 23.6          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 326      |
|    time_elapsed    | 24972    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2443.14 +/- 1118.90
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 5350000      |
| train/                  |              |
|    approx_kl            | 0.0006656636 |
|    clip_fraction        | 0.00175      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.436        |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.000165    |
|    value_loss           | 9.73         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 213      |
|    iterations      | 327      |
|    time_elapsed    | 25050    |
|    total_timesteps | 5357568  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.83e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 328          |
|    time_elapsed         | 25116        |
|    total_timesteps      | 5373952      |
| train/                  |              |
|    approx_kl            | 0.0001374459 |
|    clip_fraction        | 0.00192      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0174      |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0427       |
|    n_updates            | 3270         |
|    policy_gradient_loss | -9.94e-05    |
|    value_loss           | 1.04         |
------------------------------------------
Eval num_timesteps=5375000, episode_reward=3002.32 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.007485189 |
|    clip_fraction        | 0.00359     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0153     |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.698       |
|    n_updates            | 3280        |
|    policy_gradient_loss | 0.000359    |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 329      |
|    time_elapsed    | 25193    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=3002.25 +/- 0.10
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5400000      |
| train/                  |              |
|    approx_kl            | 0.0003496082 |
|    clip_fraction        | 0.00208      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0582       |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 1.51         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 330      |
|    time_elapsed    | 25270    |
|    total_timesteps | 5406720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.02e+03   |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 331        |
|    time_elapsed         | 25335      |
|    total_timesteps      | 5423104    |
| train/                  |            |
|    approx_kl            | 0.30899683 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.847      |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.00716   |
|    value_loss           | 9.92       |
----------------------------------------
Eval num_timesteps=5425000, episode_reward=410.72 +/- 3174.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 411          |
| time/                   |              |
|    total_timesteps      | 5425000      |
| train/                  |              |
|    approx_kl            | 0.0038277656 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.306       |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.0003       |
|    loss                 | 66.5         |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.000637    |
|    value_loss           | 430          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 549      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 332      |
|    time_elapsed    | 25412    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2423.39 +/- 1158.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.003615805 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 706         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -802     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 333      |
|    time_elapsed    | 25489    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.03e+03   |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 334         |
|    time_elapsed         | 25554       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.006444863 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 499         |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=3002.50 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.004367238 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 372         |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.21e+03 |
| time/              |           |
|    fps             | 214       |
|    iterations      | 335       |
|    time_elapsed    | 25631     |
|    total_timesteps | 5488640   |
----------------------------------
Eval num_timesteps=5500000, episode_reward=3002.88 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.027338957 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.9        |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 207         |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.71e+03 |
| time/              |           |
|    fps             | 214       |
|    iterations      | 336       |
|    time_elapsed    | 25708     |
|    total_timesteps | 5505024   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.14e+03   |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 337         |
|    time_elapsed         | 25773       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.008090632 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.573      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.8        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 312         |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=3002.71 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.037940957 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.3        |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 224         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -855     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 338      |
|    time_elapsed    | 25850    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=3002.57 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.007550047 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 510         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 339      |
|    time_elapsed    | 25927    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 601         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 340         |
|    time_elapsed         | 25992       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.008673308 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=3002.38 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.012007126 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.314      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 209         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 341      |
|    time_elapsed    | 26069    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=3002.55 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5600000      |
| train/                  |              |
|    approx_kl            | 0.0039131893 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 34.8         |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 70           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 342      |
|    time_elapsed    | 26146    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.52e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 343         |
|    time_elapsed         | 26211       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.008395689 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0369     |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 71.3        |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=2422.89 +/- 1159.20
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 2.42e+03      |
| time/                   |               |
|    total_timesteps      | 5625000       |
| train/                  |               |
|    approx_kl            | 0.00018840638 |
|    clip_fraction        | 0.00135       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0293       |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.54          |
|    n_updates            | 3430          |
|    policy_gradient_loss | 0.000171      |
|    value_loss           | 28.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 344      |
|    time_elapsed    | 26288    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=3002.49 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.004081481 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.056      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.29        |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 54          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 345      |
|    time_elapsed    | 26365    |
|    total_timesteps | 5652480  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.83e+03      |
| time/                   |               |
|    fps                  | 214           |
|    iterations           | 346           |
|    time_elapsed         | 26430         |
|    total_timesteps      | 5668864       |
| train/                  |               |
|    approx_kl            | 0.00016754057 |
|    clip_fraction        | 0.0018        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0304       |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.37          |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.000151     |
|    value_loss           | 42.2          |
-------------------------------------------
Eval num_timesteps=5675000, episode_reward=3002.50 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5675000      |
| train/                  |              |
|    approx_kl            | 0.0001386497 |
|    clip_fraction        | 0.00125      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0247      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.53         |
|    n_updates            | 3460         |
|    policy_gradient_loss | 2.42e-05     |
|    value_loss           | 40.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 347      |
|    time_elapsed    | 26507    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=3002.67 +/- 0.20
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 5700000       |
| train/                  |               |
|    approx_kl            | 0.00041317026 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.027        |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.0003        |
|    loss                 | 4.29          |
|    n_updates            | 3470          |
|    policy_gradient_loss | -0.000335     |
|    value_loss           | 10.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 348      |
|    time_elapsed    | 26584    |
|    total_timesteps | 5701632  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.94e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 349          |
|    time_elapsed         | 26649        |
|    total_timesteps      | 5718016      |
| train/                  |              |
|    approx_kl            | 0.0014711241 |
|    clip_fraction        | 0.0035       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.031       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.51         |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.000936    |
|    value_loss           | 8.6          |
------------------------------------------
Eval num_timesteps=5725000, episode_reward=3002.32 +/- 0.13
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 5725000       |
| train/                  |               |
|    approx_kl            | 0.00027563606 |
|    clip_fraction        | 0.00256       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.03          |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.000186     |
|    value_loss           | 5.49          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 350      |
|    time_elapsed    | 26726    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=3002.52 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.030973762 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.226      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.782       |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 2.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    fps             | 214      |
|    iterations      | 351      |
|    time_elapsed    | 26804    |
|    total_timesteps | 5750784  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 846          |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 352          |
|    time_elapsed         | 26869        |
|    total_timesteps      | 5767168      |
| train/                  |              |
|    approx_kl            | 0.0020234594 |
|    clip_fraction        | 0.00867      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | 77.6         |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 491          |
------------------------------------------
Eval num_timesteps=5775000, episode_reward=2422.79 +/- 1158.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 5775000      |
| train/                  |              |
|    approx_kl            | 0.0040806783 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.5         |
|    n_updates            | 3520         |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 345          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34.6     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 353      |
|    time_elapsed    | 26946    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -539        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 354         |
|    time_elapsed         | 27011       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.015872056 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 304         |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=3002.40 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.003484114 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 207         |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 511         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 355      |
|    time_elapsed    | 27088    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=2422.90 +/- 1159.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 5825000    |
| train/                  |            |
|    approx_kl            | 0.01315663 |
|    clip_fraction        | 0.042      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 84.8       |
|    n_updates            | 3550       |
|    policy_gradient_loss | -0.00472   |
|    value_loss           | 277        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 356      |
|    time_elapsed    | 27165    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.01e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 357         |
|    time_elapsed         | 27230       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.008081865 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 358         |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=3002.25 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.005138621 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.2        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 293         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 358      |
|    time_elapsed    | 27307    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=3002.58 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5875000      |
| train/                  |              |
|    approx_kl            | 0.0028891047 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0725      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.9         |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 89.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 359      |
|    time_elapsed    | 27384    |
|    total_timesteps | 5881856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.77e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 360          |
|    time_elapsed         | 27449        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0009453815 |
|    clip_fraction        | 0.00836      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0824      |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.2         |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.000273    |
|    value_loss           | 49           |
------------------------------------------
Eval num_timesteps=5900000, episode_reward=3002.46 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5900000      |
| train/                  |              |
|    approx_kl            | 0.0016155096 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0944      |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.95         |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.000786    |
|    value_loss           | 32.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 361      |
|    time_elapsed    | 27526    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=3002.72 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5925000      |
| train/                  |              |
|    approx_kl            | 0.0027720195 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0713      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.33         |
|    n_updates            | 3610         |
|    policy_gradient_loss | 5.89e-05     |
|    value_loss           | 18.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 362      |
|    time_elapsed    | 27602    |
|    total_timesteps | 5931008  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.88e+03      |
| time/                   |               |
|    fps                  | 214           |
|    iterations           | 363           |
|    time_elapsed         | 27667         |
|    total_timesteps      | 5947392       |
| train/                  |               |
|    approx_kl            | 0.00057987875 |
|    clip_fraction        | 0.00604       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0529       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.49          |
|    n_updates            | 3620          |
|    policy_gradient_loss | 1.91e-05      |
|    value_loss           | 6.31          |
-------------------------------------------
Eval num_timesteps=5950000, episode_reward=3002.53 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.003050189 |
|    clip_fraction        | 0.00744     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0398     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 15.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 364      |
|    time_elapsed    | 27744    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=3002.17 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5975000      |
| train/                  |              |
|    approx_kl            | 0.0004108507 |
|    clip_fraction        | 0.00299      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0304      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.317        |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.000112    |
|    value_loss           | 2.79         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 365      |
|    time_elapsed    | 27821    |
|    total_timesteps | 5980160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.88e+03   |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 366        |
|    time_elapsed         | 27886      |
|    total_timesteps      | 5996544    |
| train/                  |            |
|    approx_kl            | 0.15397848 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.224     |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.37       |
|    n_updates            | 3650       |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 17         |
----------------------------------------
Eval num_timesteps=6000000, episode_reward=549.82 +/- 3633.79
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 550          |
| time/                   |              |
|    total_timesteps      | 6000000      |
| train/                  |              |
|    approx_kl            | 0.0035144868 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.385       |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.0003       |
|    loss                 | 188          |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 404          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 367      |
|    time_elapsed    | 27963    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=3002.45 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6025000      |
| train/                  |              |
|    approx_kl            | 0.0063898684 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | 20.4         |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 372          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.4     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 368      |
|    time_elapsed    | 28040    |
|    total_timesteps | 6029312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -248       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 369        |
|    time_elapsed         | 28105      |
|    total_timesteps      | 6045696    |
| train/                  |            |
|    approx_kl            | 0.02302258 |
|    clip_fraction        | 0.0323     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 113        |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.00207   |
|    value_loss           | 228        |
----------------------------------------
Eval num_timesteps=6050000, episode_reward=3002.50 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.020279322 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.4        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 276         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 250      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 370      |
|    time_elapsed    | 28182    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=3002.26 +/- 0.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6075000      |
| train/                  |              |
|    approx_kl            | 0.0077129975 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 48.7         |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.00528     |
|    value_loss           | 193          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 371      |
|    time_elapsed    | 28259    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.85e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 372         |
|    time_elapsed         | 28325       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.005538837 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 306         |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=1706.36 +/- 2592.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.011036038 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.3        |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 373      |
|    time_elapsed    | 28402    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=3002.25 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6125000      |
| train/                  |              |
|    approx_kl            | 0.0055405116 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.7         |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.000925    |
|    value_loss           | 63.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 374      |
|    time_elapsed    | 28479    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.79e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 375         |
|    time_elapsed         | 28544       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.003419463 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.4        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.000133   |
|    value_loss           | 82.7        |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=3002.47 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.014455005 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0893     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 9.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 376      |
|    time_elapsed    | 28621    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=2182.73 +/- 1359.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.18e+03   |
| time/                   |            |
|    total_timesteps      | 6175000    |
| train/                  |            |
|    approx_kl            | 0.12651487 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.239     |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.43       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 34         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 377      |
|    time_elapsed    | 28698    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.75e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 378         |
|    time_elapsed         | 28763       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.021853529 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.1        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00156    |
|    value_loss           | 35.8        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=290.45 +/- 3326.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.043743324 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00963    |
|    value_loss           | 12.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 379      |
|    time_elapsed    | 28840    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=3002.26 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6225000      |
| train/                  |              |
|    approx_kl            | 0.0027783127 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.646       |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.0003       |
|    loss                 | 63.3         |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 416          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 380      |
|    time_elapsed    | 28919    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -137        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 381         |
|    time_elapsed         | 28984       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.007559876 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=1024.61 +/- 3955.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.02e+03   |
| time/                   |            |
|    total_timesteps      | 6250000    |
| train/                  |            |
|    approx_kl            | 0.03297033 |
|    clip_fraction        | 0.057      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.784     |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 114        |
|    n_updates            | 3810       |
|    policy_gradient_loss | -0.00402   |
|    value_loss           | 322        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -685     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 382      |
|    time_elapsed    | 29062    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=1586.81 +/- 2832.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.59e+03    |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.006653158 |
|    clip_fraction        | 0.0595      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 295         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -680     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 383      |
|    time_elapsed    | 29140    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -129        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 384         |
|    time_elapsed         | 29206       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.013839158 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.916      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.3        |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 368         |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=2882.42 +/- 240.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 6300000      |
| train/                  |              |
|    approx_kl            | 0.0059607024 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 71.8         |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 342          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 385      |
|    time_elapsed    | 29284    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 873         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 386         |
|    time_elapsed         | 29350       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.014489803 |
|    clip_fraction        | 0.0631      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 74          |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 236         |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=3002.86 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.005061062 |
|    clip_fraction        | 0.0557      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.3        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 387      |
|    time_elapsed    | 29428    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=2423.17 +/- 1159.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.006009773 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 444         |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00975    |
|    value_loss           | 521         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 388      |
|    time_elapsed    | 29507    |
|    total_timesteps | 6356992  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.34e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 389          |
|    time_elapsed         | 29573        |
|    total_timesteps      | 6373376      |
| train/                  |              |
|    approx_kl            | 0.0049966914 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1e+03      |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00408     |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=6375000, episode_reward=3002.32 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6375000      |
| train/                  |              |
|    approx_kl            | 0.0051943306 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.354       |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 3890         |
|    policy_gradient_loss | 7.74e-05     |
|    value_loss           | 41.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 390      |
|    time_elapsed    | 29650    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=3002.40 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6400000      |
| train/                  |              |
|    approx_kl            | 0.0072965533 |
|    clip_fraction        | 0.0635       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.1         |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.000789    |
|    value_loss           | 40.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 391      |
|    time_elapsed    | 29729    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.81e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 392         |
|    time_elapsed         | 29796       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.011135906 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 19          |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=3003.13 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.040067747 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.657       |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 10.8        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 393      |
|    time_elapsed    | 29875    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=3002.34 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.011470057 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 24.5        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.000953   |
|    value_loss           | 68.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 394      |
|    time_elapsed    | 29956    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.79e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 395         |
|    time_elapsed         | 30025       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.017695062 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.38        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 25.7        |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=3002.49 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.012302217 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 396      |
|    time_elapsed    | 30105    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2422.88 +/- 1159.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.015129782 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.343      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.2        |
|    n_updates            | 3960        |
|    policy_gradient_loss | 0.00113     |
|    value_loss           | 72.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 397      |
|    time_elapsed    | 30184    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.68e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 398         |
|    time_elapsed         | 30251       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.008275013 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 21          |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 77.4        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=2422.64 +/- 1159.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.020830035 |
|    clip_fraction        | 0.0567      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.04        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 3.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 399      |
|    time_elapsed    | 30331    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=3002.59 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.010023145 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 400      |
|    time_elapsed    | 30411    |
|    total_timesteps | 6553600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.92e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 401          |
|    time_elapsed         | 30478        |
|    total_timesteps      | 6569984      |
| train/                  |              |
|    approx_kl            | 0.0061254976 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72         |
|    n_updates            | 4000         |
|    policy_gradient_loss | -0.0185      |
|    value_loss           | 6.85         |
------------------------------------------
Eval num_timesteps=6575000, episode_reward=3002.38 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.029782543 |
|    clip_fraction        | 0.0561      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 4010        |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 2.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 402      |
|    time_elapsed    | 30557    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2443.88 +/- 1117.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.003405718 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 232         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 573         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 403      |
|    time_elapsed    | 30637    |
|    total_timesteps | 6602752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 416          |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 404          |
|    time_elapsed         | 30703        |
|    total_timesteps      | 6619136      |
| train/                  |              |
|    approx_kl            | 0.0029339637 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.405       |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0003       |
|    loss                 | 193          |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00241     |
|    value_loss           | 614          |
------------------------------------------
Eval num_timesteps=6625000, episode_reward=3002.61 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.021056902 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 352         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 405      |
|    time_elapsed    | 30783    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=3002.56 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.018256288 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.5        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 264         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 697      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 406      |
|    time_elapsed    | 30862    |
|    total_timesteps | 6651904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.85e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 407          |
|    time_elapsed         | 30929        |
|    total_timesteps      | 6668288      |
| train/                  |              |
|    approx_kl            | 0.0059843548 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.312       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 22           |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.00271     |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=6675000, episode_reward=3002.38 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6675000      |
| train/                  |              |
|    approx_kl            | 0.0035904294 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.225       |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0003       |
|    loss                 | 106          |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 98.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 408      |
|    time_elapsed    | 31008    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=3002.27 +/- 0.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6700000      |
| train/                  |              |
|    approx_kl            | 0.0043852977 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 420          |
|    n_updates            | 4080         |
|    policy_gradient_loss | 0.000857     |
|    value_loss           | 118          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 409      |
|    time_elapsed    | 31088    |
|    total_timesteps | 6701056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.58e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 410          |
|    time_elapsed         | 31155        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0034190726 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.51         |
|    n_updates            | 4090         |
|    policy_gradient_loss | 0.00175      |
|    value_loss           | 96.6         |
------------------------------------------
Eval num_timesteps=6725000, episode_reward=3002.42 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6725000      |
| train/                  |              |
|    approx_kl            | 0.0054161353 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.38         |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 74.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 411      |
|    time_elapsed    | 31234    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=3002.02 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.003850165 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.84        |
|    n_updates            | 4110        |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 75.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 215      |
|    iterations      | 412      |
|    time_elapsed    | 31314    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.7e+03    |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 413        |
|    time_elapsed         | 31381      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.00743416 |
|    clip_fraction        | 0.027      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0965    |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.639      |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00242   |
|    value_loss           | 22         |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=3002.61 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.004073335 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.097      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.5        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.000144   |
|    value_loss           | 49.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 215      |
|    iterations      | 414      |
|    time_elapsed    | 31461    |
|    total_timesteps | 6782976  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.96e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 415          |
|    time_elapsed         | 31528        |
|    total_timesteps      | 6799360      |
| train/                  |              |
|    approx_kl            | 0.0013911803 |
|    clip_fraction        | 0.00809      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0825      |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.19         |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.000643    |
|    value_loss           | 10.2         |
------------------------------------------
Eval num_timesteps=6800000, episode_reward=2882.34 +/- 239.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.029413424 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 1.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 416      |
|    time_elapsed    | 31606    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=3002.56 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.012320834 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.312      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.84        |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 55.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 417      |
|    time_elapsed    | 31684    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.78e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 418         |
|    time_elapsed         | 31750       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.013940677 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.27        |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 14.1        |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=3002.45 +/- 0.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6850000    |
| train/                  |            |
|    approx_kl            | 0.04243371 |
|    clip_fraction        | 0.0701     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.36      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.9        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.0028    |
|    value_loss           | 58.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 419      |
|    time_elapsed    | 31827    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=3002.44 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.014375789 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.000663   |
|    value_loss           | 44.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 420      |
|    time_elapsed    | 31905    |
|    total_timesteps | 6881280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.83e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 421          |
|    time_elapsed         | 31970        |
|    total_timesteps      | 6897664      |
| train/                  |              |
|    approx_kl            | 0.0021245843 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28         |
|    n_updates            | 4200         |
|    policy_gradient_loss | 0.000829     |
|    value_loss           | 52           |
------------------------------------------
Eval num_timesteps=6900000, episode_reward=3002.57 +/- 0.37
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3e+03     |
| time/                   |           |
|    total_timesteps      | 6900000   |
| train/                  |           |
|    approx_kl            | 0.0062073 |
|    clip_fraction        | 0.0172    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.152    |
|    explained_variance   | 0.873     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.43      |
|    n_updates            | 4210      |
|    policy_gradient_loss | -0.000423 |
|    value_loss           | 18.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 422      |
|    time_elapsed    | 32048    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=3002.44 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6925000      |
| train/                  |              |
|    approx_kl            | 0.0059184944 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.35        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.0003       |
|    loss                 | 113          |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.00391     |
|    value_loss           | 207          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 423      |
|    time_elapsed    | 32125    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.21e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 424         |
|    time_elapsed         | 32190       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.010235062 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 199         |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=2423.04 +/- 1159.68
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 6950000      |
| train/                  |              |
|    approx_kl            | 0.0039718277 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.1          |
|    n_updates            | 4240         |
|    policy_gradient_loss | 0.000388     |
|    value_loss           | 31.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 425      |
|    time_elapsed    | 32268    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=3002.19 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.006114797 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.74        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 43          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 426      |
|    time_elapsed    | 32345    |
|    total_timesteps | 6979584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.69e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 427          |
|    time_elapsed         | 32410        |
|    total_timesteps      | 6995968      |
| train/                  |              |
|    approx_kl            | 0.0045719035 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.39         |
|    n_updates            | 4260         |
|    policy_gradient_loss | 0.00223      |
|    value_loss           | 25.8         |
------------------------------------------
Eval num_timesteps=7000000, episode_reward=3002.31 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7000000      |
| train/                  |              |
|    approx_kl            | 0.0022908873 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.292       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 82.6         |
|    n_updates            | 4270         |
|    policy_gradient_loss | -0.000294    |
|    value_loss           | 73.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 428      |
|    time_elapsed    | 32488    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=3002.50 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.001750879 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.2         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.000767   |
|    value_loss           | 23.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 429      |
|    time_elapsed    | 32568    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 430         |
|    time_elapsed         | 32635       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.006197976 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.000373   |
|    value_loss           | 25.6        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=3002.27 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7050000      |
| train/                  |              |
|    approx_kl            | 0.0027400437 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.4         |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.00099     |
|    value_loss           | 89.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 431      |
|    time_elapsed    | 32715    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=3002.74 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.011053264 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.68        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 95.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 432      |
|    time_elapsed    | 32794    |
|    total_timesteps | 7077888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.77e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 433          |
|    time_elapsed         | 32860        |
|    total_timesteps      | 7094272      |
| train/                  |              |
|    approx_kl            | 0.0044218567 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.76         |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 35.9         |
------------------------------------------
Eval num_timesteps=7100000, episode_reward=3002.89 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.014762548 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.534       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 11          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 434      |
|    time_elapsed    | 32939    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=3002.24 +/- 0.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7125000      |
| train/                  |              |
|    approx_kl            | 0.0053577134 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 60.6         |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.00291     |
|    value_loss           | 29.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 435      |
|    time_elapsed    | 33018    |
|    total_timesteps | 7127040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 436          |
|    time_elapsed         | 33085        |
|    total_timesteps      | 7143424      |
| train/                  |              |
|    approx_kl            | 0.0025162534 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.077       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.25         |
|    n_updates            | 4350         |
|    policy_gradient_loss | -1.76e-05    |
|    value_loss           | 3            |
------------------------------------------
Eval num_timesteps=7150000, episode_reward=3002.38 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.008362841 |
|    clip_fraction        | 0.00989     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.029      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.437       |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 6.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 215      |
|    iterations      | 437      |
|    time_elapsed    | 33164    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=3002.71 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7175000      |
| train/                  |              |
|    approx_kl            | 0.0011594982 |
|    clip_fraction        | 0.00172      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.142        |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.000329    |
|    value_loss           | 2.36         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 438      |
|    time_elapsed    | 33242    |
|    total_timesteps | 7176192  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.97e+03      |
| time/                   |               |
|    fps                  | 215           |
|    iterations           | 439           |
|    time_elapsed         | 33307         |
|    total_timesteps      | 7192576       |
| train/                  |               |
|    approx_kl            | 0.00029065245 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0158       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.11          |
|    n_updates            | 4380          |
|    policy_gradient_loss | -0.000208     |
|    value_loss           | 0.724         |
-------------------------------------------
Eval num_timesteps=7200000, episode_reward=1706.38 +/- 2591.78
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.71e+03   |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.09440436 |
|    clip_fraction        | 0.0595     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0859    |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.00339   |
|    value_loss           | 0.76       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 440      |
|    time_elapsed    | 33384    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=3002.73 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7225000      |
| train/                  |              |
|    approx_kl            | 0.0019896717 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.36        |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | 320          |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.000803    |
|    value_loss           | 784          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 441      |
|    time_elapsed    | 33461    |
|    total_timesteps | 7225344  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 442          |
|    time_elapsed         | 33526        |
|    total_timesteps      | 7241728      |
| train/                  |              |
|    approx_kl            | 0.0026266144 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.331       |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.0003       |
|    loss                 | 168          |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 764          |
------------------------------------------
Eval num_timesteps=7250000, episode_reward=3002.37 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7250000      |
| train/                  |              |
|    approx_kl            | 0.0019324733 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.337       |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0003       |
|    loss                 | 190          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 578          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -975     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 443      |
|    time_elapsed    | 33603    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.04e+03   |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 444         |
|    time_elapsed         | 33668       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.009662898 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 414         |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=3002.37 +/- 0.18
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7275000      |
| train/                  |              |
|    approx_kl            | 0.0035317224 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.407       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 150          |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 472          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 216       |
|    iterations      | 445       |
|    time_elapsed    | 33745     |
|    total_timesteps | 7290880   |
----------------------------------
Eval num_timesteps=7300000, episode_reward=3002.69 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 7300000    |
| train/                  |            |
|    approx_kl            | 0.00783643 |
|    clip_fraction        | 0.0339     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 66.8       |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.00351   |
|    value_loss           | 285        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -443     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 446      |
|    time_elapsed    | 33823    |
|    total_timesteps | 7307264  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -113         |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 447          |
|    time_elapsed         | 33887        |
|    total_timesteps      | 7323648      |
| train/                  |              |
|    approx_kl            | 0.0020109874 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | 86.8         |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 409          |
------------------------------------------
Eval num_timesteps=7325000, episode_reward=3002.65 +/- 0.38
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7325000      |
| train/                  |              |
|    approx_kl            | 0.0025842143 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 250          |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 211          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 639      |
| time/              |          |
|    fps             | 216      |
|    iterations      | 448      |
|    time_elapsed    | 33964    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=3002.40 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7350000      |
| train/                  |              |
|    approx_kl            | 0.0028267298 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.5         |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 242          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 216      |
|    iterations      | 449      |
|    time_elapsed    | 34042    |
|    total_timesteps | 7356416  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.18e+03     |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 450          |
|    time_elapsed         | 34106        |
|    total_timesteps      | 7372800      |
| train/                  |              |
|    approx_kl            | 0.0042807017 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.9         |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 239          |
------------------------------------------
Eval num_timesteps=7375000, episode_reward=3002.61 +/- 0.38
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 7375000    |
| train/                  |            |
|    approx_kl            | 0.00780382 |
|    clip_fraction        | 0.0395     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.6       |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.00456   |
|    value_loss           | 385        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 451      |
|    time_elapsed    | 34183    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=2422.84 +/- 1159.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 7400000      |
| train/                  |              |
|    approx_kl            | 0.0072465627 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 139          |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.00471     |
|    value_loss           | 163          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 452      |
|    time_elapsed    | 34261    |
|    total_timesteps | 7405568  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.82e+03     |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 453          |
|    time_elapsed         | 34325        |
|    total_timesteps      | 7421952      |
| train/                  |              |
|    approx_kl            | 0.0028795528 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 48.2         |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 281          |
------------------------------------------
Eval num_timesteps=7425000, episode_reward=3002.41 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7425000      |
| train/                  |              |
|    approx_kl            | 0.0056738937 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 68.2         |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.00323     |
|    value_loss           | 166          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 216      |
|    iterations      | 454      |
|    time_elapsed    | 34402    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=3002.53 +/- 0.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 7450000    |
| train/                  |            |
|    approx_kl            | 0.00199545 |
|    clip_fraction        | 0.0222     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.8       |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.00195   |
|    value_loss           | 981        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 455      |
|    time_elapsed    | 34483    |
|    total_timesteps | 7454720  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.52e+03     |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 456          |
|    time_elapsed         | 34554        |
|    total_timesteps      | 7471104      |
| train/                  |              |
|    approx_kl            | 0.0038421326 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0527      |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=7475000, episode_reward=3002.74 +/- 0.42
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 7475000       |
| train/                  |               |
|    approx_kl            | 0.00045753852 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0381       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | 18.3          |
|    n_updates            | 4560          |
|    policy_gradient_loss | -0.000447     |
|    value_loss           | 61.2          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 457      |
|    time_elapsed    | 34638    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=3002.71 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7500000      |
| train/                  |              |
|    approx_kl            | 0.0036159838 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.085       |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.4         |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 75.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 216      |
|    iterations      | 458      |
|    time_elapsed    | 34728    |
|    total_timesteps | 7503872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.78e+03     |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 459          |
|    time_elapsed         | 34803        |
|    total_timesteps      | 7520256      |
| train/                  |              |
|    approx_kl            | 0.0027769336 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0582      |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.2         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.000433    |
|    value_loss           | 43.3         |
------------------------------------------
Eval num_timesteps=7525000, episode_reward=3002.14 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.008403391 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0415     |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.987       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 7.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 460      |
|    time_elapsed    | 34893    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=3002.35 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7550000      |
| train/                  |              |
|    approx_kl            | 0.0011824749 |
|    clip_fraction        | 0.00242      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0192      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.482        |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.000577    |
|    value_loss           | 23.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 461      |
|    time_elapsed    | 34983    |
|    total_timesteps | 7553024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.94e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 462          |
|    time_elapsed         | 35058        |
|    total_timesteps      | 7569408      |
| train/                  |              |
|    approx_kl            | 0.0065728603 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0186      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.74         |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.000666    |
|    value_loss           | 6.56         |
------------------------------------------
Eval num_timesteps=7575000, episode_reward=3002.55 +/- 0.52
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7575000      |
| train/                  |              |
|    approx_kl            | 0.0003256303 |
|    clip_fraction        | 0.00212      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.644        |
|    n_updates            | 4620         |
|    policy_gradient_loss | 0.000195     |
|    value_loss           | 4.05         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 463      |
|    time_elapsed    | 35147    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=3002.56 +/- 0.33
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 7600000       |
| train/                  |               |
|    approx_kl            | 0.00042919914 |
|    clip_fraction        | 0.0015        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.0003        |
|    loss                 | 55.9          |
|    n_updates            | 4630          |
|    policy_gradient_loss | -0.000225     |
|    value_loss           | 8.94          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 464      |
|    time_elapsed    | 35236    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.93e+03    |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 465         |
|    time_elapsed         | 35308       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.033109687 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 9.46        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=-697.56 +/- 4531.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -698       |
| time/                   |            |
|    total_timesteps      | 7625000    |
| train/                  |            |
|    approx_kl            | 0.26807976 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 4650       |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 3.57       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 466      |
|    time_elapsed    | 35395    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=3002.62 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7650000      |
| train/                  |              |
|    approx_kl            | 0.0060285237 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.716       |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.0003       |
|    loss                 | 385          |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 668          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 467      |
|    time_elapsed    | 35483    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.56e+03   |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 468         |
|    time_elapsed         | 35556       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.004741866 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 511         |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=-719.21 +/- 4558.62
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -719         |
| time/                   |              |
|    total_timesteps      | 7675000      |
| train/                  |              |
|    approx_kl            | 0.0050577368 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.4         |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 333          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.37e+03 |
| time/              |           |
|    fps             | 215       |
|    iterations      | 469       |
|    time_elapsed    | 35644     |
|    total_timesteps | 7684096   |
----------------------------------
Eval num_timesteps=7700000, episode_reward=-2872.12 +/- 3227.56
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 7700000      |
| train/                  |              |
|    approx_kl            | 0.0072213067 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 162          |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00253     |
|    value_loss           | 262          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.98e+03 |
| time/              |           |
|    fps             | 215       |
|    iterations      | 470       |
|    time_elapsed    | 35732     |
|    total_timesteps | 7700480   |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.9e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 471          |
|    time_elapsed         | 35806        |
|    total_timesteps      | 7716864      |
| train/                  |              |
|    approx_kl            | 0.0088831615 |
|    clip_fraction        | 0.0837       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 62.6         |
|    n_updates            | 4700         |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 346          |
------------------------------------------
Eval num_timesteps=7725000, episode_reward=1133.29 +/- 3738.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13e+03    |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.004956961 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.1        |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 317         |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.42e+03 |
| time/              |           |
|    fps             | 215       |
|    iterations      | 472       |
|    time_elapsed    | 35894     |
|    total_timesteps | 7733248   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -927        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 473         |
|    time_elapsed         | 35968       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.007359849 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.9        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 272         |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=410.72 +/- 3174.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 411          |
| time/                   |              |
|    total_timesteps      | 7750000      |
| train/                  |              |
|    approx_kl            | 0.0035661277 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 206          |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 409          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -434     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 474      |
|    time_elapsed    | 36057    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=1172.58 +/- 3660.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.17e+03     |
| time/                   |              |
|    total_timesteps      | 7775000      |
| train/                  |              |
|    approx_kl            | 0.0046881847 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.9         |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 157          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.6    |
| time/              |          |
|    fps             | 215      |
|    iterations      | 475      |
|    time_elapsed    | 36145    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 262         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 476         |
|    time_elapsed         | 36219       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.004331554 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 309         |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=3002.96 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.006923827 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 127         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 477      |
|    time_elapsed    | 36308    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=3002.67 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7825000      |
| train/                  |              |
|    approx_kl            | 0.0016769881 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 51.2         |
|    n_updates            | 4770         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 202          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 478      |
|    time_elapsed    | 36396    |
|    total_timesteps | 7831552  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.16e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 479          |
|    time_elapsed         | 36470        |
|    total_timesteps      | 7847936      |
| train/                  |              |
|    approx_kl            | 0.0021221621 |
|    clip_fraction        | 0.00872      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0653      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.2         |
|    n_updates            | 4780         |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 55.5         |
------------------------------------------
Eval num_timesteps=7850000, episode_reward=3002.57 +/- 0.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 7850000    |
| train/                  |            |
|    approx_kl            | 0.00117132 |
|    clip_fraction        | 0.00723    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0938    |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.7        |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.000772  |
|    value_loss           | 58.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 480      |
|    time_elapsed    | 36557    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=3002.67 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7875000      |
| train/                  |              |
|    approx_kl            | 0.0015915954 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0843      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.2         |
|    n_updates            | 4800         |
|    policy_gradient_loss | -0.000691    |
|    value_loss           | 118          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 481      |
|    time_elapsed    | 36645    |
|    total_timesteps | 7880704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.52e+03     |
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 482          |
|    time_elapsed         | 36719        |
|    total_timesteps      | 7897088      |
| train/                  |              |
|    approx_kl            | 0.0014275638 |
|    clip_fraction        | 0.00608      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0633      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 31.8         |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 339          |
------------------------------------------
Eval num_timesteps=7900000, episode_reward=3002.34 +/- 0.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7900000      |
| train/                  |              |
|    approx_kl            | 0.0014464192 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0965      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.9         |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.00095     |
|    value_loss           | 255          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 483      |
|    time_elapsed    | 36808    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=3002.56 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.005548209 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0971     |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 49.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 484      |
|    time_elapsed    | 36897    |
|    total_timesteps | 7929856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.69e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 485          |
|    time_elapsed         | 36970        |
|    total_timesteps      | 7946240      |
| train/                  |              |
|    approx_kl            | 0.0023589437 |
|    clip_fraction        | 0.0056       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.38         |
|    n_updates            | 4840         |
|    policy_gradient_loss | -0.000976    |
|    value_loss           | 9.51         |
------------------------------------------
Eval num_timesteps=7950000, episode_reward=3002.46 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.030737504 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.833       |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00963    |
|    value_loss           | 41.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 486      |
|    time_elapsed    | 37059    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=2422.76 +/- 1159.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 7975000      |
| train/                  |              |
|    approx_kl            | 0.0032806424 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.421       |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.0003       |
|    loss                 | 245          |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 730          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 487      |
|    time_elapsed    | 37147    |
|    total_timesteps | 7979008  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 459          |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 488          |
|    time_elapsed         | 37220        |
|    total_timesteps      | 7995392      |
| train/                  |              |
|    approx_kl            | 0.0011785233 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.301       |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.0003       |
|    loss                 | 269          |
|    n_updates            | 4870         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 547          |
------------------------------------------
Eval num_timesteps=8000000, episode_reward=3002.97 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.004467307 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 167         |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 379         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -223     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 489      |
|    time_elapsed    | 37308    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=3002.22 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8025000      |
| train/                  |              |
|    approx_kl            | 0.0056413217 |
|    clip_fraction        | 0.0447       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0003       |
|    loss                 | 68.6         |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 384          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 490      |
|    time_elapsed    | 37397    |
|    total_timesteps | 8028160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 448        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 491        |
|    time_elapsed         | 37471      |
|    total_timesteps      | 8044544    |
| train/                  |            |
|    approx_kl            | 0.00405451 |
|    clip_fraction        | 0.0224     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.298     |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 65.6       |
|    n_updates            | 4900       |
|    policy_gradient_loss | -0.00187   |
|    value_loss           | 278        |
----------------------------------------
Eval num_timesteps=8050000, episode_reward=3002.61 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.008655467 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 415         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 644      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 492      |
|    time_elapsed    | 37560    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=3002.26 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.005499469 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 493      |
|    time_elapsed    | 37650    |
|    total_timesteps | 8077312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.72e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 494          |
|    time_elapsed         | 37724        |
|    total_timesteps      | 8093696      |
| train/                  |              |
|    approx_kl            | 0.0024012018 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 350          |
------------------------------------------
Eval num_timesteps=8100000, episode_reward=3002.39 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8100000      |
| train/                  |              |
|    approx_kl            | 0.0038179622 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 65           |
|    n_updates            | 4940         |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 299          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 495      |
|    time_elapsed    | 37813    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=3002.73 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8125000      |
| train/                  |              |
|    approx_kl            | 0.0044825273 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 23.5         |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 36.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 496      |
|    time_elapsed    | 37902    |
|    total_timesteps | 8126464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.78e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 497          |
|    time_elapsed         | 37975        |
|    total_timesteps      | 8142848      |
| train/                  |              |
|    approx_kl            | 0.0038327442 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26         |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 16.7         |
------------------------------------------
Eval num_timesteps=8150000, episode_reward=3002.54 +/- 0.17
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8150000      |
| train/                  |              |
|    approx_kl            | 0.0039705914 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0747      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.85         |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.000464    |
|    value_loss           | 9.99         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 498      |
|    time_elapsed    | 38062    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=3002.68 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8175000      |
| train/                  |              |
|    approx_kl            | 0.0073817726 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0878      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.48         |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.000145    |
|    value_loss           | 17.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 499      |
|    time_elapsed    | 38150    |
|    total_timesteps | 8175616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.91e+03     |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 500          |
|    time_elapsed         | 38224        |
|    total_timesteps      | 8192000      |
| train/                  |              |
|    approx_kl            | 0.0002159897 |
|    clip_fraction        | 0.00305      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0515      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44         |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.000171    |
|    value_loss           | 5.94         |
------------------------------------------
Eval num_timesteps=8200000, episode_reward=3002.65 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.001109046 |
|    clip_fraction        | 0.0032      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0541     |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.643       |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.000234   |
|    value_loss           | 37.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 501      |
|    time_elapsed    | 38312    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.88e+03    |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 502         |
|    time_elapsed         | 38385       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.008086003 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.568       |
|    n_updates            | 5010        |
|    policy_gradient_loss | -4.94e-05   |
|    value_loss           | 7.86        |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=2422.85 +/- 1159.82
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 8225000      |
| train/                  |              |
|    approx_kl            | 0.0038451017 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.9         |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.000988    |
|    value_loss           | 31.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 503      |
|    time_elapsed    | 38474    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=3002.41 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.056942668 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.23        |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 5.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    fps             | 214      |
|    iterations      | 504      |
|    time_elapsed    | 38562    |
|    total_timesteps | 8257536  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 547          |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 505          |
|    time_elapsed         | 38635        |
|    total_timesteps      | 8273920      |
| train/                  |              |
|    approx_kl            | 0.0031422824 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.767        |
|    learning_rate        | 0.0003       |
|    loss                 | 263          |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.000718    |
|    value_loss           | 834          |
------------------------------------------
Eval num_timesteps=8275000, episode_reward=3002.30 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8275000      |
| train/                  |              |
|    approx_kl            | 0.0026678713 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.44        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.0003       |
|    loss                 | 294          |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 652          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -776     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 506      |
|    time_elapsed    | 38723    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=3002.78 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8300000      |
| train/                  |              |
|    approx_kl            | 0.0076469798 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 229          |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.00332     |
|    value_loss           | 634          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    fps             | 214       |
|    iterations      | 507       |
|    time_elapsed    | 38811     |
|    total_timesteps | 8306688   |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.25e+03    |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 508          |
|    time_elapsed         | 38885        |
|    total_timesteps      | 8323072      |
| train/                  |              |
|    approx_kl            | 0.0036994722 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 136          |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 466          |
------------------------------------------
Eval num_timesteps=8325000, episode_reward=3002.64 +/- 0.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8325000      |
| train/                  |              |
|    approx_kl            | 0.0064521926 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 77.5         |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 333          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -471     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 509      |
|    time_elapsed    | 38974    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=3002.20 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.006705399 |
|    clip_fraction        | 0.0504      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.8        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 318         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 196      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 510      |
|    time_elapsed    | 39062    |
|    total_timesteps | 8355840  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.37e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 511          |
|    time_elapsed         | 39138        |
|    total_timesteps      | 8372224      |
| train/                  |              |
|    approx_kl            | 0.0062453016 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 57.4         |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.00435     |
|    value_loss           | 269          |
------------------------------------------
Eval num_timesteps=8375000, episode_reward=3002.43 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8375000      |
| train/                  |              |
|    approx_kl            | 0.0032623713 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 139          |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 244          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 512      |
|    time_elapsed    | 39227    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=3002.59 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.004233384 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.8        |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 175         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 513      |
|    time_elapsed    | 39316    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 514         |
|    time_elapsed         | 39390       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.005806562 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 31          |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 69.8        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=2882.46 +/- 240.04
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 8425000      |
| train/                  |              |
|    approx_kl            | 0.0037206325 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 67.8         |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 74.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 515      |
|    time_elapsed    | 39479    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=3002.24 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8450000      |
| train/                  |              |
|    approx_kl            | 0.0099145025 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.111       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.33         |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00407     |
|    value_loss           | 26.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 516      |
|    time_elapsed    | 39568    |
|    total_timesteps | 8454144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.5e+03      |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 517          |
|    time_elapsed         | 39641        |
|    total_timesteps      | 8470528      |
| train/                  |              |
|    approx_kl            | 0.0038386383 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.266       |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 122          |
|    n_updates            | 5160         |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 254          |
------------------------------------------
Eval num_timesteps=8475000, episode_reward=3002.57 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.013322154 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.31        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.000558   |
|    value_loss           | 31.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 518      |
|    time_elapsed    | 39729    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=3002.40 +/- 0.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.002041031 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.82        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.000278   |
|    value_loss           | 47          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 519      |
|    time_elapsed    | 39818    |
|    total_timesteps | 8503296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.8e+03      |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 520          |
|    time_elapsed         | 39891        |
|    total_timesteps      | 8519680      |
| train/                  |              |
|    approx_kl            | 0.0032511326 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.56         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.000485    |
|    value_loss           | 51.6         |
------------------------------------------
Eval num_timesteps=8525000, episode_reward=3002.78 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 8525000    |
| train/                  |            |
|    approx_kl            | 0.04630254 |
|    clip_fraction        | 0.0403     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.894      |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 15.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 521      |
|    time_elapsed    | 39979    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=1706.41 +/- 2592.11
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 8550000      |
| train/                  |              |
|    approx_kl            | 0.0022804087 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.0003       |
|    loss                 | 37.2         |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.000918    |
|    value_loss           | 590          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 522      |
|    time_elapsed    | 40067    |
|    total_timesteps | 8552448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 523        |
|    time_elapsed         | 40140      |
|    total_timesteps      | 8568832    |
| train/                  |            |
|    approx_kl            | 0.00785386 |
|    clip_fraction        | 0.0736     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.3       |
|    n_updates            | 5220       |
|    policy_gradient_loss | -0.00207   |
|    value_loss           | 519        |
----------------------------------------
Eval num_timesteps=8575000, episode_reward=3002.34 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.017715741 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.1        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 452         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -834     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 524      |
|    time_elapsed    | 40229    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=2422.87 +/- 1159.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.022710197 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.583      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.2        |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 398         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -357     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 525      |
|    time_elapsed    | 40318    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 655         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 526         |
|    time_elapsed         | 40392       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.008897811 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 315         |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=3002.16 +/- 0.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8625000      |
| train/                  |              |
|    approx_kl            | 0.0060812286 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.5         |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.00599     |
|    value_loss           | 116          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 527      |
|    time_elapsed    | 40481    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=3002.65 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.004320302 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.1        |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 528      |
|    time_elapsed    | 40570    |
|    total_timesteps | 8650752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.77e+03     |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 529          |
|    time_elapsed         | 40644        |
|    total_timesteps      | 8667136      |
| train/                  |              |
|    approx_kl            | 0.0054208115 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.3         |
|    n_updates            | 5280         |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 65.3         |
------------------------------------------
Eval num_timesteps=8675000, episode_reward=3002.44 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8675000      |
| train/                  |              |
|    approx_kl            | 0.0066627283 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.63         |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.000304    |
|    value_loss           | 60.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 530      |
|    time_elapsed    | 40732    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.73e+03    |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 531         |
|    time_elapsed         | 40805       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.008341625 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.02        |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.000273   |
|    value_loss           | 35.4        |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=3002.63 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8700000      |
| train/                  |              |
|    approx_kl            | 0.0037025458 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 51           |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.000949    |
|    value_loss           | 93.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 532      |
|    time_elapsed    | 40893    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=3002.55 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8725000      |
| train/                  |              |
|    approx_kl            | 0.0113284215 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.1         |
|    n_updates            | 5320         |
|    policy_gradient_loss | 6.08e-05     |
|    value_loss           | 27.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 533      |
|    time_elapsed    | 40981    |
|    total_timesteps | 8732672  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.16e+03  |
| time/                   |           |
|    fps                  | 213       |
|    iterations           | 534       |
|    time_elapsed         | 41055     |
|    total_timesteps      | 8749056   |
| train/                  |           |
|    approx_kl            | 0.0587382 |
|    clip_fraction        | 0.05      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.215    |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 5330      |
|    policy_gradient_loss | -0.00445  |
|    value_loss           | 4.75      |
---------------------------------------
Eval num_timesteps=8750000, episode_reward=1706.40 +/- 2592.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.71e+03   |
| time/                   |            |
|    total_timesteps      | 8750000    |
| train/                  |            |
|    approx_kl            | 0.02342942 |
|    clip_fraction        | 0.0456     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.52      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 213        |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.00197   |
|    value_loss           | 269        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 535      |
|    time_elapsed    | 41142    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=1126.88 +/- 2561.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.13e+03   |
| time/                   |            |
|    total_timesteps      | 8775000    |
| train/                  |            |
|    approx_kl            | 0.00548389 |
|    clip_fraction        | 0.0477     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 187        |
|    n_updates            | 5350       |
|    policy_gradient_loss | -0.00165   |
|    value_loss           | 331        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 212      |
|    iterations      | 536      |
|    time_elapsed    | 41231    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 298         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 537         |
|    time_elapsed         | 41305       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.005854409 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.1        |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.000526   |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=2422.67 +/- 1159.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.023999508 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.9        |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 99.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 212      |
|    iterations      | 538      |
|    time_elapsed    | 41394    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=3002.41 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.012303373 |
|    clip_fraction        | 0.0672      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.2        |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 201         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 539      |
|    time_elapsed    | 41482    |
|    total_timesteps | 8830976  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.51e+03     |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 540          |
|    time_elapsed         | 41556        |
|    total_timesteps      | 8847360      |
| train/                  |              |
|    approx_kl            | 0.0067108814 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.8         |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.00525     |
|    value_loss           | 163          |
------------------------------------------
Eval num_timesteps=8850000, episode_reward=3002.75 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8850000      |
| train/                  |              |
|    approx_kl            | 0.0026611933 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.69         |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 172          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 541      |
|    time_elapsed    | 41644    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=2443.86 +/- 1117.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.005357315 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.376      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 206         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 542      |
|    time_elapsed    | 41733    |
|    total_timesteps | 8880128  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.46e+03     |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 543          |
|    time_elapsed         | 41807        |
|    total_timesteps      | 8896512      |
| train/                  |              |
|    approx_kl            | 0.0066038724 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0781      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.9         |
|    n_updates            | 5420         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 12.9         |
------------------------------------------
Eval num_timesteps=8900000, episode_reward=3002.60 +/- 0.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8900000      |
| train/                  |              |
|    approx_kl            | 0.0033895252 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0003       |
|    loss                 | 142          |
|    n_updates            | 5430         |
|    policy_gradient_loss | 0.000202     |
|    value_loss           | 110          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 212      |
|    iterations      | 544      |
|    time_elapsed    | 41895    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=3002.31 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.003915855 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.8        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.000648   |
|    value_loss           | 94.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 545      |
|    time_elapsed    | 41982    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.49e+03    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 546         |
|    time_elapsed         | 42056       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.008329876 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.3        |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 48.2        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=2882.32 +/- 240.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 8950000      |
| train/                  |              |
|    approx_kl            | 0.0013428968 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0927      |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 674          |
|    n_updates            | 5460         |
|    policy_gradient_loss | 0.00243      |
|    value_loss           | 45.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 547      |
|    time_elapsed    | 42143    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=3002.61 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8975000      |
| train/                  |              |
|    approx_kl            | 0.0009973407 |
|    clip_fraction        | 0.00911      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 199          |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 191          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 548      |
|    time_elapsed    | 42231    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.44e+03    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 549         |
|    time_elapsed         | 42305       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.008474918 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.9         |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 83.4        |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=3002.57 +/- 0.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9000000      |
| train/                  |              |
|    approx_kl            | 0.0020207902 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 71.5         |
|    n_updates            | 5490         |
|    policy_gradient_loss | 0.000323     |
|    value_loss           | 140          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 550      |
|    time_elapsed    | 42393    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=3002.40 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.004080466 |
|    clip_fraction        | 0.00331     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0154     |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.395       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 15.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 551      |
|    time_elapsed    | 42481    |
|    total_timesteps | 9027584  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.7e+03       |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 552           |
|    time_elapsed         | 42555         |
|    total_timesteps      | 9043968       |
| train/                  |               |
|    approx_kl            | 0.00029380692 |
|    clip_fraction        | 0.00102       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00841      |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.7          |
|    n_updates            | 5510          |
|    policy_gradient_loss | 6.94e-06      |
|    value_loss           | 4.29          |
-------------------------------------------
Eval num_timesteps=9050000, episode_reward=3002.48 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9050000      |
| train/                  |              |
|    approx_kl            | 0.0024362714 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0695      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 469          |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 52.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 553      |
|    time_elapsed    | 42643    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=3002.60 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9075000      |
| train/                  |              |
|    approx_kl            | 8.138113e-05 |
|    clip_fraction        | 0.000403     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00652     |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.647        |
|    n_updates            | 5530         |
|    policy_gradient_loss | -4.32e-05    |
|    value_loss           | 5.99         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 554      |
|    time_elapsed    | 42732    |
|    total_timesteps | 9076736  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.85e+03      |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 555           |
|    time_elapsed         | 42805         |
|    total_timesteps      | 9093120       |
| train/                  |               |
|    approx_kl            | 2.4953046e-05 |
|    clip_fraction        | 0.000208      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00501      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.167         |
|    n_updates            | 5540          |
|    policy_gradient_loss | -1.52e-05     |
|    value_loss           | 1.11          |
-------------------------------------------
Eval num_timesteps=9100000, episode_reward=3002.46 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9100000      |
| train/                  |              |
|    approx_kl            | 0.0018069407 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0386      |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.182        |
|    n_updates            | 5550         |
|    policy_gradient_loss | 0.000601     |
|    value_loss           | 42.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 556      |
|    time_elapsed    | 42894    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=3002.79 +/- 0.63
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9125000       |
| train/                  |               |
|    approx_kl            | 2.9363986e-05 |
|    clip_fraction        | 0.000433      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00458      |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.168         |
|    n_updates            | 5560          |
|    policy_gradient_loss | -7.85e-05     |
|    value_loss           | 10.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 557      |
|    time_elapsed    | 42983    |
|    total_timesteps | 9125888  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.86e+03      |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 558           |
|    time_elapsed         | 43057         |
|    total_timesteps      | 9142272       |
| train/                  |               |
|    approx_kl            | 2.1789823e-05 |
|    clip_fraction        | 0.000214      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00525      |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.154         |
|    n_updates            | 5570          |
|    policy_gradient_loss | -2.72e-05     |
|    value_loss           | 45.1          |
-------------------------------------------
Eval num_timesteps=9150000, episode_reward=3002.66 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9150000      |
| train/                  |              |
|    approx_kl            | 0.0009046211 |
|    clip_fraction        | 0.00392      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.022       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.331        |
|    n_updates            | 5580         |
|    policy_gradient_loss | 0.000552     |
|    value_loss           | 17.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 559      |
|    time_elapsed    | 43146    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=3002.87 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 9175000    |
| train/                  |            |
|    approx_kl            | 0.00249452 |
|    clip_fraction        | 0.00135    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00659   |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0495     |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.000321  |
|    value_loss           | 2.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 560      |
|    time_elapsed    | 43234    |
|    total_timesteps | 9175040  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.89e+03      |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 561           |
|    time_elapsed         | 43307         |
|    total_timesteps      | 9191424       |
| train/                  |               |
|    approx_kl            | 0.00026365736 |
|    clip_fraction        | 0.000891      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00462      |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.25          |
|    n_updates            | 5600          |
|    policy_gradient_loss | -9.47e-06     |
|    value_loss           | 3.89          |
-------------------------------------------
Eval num_timesteps=9200000, episode_reward=3002.68 +/- 0.33
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9200000       |
| train/                  |               |
|    approx_kl            | 1.1950633e-05 |
|    clip_fraction        | 9.16e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00215      |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.11          |
|    n_updates            | 5610          |
|    policy_gradient_loss | -1.1e-05      |
|    value_loss           | 8.04          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 562      |
|    time_elapsed    | 43395    |
|    total_timesteps | 9207808  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.92e+03      |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 563           |
|    time_elapsed         | 43468         |
|    total_timesteps      | 9224192       |
| train/                  |               |
|    approx_kl            | 4.0140472e-05 |
|    clip_fraction        | 0.000159      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00149      |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.212         |
|    n_updates            | 5620          |
|    policy_gradient_loss | -4.59e-05     |
|    value_loss           | 5.72          |
-------------------------------------------
Eval num_timesteps=9225000, episode_reward=3002.41 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9225000      |
| train/                  |              |
|    approx_kl            | 3.314738e-05 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000876    |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.239        |
|    n_updates            | 5630         |
|    policy_gradient_loss | -5.19e-05    |
|    value_loss           | 16.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 564      |
|    time_elapsed    | 43556    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=2422.91 +/- 1159.71
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.42e+03  |
| time/                   |           |
|    total_timesteps      | 9250000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000778 |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.183     |
|    n_updates            | 5640      |
|    policy_gradient_loss | -2.99e-07 |
|    value_loss           | 8.12      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.95e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 565      |
|    time_elapsed    | 43644    |
|    total_timesteps | 9256960  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.95e+03  |
| time/                   |           |
|    fps                  | 212       |
|    iterations           | 566       |
|    time_elapsed         | 43717     |
|    total_timesteps      | 9273344   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000795 |
|    explained_variance   | 0.961     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.186     |
|    n_updates            | 5650      |
|    policy_gradient_loss | -2.99e-07 |
|    value_loss           | 0.635     |
---------------------------------------
Eval num_timesteps=9275000, episode_reward=3002.39 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9275000      |
| train/                  |              |
|    approx_kl            | 3.513811e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000737    |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.191        |
|    n_updates            | 5660         |
|    policy_gradient_loss | -2.83e-06    |
|    value_loss           | 13.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 567      |
|    time_elapsed    | 43805    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=2423.00 +/- 1159.58
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 2.42e+03      |
| time/                   |               |
|    total_timesteps      | 9300000       |
| train/                  |               |
|    approx_kl            | 4.3291948e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000883     |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0859        |
|    n_updates            | 5670          |
|    policy_gradient_loss | -4.26e-06     |
|    value_loss           | 29.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 212      |
|    iterations      | 568      |
|    time_elapsed    | 43893    |
|    total_timesteps | 9306112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.8e+03      |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 569          |
|    time_elapsed         | 43966        |
|    total_timesteps      | 9322496      |
| train/                  |              |
|    approx_kl            | 6.475602e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00153     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.272        |
|    n_updates            | 5680         |
|    policy_gradient_loss | -2.28e-06    |
|    value_loss           | 8.46         |
------------------------------------------
Eval num_timesteps=9325000, episode_reward=3002.46 +/- 0.26
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9325000       |
| train/                  |               |
|    approx_kl            | 1.8987412e-06 |
|    clip_fraction        | 3.05e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00216      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.129         |
|    n_updates            | 5690          |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 0.571         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 570      |
|    time_elapsed    | 44054    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=3002.49 +/- 0.37
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9350000       |
| train/                  |               |
|    approx_kl            | 0.00030584805 |
|    clip_fraction        | 0.000812      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.395         |
|    n_updates            | 5700          |
|    policy_gradient_loss | -0.000103     |
|    value_loss           | 0.391         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 571      |
|    time_elapsed    | 44142    |
|    total_timesteps | 9355264  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 572           |
|    time_elapsed         | 44216         |
|    total_timesteps      | 9371648       |
| train/                  |               |
|    approx_kl            | 1.7808678e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00431      |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0836        |
|    n_updates            | 5710          |
|    policy_gradient_loss | 6.11e-05      |
|    value_loss           | 0.209         |
-------------------------------------------
Eval num_timesteps=9375000, episode_reward=2422.83 +/- 1159.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 9375000      |
| train/                  |              |
|    approx_kl            | 3.663494e-05 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00306     |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.129        |
|    n_updates            | 5720         |
|    policy_gradient_loss | -4.67e-05    |
|    value_loss           | 0.339        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    fps             | 211      |
|    iterations      | 573      |
|    time_elapsed    | 44304    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=3002.37 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9400000      |
| train/                  |              |
|    approx_kl            | 5.330915e-05 |
|    clip_fraction        | 0.000543     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0033      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.256        |
|    n_updates            | 5730         |
|    policy_gradient_loss | -7.52e-05    |
|    value_loss           | 0.183        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    fps             | 211      |
|    iterations      | 574      |
|    time_elapsed    | 44392    |
|    total_timesteps | 9404416  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 575           |
|    time_elapsed         | 44465         |
|    total_timesteps      | 9420800       |
| train/                  |               |
|    approx_kl            | 4.0363335e-05 |
|    clip_fraction        | 0.000128      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00251      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0523        |
|    n_updates            | 5740          |
|    policy_gradient_loss | -2.58e-05     |
|    value_loss           | 0.286         |
-------------------------------------------
Eval num_timesteps=9425000, episode_reward=3002.28 +/- 0.08
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9425000       |
| train/                  |               |
|    approx_kl            | 9.1439266e-05 |
|    clip_fraction        | 9.16e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00125      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.129         |
|    n_updates            | 5750          |
|    policy_gradient_loss | -2.73e-05     |
|    value_loss           | 0.167         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 576      |
|    time_elapsed    | 44555    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=3002.36 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.006524485 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0859     |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00976     |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 8.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 577      |
|    time_elapsed    | 44644    |
|    total_timesteps | 9453568  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.63e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 578          |
|    time_elapsed         | 44717        |
|    total_timesteps      | 9469952      |
| train/                  |              |
|    approx_kl            | 0.0010764159 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.0003       |
|    loss                 | 140          |
|    n_updates            | 5770         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 587          |
------------------------------------------
Eval num_timesteps=9475000, episode_reward=3002.45 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9475000      |
| train/                  |              |
|    approx_kl            | 0.0006496535 |
|    clip_fraction        | 0.00499      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.25        |
|    explained_variance   | 0.853        |
|    learning_rate        | 0.0003       |
|    loss                 | 707          |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000483    |
|    value_loss           | 895          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 975      |
| time/              |          |
|    fps             | 211      |
|    iterations      | 579      |
|    time_elapsed    | 44806    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=3002.68 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9500000      |
| train/                  |              |
|    approx_kl            | 0.0024361452 |
|    clip_fraction        | 0.00978      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0003       |
|    loss                 | 23.9         |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00075     |
|    value_loss           | 377          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 580      |
|    time_elapsed    | 44894    |
|    total_timesteps | 9502720  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.33e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 581          |
|    time_elapsed         | 44968        |
|    total_timesteps      | 9519104      |
| train/                  |              |
|    approx_kl            | 0.0031250804 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.4         |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 278          |
------------------------------------------
Eval num_timesteps=9525000, episode_reward=3002.70 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9525000      |
| train/                  |              |
|    approx_kl            | 0.0059814826 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.5         |
|    n_updates            | 5810         |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 126          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 582      |
|    time_elapsed    | 45056    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=3002.35 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.008869313 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 97.3        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 160         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 583      |
|    time_elapsed    | 45144    |
|    total_timesteps | 9551872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.4e+03      |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 584          |
|    time_elapsed         | 45220        |
|    total_timesteps      | 9568256      |
| train/                  |              |
|    approx_kl            | 0.0072670253 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.08         |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.00529     |
|    value_loss           | 83.1         |
------------------------------------------
Eval num_timesteps=9575000, episode_reward=3002.76 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9575000      |
| train/                  |              |
|    approx_kl            | 0.0024984176 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.53         |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.000265    |
|    value_loss           | 45.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 585      |
|    time_elapsed    | 45308    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=3002.56 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.009278814 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0889     |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.592       |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 13.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 586      |
|    time_elapsed    | 45396    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.83e+03    |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 587         |
|    time_elapsed         | 45469       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.008625841 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0688     |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.361       |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 26.1        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=3002.39 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9625000      |
| train/                  |              |
|    approx_kl            | 0.0018169796 |
|    clip_fraction        | 0.00543      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0357      |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.9         |
|    n_updates            | 5870         |
|    policy_gradient_loss | 0.000404     |
|    value_loss           | 36.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 588      |
|    time_elapsed    | 45557    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=3002.43 +/- 0.46
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9650000       |
| train/                  |               |
|    approx_kl            | 0.00067978865 |
|    clip_fraction        | 0.00408       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0281       |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.32          |
|    n_updates            | 5880          |
|    policy_gradient_loss | -0.000374     |
|    value_loss           | 5.27          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 589      |
|    time_elapsed    | 45645    |
|    total_timesteps | 9650176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.84e+03   |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 590        |
|    time_elapsed         | 45718      |
|    total_timesteps      | 9666560    |
| train/                  |            |
|    approx_kl            | 0.00479645 |
|    clip_fraction        | 0.00579    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0704    |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 5890       |
|    policy_gradient_loss | 0.000501   |
|    value_loss           | 17.9       |
----------------------------------------
Eval num_timesteps=9675000, episode_reward=3002.37 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.024186399 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 61.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 591      |
|    time_elapsed    | 45807    |
|    total_timesteps | 9682944  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.65e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 592          |
|    time_elapsed         | 45880        |
|    total_timesteps      | 9699328      |
| train/                  |              |
|    approx_kl            | 0.0033612014 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.335       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | 41.5         |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.000973    |
|    value_loss           | 385          |
------------------------------------------
Eval num_timesteps=9700000, episode_reward=3002.35 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.015863564 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.4        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 584         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 593      |
|    time_elapsed    | 45972    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=3002.54 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 9725000    |
| train/                  |            |
|    approx_kl            | 0.02409115 |
|    clip_fraction        | 0.029      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 207        |
|    n_updates            | 5930       |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 242        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 953      |
| time/              |          |
|    fps             | 211      |
|    iterations      | 594      |
|    time_elapsed    | 46059    |
|    total_timesteps | 9732096  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 959          |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 595          |
|    time_elapsed         | 46132        |
|    total_timesteps      | 9748480      |
| train/                  |              |
|    approx_kl            | 0.0071045225 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.478       |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | 22.3         |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 343          |
------------------------------------------
Eval num_timesteps=9750000, episode_reward=3002.36 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.004555678 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.2        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 191         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 596      |
|    time_elapsed    | 46219    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=2422.72 +/- 1159.18
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 9775000      |
| train/                  |              |
|    approx_kl            | 0.0067244694 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.399       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 54.3         |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 171          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 597      |
|    time_elapsed    | 46307    |
|    total_timesteps | 9781248  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.01e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 598          |
|    time_elapsed         | 46380        |
|    total_timesteps      | 9797632      |
| train/                  |              |
|    approx_kl            | 0.0044485605 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.000445    |
|    value_loss           | 110          |
------------------------------------------
Eval num_timesteps=9800000, episode_reward=3002.28 +/- 0.18
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9800000      |
| train/                  |              |
|    approx_kl            | 0.0028911042 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 32           |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.000403    |
|    value_loss           | 67.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.27e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 599      |
|    time_elapsed    | 46467    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=3002.68 +/- 0.70
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9825000      |
| train/                  |              |
|    approx_kl            | 0.0016842973 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.305       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 79.1         |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.000598    |
|    value_loss           | 135          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 211      |
|    iterations      | 600      |
|    time_elapsed    | 46555    |
|    total_timesteps | 9830400  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.51e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 601          |
|    time_elapsed         | 46628        |
|    total_timesteps      | 9846784      |
| train/                  |              |
|    approx_kl            | 0.0018570039 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 49           |
|    n_updates            | 6000         |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 92           |
------------------------------------------
Eval num_timesteps=9850000, episode_reward=3002.56 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.005520768 |
|    clip_fraction        | 0.0168      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0871     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.47        |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 17.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 211      |
|    iterations      | 602      |
|    time_elapsed    | 46715    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=3002.66 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.003974681 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0556     |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.000846   |
|    value_loss           | 34.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 603      |
|    time_elapsed    | 46803    |
|    total_timesteps | 9879552  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.89e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 604          |
|    time_elapsed         | 46877        |
|    total_timesteps      | 9895936      |
| train/                  |              |
|    approx_kl            | 0.0029416443 |
|    clip_fraction        | 0.00587      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0566      |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 44           |
|    n_updates            | 6030         |
|    policy_gradient_loss | 0.000579     |
|    value_loss           | 21.8         |
------------------------------------------
Eval num_timesteps=9900000, episode_reward=3002.39 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9900000      |
| train/                  |              |
|    approx_kl            | 0.0028239698 |
|    clip_fraction        | 0.00651      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.389        |
|    n_updates            | 6040         |
|    policy_gradient_loss | -0.000402    |
|    value_loss           | 67.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 605      |
|    time_elapsed    | 46964    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=3002.75 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.002202677 |
|    clip_fraction        | 0.00784     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0375     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00069    |
|    value_loss           | 14.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 606      |
|    time_elapsed    | 47051    |
|    total_timesteps | 9928704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.82e+03     |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 607          |
|    time_elapsed         | 47125        |
|    total_timesteps      | 9945088      |
| train/                  |              |
|    approx_kl            | 7.965643e-05 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.036       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.822        |
|    n_updates            | 6060         |
|    policy_gradient_loss | -2.05e-05    |
|    value_loss           | 33.2         |
------------------------------------------
Eval num_timesteps=9950000, episode_reward=3003.04 +/- 0.35
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9950000       |
| train/                  |               |
|    approx_kl            | 0.00028968832 |
|    clip_fraction        | 0.00358       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0376       |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.164         |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000454     |
|    value_loss           | 3.52          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 608      |
|    time_elapsed    | 47213    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=3002.51 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.017943442 |
|    clip_fraction        | 0.00242     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0449     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.9         |
|    n_updates            | 6080        |
|    policy_gradient_loss | 0.0004      |
|    value_loss           | 4.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 609      |
|    time_elapsed    | 47302    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.13e+03    |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 610         |
|    time_elapsed         | 47375       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.041525673 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.186       |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 2.28        |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=3002.37 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.005898983 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 1.65e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 611      |
|    time_elapsed    | 47463    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v9_1
