========== uav-v0 ==========
Seed: 341590409
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v0_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.03e+03 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 1         |
|    time_elapsed    | 104       |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-1319.18 +/- 240.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008490553 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.00174     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.16e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00847    |
|    value_loss           | 4.37e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.08e+03 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 2         |
|    time_elapsed    | 258       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.14e+03   |
| time/                   |             |
|    fps                  | 126         |
|    iterations           | 3           |
|    time_elapsed         | 387         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008955717 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -5.01e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 9.38e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 2.39e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-1199.25 +/- 0.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008899264 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 5.96e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 5.79e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 8.9e+03     |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.58e+03 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 4         |
|    time_elapsed    | 541       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-1199.10 +/- 379.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.008373883 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.82e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 6.49e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.19e+03 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 5         |
|    time_elapsed    | 695       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.47e+03   |
| time/                   |             |
|    fps                  | 119         |
|    iterations           | 6           |
|    time_elapsed         | 824         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.010625635 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | 584         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1199.15 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.013439048 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 253         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 319         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -994     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 7        |
|    time_elapsed    | 978      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-1079.20 +/- 239.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.010583005 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 168         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 8        |
|    time_elapsed    | 1131     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -437        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 9           |
|    time_elapsed         | 1261        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.017029755 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 33.6        |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-598.80 +/- 379.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -599        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.011785867 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | -0.381      |
|    learning_rate        | 0.0003      |
|    loss                 | 863         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 335         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -434     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 10       |
|    time_elapsed    | 1415     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-239.07 +/- 292.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -239        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.009640942 |
|    clip_fraction        | 0.0976      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | -0.184      |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 424         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -366     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 11       |
|    time_elapsed    | 1568     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -375        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 12          |
|    time_elapsed         | 1697        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.016915532 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-118.70 +/- 239.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.015858803 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0003      |
|    loss                 | 241         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00917    |
|    value_loss           | 213         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 13       |
|    time_elapsed    | 1850     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=1.19 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.018135557 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.2        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 100         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 14       |
|    time_elapsed    | 2004     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -340        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 15          |
|    time_elapsed         | 2133        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.016562141 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.0003      |
|    loss                 | 153         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 386         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1.07 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.010596693 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 3.26e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -475     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 16       |
|    time_elapsed    | 2287     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=1.04 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.04        |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.009449659 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 954         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 3.05e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -467     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 17       |
|    time_elapsed    | 2440     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -462        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 18          |
|    time_elapsed         | 2569        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.015501727 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.1        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 987         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=0.99 +/- 0.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.988      |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.01704454 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.7       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.00881   |
|    value_loss           | 107        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 19       |
|    time_elapsed    | 2722     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=0.96 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.964       |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.010362852 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.8        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 2.38e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -476     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 20       |
|    time_elapsed    | 2875     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -420        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 21          |
|    time_elapsed         | 3004        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.010361932 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 150         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 3.74e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=1.29 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.018964164 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 22          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -291     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 22       |
|    time_elapsed    | 3159     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=2.14 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.14        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.020042825 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 249         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 23       |
|    time_elapsed    | 3313     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -139        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 24          |
|    time_elapsed         | 3442        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.020490548 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0003      |
|    loss                 | 18          |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 86.5        |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-102.29 +/- 207.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -102        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.015379302 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.00501     |
|    learning_rate        | 0.0003      |
|    loss                 | 99.3        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 223         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 25       |
|    time_elapsed    | 3595     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=1.25 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.018854838 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.38        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 45.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 26       |
|    time_elapsed    | 3749     |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -94.8     |
| time/                   |           |
|    fps                  | 114       |
|    iterations           | 27        |
|    time_elapsed         | 3878      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0237267 |
|    clip_fraction        | 0.328     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.64     |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.8      |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.00253  |
|    value_loss           | 87.3      |
---------------------------------------
Eval num_timesteps=450000, episode_reward=2.04 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.021901568 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.408       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 10.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.3    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 28       |
|    time_elapsed    | 4032     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.61 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.021245122 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 1.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 29       |
|    time_elapsed    | 4186     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -182        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 30          |
|    time_elapsed         | 4315        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.011368424 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 607         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=1.79 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.019736778 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.908       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 6.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 31       |
|    time_elapsed    | 4469     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -290        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 32          |
|    time_elapsed         | 4598        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.019148178 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.516       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=1.61 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.018633772 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.5e+03     |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 1.99e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 33       |
|    time_elapsed    | 4752     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2.03 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.021397935 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.2        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 8.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 34       |
|    time_elapsed    | 4906     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -270        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 35          |
|    time_elapsed         | 5036        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.029253356 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 42.7        |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=1.96 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.016775776 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.93e+03    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.000929   |
|    value_loss           | 3.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 36       |
|    time_elapsed    | 5190     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2.12 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.020233566 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 5.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 37       |
|    time_elapsed    | 5343     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 38          |
|    time_elapsed         | 5472        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.032996543 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.5        |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.00712     |
|    value_loss           | 287         |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=1.56 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.021031782 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.202       |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.00349     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -244     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 39       |
|    time_elapsed    | 5625     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1.95 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.022536015 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.0244      |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00442     |
|    value_loss           | 296         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -254     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 40       |
|    time_elapsed    | 5779     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -242        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 41          |
|    time_elapsed         | 5908        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.025928503 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.179       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 91.4        |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=1.49 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.019361563 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 19.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 42       |
|    time_elapsed    | 6062     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=1.46 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.022746895 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00019     |
|    value_loss           | 383         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.2    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 43       |
|    time_elapsed    | 6213     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 44          |
|    time_elapsed         | 6339        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.023004109 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.88        |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=1.49 +/- 0.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.49       |
| time/                   |            |
|    total_timesteps      | 725000     |
| train/                  |            |
|    approx_kl            | 0.01615246 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.23       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.000337  |
|    value_loss           | 379        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 45       |
|    time_elapsed    | 6489     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1.48 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.021072853 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.28        |
|    n_updates            | 450         |
|    policy_gradient_loss | 0.000608    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -309     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 46       |
|    time_elapsed    | 6640     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -262        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 47          |
|    time_elapsed         | 6766        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.008227132 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 589         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=1.37 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.015687171 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 371         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 187         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -334     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 48       |
|    time_elapsed    | 6916     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=1.56 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.007515042 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.2        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 345         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 49       |
|    time_elapsed    | 7066     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -147        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 50          |
|    time_elapsed         | 7192        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.013314752 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=1.82 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.021409215 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.79        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 32.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -89.3    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 51       |
|    time_elapsed    | 7342     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2.30 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.017155591 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 33.3        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 52       |
|    time_elapsed    | 7492     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -287        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 53          |
|    time_elapsed         | 7618        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.009603871 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 319         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 910         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=1.75 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 875000       |
| train/                  |              |
|    approx_kl            | 0.0088431435 |
|    clip_fraction        | 0.0843       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.6         |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | 760          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00892     |
|    value_loss           | 1.4e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 54       |
|    time_elapsed    | 7768     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=1.61 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.020280331 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.984       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 11.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -307     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 55       |
|    time_elapsed    | 7919     |
|    total_timesteps | 901120   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 56         |
|    time_elapsed         | 8046       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.01871561 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00266   |
|    value_loss           | 123        |
----------------------------------------
Eval num_timesteps=925000, episode_reward=1.33 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.33         |
| time/                   |              |
|    total_timesteps      | 925000       |
| train/                  |              |
|    approx_kl            | 0.0140210595 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.61        |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.0003       |
|    loss                 | 83.9         |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 103          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.8    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 57       |
|    time_elapsed    | 8197     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1.24 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.023167681 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.9    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 58       |
|    time_elapsed    | 8348     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -46.3       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 59          |
|    time_elapsed         | 8474        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.013679601 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.8         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=1.74 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.017666748 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.263       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 8.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.4    |
| time/              |          |
|    fps             | 113      |
|    iterations      | 60       |
|    time_elapsed    | 8625     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -102        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 61          |
|    time_elapsed         | 8751        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.018680964 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 88.9        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=2.07 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.013366667 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.3        |
|    n_updates            | 610         |
|    policy_gradient_loss | 0.000573    |
|    value_loss           | 220         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 62       |
|    time_elapsed    | 8902     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=1.32 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.32       |
| time/                   |            |
|    total_timesteps      | 1025000    |
| train/                  |            |
|    approx_kl            | 0.01723594 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 367        |
|    n_updates            | 620        |
|    policy_gradient_loss | -3.98e-06  |
|    value_loss           | 699        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 63       |
|    time_elapsed    | 9052     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -190        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 64          |
|    time_elapsed         | 9178        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.019435126 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.98        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=1.41 +/- 0.14
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.41       |
| time/                   |            |
|    total_timesteps      | 1050000    |
| train/                  |            |
|    approx_kl            | 0.01849841 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.57      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.00416   |
|    value_loss           | 7.9        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 65       |
|    time_elapsed    | 9329     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=1.49 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.017365437 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 300         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 83.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 66       |
|    time_elapsed    | 9479     |
|    total_timesteps | 1081344  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -29.8      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 67         |
|    time_elapsed         | 9605       |
|    total_timesteps      | 1097728    |
| train/                  |            |
|    approx_kl            | 0.01855842 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.6       |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.00385   |
|    value_loss           | 22.4       |
----------------------------------------
Eval num_timesteps=1100000, episode_reward=2.61 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.61        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.017217062 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.601       |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 12.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 68       |
|    time_elapsed    | 9756     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=1.22 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.22        |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.019022614 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.766       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 6.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 69       |
|    time_elapsed    | 9906     |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -79.1      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 70         |
|    time_elapsed         | 10032      |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.01736345 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.695      |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00498   |
|    value_loss           | 6.91       |
----------------------------------------
Eval num_timesteps=1150000, episode_reward=1.46 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.013307219 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.5        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 71       |
|    time_elapsed    | 10182    |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=1.85 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.023243712 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.44        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 28          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 72       |
|    time_elapsed    | 10333    |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -143        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 73          |
|    time_elapsed         | 10459       |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.007944502 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 304         |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 455         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=1.23 +/- 0.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.23       |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.02053254 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0064    |
|    value_loss           | 1.71       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 74       |
|    time_elapsed    | 10609    |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=1.17 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.018435884 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 9.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.6    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 75       |
|    time_elapsed    | 10759    |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -21.3       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 76          |
|    time_elapsed         | 10885       |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.016638028 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.8         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 62.2        |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=1.32 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.020019297 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.437       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00868    |
|    value_loss           | 6.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 77       |
|    time_elapsed    | 11035    |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=2.17 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.025431503 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 770         |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 31.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 78       |
|    time_elapsed    | 11186    |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -222        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 79          |
|    time_elapsed         | 11311       |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.012651624 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.000579   |
|    value_loss           | 618         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=1.58 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.017557072 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.5        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 429         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -296     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 80       |
|    time_elapsed    | 11462    |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=1.74 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.021093315 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.8        |
|    n_updates            | 800         |
|    policy_gradient_loss | 0.00233     |
|    value_loss           | 193         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -435     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 81       |
|    time_elapsed    | 11612    |
|    total_timesteps | 1327104  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -354         |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 82           |
|    time_elapsed         | 11738        |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0088230595 |
|    clip_fraction        | 0.062        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.1         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.0035      |
|    value_loss           | 2.03e+03     |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=1.33 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.015393583 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 411         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 344         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -264     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 83       |
|    time_elapsed    | 11889    |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=2.01 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.018858252 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.9        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 43.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -255     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 84       |
|    time_elapsed    | 12039    |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -94         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 85          |
|    time_elapsed         | 12165       |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.023441853 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.202       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 29.3        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=1.27 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.019042298 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.84        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0072     |
|    value_loss           | 7.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 86       |
|    time_elapsed    | 12315    |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=1.66 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.026645424 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00039    |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 87       |
|    time_elapsed    | 12466    |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -379        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 88          |
|    time_elapsed         | 12592       |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.008197811 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 480         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00456    |
|    value_loss           | 471         |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=1.58 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.017929683 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.29        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -390     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 89       |
|    time_elapsed    | 12742    |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -255        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 90          |
|    time_elapsed         | 12868       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.015653184 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 55.8        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=1.35 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.35        |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.021369265 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 91       |
|    time_elapsed    | 13018    |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1.92 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.017975591 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | -0.516      |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 92       |
|    time_elapsed    | 13168    |
|    total_timesteps | 1507328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -52.4      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 93         |
|    time_elapsed         | 13294      |
|    total_timesteps      | 1523712    |
| train/                  |            |
|    approx_kl            | 0.02093895 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.927      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00518   |
|    value_loss           | 18.7       |
----------------------------------------
Eval num_timesteps=1525000, episode_reward=1.70 +/- 0.44
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.7        |
| time/                   |            |
|    total_timesteps      | 1525000    |
| train/                  |            |
|    approx_kl            | 0.02256166 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.03       |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.000466  |
|    value_loss           | 68.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 94       |
|    time_elapsed    | 13444    |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2.82 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.011672163 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.4        |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 1.94e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -249     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 95       |
|    time_elapsed    | 13595    |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -270        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 96          |
|    time_elapsed         | 13721       |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.019892575 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 950         |
|    policy_gradient_loss | 0.00302     |
|    value_loss           | 640         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=1.68 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.011142115 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.9        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 97       |
|    time_elapsed    | 13871    |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1.52 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.015512286 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.4        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 56.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 98       |
|    time_elapsed    | 14022    |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 99          |
|    time_elapsed         | 14148       |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.028689522 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.6        |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 89.5        |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=1.87 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.018672826 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 53.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 100      |
|    time_elapsed    | 14298    |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=1.27 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.021730538 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -3.59e-05   |
|    value_loss           | 29.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -98.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 101      |
|    time_elapsed    | 14448    |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -79.9       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 102         |
|    time_elapsed         | 14574       |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.022200638 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00155     |
|    value_loss           | 249         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=1.25 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.016824365 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.36        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 9.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 103      |
|    time_elapsed    | 14724    |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1.36 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.024492443 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.4        |
|    n_updates            | 1030        |
|    policy_gradient_loss | 0.0034      |
|    value_loss           | 48.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -87      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 104      |
|    time_elapsed    | 14874    |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -58.2      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 105        |
|    time_elapsed         | 15000      |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 0.01717355 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.2        |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00255   |
|    value_loss           | 154        |
----------------------------------------
Eval num_timesteps=1725000, episode_reward=1.69 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.69        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.023864362 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0404      |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.000357    |
|    value_loss           | 25.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 106      |
|    time_elapsed    | 15151    |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=1.64 +/- 0.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.64         |
| time/                   |              |
|    total_timesteps      | 1750000      |
| train/                  |              |
|    approx_kl            | 0.0052602133 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | 217          |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 338          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 107      |
|    time_elapsed    | 15301    |
|    total_timesteps | 1753088  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -213         |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 108          |
|    time_elapsed         | 15427        |
|    total_timesteps      | 1769472      |
| train/                  |              |
|    approx_kl            | 0.0062235584 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.0003       |
|    loss                 | 57.7         |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 360          |
------------------------------------------
Eval num_timesteps=1775000, episode_reward=1.72 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.018306352 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.2        |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 284         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 109      |
|    time_elapsed    | 15577    |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2.77 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.020073911 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 34.9        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 39.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 110      |
|    time_elapsed    | 15727    |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 111         |
|    time_elapsed         | 15854       |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.024734525 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.9        |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.00434     |
|    value_loss           | 133         |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=1.97 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.015816756 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 4.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 112      |
|    time_elapsed    | 16005    |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2.49 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.016758451 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 1.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 113      |
|    time_elapsed    | 16155    |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 114         |
|    time_elapsed         | 16281       |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.017656876 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.389       |
|    n_updates            | 1130        |
|    policy_gradient_loss | 5.31e-05    |
|    value_loss           | 11.8        |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=2.50 +/- 0.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.5          |
| time/                   |              |
|    total_timesteps      | 1875000      |
| train/                  |              |
|    approx_kl            | 0.0071959463 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | -0.0954      |
|    learning_rate        | 0.0003       |
|    loss                 | 67.3         |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00282     |
|    value_loss           | 176          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 115      |
|    time_elapsed    | 16432    |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2.23 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.015148572 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 142         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 116      |
|    time_elapsed    | 16582    |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -249        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 117         |
|    time_elapsed         | 16708       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.019838385 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=1.73 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.008069116 |
|    clip_fraction        | 0.0665      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.9        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 405         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -210     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 118      |
|    time_elapsed    | 16858    |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -393        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 119         |
|    time_elapsed         | 16984       |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.014699691 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.94        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=2.54 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.012375282 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 427         |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 717         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 120      |
|    time_elapsed    | 17135    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=2.05 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.05        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.010085668 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 842         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 121      |
|    time_elapsed    | 17285    |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -376        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 122         |
|    time_elapsed         | 17411       |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.016957875 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 22.3        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=2.56 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.013693551 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.991       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 123      |
|    time_elapsed    | 17561    |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=2.42 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.019461203 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.381       |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 9.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 124      |
|    time_elapsed    | 17712    |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23         |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 125         |
|    time_elapsed         | 17838       |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.011233015 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.56        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 46.8        |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=2.29 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.014628144 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 53          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 126      |
|    time_elapsed    | 17988    |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=2.69 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.69        |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.017744612 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 76.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -223     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 127      |
|    time_elapsed    | 18138    |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -221        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 128         |
|    time_elapsed         | 18264       |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.015592311 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.4        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00065    |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=2.40 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.016456973 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -232     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 129      |
|    time_elapsed    | 18415    |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=2.83 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.014175606 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.36        |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 81.9        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 130      |
|    time_elapsed    | 18565    |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 131         |
|    time_elapsed         | 18691       |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.015902963 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 9.87        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=2.79 +/- 0.91
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.79       |
| time/                   |            |
|    total_timesteps      | 2150000    |
| train/                  |            |
|    approx_kl            | 0.01672337 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.45      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 1310       |
|    policy_gradient_loss | 0.000733   |
|    value_loss           | 84.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 132      |
|    time_elapsed    | 18842    |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=2.22 +/- 0.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.22       |
| time/                   |            |
|    total_timesteps      | 2175000    |
| train/                  |            |
|    approx_kl            | 0.01336994 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.39      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.41       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.00521   |
|    value_loss           | 26.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 133      |
|    time_elapsed    | 18992    |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -50.1       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 134         |
|    time_elapsed         | 19118       |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.014334302 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.67        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 12.8        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=1.92 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.012759216 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.5        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 208         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.2    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 135      |
|    time_elapsed    | 19268    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=2.68 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.025921471 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.79        |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.00593     |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 136      |
|    time_elapsed    | 19419    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -79.1       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 137         |
|    time_elapsed         | 19545       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.016553223 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.466       |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 19.6        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=2.04 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.018243408 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.934       |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.000281   |
|    value_loss           | 3.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.6    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 138      |
|    time_elapsed    | 19696    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=2.24 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.011451481 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.44        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.6    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 139      |
|    time_elapsed    | 19846    |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -62.5       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 140         |
|    time_elapsed         | 19972       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.028657403 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.84        |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.0043      |
|    value_loss           | 106         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=2.46 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.008015737 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -117     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 141      |
|    time_elapsed    | 20122    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=2.00 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.045130845 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    n_updates            | 1410        |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 142      |
|    time_elapsed    | 20273    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -131        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 143         |
|    time_elapsed         | 20399       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.015680982 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 153         |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 309         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=1.84 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.020425746 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.83        |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 6.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -76.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 144      |
|    time_elapsed    | 20549    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=1.51 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.014175994 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.303       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00485    |
|    value_loss           | 3.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 145      |
|    time_elapsed    | 20700    |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -219        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 146         |
|    time_elapsed         | 20825       |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.009586548 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.5        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=2.56 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.017381245 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 174         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.000895   |
|    value_loss           | 392         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 147      |
|    time_elapsed    | 20976    |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -220        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 148         |
|    time_elapsed         | 21102       |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.020066166 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.245       |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 15.6        |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=2.70 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.014622198 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 2.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 149      |
|    time_elapsed    | 21252    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=-180.94 +/- 367.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -181       |
| time/                   |            |
|    total_timesteps      | 2450000    |
| train/                  |            |
|    approx_kl            | 0.01881383 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 22         |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.00184   |
|    value_loss           | 7.6        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.69    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 150      |
|    time_elapsed    | 21403    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.04       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 151         |
|    time_elapsed         | 21530       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.014725904 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.004      |
|    value_loss           | 2.53        |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=2.50 +/- 0.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.5        |
| time/                   |            |
|    total_timesteps      | 2475000    |
| train/                  |            |
|    approx_kl            | 0.02338416 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.3       |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.00119   |
|    value_loss           | 21.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 152      |
|    time_elapsed    | 21680    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2.25 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.020258881 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0876      |
|    n_updates            | 1520        |
|    policy_gradient_loss | 0.000477    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 153      |
|    time_elapsed    | 21830    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -212        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 154         |
|    time_elapsed         | 21956       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.034111865 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.1        |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.00275     |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=1.99 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.019480664 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.000778   |
|    value_loss           | 71.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -305     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 155      |
|    time_elapsed    | 22106    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2.40 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.013413487 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 305         |
|    n_updates            | 1550        |
|    policy_gradient_loss | 0.00143     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -326     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 156      |
|    time_elapsed    | 22257    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -496        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 157         |
|    time_elapsed         | 22383       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.014252771 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 679         |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 974         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=2.21 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.007430484 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.6        |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 3.63e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 158      |
|    time_elapsed    | 22533    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2.19 +/- 0.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.19       |
| time/                   |            |
|    total_timesteps      | 2600000    |
| train/                  |            |
|    approx_kl            | 0.01792945 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 1580       |
|    policy_gradient_loss | 0.000386   |
|    value_loss           | 220        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -633     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 159      |
|    time_elapsed    | 22683    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -492        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 160         |
|    time_elapsed         | 22809       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.007945584 |
|    clip_fraction        | 0.0572      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 593         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 976         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=2.33 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.016060056 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 82.7        |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 554         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -336     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 161      |
|    time_elapsed    | 22960    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=3.81 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.81        |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.012311354 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.67        |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 295         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -462     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 162      |
|    time_elapsed    | 23110    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -256        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 163         |
|    time_elapsed         | 23236       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.011735391 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 691         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=2.78 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.014024123 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.604       |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 4.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -265     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 164      |
|    time_elapsed    | 23386    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2.22 +/- 0.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.22         |
| time/                   |              |
|    total_timesteps      | 2700000      |
| train/                  |              |
|    approx_kl            | 0.0062071923 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.48        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 296          |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00498     |
|    value_loss           | 793          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 165      |
|    time_elapsed    | 23537    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 166         |
|    time_elapsed         | 23664       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.009041589 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.1        |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 224         |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=2.32 +/- 0.91
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.32       |
| time/                   |            |
|    total_timesteps      | 2725000    |
| train/                  |            |
|    approx_kl            | 0.01931354 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.00103   |
|    value_loss           | 229        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 167      |
|    time_elapsed    | 23815    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=1.90 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.017999925 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.537       |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 7.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 168      |
|    time_elapsed    | 23966    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -82.3       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 169         |
|    time_elapsed         | 24092       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.013419578 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.56        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 10.1        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=1.95 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.012365719 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 281         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 170      |
|    time_elapsed    | 24242    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2.12 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.014566861 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 85.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 171      |
|    time_elapsed    | 24393    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69.6       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 172         |
|    time_elapsed         | 24519       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.015707968 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 11.5        |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=2.30 +/- 0.38
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 2825000      |
| train/                  |              |
|    approx_kl            | 0.0118254125 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.08         |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00444     |
|    value_loss           | 149          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.6    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 173      |
|    time_elapsed    | 24669    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2.95 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.95        |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.034424953 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.37        |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 65.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 174      |
|    time_elapsed    | 24819    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -40.6       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 175         |
|    time_elapsed         | 24946       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.021027554 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.202       |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 5.64        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=4.48 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.48        |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.012893178 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.39        |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 26.3        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 176      |
|    time_elapsed    | 25096    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.99       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 177         |
|    time_elapsed         | 25222       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.013738937 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 5.13        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=2.44 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.015457958 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.61        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 7.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -119     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 178      |
|    time_elapsed    | 25372    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=2.86 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.006954046 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 582         |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 764         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -119     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 179      |
|    time_elapsed    | 25523    |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 180         |
|    time_elapsed         | 25649       |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.015040154 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.46        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 1.82        |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=2.43 +/- 0.78
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.43         |
| time/                   |              |
|    total_timesteps      | 2950000      |
| train/                  |              |
|    approx_kl            | 0.0136893075 |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0003       |
|    loss                 | 19           |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.0072      |
|    value_loss           | 23.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 181      |
|    time_elapsed    | 25799    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=2.58 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.015636053 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.265       |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 2.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 182      |
|    time_elapsed    | 25950    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -68.5       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 183         |
|    time_elapsed         | 26076       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.024841635 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 51.1        |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=2.92 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.92        |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.019418081 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.72        |
|    n_updates            | 1830        |
|    policy_gradient_loss | 8.46e-05    |
|    value_loss           | 9.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 184      |
|    time_elapsed    | 26227    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=2.49 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.018289944 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0936      |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.000276   |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 185      |
|    time_elapsed    | 26377    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.7       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 186         |
|    time_elapsed         | 26503       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.012652066 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 58.3        |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=2.94 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.022282729 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.4         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 187      |
|    time_elapsed    | 26654    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=2.10 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.020410161 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.3        |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.000143   |
|    value_loss           | 150         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 188      |
|    time_elapsed    | 26804    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -72.2       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 189         |
|    time_elapsed         | 26930       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.016735815 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.28        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 4.73        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=1.87 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.020671222 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.66        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 4.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 190      |
|    time_elapsed    | 27080    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=2.97 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.011313784 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 96.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 191      |
|    time_elapsed    | 27231    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.9       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 192         |
|    time_elapsed         | 27357       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.020024937 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0772      |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 0.579       |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=2.26 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.010076276 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 60.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 193      |
|    time_elapsed    | 27507    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=2.31 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.021448731 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.12        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 9.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 194      |
|    time_elapsed    | 27658    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.5       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 195         |
|    time_elapsed         | 27784       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.019617228 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.498       |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 3.18        |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=3.10 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.019581418 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.72        |
|    n_updates            | 1950        |
|    policy_gradient_loss | -7.86e-05   |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.8     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 196      |
|    time_elapsed    | 27934    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=2.94 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.013911402 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 197      |
|    time_elapsed    | 28085    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -77.4       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 198         |
|    time_elapsed         | 28211       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.020812085 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.0706      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.05        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 60.5        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=2.97 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.017047549 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 38          |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 313         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 199      |
|    time_elapsed    | 28361    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=2.36 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.027225504 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1990        |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 75.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 200      |
|    time_elapsed    | 28512    |
|    total_timesteps | 3276800  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 201        |
|    time_elapsed         | 28638      |
|    total_timesteps      | 3293184    |
| train/                  |            |
|    approx_kl            | 0.02251558 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.00103   |
|    value_loss           | 21.1       |
----------------------------------------
Eval num_timesteps=3300000, episode_reward=2.81 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.023488805 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 5.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 202      |
|    time_elapsed    | 28788    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=1.83 +/- 0.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.83       |
| time/                   |            |
|    total_timesteps      | 3325000    |
| train/                  |            |
|    approx_kl            | 0.01984746 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.786      |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.002     |
|    value_loss           | 8.47       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 203      |
|    time_elapsed    | 28939    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.3       |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 204         |
|    time_elapsed         | 29065       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.021258064 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0643      |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 6.7         |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=2.11 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.019410215 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0554      |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 11          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.3    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 205      |
|    time_elapsed    | 29215    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=2.83 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.012204801 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.87        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 53.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 206      |
|    time_elapsed    | 29366    |
|    total_timesteps | 3375104  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -69.7      |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 207        |
|    time_elapsed         | 29492      |
|    total_timesteps      | 3391488    |
| train/                  |            |
|    approx_kl            | 0.01577089 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.45      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.432      |
|    n_updates            | 2060       |
|    policy_gradient_loss | 1.32e-05   |
|    value_loss           | 5.36       |
----------------------------------------
Eval num_timesteps=3400000, episode_reward=2.13 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.034708314 |
|    clip_fraction        | 0.395       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.00437     |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 208      |
|    time_elapsed    | 29642    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 209         |
|    time_elapsed         | 29769       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.013180392 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.4        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00037    |
|    value_loss           | 251         |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=2.33 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.019740365 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 6.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 210      |
|    time_elapsed    | 29919    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=2.24 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.024558147 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 2100        |
|    policy_gradient_loss | 0.000141    |
|    value_loss           | 13          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 211      |
|    time_elapsed    | 30069    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -170        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 212         |
|    time_elapsed         | 30195       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.032897145 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 526         |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=3.45 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.45        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.023281451 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.12        |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.703       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 213      |
|    time_elapsed    | 30346    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=2.12 +/- 0.63
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.12      |
| time/                   |           |
|    total_timesteps      | 3500000   |
| train/                  |           |
|    approx_kl            | 0.0143237 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.44     |
|    explained_variance   | 0.263     |
|    learning_rate        | 0.0003    |
|    loss                 | 71.7      |
|    n_updates            | 2130      |
|    policy_gradient_loss | -0.0066   |
|    value_loss           | 34.9      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 214      |
|    time_elapsed    | 30496    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.9       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 215         |
|    time_elapsed         | 30622       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.023490753 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.6        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 64.5        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=1.67 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.67       |
| time/                   |            |
|    total_timesteps      | 3525000    |
| train/                  |            |
|    approx_kl            | 0.02471871 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.39      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.25       |
|    n_updates            | 2150       |
|    policy_gradient_loss | 0.00167    |
|    value_loss           | 22.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.2    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 216      |
|    time_elapsed    | 30773    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=2.13 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.018092621 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.02        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.000962   |
|    value_loss           | 16.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 217      |
|    time_elapsed    | 30923    |
|    total_timesteps | 3555328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -215       |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 218        |
|    time_elapsed         | 31049      |
|    total_timesteps      | 3571712    |
| train/                  |            |
|    approx_kl            | 0.04759986 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.9        |
|    n_updates            | 2170       |
|    policy_gradient_loss | 0.00607    |
|    value_loss           | 168        |
----------------------------------------
Eval num_timesteps=3575000, episode_reward=1.62 +/- 0.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.62       |
| time/                   |            |
|    total_timesteps      | 3575000    |
| train/                  |            |
|    approx_kl            | 0.02201518 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.64       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.000834  |
|    value_loss           | 163        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 219      |
|    time_elapsed    | 31199    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=2.03 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.012537472 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 243         |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.000635   |
|    value_loss           | 757         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -439     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 220      |
|    time_elapsed    | 31350    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -361        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 221         |
|    time_elapsed         | 31478       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.015391035 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 332         |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=2.05 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.05        |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.015648916 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.5        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 63.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 222      |
|    time_elapsed    | 31628    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=2.23 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.022112295 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.1        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.000213   |
|    value_loss           | 48.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.2    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 223      |
|    time_elapsed    | 31778    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -110        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 224         |
|    time_elapsed         | 31905       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.020612461 |
|    clip_fraction        | 0.375       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.9        |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.000514    |
|    value_loss           | 52.7        |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=2.02 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.009311873 |
|    clip_fraction        | 0.0665      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.8        |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 178         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 225      |
|    time_elapsed    | 32055    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=2.56 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.018319178 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.319       |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.000191    |
|    value_loss           | 94.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -286     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 226      |
|    time_elapsed    | 32205    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -287        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 227         |
|    time_elapsed         | 32331       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.011567391 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 396         |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=1.62 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.018860959 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.404       |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 0.925       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 228      |
|    time_elapsed    | 32482    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=2.04 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.016550228 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 0.366       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 229      |
|    time_elapsed    | 32632    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.6        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 230         |
|    time_elapsed         | 32758       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.019583885 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 11.2        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=2.90 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.024043662 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.269       |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00093    |
|    value_loss           | 2.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.74    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 231      |
|    time_elapsed    | 32909    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=1.83 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.026538279 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 5.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.76    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 232      |
|    time_elapsed    | 33060    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.36       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 233         |
|    time_elapsed         | 33187       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.021309918 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.703       |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.000664   |
|    value_loss           | 3.59        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=2.22 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.018464703 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0454      |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.5    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 234      |
|    time_elapsed    | 33338    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2.49 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.029705929 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.000982    |
|    value_loss           | 29.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 235      |
|    time_elapsed    | 33488    |
|    total_timesteps | 3850240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -299         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 236          |
|    time_elapsed         | 33614        |
|    total_timesteps      | 3866624      |
| train/                  |              |
|    approx_kl            | 0.0033315076 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.43        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 63.4         |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00327     |
|    value_loss           | 215          |
------------------------------------------
Eval num_timesteps=3875000, episode_reward=2.33 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.017346442 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.000893   |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -298     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 237      |
|    time_elapsed    | 33765    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -465        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 238         |
|    time_elapsed         | 33891       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.009554408 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 262         |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 945         |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=2.04 +/- 0.56
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.04      |
| time/                   |           |
|    total_timesteps      | 3900000   |
| train/                  |           |
|    approx_kl            | 0.0212692 |
|    clip_fraction        | 0.333     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.47     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.69      |
|    n_updates            | 2380      |
|    policy_gradient_loss | 0.000257  |
|    value_loss           | 6.93      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -343     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 239      |
|    time_elapsed    | 34041    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=1.68 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.015727144 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0101      |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 0.0935      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 240      |
|    time_elapsed    | 34192    |
|    total_timesteps | 3932160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -119         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 241          |
|    time_elapsed         | 34318        |
|    total_timesteps      | 3948544      |
| train/                  |              |
|    approx_kl            | 0.0061899563 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.33        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 373          |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 763          |
------------------------------------------
Eval num_timesteps=3950000, episode_reward=2.99 +/- 0.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.99       |
| time/                   |            |
|    total_timesteps      | 3950000    |
| train/                  |            |
|    approx_kl            | 0.01984135 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.502      |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.00418   |
|    value_loss           | 0.19       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 242      |
|    time_elapsed    | 34468    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=1.87 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.015144429 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 0.381       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 243      |
|    time_elapsed    | 34619    |
|    total_timesteps | 3981312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -7.19        |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 244          |
|    time_elapsed         | 34745        |
|    total_timesteps      | 3997696      |
| train/                  |              |
|    approx_kl            | 0.0060631046 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.9          |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 28.1         |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=2.19 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.018357906 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 0.278       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.66    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 245      |
|    time_elapsed    | 34895    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=1.71 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.014991837 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.93    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 246      |
|    time_elapsed    | 35046    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.89       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 247         |
|    time_elapsed         | 35172       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.018575843 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0497      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 4.58        |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=1.65 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.65       |
| time/                   |            |
|    total_timesteps      | 4050000    |
| train/                  |            |
|    approx_kl            | 0.01713447 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.138      |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.00485   |
|    value_loss           | 0.311      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.73    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 248      |
|    time_elapsed    | 35323    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=2.07 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.020019826 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0668      |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 1.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.45    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 249      |
|    time_elapsed    | 35473    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.44       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 250         |
|    time_elapsed         | 35600       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.021362625 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 3.17        |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=2.48 +/- 0.68
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.48       |
| time/                   |            |
|    total_timesteps      | 4100000    |
| train/                  |            |
|    approx_kl            | 0.01997831 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.45      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.00187   |
|    value_loss           | 3.12       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.82    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 251      |
|    time_elapsed    | 35751    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=2.33 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.007691893 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00196    |
|    value_loss           | 98.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 252      |
|    time_elapsed    | 35901    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -33.9       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 253         |
|    time_elapsed         | 36027       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.019889046 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.06        |
|    n_updates            | 2520        |
|    policy_gradient_loss | 0.000785    |
|    value_loss           | 58.2        |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=1.87 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.024496345 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.094       |
|    n_updates            | 2530        |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 1.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 254      |
|    time_elapsed    | 36178    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2.01 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.021080803 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0076     |
|    value_loss           | 5.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.92    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 255      |
|    time_elapsed    | 36328    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 256         |
|    time_elapsed         | 36454       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.027710387 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.266       |
|    n_updates            | 2550        |
|    policy_gradient_loss | 0.00255     |
|    value_loss           | 9.44        |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=2.69 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.69        |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.010683693 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 77.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 257      |
|    time_elapsed    | 36605    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2.16 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.016642384 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 2.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 258      |
|    time_elapsed    | 36755    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.6       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 259         |
|    time_elapsed         | 36881       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.017797075 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0637      |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 5.04        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=2.48 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.019933268 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.00693     |
|    learning_rate        | 0.0003      |
|    loss                 | 44.1        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 23.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 260      |
|    time_elapsed    | 37032    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=2.47 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.47        |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.009662714 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.5        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 32          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 261      |
|    time_elapsed    | 37182    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 262         |
|    time_elapsed         | 37308       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.019755637 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 2610        |
|    policy_gradient_loss | 0.000473    |
|    value_loss           | 1.92        |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=2.68 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.016350646 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.492       |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 7.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 263      |
|    time_elapsed    | 37458    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=3.50 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.019567002 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.479       |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 2.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.61    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 264      |
|    time_elapsed    | 37609    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -163        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 265         |
|    time_elapsed         | 37734       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.021599181 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.4         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 27          |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=2.23 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.007333546 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 200         |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 923         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 266      |
|    time_elapsed    | 37885    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -163        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 267         |
|    time_elapsed         | 38011       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.010369562 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.87        |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 19.2        |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=1.69 +/- 0.47
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 4375000    |
| train/                  |            |
|    approx_kl            | 0.02547374 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.582      |
|    n_updates            | 2670       |
|    policy_gradient_loss | -0.00305   |
|    value_loss           | 7.02       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 268      |
|    time_elapsed    | 38161    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=1.89 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.005676208 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 156         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 269      |
|    time_elapsed    | 38311    |
|    total_timesteps | 4407296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 270        |
|    time_elapsed         | 38437      |
|    total_timesteps      | 4423680    |
| train/                  |            |
|    approx_kl            | 0.03426747 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.91       |
|    n_updates            | 2690       |
|    policy_gradient_loss | 0.00818    |
|    value_loss           | 420        |
----------------------------------------
Eval num_timesteps=4425000, episode_reward=1.86 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.017355729 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -7.05e-05   |
|    value_loss           | 9           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 271      |
|    time_elapsed    | 38588    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=2.42 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.013798853 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.511       |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 17.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 272      |
|    time_elapsed    | 38738    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.6       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 273         |
|    time_elapsed         | 38864       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.030086674 |
|    clip_fraction        | 0.413       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 230         |
|    n_updates            | 2720        |
|    policy_gradient_loss | 0.00572     |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=1.93 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.013190288 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.56        |
|    n_updates            | 2730        |
|    policy_gradient_loss | 0.00189     |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 274      |
|    time_elapsed    | 39014    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=1.57 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.017733395 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 31.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 275      |
|    time_elapsed    | 39165    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 276         |
|    time_elapsed         | 39293       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.020752804 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 2.08        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=2.18 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.023493895 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.472       |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 3.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 277      |
|    time_elapsed    | 39444    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=2.46 +/- 0.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.46       |
| time/                   |            |
|    total_timesteps      | 4550000    |
| train/                  |            |
|    approx_kl            | 0.00598426 |
|    clip_fraction        | 0.0298     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.42       |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.00352   |
|    value_loss           | 115        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 278      |
|    time_elapsed    | 39595    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 279         |
|    time_elapsed         | 39722       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.015047863 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.97        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 48.5        |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=1.74 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.026016438 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0739      |
|    n_updates            | 2790        |
|    policy_gradient_loss | 0.00393     |
|    value_loss           | 6.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 280      |
|    time_elapsed    | 39872    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=1.76 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.76         |
| time/                   |              |
|    total_timesteps      | 4600000      |
| train/                  |              |
|    approx_kl            | 0.0064325556 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.66         |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 10.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.04    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 281      |
|    time_elapsed    | 40023    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.78       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 282         |
|    time_elapsed         | 40149       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.020331321 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0798      |
|    n_updates            | 2810        |
|    policy_gradient_loss | 0.00277     |
|    value_loss           | 2.93        |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=1.62 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.020322682 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.533       |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.000599   |
|    value_loss           | 1.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.9     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 283      |
|    time_elapsed    | 40299    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2.46 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.019736309 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.502       |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 16.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.71    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 284      |
|    time_elapsed    | 40451    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 285         |
|    time_elapsed         | 40577       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.026736189 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.507       |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.000287   |
|    value_loss           | 9.26        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=2.50 +/- 0.72
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.5          |
| time/                   |              |
|    total_timesteps      | 4675000      |
| train/                  |              |
|    approx_kl            | 0.0062878295 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.68        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 87.6         |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.00408     |
|    value_loss           | 116          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 286      |
|    time_elapsed    | 40728    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=1.96 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.015750222 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 18          |
|    n_updates            | 2860        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 18          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -94.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 287      |
|    time_elapsed    | 40878    |
|    total_timesteps | 4702208  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -93.5        |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 288          |
|    time_elapsed         | 41005        |
|    total_timesteps      | 4718592      |
| train/                  |              |
|    approx_kl            | 0.0085277725 |
|    clip_fraction        | 0.0721       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.67        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0003       |
|    loss                 | 89.3         |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 304          |
------------------------------------------
Eval num_timesteps=4725000, episode_reward=1.86 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.019746572 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.97        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 4.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 289      |
|    time_elapsed    | 41155    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=1.80 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.018727545 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.57        |
|    n_updates            | 2890        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 199         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -99      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 290      |
|    time_elapsed    | 41305    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -33         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 291         |
|    time_elapsed         | 41432       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.018662913 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.9         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 14.6        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=2.01 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.025161609 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 6.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 292      |
|    time_elapsed    | 41582    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=1.90 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.019256797 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0306      |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 0.781       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.04    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 293      |
|    time_elapsed    | 41733    |
|    total_timesteps | 4800512  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -5.83        |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 294          |
|    time_elapsed         | 41859        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 0.0146549335 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.183        |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 4.41         |
------------------------------------------
Eval num_timesteps=4825000, episode_reward=1.71 +/- 0.70
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.71       |
| time/                   |            |
|    total_timesteps      | 4825000    |
| train/                  |            |
|    approx_kl            | 0.01406319 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.835      |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.00287   |
|    value_loss           | 6.3        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 295      |
|    time_elapsed    | 42010    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 296         |
|    time_elapsed         | 42136       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.023114756 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.518       |
|    n_updates            | 2950        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 34          |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=1.89 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.013459376 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.55        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 61.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 297      |
|    time_elapsed    | 42286    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=1.62 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.020718027 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.47        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 3.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 298      |
|    time_elapsed    | 42437    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 299         |
|    time_elapsed         | 42563       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.043575954 |
|    clip_fraction        | 0.528       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.4        |
|    n_updates            | 2980        |
|    policy_gradient_loss | 0.0134      |
|    value_loss           | 35.1        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=2.04 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.019574951 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 237         |
|    n_updates            | 2990        |
|    policy_gradient_loss | 0.000114    |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 300      |
|    time_elapsed    | 42713    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=2.58 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.018847667 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0136      |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 0.138       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 301      |
|    time_elapsed    | 42864    |
|    total_timesteps | 4931584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -110         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 302          |
|    time_elapsed         | 42991        |
|    total_timesteps      | 4947968      |
| train/                  |              |
|    approx_kl            | 0.0104891425 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.106        |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 2.81         |
------------------------------------------
Eval num_timesteps=4950000, episode_reward=1.97 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.030455483 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.892       |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.000255   |
|    value_loss           | 5.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 303      |
|    time_elapsed    | 43142    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=2.83 +/- 0.92
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.83         |
| time/                   |              |
|    total_timesteps      | 4975000      |
| train/                  |              |
|    approx_kl            | 0.0057039917 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.0003       |
|    loss                 | 223          |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 361          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 304      |
|    time_elapsed    | 43292    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -209        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 305         |
|    time_elapsed         | 43418       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.027917296 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.27        |
|    n_updates            | 3040        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 30.7        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=1.79 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.010261227 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 45.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -210     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 306      |
|    time_elapsed    | 43568    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=2.86 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.021038327 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.8        |
|    n_updates            | 3060        |
|    policy_gradient_loss | 0.00999     |
|    value_loss           | 38.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -237     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 307      |
|    time_elapsed    | 43718    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -124        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 308         |
|    time_elapsed         | 43845       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.025757972 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.2        |
|    n_updates            | 3070        |
|    policy_gradient_loss | 0.00357     |
|    value_loss           | 77.7        |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=1.95 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.019915393 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0795      |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 2.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 309      |
|    time_elapsed    | 43995    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=2.00 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.022251684 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0381     |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 1.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.58    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 310      |
|    time_elapsed    | 44146    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 311         |
|    time_elapsed         | 44272       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.020009927 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 3.85        |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=1.93 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.011714497 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 17.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 312      |
|    time_elapsed    | 44422    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=2.36 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.026020672 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 22.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -52.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 313      |
|    time_elapsed    | 44572    |
|    total_timesteps | 5128192  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -52.6      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 314        |
|    time_elapsed         | 44698      |
|    total_timesteps      | 5144576    |
| train/                  |            |
|    approx_kl            | 0.02943075 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.08      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0948     |
|    n_updates            | 3130       |
|    policy_gradient_loss | 0.000353   |
|    value_loss           | 2.95       |
----------------------------------------
Eval num_timesteps=5150000, episode_reward=1.94 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.016819404 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0266      |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 0.108       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 315      |
|    time_elapsed    | 44849    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=1.98 +/- 0.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.98       |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.02162728 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.36      |
|    explained_variance   | 0.122      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.5       |
|    n_updates            | 3150       |
|    policy_gradient_loss | -1.86e-05  |
|    value_loss           | 201        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 316      |
|    time_elapsed    | 44999    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 317         |
|    time_elapsed         | 45125       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.024147388 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.673       |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 15.7        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=1.68 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.013936792 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 16          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 318      |
|    time_elapsed    | 45275    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=1.94 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.021222126 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3180        |
|    policy_gradient_loss | 0.0016      |
|    value_loss           | 165         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 319      |
|    time_elapsed    | 45426    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.2       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 320         |
|    time_elapsed         | 45552       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.019582693 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0318      |
|    n_updates            | 3190        |
|    policy_gradient_loss | 0.000135    |
|    value_loss           | 0.179       |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=1.30 +/- 0.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 5250000    |
| train/                  |            |
|    approx_kl            | 0.01536452 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.0068    |
|    value_loss           | 0.302      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 321      |
|    time_elapsed    | 45702    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=1.79 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.79       |
| time/                   |            |
|    total_timesteps      | 5275000    |
| train/                  |            |
|    approx_kl            | 0.02541122 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4         |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0893     |
|    n_updates            | 3210       |
|    policy_gradient_loss | -0.000581  |
|    value_loss           | 1.55       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.35    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 322      |
|    time_elapsed    | 45852    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.22       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 323         |
|    time_elapsed         | 45978       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.014958104 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.672       |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 33.4        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=1.78 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.029576834 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 3230        |
|    policy_gradient_loss | 0.00355     |
|    value_loss           | 11.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 324      |
|    time_elapsed    | 46129    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.2       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 325         |
|    time_elapsed         | 46255       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.032346204 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | -0.706      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.87        |
|    n_updates            | 3240        |
|    policy_gradient_loss | 0.003       |
|    value_loss           | 35          |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=2.54 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.019717164 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.035       |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 3.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 326      |
|    time_elapsed    | 46406    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2.43 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.020241516 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0621      |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 1.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 327      |
|    time_elapsed    | 46556    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -79.9       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 328         |
|    time_elapsed         | 46682       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.021067277 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.392       |
|    n_updates            | 3270        |
|    policy_gradient_loss | 0.000267    |
|    value_loss           | 250         |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=2.36 +/- 0.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.36       |
| time/                   |            |
|    total_timesteps      | 5375000    |
| train/                  |            |
|    approx_kl            | 0.01840748 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.248      |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.002     |
|    value_loss           | 8.51       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 329      |
|    time_elapsed    | 46833    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=1.90 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.017931927 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.489       |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 2.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 330      |
|    time_elapsed    | 46984    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 331         |
|    time_elapsed         | 47112       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.016479753 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 2.08        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=1.55 +/- 0.87
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.55       |
| time/                   |            |
|    total_timesteps      | 5425000    |
| train/                  |            |
|    approx_kl            | 0.02938116 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.1       |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.762      |
|    n_updates            | 3310       |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 93.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 332      |
|    time_elapsed    | 47262    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2.17 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.022947617 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.287       |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 2.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 333      |
|    time_elapsed    | 47412    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -117        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 334         |
|    time_elapsed         | 47539       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.019282945 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 3330        |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 1.27        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=1.93 +/- 0.71
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.93         |
| time/                   |              |
|    total_timesteps      | 5475000      |
| train/                  |              |
|    approx_kl            | 0.0065874523 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.73         |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 134          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 335      |
|    time_elapsed    | 47689    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=1.36 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.019627796 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    n_updates            | 3350        |
|    policy_gradient_loss | 0.00271     |
|    value_loss           | 7.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 336      |
|    time_elapsed    | 47840    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -107        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 337         |
|    time_elapsed         | 47967       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.021700762 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 3360        |
|    policy_gradient_loss | 0.000489    |
|    value_loss           | 11          |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=1.91 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.015188845 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0558      |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.000955   |
|    value_loss           | 1.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.3     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 338      |
|    time_elapsed    | 48118    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=1.69 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.69        |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.017456327 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0735      |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 0.744       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.72    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 339      |
|    time_elapsed    | 48270    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.92       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 340         |
|    time_elapsed         | 48396       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.024267348 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.445       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 24.2        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=2.00 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.011356775 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 5           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 341      |
|    time_elapsed    | 48546    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=1.66 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.011828658 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 75.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 342      |
|    time_elapsed    | 48697    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 343         |
|    time_elapsed         | 48823       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.026978122 |
|    clip_fraction        | 0.45        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.338       |
|    n_updates            | 3420        |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=2.01 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.020533122 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 344      |
|    time_elapsed    | 48973    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=2.11 +/- 0.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.11       |
| time/                   |            |
|    total_timesteps      | 5650000    |
| train/                  |            |
|    approx_kl            | 0.01922283 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.14      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.255      |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.00414   |
|    value_loss           | 1.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 345      |
|    time_elapsed    | 49124    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.35       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 346         |
|    time_elapsed         | 49250       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.014627039 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=1.63 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.018515565 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 0.413       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.98    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 347      |
|    time_elapsed    | 49400    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=1.32 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.014877082 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.412       |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 2.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.84    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 348      |
|    time_elapsed    | 49551    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.24       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 349         |
|    time_elapsed         | 49677       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.017504757 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 0.762       |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=1.74 +/- 0.50
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 5725000    |
| train/                  |            |
|    approx_kl            | 0.01872843 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0131    |
|    n_updates            | 3490       |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.0672     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.66    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 350      |
|    time_elapsed    | 49827    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2.34 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.013955728 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0492      |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 0.781       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.12    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 351      |
|    time_elapsed    | 49978    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.8        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 352         |
|    time_elapsed         | 50104       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.019069493 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0751      |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 0.49        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=1.81 +/- 0.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 5775000    |
| train/                  |            |
|    approx_kl            | 0.01843797 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.59       |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.00641   |
|    value_loss           | 4.28       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.08    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 353      |
|    time_elapsed    | 50254    |
|    total_timesteps | 5783552  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -2.55     |
| time/                   |           |
|    fps                  | 115       |
|    iterations           | 354       |
|    time_elapsed         | 50380     |
|    total_timesteps      | 5799936   |
| train/                  |           |
|    approx_kl            | 0.0192909 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.18     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0314    |
|    n_updates            | 3530      |
|    policy_gradient_loss | -0.01     |
|    value_loss           | 0.0668    |
---------------------------------------
Eval num_timesteps=5800000, episode_reward=2.00 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.010999365 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0561      |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 1.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.71    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 355      |
|    time_elapsed    | 50531    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=1.95 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.017825257 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.58        |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 1.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.41    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 356      |
|    time_elapsed    | 50682    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.376      |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 357         |
|    time_elapsed         | 50808       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.016249973 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 1.23        |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=2.37 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.021134524 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 3570        |
|    policy_gradient_loss | 0.00093     |
|    value_loss           | 0.727       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.48    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 358      |
|    time_elapsed    | 50959    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=2.32 +/- 1.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.023600196 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0719      |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 2.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.28    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 359      |
|    time_elapsed    | 51109    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.4        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 360         |
|    time_elapsed         | 51236       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.008306697 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.59        |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 3.34        |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=2.04 +/- 0.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.04       |
| time/                   |            |
|    total_timesteps      | 5900000    |
| train/                  |            |
|    approx_kl            | 0.01712856 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0392     |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.00326   |
|    value_loss           | 2.95       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.16    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 361      |
|    time_elapsed    | 51386    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=2.18 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.024475753 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.908       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 362      |
|    time_elapsed    | 51536    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -138        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 363         |
|    time_elapsed         | 51662       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.013535691 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.884       |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.000648   |
|    value_loss           | 94.6        |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=2.19 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.030315898 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.000382   |
|    value_loss           | 68.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 364      |
|    time_elapsed    | 51812    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=2.53 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.022971231 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.946       |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 2.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 365      |
|    time_elapsed    | 51963    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -394        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 366         |
|    time_elapsed         | 52089       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.026877237 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.582       |
|    n_updates            | 3650        |
|    policy_gradient_loss | 0.00613     |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=1.98 +/- 1.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 6000000      |
| train/                  |              |
|    approx_kl            | 0.0028270776 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0003       |
|    loss                 | 425          |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 1.02e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 367      |
|    time_elapsed    | 52239    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=2.24 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.027655017 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.216       |
|    n_updates            | 3670        |
|    policy_gradient_loss | 0.00466     |
|    value_loss           | 260         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -483     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 368      |
|    time_elapsed    | 52389    |
|    total_timesteps | 6029312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 369        |
|    time_elapsed         | 52515      |
|    total_timesteps      | 6045696    |
| train/                  |            |
|    approx_kl            | 0.02719979 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.28      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 211        |
|    n_updates            | 3680       |
|    policy_gradient_loss | 0.0025     |
|    value_loss           | 174        |
----------------------------------------
Eval num_timesteps=6050000, episode_reward=1.74 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.024716478 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.381       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 18.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 370      |
|    time_elapsed    | 52665    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=1.80 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 6075000      |
| train/                  |              |
|    approx_kl            | 0.0063396906 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 58.3         |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 167          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -303     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 371      |
|    time_elapsed    | 52816    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -259        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 372         |
|    time_elapsed         | 52942       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.008029062 |
|    clip_fraction        | 0.0416      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 193         |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 552         |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=1.59 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.59        |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.030494506 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 17.8        |
|    n_updates            | 3720        |
|    policy_gradient_loss | 0.00466     |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -255     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 373      |
|    time_elapsed    | 53093    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=3.39 +/- 2.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.39        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.016171355 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.351       |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 5.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 374      |
|    time_elapsed    | 53243    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -84.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 375         |
|    time_elapsed         | 53369       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.029443625 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 3740        |
|    policy_gradient_loss | 0.00499     |
|    value_loss           | 91          |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=1.93 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.034991056 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.912       |
|    n_updates            | 3750        |
|    policy_gradient_loss | 1.99e-05    |
|    value_loss           | 7.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 376      |
|    time_elapsed    | 53520    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=1.93 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.017082471 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 0.372       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 377      |
|    time_elapsed    | 53670    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.01       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 378         |
|    time_elapsed         | 53796       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.014537662 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.289       |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0005     |
|    value_loss           | 20.1        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=1.72 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.014849906 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.559       |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 13.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 379      |
|    time_elapsed    | 53946    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2.94 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.008194626 |
|    clip_fraction        | 0.0577      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 54.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 380      |
|    time_elapsed    | 54097    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.6       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 381         |
|    time_elapsed         | 54223       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.020836234 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0136      |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 4.2         |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=1.38 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.011581209 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.228       |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 4.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 382      |
|    time_elapsed    | 54373    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=1.25 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.25       |
| time/                   |            |
|    total_timesteps      | 6275000    |
| train/                  |            |
|    approx_kl            | 0.02102885 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.291      |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.00352   |
|    value_loss           | 2.25       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.18    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 383      |
|    time_elapsed    | 54524    |
|    total_timesteps | 6275072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -1.25      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 384        |
|    time_elapsed         | 54650      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.02132219 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.14      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0448     |
|    n_updates            | 3830       |
|    policy_gradient_loss | -0.00408   |
|    value_loss           | 0.884      |
----------------------------------------
Eval num_timesteps=6300000, episode_reward=2.95 +/- 3.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.95       |
| time/                   |            |
|    total_timesteps      | 6300000    |
| train/                  |            |
|    approx_kl            | 0.03508454 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 3840       |
|    policy_gradient_loss | 0.00443    |
|    value_loss           | 14.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.571   |
| time/              |          |
|    fps             | 115      |
|    iterations      | 385      |
|    time_elapsed    | 54800    |
|    total_timesteps | 6307840  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -9.11     |
| time/                   |           |
|    fps                  | 115       |
|    iterations           | 386       |
|    time_elapsed         | 54928     |
|    total_timesteps      | 6324224   |
| train/                  |           |
|    approx_kl            | 0.0207818 |
|    clip_fraction        | 0.295     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.24     |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0104   |
|    n_updates            | 3850      |
|    policy_gradient_loss | -0.0142   |
|    value_loss           | 0.03      |
---------------------------------------
Eval num_timesteps=6325000, episode_reward=1.58 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.016162599 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 176         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 387      |
|    time_elapsed    | 55079    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=2.29 +/- 0.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.29       |
| time/                   |            |
|    total_timesteps      | 6350000    |
| train/                  |            |
|    approx_kl            | 0.01076249 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.97      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.1       |
|    n_updates            | 3870       |
|    policy_gradient_loss | -0.00131   |
|    value_loss           | 10.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 388      |
|    time_elapsed    | 55230    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 389         |
|    time_elapsed         | 55356       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.009735151 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.389       |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 48.3        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=1.34 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.021511134 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0707      |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00998    |
|    value_loss           | 0.477       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 390      |
|    time_elapsed    | 55506    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=2.17 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.005731417 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.000657   |
|    value_loss           | 160         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 391      |
|    time_elapsed    | 55656    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 392         |
|    time_elapsed         | 55782       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.008698849 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.864       |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 21.4        |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=1.94 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.015319303 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.503       |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 7.9         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.9     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 393      |
|    time_elapsed    | 55933    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=1.91 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.91       |
| time/                   |            |
|    total_timesteps      | 6450000    |
| train/                  |            |
|    approx_kl            | 0.01657702 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 3930       |
|    policy_gradient_loss | -0.00554   |
|    value_loss           | 37         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 394      |
|    time_elapsed    | 56083    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 395         |
|    time_elapsed         | 56209       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.013096726 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.6        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 40          |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=2.21 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.016452093 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.35        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 4.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 396      |
|    time_elapsed    | 56359    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=1.83 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.018426728 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.588       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 8.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.84    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 397      |
|    time_elapsed    | 56509    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.3       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 398         |
|    time_elapsed         | 56635       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.018410437 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.771       |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 7.34        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=1.24 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.24       |
| time/                   |            |
|    total_timesteps      | 6525000    |
| train/                  |            |
|    approx_kl            | 0.01329501 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.889      |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.000183  |
|    value_loss           | 63.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 399      |
|    time_elapsed    | 56786    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=2.57 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57        |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.016432252 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 2.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 400      |
|    time_elapsed    | 56936    |
|    total_timesteps | 6553600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -242       |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 401        |
|    time_elapsed         | 57062      |
|    total_timesteps      | 6569984    |
| train/                  |            |
|    approx_kl            | 0.03534528 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 4000       |
|    policy_gradient_loss | 0.00793    |
|    value_loss           | 115        |
----------------------------------------
Eval num_timesteps=6575000, episode_reward=1.56 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.56       |
| time/                   |            |
|    total_timesteps      | 6575000    |
| train/                  |            |
|    approx_kl            | 0.03427755 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.8       |
|    n_updates            | 4010       |
|    policy_gradient_loss | 0.00333    |
|    value_loss           | 113        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 402      |
|    time_elapsed    | 57212    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2.67 +/- 1.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67        |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.018672038 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 7.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 403      |
|    time_elapsed    | 57363    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 404         |
|    time_elapsed         | 57489       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.019230489 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.208       |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 2.62        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=1.48 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.014303476 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 2.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 405      |
|    time_elapsed    | 57639    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=1.45 +/- 1.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.014082415 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.5        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 221         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 406      |
|    time_elapsed    | 57789    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -170        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 407         |
|    time_elapsed         | 57915       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.019241743 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.0986      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.497       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 39.8        |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=1.88 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.012772739 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.256       |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 1.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 408      |
|    time_elapsed    | 58066    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=1.55 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.015873786 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.579       |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 409      |
|    time_elapsed    | 58216    |
|    total_timesteps | 6701056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -13.2        |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 410          |
|    time_elapsed         | 58342        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0147883035 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.9         |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.0066      |
|    value_loss           | 20.4         |
------------------------------------------
Eval num_timesteps=6725000, episode_reward=2.48 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.015315217 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.573       |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 11.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 411      |
|    time_elapsed    | 58492    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2.42 +/- 1.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.016737206 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 7.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 412      |
|    time_elapsed    | 58643    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -7.55      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 413        |
|    time_elapsed         | 58770      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.01670751 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.15       |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00811   |
|    value_loss           | 0.406      |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=1.43 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.015089769 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 2.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.35    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 414      |
|    time_elapsed    | 58921    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.64       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 415         |
|    time_elapsed         | 59047       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.017584763 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.92        |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 3.89        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=1.39 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.39        |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.020672511 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.86        |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 4.85        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.69    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 416      |
|    time_elapsed    | 59197    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=2.12 +/- 0.54
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.12      |
| time/                   |           |
|    total_timesteps      | 6825000   |
| train/                  |           |
|    approx_kl            | 0.0189956 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.25     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.238     |
|    n_updates            | 4160      |
|    policy_gradient_loss | -0.00446  |
|    value_loss           | 2.88      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.1     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 417      |
|    time_elapsed    | 59348    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.3       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 418         |
|    time_elapsed         | 59475       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.018332299 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 17.5        |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=2.39 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.39        |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.019411474 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.299       |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 2.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 419      |
|    time_elapsed    | 59626    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=1.90 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.023176614 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 4190        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 23.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 420      |
|    time_elapsed    | 59776    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.16       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 421         |
|    time_elapsed         | 59902       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.018006291 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.682       |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 6.95        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=1.26 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.012382952 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.813       |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 11.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.18    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 422      |
|    time_elapsed    | 60053    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=1.65 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.018631535 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.416       |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.000562   |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.32    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 423      |
|    time_elapsed    | 60203    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 424         |
|    time_elapsed         | 60330       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.019864144 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.643       |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 11.3        |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=1.33 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.33         |
| time/                   |              |
|    total_timesteps      | 6950000      |
| train/                  |              |
|    approx_kl            | 0.0118226325 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.848        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.88         |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.00716     |
|    value_loss           | 47.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.92    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 425      |
|    time_elapsed    | 60480    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=2.03 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.016501289 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 1.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.5     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 426      |
|    time_elapsed    | 60631    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.18       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 427         |
|    time_elapsed         | 60757       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.015418402 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 2.99        |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=1.44 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.019553278 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.469       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.000275   |
|    value_loss           | 3.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.27    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 428      |
|    time_elapsed    | 60908    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=1.73 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.017177587 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 6.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.13    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 429      |
|    time_elapsed    | 61058    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.46       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 430         |
|    time_elapsed         | 61185       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.017556412 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.521       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 3.33        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=1.52 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.018018045 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.447       |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.000756   |
|    value_loss           | 0.514       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.67    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 431      |
|    time_elapsed    | 61336    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=2.25 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.014895884 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.218       |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.22    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 432      |
|    time_elapsed    | 61487    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.2        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 433         |
|    time_elapsed         | 61612       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.021281527 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 3.35        |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=1.41 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.022862002 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0585      |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 1.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.32    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 434      |
|    time_elapsed    | 61763    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=1.13 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13        |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.023023564 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0448      |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 15.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 435      |
|    time_elapsed    | 61913    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 436         |
|    time_elapsed         | 62039       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.022379035 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.12        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 91.8        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=1.55 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.009649283 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.3        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 63.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 437      |
|    time_elapsed    | 62189    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=1.59 +/- 0.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.59       |
| time/                   |            |
|    total_timesteps      | 7175000    |
| train/                  |            |
|    approx_kl            | 0.01981952 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.96      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 4370       |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 20.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 438      |
|    time_elapsed    | 62340    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.3       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 439         |
|    time_elapsed         | 62466       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.022951985 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.277       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 14.5        |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=2.07 +/- 0.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.07       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.01622791 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.4       |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.00507   |
|    value_loss           | 14         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 440      |
|    time_elapsed    | 62617    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=1.47 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.017381176 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 156         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 441      |
|    time_elapsed    | 62767    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -29.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 442         |
|    time_elapsed         | 62893       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.034202423 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.84        |
|    n_updates            | 4410        |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 40.6        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=2.16 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.021449126 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.382       |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 13.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 443      |
|    time_elapsed    | 63044    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -28.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 444         |
|    time_elapsed         | 63170       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.009585204 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.77        |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=1.60 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.6        |
| time/                   |            |
|    total_timesteps      | 7275000    |
| train/                  |            |
|    approx_kl            | 0.01729928 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.239      |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.00472   |
|    value_loss           | 9.03       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 445      |
|    time_elapsed    | 63320    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=2.14 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.14        |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.016200414 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.69        |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 21.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 446      |
|    time_elapsed    | 63470    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.99       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 447         |
|    time_elapsed         | 63597       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.015592855 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94        |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 18.1        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=2.11 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.016277762 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 3.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.41    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 448      |
|    time_elapsed    | 63747    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=1.54 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.015118798 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.577       |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 4.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 449      |
|    time_elapsed    | 63897    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -112        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 450         |
|    time_elapsed         | 64023       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.023130361 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | 185         |
|    n_updates            | 4490        |
|    policy_gradient_loss | 0.000791    |
|    value_loss           | 337         |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=1.84 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.018222122 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 1.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 451      |
|    time_elapsed    | 64174    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=2.00 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.010631047 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.000778   |
|    value_loss           | 20.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 452      |
|    time_elapsed    | 64324    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.04       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 453         |
|    time_elapsed         | 64450       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.016148243 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 1.83        |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=1.80 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.013072999 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.474       |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 5.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.61    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 454      |
|    time_elapsed    | 64600    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=1.53 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.019780282 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 1.87        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.36    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 455      |
|    time_elapsed    | 64751    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.24       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 456         |
|    time_elapsed         | 64877       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.016977537 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.326       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 1.29        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=2.40 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.014619868 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 31.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 457      |
|    time_elapsed    | 65028    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=1.67 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.017263893 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.325       |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 47.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 458      |
|    time_elapsed    | 65178    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 459         |
|    time_elapsed         | 65304       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.018352225 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=1.94 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.013020016 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.549       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00854    |
|    value_loss           | 16.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 460      |
|    time_elapsed    | 65454    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=1.86 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.019308543 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00782    |
|    value_loss           | 27.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 461      |
|    time_elapsed    | 65605    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 462         |
|    time_elapsed         | 65732       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.017762251 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.6         |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 22.2        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=2.42 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.019189443 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.137       |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.000501   |
|    value_loss           | 34.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 463      |
|    time_elapsed    | 65883    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=2.19 +/- 1.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.19       |
| time/                   |            |
|    total_timesteps      | 7600000    |
| train/                  |            |
|    approx_kl            | 0.01451838 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.337      |
|    n_updates            | 4630       |
|    policy_gradient_loss | -0.00714   |
|    value_loss           | 1.04       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 464      |
|    time_elapsed    | 66033    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 465         |
|    time_elapsed         | 66160       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.017090779 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.02        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 1.53        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=1.87 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.015317642 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.004      |
|    value_loss           | 2.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.09    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 466      |
|    time_elapsed    | 66310    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=2.55 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55        |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.013347454 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.559       |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 11.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.49    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 467      |
|    time_elapsed    | 66461    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 468         |
|    time_elapsed         | 66589       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.025246935 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.9        |
|    n_updates            | 4670        |
|    policy_gradient_loss | 0.000292    |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=1.71 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.020852115 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.239       |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 67          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 469      |
|    time_elapsed    | 66739    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=1.53 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.018022235 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.972       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 470      |
|    time_elapsed    | 66890    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.3       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 471         |
|    time_elapsed         | 67016       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.017617758 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.352       |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 2.8         |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=2.65 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65        |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.017895088 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.93        |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 10.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.32    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 472      |
|    time_elapsed    | 67167    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.24       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 473         |
|    time_elapsed         | 67293       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.016061816 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.72        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 8.05        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=2.78 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.015312847 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.752       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 4.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.92    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 474      |
|    time_elapsed    | 67443    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=1.67 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.019366363 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.895       |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 115         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 475      |
|    time_elapsed    | 67594    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.2       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 476         |
|    time_elapsed         | 67720       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.018290617 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.415       |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 1.75        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=2.24 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.020028563 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.4         |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 1.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 477      |
|    time_elapsed    | 67870    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=1.31 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.31        |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.015450808 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.377       |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 1.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 478      |
|    time_elapsed    | 68021    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 479         |
|    time_elapsed         | 68147       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.021692375 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=1.69 +/- 0.61
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 7850000    |
| train/                  |            |
|    approx_kl            | 0.01908819 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0513     |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.00648   |
|    value_loss           | 1.86       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 480      |
|    time_elapsed    | 68298    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=1.74 +/- 0.68
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 7875000    |
| train/                  |            |
|    approx_kl            | 0.01763444 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.15      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0763     |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.00336   |
|    value_loss           | 2.7        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 481      |
|    time_elapsed    | 68448    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.83       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 482         |
|    time_elapsed         | 68574       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.020503176 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 24.1        |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=2.30 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.015790984 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.822       |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 7.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 483      |
|    time_elapsed    | 68725    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=1.99 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.020192107 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00633    |
|    value_loss           | 23.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 484      |
|    time_elapsed    | 68875    |
|    total_timesteps | 7929856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -10.6      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 485        |
|    time_elapsed         | 69001      |
|    total_timesteps      | 7946240    |
| train/                  |            |
|    approx_kl            | 0.02192452 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.3       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.00887   |
|    value_loss           | 9.72       |
----------------------------------------
Eval num_timesteps=7950000, episode_reward=2.97 +/- 1.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.020818902 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.164       |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 4.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.87    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 486      |
|    time_elapsed    | 69152    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=2.24 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.017453812 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 1.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2       |
| time/              |          |
|    fps             | 115      |
|    iterations      | 487      |
|    time_elapsed    | 69303    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.54       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 488         |
|    time_elapsed         | 69429       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.018292183 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.274       |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 1.7         |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2.30 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.021886054 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0217      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 0.414       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.826   |
| time/              |          |
|    fps             | 115      |
|    iterations      | 489      |
|    time_elapsed    | 69579    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=2.42 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.018165682 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.647       |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 1.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.342   |
| time/              |          |
|    fps             | 115      |
|    iterations      | 490      |
|    time_elapsed    | 69730    |
|    total_timesteps | 8028160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -0.625     |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 491        |
|    time_elapsed         | 69856      |
|    total_timesteps      | 8044544    |
| train/                  |            |
|    approx_kl            | 0.01906293 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0839     |
|    n_updates            | 4900       |
|    policy_gradient_loss | -0.00657   |
|    value_loss           | 0.209      |
----------------------------------------
Eval num_timesteps=8050000, episode_reward=2.08 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.016642015 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0893      |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 0.463       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.157   |
| time/              |          |
|    fps             | 115      |
|    iterations      | 492      |
|    time_elapsed    | 70007    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=1.57 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.013756851 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 0.553       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.73    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 493      |
|    time_elapsed    | 70157    |
|    total_timesteps | 8077312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -0.863     |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 494        |
|    time_elapsed         | 70283      |
|    total_timesteps      | 8093696    |
| train/                  |            |
|    approx_kl            | 0.02141478 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.14      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 4930       |
|    policy_gradient_loss | -0.00607   |
|    value_loss           | 0.531      |
----------------------------------------
Eval num_timesteps=8100000, episode_reward=3.04 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.04        |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.017164636 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0592      |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 0.495       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.33    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 495      |
|    time_elapsed    | 70435    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=1.81 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.019900594 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 9.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.1     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 496      |
|    time_elapsed    | 70586    |
|    total_timesteps | 8126464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.91      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 497        |
|    time_elapsed         | 70713      |
|    total_timesteps      | 8142848    |
| train/                  |            |
|    approx_kl            | 0.02087051 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 4960       |
|    policy_gradient_loss | 0.000684   |
|    value_loss           | 8.74       |
----------------------------------------
Eval num_timesteps=8150000, episode_reward=4.25 +/- 2.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.25        |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.019022971 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 13.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.82    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 498      |
|    time_elapsed    | 70864    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=1.84 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.018232243 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 4.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 499      |
|    time_elapsed    | 71014    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.1       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 500         |
|    time_elapsed         | 71140       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.020558368 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.97        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=1.86 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.022604953 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 2.87        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 501      |
|    time_elapsed    | 71291    |
|    total_timesteps | 8208384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -10.6      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 502        |
|    time_elapsed         | 71417      |
|    total_timesteps      | 8224768    |
| train/                  |            |
|    approx_kl            | 0.02064382 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.85      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.593      |
|    n_updates            | 5010       |
|    policy_gradient_loss | -0.00889   |
|    value_loss           | 5.22       |
----------------------------------------
Eval num_timesteps=8225000, episode_reward=1.59 +/- 0.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.59       |
| time/                   |            |
|    total_timesteps      | 8225000    |
| train/                  |            |
|    approx_kl            | 0.01928451 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.252      |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.00649   |
|    value_loss           | 1.79       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.06    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 503      |
|    time_elapsed    | 71567    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=3.34 +/- 2.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.016032726 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 20.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 504      |
|    time_elapsed    | 71718    |
|    total_timesteps | 8257536  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11        |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 505        |
|    time_elapsed         | 71844      |
|    total_timesteps      | 8273920    |
| train/                  |            |
|    approx_kl            | 0.02116587 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.72      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.81       |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.00679   |
|    value_loss           | 4.62       |
----------------------------------------
Eval num_timesteps=8275000, episode_reward=1.85 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.018164843 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.1        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 5.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 506      |
|    time_elapsed    | 71995    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=2.12 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.020457022 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 3.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.38    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 507      |
|    time_elapsed    | 72145    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 508         |
|    time_elapsed         | 72271       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.015684422 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.46        |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 6.34        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=1.34 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.010730355 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 44.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 509      |
|    time_elapsed    | 72421    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=1.61 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.019962175 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.268       |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 1.94        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 510      |
|    time_elapsed    | 72572    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.3       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 511         |
|    time_elapsed         | 72698       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.015841236 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 64.7        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=1.68 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.024019854 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 0.606       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 512      |
|    time_elapsed    | 72849    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=2.46 +/- 1.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.46       |
| time/                   |            |
|    total_timesteps      | 8400000    |
| train/                  |            |
|    approx_kl            | 0.01673356 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.208      |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.00954   |
|    value_loss           | 1.86       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 513      |
|    time_elapsed    | 72999    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.48       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 514         |
|    time_elapsed         | 73126       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.021169111 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 20.3        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=1.46 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.018802177 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0735      |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 0.248       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 515      |
|    time_elapsed    | 73276    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=2.46 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.017053742 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.0149      |
|    learning_rate        | 0.0003      |
|    loss                 | 148         |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 516      |
|    time_elapsed    | 73427    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -46.5       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 517         |
|    time_elapsed         | 73553       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.020376636 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0414      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 0.989       |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=1.77 +/- 0.32
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.77       |
| time/                   |            |
|    total_timesteps      | 8475000    |
| train/                  |            |
|    approx_kl            | 0.02057346 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0984     |
|    n_updates            | 5170       |
|    policy_gradient_loss | -0.0078    |
|    value_loss           | 1.38       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.48    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 518      |
|    time_elapsed    | 73704    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=1.54 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.021596566 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 3.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.89    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 519      |
|    time_elapsed    | 73854    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.61       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 520         |
|    time_elapsed         | 73980       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.018911308 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.13        |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 4.85        |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=1.42 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.018705262 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 0.422       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.77    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 521      |
|    time_elapsed    | 74131    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=1.73 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.018510288 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0802      |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 0.854       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 522      |
|    time_elapsed    | 74282    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.11       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 523         |
|    time_elapsed         | 74410       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.023591695 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0783      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 0.265       |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=1.76 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.013064904 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 5.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.05    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 524      |
|    time_elapsed    | 74561    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=1.94 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.018269699 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.398       |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 9.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.39    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 525      |
|    time_elapsed    | 74712    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.95       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 526         |
|    time_elapsed         | 74839       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.021538325 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 14.5        |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=1.65 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.022721637 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.488       |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 6.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 527      |
|    time_elapsed    | 74989    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=1.49 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.017392613 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.118       |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 2.46        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.84    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 528      |
|    time_elapsed    | 75140    |
|    total_timesteps | 8650752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.3        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 529         |
|    time_elapsed         | 75266       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.020414535 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.832       |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 10.4        |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=1.93 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.020171631 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0818      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 0.489       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.14    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 530      |
|    time_elapsed    | 75417    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.23       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 531         |
|    time_elapsed         | 75543       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.016690869 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 1.79        |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=1.85 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.019461248 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 1.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.1     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 532      |
|    time_elapsed    | 75693    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=2.04 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.018102882 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 1.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.86    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 533      |
|    time_elapsed    | 75844    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.96       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 534         |
|    time_elapsed         | 75970       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.019822191 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0142      |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 0.138       |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=1.39 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.39        |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.020461192 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.0888      |
|    learning_rate        | 0.0003      |
|    loss                 | 75.9        |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.00067    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 535      |
|    time_elapsed    | 76120    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=1.44 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.015437773 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.23        |
|    learning_rate        | 0.0003      |
|    loss                 | 124         |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 536      |
|    time_elapsed    | 76271    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -74.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 537         |
|    time_elapsed         | 76397       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.040188923 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.974       |
|    n_updates            | 5360        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 9           |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=1.30 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.027387913 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 5370        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 5.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.3    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 538      |
|    time_elapsed    | 76547    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=1.45 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.017982941 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0147      |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 0.787       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 539      |
|    time_elapsed    | 76698    |
|    total_timesteps | 8830976  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -102         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 540          |
|    time_elapsed         | 76824        |
|    total_timesteps      | 8847360      |
| train/                  |              |
|    approx_kl            | 0.0145869795 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.52         |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.000788    |
|    value_loss           | 211          |
------------------------------------------
Eval num_timesteps=8850000, episode_reward=1.20 +/- 0.41
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.2       |
| time/                   |           |
|    total_timesteps      | 8850000   |
| train/                  |           |
|    approx_kl            | 0.0280594 |
|    clip_fraction        | 0.235     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.75     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.84      |
|    n_updates            | 5400      |
|    policy_gradient_loss | -0.00185  |
|    value_loss           | 71.7      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 541      |
|    time_elapsed    | 76974    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=1.23 +/- 0.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.23        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.018207798 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.73        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 18.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 542      |
|    time_elapsed    | 77124    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.7       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 543         |
|    time_elapsed         | 77250       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.018718323 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 3.14        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=1.23 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.23        |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.021422323 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.49        |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 7.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 544      |
|    time_elapsed    | 77401    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=1.25 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.019285176 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.5        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 17.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 545      |
|    time_elapsed    | 77551    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 546         |
|    time_elapsed         | 77677       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.022699296 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.07        |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 6.96        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=1.58 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.027606942 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 1.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 547      |
|    time_elapsed    | 77828    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=1.42 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.015737426 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 9.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 548      |
|    time_elapsed    | 77979    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30         |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 549         |
|    time_elapsed         | 78105       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.023303002 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.8         |
|    n_updates            | 5480        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=1.51 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.025539536 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 15.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 550      |
|    time_elapsed    | 78255    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=1.42 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.021057129 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 0.328       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 551      |
|    time_elapsed    | 78406    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.58       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 552         |
|    time_elapsed         | 78532       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.017738115 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 29.1        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=1.98 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.019164898 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.368       |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 9.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.85    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 553      |
|    time_elapsed    | 78682    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=2.79 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.023500176 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0334      |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 2.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.4     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 554      |
|    time_elapsed    | 78833    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.19       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 555         |
|    time_elapsed         | 78959       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.016835956 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.128       |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 1.12        |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=1.48 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.015832543 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.192       |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 10.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.85    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 556      |
|    time_elapsed    | 79110    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=2.12 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.019204821 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.299       |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 1.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 557      |
|    time_elapsed    | 79260    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.62       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 558         |
|    time_elapsed         | 79386       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.016780894 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.02        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 4.58        |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=1.77 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.77       |
| time/                   |            |
|    total_timesteps      | 9150000    |
| train/                  |            |
|    approx_kl            | 0.01902857 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.44      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0371     |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.00167   |
|    value_loss           | 1.09       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 559      |
|    time_elapsed    | 79537    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=2.93 +/- 1.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.020175021 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0308      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00731    |
|    value_loss           | 0.263       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 560      |
|    time_elapsed    | 79688    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.25       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 561         |
|    time_elapsed         | 79815       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.023097338 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0177      |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 1.09        |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=1.86 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.011690581 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 84.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 562      |
|    time_elapsed    | 79966    |
|    total_timesteps | 9207808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.4      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 563        |
|    time_elapsed         | 80093      |
|    total_timesteps      | 9224192    |
| train/                  |            |
|    approx_kl            | 0.02069015 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0756     |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.00846   |
|    value_loss           | 1.47       |
----------------------------------------
Eval num_timesteps=9225000, episode_reward=1.87 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.021896655 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 11.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 564      |
|    time_elapsed    | 80243    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=-945.37 +/- 1895.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -945        |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.026316844 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.178       |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 7.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.32    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 565      |
|    time_elapsed    | 80393    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -63.8       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 566         |
|    time_elapsed         | 80519       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.019936852 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.251       |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 48          |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=1.63 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.029995527 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 5660        |
|    policy_gradient_loss | 0.00487     |
|    value_loss           | 202         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 567      |
|    time_elapsed    | 80670    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=1.31 +/- 0.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.31       |
| time/                   |            |
|    total_timesteps      | 9300000    |
| train/                  |            |
|    approx_kl            | 0.02170729 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 5670       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 2.49       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.5    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 568      |
|    time_elapsed    | 80820    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.51       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 569         |
|    time_elapsed         | 80946       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.019220937 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.2        |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 17          |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=1.73 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.018637784 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.66        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 17.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 570      |
|    time_elapsed    | 81097    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=1.89 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.024612218 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.574       |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.000828   |
|    value_loss           | 23.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 571      |
|    time_elapsed    | 81247    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.93       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 572         |
|    time_elapsed         | 81373       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.016058594 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.3        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 10.5        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=1.30 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.022686336 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.16        |
|    n_updates            | 5720        |
|    policy_gradient_loss | 0.000777    |
|    value_loss           | 87          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -246     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 573      |
|    time_elapsed    | 81524    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=1.57 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.019796249 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.579       |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -243     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 574      |
|    time_elapsed    | 81675    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -242        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 575         |
|    time_elapsed         | 81801       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.021765072 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0805      |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.985       |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=1.89 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.020218734 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0078      |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 0.471       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.19    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 576      |
|    time_elapsed    | 81951    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=1.75 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.013224672 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.336       |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00606    |
|    value_loss           | 86          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.42    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 577      |
|    time_elapsed    | 82102    |
|    total_timesteps | 9453568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -9.37      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 578        |
|    time_elapsed         | 82229      |
|    total_timesteps      | 9469952    |
| train/                  |            |
|    approx_kl            | 0.02120759 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.056      |
|    n_updates            | 5770       |
|    policy_gradient_loss | -0.00324   |
|    value_loss           | 0.963      |
----------------------------------------
Eval num_timesteps=9475000, episode_reward=2.12 +/- 1.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.12       |
| time/                   |            |
|    total_timesteps      | 9475000    |
| train/                  |            |
|    approx_kl            | 0.01939325 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.76      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0396     |
|    n_updates            | 5780       |
|    policy_gradient_loss | -0.0031    |
|    value_loss           | 1.94       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 579      |
|    time_elapsed    | 82380    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=1.06 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.06        |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.014270905 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.6        |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 52.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.26    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 580      |
|    time_elapsed    | 82530    |
|    total_timesteps | 9502720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.7      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 581        |
|    time_elapsed         | 82656      |
|    total_timesteps      | 9519104    |
| train/                  |            |
|    approx_kl            | 0.01943691 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0643     |
|    n_updates            | 5800       |
|    policy_gradient_loss | -0.00496   |
|    value_loss           | 1          |
----------------------------------------
Eval num_timesteps=9525000, episode_reward=1.29 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.012584088 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.604       |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 52.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 582      |
|    time_elapsed    | 82806    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=1.71 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.022338146 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 1.85        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.93    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 583      |
|    time_elapsed    | 82957    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.74       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 584         |
|    time_elapsed         | 83083       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.020498175 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.298       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 13          |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=1.51 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.021421231 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 6.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.7     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 585      |
|    time_elapsed    | 83233    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=1.57 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.022747744 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.2        |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 5.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.99    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 586      |
|    time_elapsed    | 83383    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.1        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 587         |
|    time_elapsed         | 83509       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.024103586 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0198      |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 7.82        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=2.61 +/- 1.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.61       |
| time/                   |            |
|    total_timesteps      | 9625000    |
| train/                  |            |
|    approx_kl            | 0.01777625 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0506     |
|    n_updates            | 5870       |
|    policy_gradient_loss | -0.00878   |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.83    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 588      |
|    time_elapsed    | 83660    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=1.49 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.017004691 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0613      |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 0.495       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.576   |
| time/              |          |
|    fps             | 115      |
|    iterations      | 589      |
|    time_elapsed    | 83810    |
|    total_timesteps | 9650176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -0.956     |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 590        |
|    time_elapsed         | 83936      |
|    total_timesteps      | 9666560    |
| train/                  |            |
|    approx_kl            | 0.01907311 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.277      |
|    n_updates            | 5890       |
|    policy_gradient_loss | -0.00594   |
|    value_loss           | 3.52       |
----------------------------------------
Eval num_timesteps=9675000, episode_reward=2.25 +/- 1.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.019943058 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.000872   |
|    value_loss           | 2.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.98    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 591      |
|    time_elapsed    | 84087    |
|    total_timesteps | 9682944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -19.1      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 592        |
|    time_elapsed         | 84212      |
|    total_timesteps      | 9699328    |
| train/                  |            |
|    approx_kl            | 0.02009052 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.56      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.18       |
|    n_updates            | 5910       |
|    policy_gradient_loss | -0.00726   |
|    value_loss           | 10.7       |
----------------------------------------
Eval num_timesteps=9700000, episode_reward=1.61 +/- 0.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.61       |
| time/                   |            |
|    total_timesteps      | 9700000    |
| train/                  |            |
|    approx_kl            | 0.02506721 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.76      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 5920       |
|    policy_gradient_loss | 0.00238    |
|    value_loss           | 76.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 593      |
|    time_elapsed    | 84363    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=1.68 +/- 0.57
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.68       |
| time/                   |            |
|    total_timesteps      | 9725000    |
| train/                  |            |
|    approx_kl            | 0.02037419 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.59      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.704      |
|    n_updates            | 5930       |
|    policy_gradient_loss | -0.00507   |
|    value_loss           | 11.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 594      |
|    time_elapsed    | 84513    |
|    total_timesteps | 9732096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -16.3      |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 595        |
|    time_elapsed         | 84639      |
|    total_timesteps      | 9748480    |
| train/                  |            |
|    approx_kl            | 0.01986792 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.46      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.00389   |
|    value_loss           | 10.7       |
----------------------------------------
Eval num_timesteps=9750000, episode_reward=1.41 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.014108501 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.3        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 43.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 596      |
|    time_elapsed    | 84790    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=-116.38 +/- 236.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.019794166 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.81        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 3.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 597      |
|    time_elapsed    | 84940    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.4       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 598         |
|    time_elapsed         | 85066       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.019560669 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.459       |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 15.6        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=1.15 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.15        |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.020893369 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.565       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0319      |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 24.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    fps             | 115      |
|    iterations      | 599      |
|    time_elapsed    | 85216    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=1.12 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.12        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.017656472 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.594       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00987    |
|    value_loss           | 6.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 600      |
|    time_elapsed    | 85368    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -151        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 601         |
|    time_elapsed         | 85495       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.016311858 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.8         |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 4.47        |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=1.44 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.022605028 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 6010        |
|    policy_gradient_loss | 0.00077     |
|    value_loss           | 79.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 602      |
|    time_elapsed    | 85645    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=2.09 +/- 2.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.021820316 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.24        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 84.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -208     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 603      |
|    time_elapsed    | 85796    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -212        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 604         |
|    time_elapsed         | 85922       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.014917536 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.36        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 47.2        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=1.15 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.15       |
| time/                   |            |
|    total_timesteps      | 9900000    |
| train/                  |            |
|    approx_kl            | 0.01667462 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.38      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.33       |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.00232   |
|    value_loss           | 46.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -198     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 605      |
|    time_elapsed    | 86073    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=1.26 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.008381285 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 505         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -257     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 606      |
|    time_elapsed    | 86223    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -265        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 607         |
|    time_elapsed         | 86349       |
|    total_timesteps      | 9945088     |
| train/                  |             |
|    approx_kl            | 0.017992852 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.64        |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 145         |
-----------------------------------------
slurmstepd: error: *** JOB 26565265 ON m3i031 CANCELLED AT 2022-06-28T01:06:27 DUE TO TIME LIMIT ***
