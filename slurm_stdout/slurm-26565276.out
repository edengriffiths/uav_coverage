========== uav-v11 ==========
Seed: 3733369590
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v11_1
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.61e+03 |
| time/              |           |
|    fps             | 284       |
|    iterations      | 1         |
|    time_elapsed    | 57        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-5251.79 +/- 4196.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -5.25e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008326032 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | -0.000689   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.72e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 2.8e+04     |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.13e+03 |
| time/              |           |
|    fps             | 238       |
|    iterations      | 2         |
|    time_elapsed    | 137       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.34e+03   |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 3           |
|    time_elapsed         | 204         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.007993244 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.139       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 2.47e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-1557.51 +/- 294.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008832627 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 1.03e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.55e+03 |
| time/              |           |
|    fps             | 230       |
|    iterations      | 4         |
|    time_elapsed    | 284       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-1657.65 +/- 233.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.66e+03    |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0097315125 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.0003       |
|    loss                 | 641          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 1.86e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.19e+03 |
| time/              |           |
|    fps             | 224       |
|    iterations      | 5         |
|    time_elapsed    | 364       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -443        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 6           |
|    time_elapsed         | 431         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.012060227 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 575         |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1557.19 +/- 294.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.011762209 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 82.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 7        |
|    time_elapsed    | 511      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-1056.59 +/- 282.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.06e+03   |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.011817685 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.15        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 81.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.49    |
| time/              |          |
|    fps             | 221      |
|    iterations      | 8        |
|    time_elapsed    | 591      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 32.3        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 9           |
|    time_elapsed         | 658         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.009430233 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.8        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 438         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-295.32 +/- 285.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.011985147 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 251         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 10       |
|    time_elapsed    | 738      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-1278.36 +/- 1880.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.28e+03   |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.012280728 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.3        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 352         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 11       |
|    time_elapsed    | 818      |
|    total_timesteps | 180224   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 97.2      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 12        |
|    time_elapsed         | 885       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0083833 |
|    clip_fraction        | 0.0639    |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.78     |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.0003    |
|    loss                 | 356       |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.00866  |
|    value_loss           | 1.08e+03  |
---------------------------------------
Eval num_timesteps=200000, episode_reward=1.13 +/- 363.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.008176872 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 611         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 2e+03       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 59.7     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 13       |
|    time_elapsed    | 965      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-114.56 +/- 474.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.009003978 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 968         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 1.51e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 51.2     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 14       |
|    time_elapsed    | 1045     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 201         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 15          |
|    time_elapsed         | 1112        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.009513367 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 637         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 890         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=247.95 +/- 382.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 248         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.010894068 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.4        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 705         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 16       |
|    time_elapsed    | 1192     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=140.28 +/- 414.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 140         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.013147563 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.7        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 293         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 477      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 17       |
|    time_elapsed    | 1272     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 555         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 18          |
|    time_elapsed         | 1339        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.012558095 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 562         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=191.24 +/- 206.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.013334748 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.1        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 620      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 19       |
|    time_elapsed    | 1419     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-2387.35 +/- 5040.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.39e+03   |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.010826368 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.3        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 531         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 661      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 20       |
|    time_elapsed    | 1499     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 749         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 21          |
|    time_elapsed         | 1566        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.014980108 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.3        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 220         |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=483.34 +/- 343.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 483         |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.015648723 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 237         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 22       |
|    time_elapsed    | 1646     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=562.80 +/- 422.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 563         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.011463375 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 112         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 555      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 23       |
|    time_elapsed    | 1726     |
|    total_timesteps | 376832   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 408          |
| time/                   |              |
|    fps                  | 219          |
|    iterations           | 24           |
|    time_elapsed         | 1793         |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0089784935 |
|    clip_fraction        | 0.0874       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.00968      |
|    learning_rate        | 0.0003       |
|    loss                 | 223          |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00543     |
|    value_loss           | 629          |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-3255.11 +/- 6980.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.26e+03   |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.008654617 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0003      |
|    loss                 | 557         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 494         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 25       |
|    time_elapsed    | 1873     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=1331.37 +/- 1084.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.013375325 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.7        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 477         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 524      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 26       |
|    time_elapsed    | 1953     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 841         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 27          |
|    time_elapsed         | 2020        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.015023546 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.5        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=555.82 +/- 349.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 556         |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.015552939 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 28       |
|    time_elapsed    | 2100     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1513.81 +/- 797.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51e+03    |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.012552071 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 340         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 29       |
|    time_elapsed    | 2180     |
|    total_timesteps | 475136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 910        |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 30         |
|    time_elapsed         | 2247       |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.01381774 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.6       |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 331        |
----------------------------------------
Eval num_timesteps=500000, episode_reward=827.64 +/- 465.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 828         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.010136988 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03e+03    |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 1.56e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 946      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 31       |
|    time_elapsed    | 2327     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.01e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 32          |
|    time_elapsed         | 2394        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.014794003 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 83          |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 219         |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=1104.71 +/- 762.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.1e+03     |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.011850862 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 332         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 553         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 33       |
|    time_elapsed    | 2474     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=1199.92 +/- 824.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2e+03     |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.012077823 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 434         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 34       |
|    time_elapsed    | 2554     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.13e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 35          |
|    time_elapsed         | 2621        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.016082324 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 298         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=760.83 +/- 239.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.013032697 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 467         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 470         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 36       |
|    time_elapsed    | 2701     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-2423.11 +/- 7119.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.42e+03   |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.008951185 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 856         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 37       |
|    time_elapsed    | 2781     |
|    total_timesteps | 606208   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 994       |
| time/                   |           |
|    fps                  | 218       |
|    iterations           | 38        |
|    time_elapsed         | 2848      |
|    total_timesteps      | 622592    |
| train/                  |           |
|    approx_kl            | 0.0107642 |
|    clip_fraction        | 0.121     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.59     |
|    explained_variance   | 0.856     |
|    learning_rate        | 0.0003    |
|    loss                 | 483       |
|    n_updates            | 370       |
|    policy_gradient_loss | -0.00945  |
|    value_loss           | 644       |
---------------------------------------
Eval num_timesteps=625000, episode_reward=1530.14 +/- 1142.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53e+03    |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.009187091 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 238         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 760         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 39       |
|    time_elapsed    | 2928     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1069.55 +/- 212.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07e+03    |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.016732743 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 80.6        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 167         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 40       |
|    time_elapsed    | 3008     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 41          |
|    time_elapsed         | 3075        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.014425503 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 155         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 369         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=1236.04 +/- 726.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.24e+03   |
| time/                   |            |
|    total_timesteps      | 675000     |
| train/                  |            |
|    approx_kl            | 0.01552579 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.6       |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.00962   |
|    value_loss           | 328        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 42       |
|    time_elapsed    | 3155     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=941.51 +/- 195.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 942         |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.013273963 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 225         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 455         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 43       |
|    time_elapsed    | 3235     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.53e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 44          |
|    time_elapsed         | 3302        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.015137469 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 53.9        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=1130.63 +/- 600.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13e+03    |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.013645338 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 244         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 260         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 45       |
|    time_elapsed    | 3382     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1845.36 +/- 663.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.015489763 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 82.7        |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00892    |
|    value_loss           | 396         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 46       |
|    time_elapsed    | 3462     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.34e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 47          |
|    time_elapsed         | 3529        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.009899836 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 209         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 639         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=2152.30 +/- 798.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.15e+03    |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.013785747 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.8        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 348         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 48       |
|    time_elapsed    | 3609     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=1581.63 +/- 467.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58e+03    |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.015414985 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.1        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 227         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 49       |
|    time_elapsed    | 3689     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 50          |
|    time_elapsed         | 3757        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.014883565 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.2        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=1814.39 +/- 765.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.014629824 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.6        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 221         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 51       |
|    time_elapsed    | 3836     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=1740.57 +/- 857.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74e+03   |
| time/                   |            |
|    total_timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.01522722 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.4       |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 149        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 52       |
|    time_elapsed    | 3916     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 53          |
|    time_elapsed         | 3984        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.015681932 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.3        |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 182         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=1515.39 +/- 284.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.52e+03     |
| time/                   |              |
|    total_timesteps      | 875000       |
| train/                  |              |
|    approx_kl            | 0.0151352845 |
|    clip_fraction        | 0.187        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0003       |
|    loss                 | 28.8         |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.0098      |
|    value_loss           | 270          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 54       |
|    time_elapsed    | 4064     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=1683.68 +/- 389.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68e+03    |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.018515494 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 28.6        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 55       |
|    time_elapsed    | 4143     |
|    total_timesteps | 901120   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.58e+03   |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 56         |
|    time_elapsed         | 4211       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.01482286 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0003     |
|    loss                 | 175        |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00838   |
|    value_loss           | 357        |
----------------------------------------
Eval num_timesteps=925000, episode_reward=1603.41 +/- 499.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6e+03     |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.013629393 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 374         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 320         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 57       |
|    time_elapsed    | 4290     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1646.68 +/- 486.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65e+03    |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.017379258 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.4        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 211         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 58       |
|    time_elapsed    | 4370     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.55e+03    |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 59          |
|    time_elapsed         | 4437        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.010686113 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 243         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 703         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=1699.59 +/- 311.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.7e+03    |
| time/                   |            |
|    total_timesteps      | 975000     |
| train/                  |            |
|    approx_kl            | 0.01814314 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.7       |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 176        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    fps             | 217      |
|    iterations      | 60       |
|    time_elapsed    | 4517     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.55e+03    |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 61          |
|    time_elapsed         | 4584        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.014604076 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 74          |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 227         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=1752.68 +/- 321.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75e+03    |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.016726278 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.9        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00933    |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 62       |
|    time_elapsed    | 4664     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=1743.22 +/- 619.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.013246726 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 251         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 63       |
|    time_elapsed    | 4742     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.69e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 64          |
|    time_elapsed         | 4808        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.017558882 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 80.6        |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=1621.54 +/- 691.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62e+03    |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.015937945 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.5        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    fps             | 217      |
|    iterations      | 65       |
|    time_elapsed    | 4886     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=1900.34 +/- 814.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9e+03     |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.011651499 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    fps             | 217      |
|    iterations      | 66       |
|    time_elapsed    | 4965     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.77e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 67          |
|    time_elapsed         | 5031        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.016256291 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.2        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 226         |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=1401.68 +/- 544.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4e+03     |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.017233055 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.3        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 68       |
|    time_elapsed    | 5109     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=1401.90 +/- 233.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4e+03     |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.015678097 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 25.6        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00826    |
|    value_loss           | 245         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    fps             | 217      |
|    iterations      | 69       |
|    time_elapsed    | 5187     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.71e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 70          |
|    time_elapsed         | 5253        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.015314761 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 74.4        |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 428         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=2232.81 +/- 449.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23e+03    |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.017768487 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.2        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 181         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 71       |
|    time_elapsed    | 5332     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=1700.36 +/- 659.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.7e+03     |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.018610802 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.4        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 82.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 72       |
|    time_elapsed    | 5410     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.81e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 73          |
|    time_elapsed         | 5476        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.017442588 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.8        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=2168.93 +/- 413.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17e+03    |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.016537491 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 144         |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 205         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 74       |
|    time_elapsed    | 5554     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=1973.56 +/- 513.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97e+03    |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.015514242 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.5        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 247         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 75       |
|    time_elapsed    | 5632     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.86e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 76          |
|    time_elapsed         | 5698        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.018433237 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.1        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 118         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=1863.81 +/- 341.68
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.86e+03  |
| time/                   |           |
|    total_timesteps      | 1250000   |
| train/                  |           |
|    approx_kl            | 0.0187065 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.42     |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | 73.4      |
|    n_updates            | 760       |
|    policy_gradient_loss | -0.00817  |
|    value_loss           | 225       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 77       |
|    time_elapsed    | 5776     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=1885.44 +/- 546.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89e+03    |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.014834448 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 157         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 367         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 78       |
|    time_elapsed    | 5855     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.79e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 79          |
|    time_elapsed         | 5921        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.015108461 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.4        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00885    |
|    value_loss           | 186         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=2038.18 +/- 274.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04e+03    |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.017519329 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 201         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 80       |
|    time_elapsed    | 5999     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=1662.87 +/- 237.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66e+03    |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.016902022 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 143         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 81       |
|    time_elapsed    | 6077     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.83e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 82          |
|    time_elapsed         | 6143        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.014373418 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 564         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=989.77 +/- 1581.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 990         |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.017422471 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 83       |
|    time_elapsed    | 6221     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=2228.69 +/- 718.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23e+03    |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.018938389 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.2        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 207         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 84       |
|    time_elapsed    | 6299     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.79e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 85          |
|    time_elapsed         | 6365        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.014547173 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.3        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 341         |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=2004.83 +/- 359.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2e+03       |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.018504802 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.3        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00939    |
|    value_loss           | 127         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 86       |
|    time_elapsed    | 6443     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=2092.31 +/- 776.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09e+03    |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.013355213 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 436         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 389         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 87       |
|    time_elapsed    | 6521     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.84e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 88          |
|    time_elapsed         | 6587        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.016955871 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.7        |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 246         |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=978.26 +/- 1605.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 978        |
| time/                   |            |
|    total_timesteps      | 1450000    |
| train/                  |            |
|    approx_kl            | 0.02309484 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 55.7       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 127        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 89       |
|    time_elapsed    | 6665     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.86e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 90          |
|    time_elapsed         | 6730        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.018296387 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.2        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=1790.20 +/- 286.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.013064173 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 616         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 91       |
|    time_elapsed    | 6808     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2139.74 +/- 583.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.14e+03   |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.01489785 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 127        |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0086    |
|    value_loss           | 250        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 92       |
|    time_elapsed    | 6886     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.79e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 93          |
|    time_elapsed         | 6952        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.014744275 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 223         |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 259         |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=1686.31 +/- 827.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.69e+03    |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.018468967 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.8        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00963    |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 94       |
|    time_elapsed    | 7030     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2560.88 +/- 407.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56e+03    |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.019126713 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.7        |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 159         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 95       |
|    time_elapsed    | 7108     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 96          |
|    time_elapsed         | 7173        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.016480934 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.3        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 336         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=2066.26 +/- 549.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07e+03    |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.018156718 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.3        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 311         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 97       |
|    time_elapsed    | 7251     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1837.18 +/- 222.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.018858183 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.1        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00962    |
|    value_loss           | 67.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 98       |
|    time_elapsed    | 7329     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 99          |
|    time_elapsed         | 7395        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.016027348 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.8        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=2546.13 +/- 719.05
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.55e+03   |
| time/                   |            |
|    total_timesteps      | 1625000    |
| train/                  |            |
|    approx_kl            | 0.01941925 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.7       |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.00879   |
|    value_loss           | 168        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 100      |
|    time_elapsed    | 7473     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2016.50 +/- 393.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02e+03    |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.016326372 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.6        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 291         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 101      |
|    time_elapsed    | 7551     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.97e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 102         |
|    time_elapsed         | 7616        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.017979085 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.3        |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00913    |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=2464.87 +/- 488.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46e+03    |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.017118145 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.5        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00962    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 103      |
|    time_elapsed    | 7694     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1765.09 +/- 450.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.016337816 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.4        |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 352         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 104      |
|    time_elapsed    | 7772     |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.06e+03   |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 105        |
|    time_elapsed         | 7838       |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 0.01768542 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.11      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.6       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00968   |
|    value_loss           | 93.3       |
----------------------------------------
Eval num_timesteps=1725000, episode_reward=2113.30 +/- 512.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11e+03    |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.019605864 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 31          |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 53.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 106      |
|    time_elapsed    | 7916     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2024.38 +/- 544.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02e+03    |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.018002424 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 107      |
|    time_elapsed    | 7994     |
|    total_timesteps | 1753088  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2e+03        |
| time/                   |              |
|    fps                  | 219          |
|    iterations           | 108          |
|    time_elapsed         | 8060         |
|    total_timesteps      | 1769472      |
| train/                  |              |
|    approx_kl            | 0.0114749195 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.658        |
|    learning_rate        | 0.0003       |
|    loss                 | 398          |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00627     |
|    value_loss           | 606          |
------------------------------------------
Eval num_timesteps=1775000, episode_reward=1836.52 +/- 411.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.020357871 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 35.5        |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 109      |
|    time_elapsed    | 8138     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2677.70 +/- 198.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68e+03    |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.014554879 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.2        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 328         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 110      |
|    time_elapsed    | 8216     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 111         |
|    time_elapsed         | 8282        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.019074287 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 24          |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 121         |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=2179.37 +/- 841.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.015487237 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.8        |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 268         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 112      |
|    time_elapsed    | 8359     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2608.88 +/- 527.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.61e+03    |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.017417237 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 185         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 345         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 113      |
|    time_elapsed    | 8437     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 114         |
|    time_elapsed         | 8503        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.018274516 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.3        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=1979.70 +/- 212.80
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.98e+03   |
| time/                   |            |
|    total_timesteps      | 1875000    |
| train/                  |            |
|    approx_kl            | 0.01960688 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 121        |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 92.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 115      |
|    time_elapsed    | 8581     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2018.26 +/- 396.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02e+03    |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.011282438 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 72.6        |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 426         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 116      |
|    time_elapsed    | 8659     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.91e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 117         |
|    time_elapsed         | 8724        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.015649306 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 335         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 492         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=2414.92 +/- 268.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.020283392 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 37.9        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 144         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 118      |
|    time_elapsed    | 8802     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.15e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 119         |
|    time_elapsed         | 8868        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.018981822 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.2        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 53          |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=1209.74 +/- 2188.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.21e+03   |
| time/                   |            |
|    total_timesteps      | 1950000    |
| train/                  |            |
|    approx_kl            | 0.01966523 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.79      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.7       |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 121        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 120      |
|    time_elapsed    | 8946     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=709.50 +/- 2837.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 709        |
| time/                   |            |
|    total_timesteps      | 1975000    |
| train/                  |            |
|    approx_kl            | 0.01569382 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 85.3       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.00846   |
|    value_loss           | 410        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 121      |
|    time_elapsed    | 9024     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 122         |
|    time_elapsed         | 9089        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.019553922 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.1        |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=1952.09 +/- 303.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95e+03    |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.016038124 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.7        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 192         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 123      |
|    time_elapsed    | 9167     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=2136.50 +/- 369.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.14e+03    |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.017213915 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 124      |
|    time_elapsed    | 9245     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 125         |
|    time_elapsed         | 9311        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.018554136 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.4        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 272         |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=2084.84 +/- 424.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.08e+03    |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.021779858 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 50.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 126      |
|    time_elapsed    | 9389     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=595.11 +/- 2868.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 595         |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.018170787 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.4        |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 150         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 127      |
|    time_elapsed    | 9467     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 128         |
|    time_elapsed         | 9532        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.018729221 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.4        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=2366.01 +/- 587.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37e+03    |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.020190286 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 129      |
|    time_elapsed    | 9610     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=2062.35 +/- 765.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06e+03    |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.021035694 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 226         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 130      |
|    time_elapsed    | 9688     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 131         |
|    time_elapsed         | 9754        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.019194607 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 132         |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=2027.74 +/- 522.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03e+03    |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.014570113 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.8        |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 366         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 132      |
|    time_elapsed    | 9832     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=2059.75 +/- 548.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06e+03    |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.020275105 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.01        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 39.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 133      |
|    time_elapsed    | 9910     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.08e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 134         |
|    time_elapsed         | 9975        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.020700919 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.8        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 80.1        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=2328.34 +/- 539.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33e+03    |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.023261126 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 135      |
|    time_elapsed    | 10053    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=2166.46 +/- 721.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17e+03    |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.018994395 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 136      |
|    time_elapsed    | 10131    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.24e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 137         |
|    time_elapsed         | 10197       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.021774195 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=660.84 +/- 3761.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 661         |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.021012288 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 102         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 138      |
|    time_elapsed    | 10275    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=1975.00 +/- 527.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.98e+03   |
| time/                   |            |
|    total_timesteps      | 2275000    |
| train/                  |            |
|    approx_kl            | 0.01879546 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.23      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.5       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.00843   |
|    value_loss           | 165        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 139      |
|    time_elapsed    | 10353    |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 140         |
|    time_elapsed         | 10418       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.017834784 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.5        |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=2489.52 +/- 682.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49e+03    |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.021459013 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 410         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 141      |
|    time_elapsed    | 10496    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=2181.88 +/- 630.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.014522076 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 533         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 142      |
|    time_elapsed    | 10574    |
|    total_timesteps | 2326528  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.25e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 143        |
|    time_elapsed         | 10640      |
|    total_timesteps      | 2342912    |
| train/                  |            |
|    approx_kl            | 0.02317245 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.29      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.00919   |
|    value_loss           | 145        |
----------------------------------------
Eval num_timesteps=2350000, episode_reward=2531.58 +/- 827.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.022993073 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.03       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 88.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 144      |
|    time_elapsed    | 10718    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=2119.29 +/- 310.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12e+03    |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.019381614 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.4        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 177         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 145      |
|    time_elapsed    | 10796    |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.19e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 146         |
|    time_elapsed         | 10862       |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.016677652 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.8        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=2239.80 +/- 475.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24e+03    |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.021390216 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.4        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 147      |
|    time_elapsed    | 10940    |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 148         |
|    time_elapsed         | 11005       |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.022399891 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=1867.35 +/- 474.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.87e+03   |
| time/                   |            |
|    total_timesteps      | 2425000    |
| train/                  |            |
|    approx_kl            | 0.01633098 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.22      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 102        |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 420        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 149      |
|    time_elapsed    | 11083    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2298.02 +/- 516.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.021275956 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.24e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 150      |
|    time_elapsed    | 11161    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.28e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 151         |
|    time_elapsed         | 11227       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.015291685 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 431         |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=2143.90 +/- 427.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.14e+03    |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.020547546 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 49          |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 41.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 152      |
|    time_elapsed    | 11305    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2179.29 +/- 516.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.020054653 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 153      |
|    time_elapsed    | 11383    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.36e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 154         |
|    time_elapsed         | 11448       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.018564079 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 24.6        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 246         |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=2071.63 +/- 22.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07e+03    |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.015914079 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.2        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 253         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 155      |
|    time_elapsed    | 11526    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2025.38 +/- 390.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03e+03    |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.018650033 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 24.5        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00943    |
|    value_loss           | 433         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 156      |
|    time_elapsed    | 11604    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 157         |
|    time_elapsed         | 11670       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.018103527 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 206         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=2696.63 +/- 387.78
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.7e+03   |
| time/                   |           |
|    total_timesteps      | 2575000   |
| train/                  |           |
|    approx_kl            | 0.0201511 |
|    clip_fraction        | 0.208     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.84     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | 20.3      |
|    n_updates            | 1570      |
|    policy_gradient_loss | -0.00923  |
|    value_loss           | 111       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.27e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 158      |
|    time_elapsed    | 11748    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2234.09 +/- 234.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23e+03    |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.019070953 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.4        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 269         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 159      |
|    time_elapsed    | 11826    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.3e+03     |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 160         |
|    time_elapsed         | 11892       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.016582891 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 316         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 235         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=2435.33 +/- 487.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.021555077 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 436         |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 337         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 161      |
|    time_elapsed    | 11970    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2515.94 +/- 733.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.018575791 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.3        |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.000738   |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 162      |
|    time_elapsed    | 12048    |
|    total_timesteps | 2654208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.17e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 163        |
|    time_elapsed         | 12114      |
|    total_timesteps      | 2670592    |
| train/                  |            |
|    approx_kl            | 0.02094749 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.76      |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.4       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 250        |
----------------------------------------
Eval num_timesteps=2675000, episode_reward=2300.89 +/- 318.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.016049363 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 31.5        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 280         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 164      |
|    time_elapsed    | 12192    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2274.95 +/- 599.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.27e+03   |
| time/                   |            |
|    total_timesteps      | 2700000    |
| train/                  |            |
|    approx_kl            | 0.01940413 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.35      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.00299   |
|    value_loss           | 232        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 165      |
|    time_elapsed    | 12270    |
|    total_timesteps | 2703360  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.27e+03  |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 166       |
|    time_elapsed         | 12336     |
|    total_timesteps      | 2719744   |
| train/                  |           |
|    approx_kl            | 0.0192797 |
|    clip_fraction        | 0.171     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.47     |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | 27.3      |
|    n_updates            | 1650      |
|    policy_gradient_loss | -0.00492  |
|    value_loss           | 539       |
---------------------------------------
Eval num_timesteps=2725000, episode_reward=2498.09 +/- 424.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.019691799 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 208         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 167      |
|    time_elapsed    | 12414    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2190.85 +/- 775.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19e+03    |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.013908742 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.9        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 751         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.27e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 168      |
|    time_elapsed    | 12493    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 169         |
|    time_elapsed         | 12558       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.014558186 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 314         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 730         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=2696.91 +/- 562.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.020983286 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.1        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 160         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 170      |
|    time_elapsed    | 12637    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2181.36 +/- 803.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.015931645 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 185         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00696    |
|    value_loss           | 219         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 171      |
|    time_elapsed    | 12715    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.35e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 172         |
|    time_elapsed         | 12780       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.018579952 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.4        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=2408.31 +/- 538.54
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.41e+03   |
| time/                   |            |
|    total_timesteps      | 2825000    |
| train/                  |            |
|    approx_kl            | 0.01589652 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.18      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.46       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 242        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 173      |
|    time_elapsed    | 12858    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2745.62 +/- 564.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.020066714 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 103         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 174      |
|    time_elapsed    | 12937    |
|    total_timesteps | 2850816  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.2e+03    |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 175        |
|    time_elapsed         | 13002      |
|    total_timesteps      | 2867200    |
| train/                  |            |
|    approx_kl            | 0.01381714 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.36      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 51.8       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.00594   |
|    value_loss           | 212        |
----------------------------------------
Eval num_timesteps=2875000, episode_reward=2317.91 +/- 717.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32e+03    |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.010588707 |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 262         |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 1.82e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 176      |
|    time_elapsed    | 13080    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 177         |
|    time_elapsed         | 13146       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.012238193 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 248         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 802         |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=2983.29 +/- 418.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.98e+03    |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.017701492 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.3        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 355         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 178      |
|    time_elapsed    | 13224    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=2312.11 +/- 577.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.31e+03     |
| time/                   |              |
|    total_timesteps      | 2925000      |
| train/                  |              |
|    approx_kl            | 0.0145008415 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.0003       |
|    loss                 | 647          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00841     |
|    value_loss           | 1.03e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 179      |
|    time_elapsed    | 13302    |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 180         |
|    time_elapsed         | 13367       |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.015349248 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.7        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 285         |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=2333.78 +/- 435.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33e+03    |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.020763777 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 181      |
|    time_elapsed    | 13445    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=2926.35 +/- 339.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93e+03    |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.017503725 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.9        |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 182      |
|    time_elapsed    | 13523    |
|    total_timesteps | 2981888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.34e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 183        |
|    time_elapsed         | 13589      |
|    total_timesteps      | 2998272    |
| train/                  |            |
|    approx_kl            | 0.01583771 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.3       |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.7       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.00671   |
|    value_loss           | 361        |
----------------------------------------
Eval num_timesteps=3000000, episode_reward=2604.30 +/- 450.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.011994064 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 403         |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 692         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 184      |
|    time_elapsed    | 13667    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=2724.95 +/- 680.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72e+03    |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.020404486 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.1        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00983    |
|    value_loss           | 717         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 185      |
|    time_elapsed    | 13745    |
|    total_timesteps | 3031040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.29e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 186        |
|    time_elapsed         | 13810      |
|    total_timesteps      | 3047424    |
| train/                  |            |
|    approx_kl            | 0.02690099 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.41      |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0003     |
|    loss                 | 41.5       |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.00525   |
|    value_loss           | 248        |
----------------------------------------
Eval num_timesteps=3050000, episode_reward=2572.13 +/- 364.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.018450886 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 338         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 187      |
|    time_elapsed    | 13888    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=2588.56 +/- 246.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.021198194 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 663         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00778    |
|    value_loss           | 594         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 188      |
|    time_elapsed    | 13966    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.1e+03     |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 189         |
|    time_elapsed         | 14032       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.011715883 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 154         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 558         |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=2753.68 +/- 307.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.010647725 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 396         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 523         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 190      |
|    time_elapsed    | 14110    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=2412.71 +/- 450.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.021937625 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 233         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 191      |
|    time_elapsed    | 14188    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.21e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 192         |
|    time_elapsed         | 14253       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.018810917 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 207         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 220         |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=2450.93 +/- 830.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45e+03    |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.013941952 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 510         |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 638         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 193      |
|    time_elapsed    | 14331    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=2097.88 +/- 407.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1e+03     |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.016599068 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.4        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 276         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 194      |
|    time_elapsed    | 14410    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.32e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 195         |
|    time_elapsed         | 14475       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.020240761 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 74          |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 241         |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=2374.93 +/- 350.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37e+03    |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.016831044 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 195         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 196      |
|    time_elapsed    | 14553    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=3073.96 +/- 280.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.07e+03   |
| time/                   |            |
|    total_timesteps      | 3225000    |
| train/                  |            |
|    approx_kl            | 0.01932073 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.3       |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.00953   |
|    value_loss           | 151        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 197      |
|    time_elapsed    | 14631    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.37e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 198         |
|    time_elapsed         | 14697       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.021460492 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.4        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 96.7        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=2666.61 +/- 447.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.016144723 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 10          |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 356         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 199      |
|    time_elapsed    | 14775    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=2350.22 +/- 488.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35e+03    |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.013457965 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 89.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 200      |
|    time_elapsed    | 14853    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.33e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 201         |
|    time_elapsed         | 14919       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.016305413 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 83.2        |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=2560.74 +/- 465.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56e+03    |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.017874919 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 240         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 202      |
|    time_elapsed    | 14997    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=2180.02 +/- 286.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.019812023 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 503         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 203      |
|    time_elapsed    | 15075    |
|    total_timesteps | 3325952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.3e+03    |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 204        |
|    time_elapsed         | 15140      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.01378694 |
|    clip_fraction        | 0.0993     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.4       |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.00603   |
|    value_loss           | 607        |
----------------------------------------
Eval num_timesteps=3350000, episode_reward=2681.72 +/- 499.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68e+03    |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.018867623 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.2        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 256         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 205      |
|    time_elapsed    | 15218    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=2171.14 +/- 520.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17e+03    |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.011444289 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 206      |
|    time_elapsed    | 15296    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.34e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 207         |
|    time_elapsed         | 15362       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.017987292 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=2200.65 +/- 702.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.010764908 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 150         |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 243         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 208      |
|    time_elapsed    | 15439    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.31e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 209         |
|    time_elapsed         | 15505       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.015319787 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.6        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 385         |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=2687.50 +/- 453.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.69e+03    |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.016120972 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 80.4        |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 252         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 210      |
|    time_elapsed    | 15583    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=2905.83 +/- 548.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91e+03    |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.010533307 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 303         |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 375         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 211      |
|    time_elapsed    | 15661    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.24e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 212         |
|    time_elapsed         | 15727       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.022570893 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 293         |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=2756.28 +/- 568.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.014374158 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 751         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 213      |
|    time_elapsed    | 15805    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=2574.82 +/- 143.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.020895973 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 60.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 214      |
|    time_elapsed    | 15883    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.32e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 215         |
|    time_elapsed         | 15948       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.015558244 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 176         |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=2993.96 +/- 291.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.99e+03    |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.016024552 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00854    |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 216      |
|    time_elapsed    | 16027    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=2022.65 +/- 556.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02e+03    |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.020065779 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 79.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 217      |
|    time_elapsed    | 16104    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.33e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 218         |
|    time_elapsed         | 16170       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.015375279 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00769    |
|    value_loss           | 355         |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=2484.19 +/- 673.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48e+03    |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.014303513 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.2        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 186         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 219      |
|    time_elapsed    | 16248    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=2481.98 +/- 452.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.48e+03   |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.01223171 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.88      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.23       |
|    n_updates            | 2190       |
|    policy_gradient_loss | 0.00078    |
|    value_loss           | 509        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 220      |
|    time_elapsed    | 16326    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.1e+03    |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 221        |
|    time_elapsed         | 16392      |
|    total_timesteps      | 3620864    |
| train/                  |            |
|    approx_kl            | 0.01824563 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16e+03   |
|    n_updates            | 2200       |
|    policy_gradient_loss | 0.000531   |
|    value_loss           | 1.33e+03   |
----------------------------------------
Eval num_timesteps=3625000, episode_reward=2058.95 +/- 397.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06e+03    |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.008987186 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 437         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 222      |
|    time_elapsed    | 16470    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=2417.12 +/- 764.44
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.42e+03  |
| time/                   |           |
|    total_timesteps      | 3650000   |
| train/                  |           |
|    approx_kl            | 0.0255311 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.56     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | 146       |
|    n_updates            | 2220      |
|    policy_gradient_loss | -0.0122   |
|    value_loss           | 104       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 223      |
|    time_elapsed    | 16548    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.24e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 224         |
|    time_elapsed         | 16613       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.014955757 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 898         |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00502    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=2383.10 +/- 243.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.008652575 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 609         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 225      |
|    time_elapsed    | 16691    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=2222.84 +/- 558.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22e+03    |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.015906362 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.5        |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.00453     |
|    value_loss           | 93.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 226      |
|    time_elapsed    | 16769    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.32e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 227         |
|    time_elapsed         | 16835       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.012495516 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.01        |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 363         |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=2517.30 +/- 684.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.013328859 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 162         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 228      |
|    time_elapsed    | 16913    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=2315.94 +/- 153.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.32e+03   |
| time/                   |            |
|    total_timesteps      | 3750000    |
| train/                  |            |
|    approx_kl            | 0.01437301 |
|    clip_fraction        | 0.0881     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | 762        |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 611        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 229      |
|    time_elapsed    | 16991    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 230         |
|    time_elapsed         | 17057       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.010680132 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0003      |
|    loss                 | 242         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=2194.47 +/- 263.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19e+03    |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.008598446 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 45.1        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 285         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 231      |
|    time_elapsed    | 17135    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=2876.44 +/- 418.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.020384401 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 20.8        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 372         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 232      |
|    time_elapsed    | 17213    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 233         |
|    time_elapsed         | 17278       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.011127961 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 614         |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=2111.41 +/- 475.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11e+03    |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.014482269 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 192         |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 503         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 234      |
|    time_elapsed    | 17356    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2977.12 +/- 340.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.98e+03    |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.015608899 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 243         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 235      |
|    time_elapsed    | 17434    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.45e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 236         |
|    time_elapsed         | 17500       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.020209443 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 71.3        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=2486.33 +/- 273.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49e+03    |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.014105028 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.3        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 510         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 237      |
|    time_elapsed    | 17578    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 238         |
|    time_elapsed         | 17643       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.019322641 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.4        |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 446         |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=2382.82 +/- 271.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.022467338 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.966      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 80.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 239      |
|    time_elapsed    | 17721    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=2288.36 +/- 488.78
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.29e+03     |
| time/                   |              |
|    total_timesteps      | 3925000      |
| train/                  |              |
|    approx_kl            | 0.0076301945 |
|    clip_fraction        | 0.0575       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.575        |
|    learning_rate        | 0.0003       |
|    loss                 | 151          |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 1.67e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 240      |
|    time_elapsed    | 17799    |
|    total_timesteps | 3932160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.42e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 241        |
|    time_elapsed         | 17865      |
|    total_timesteps      | 3948544    |
| train/                  |            |
|    approx_kl            | 0.01966802 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 662        |
----------------------------------------
Eval num_timesteps=3950000, episode_reward=2849.78 +/- 221.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.85e+03    |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.021671142 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 198         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 242      |
|    time_elapsed    | 17943    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=2182.73 +/- 597.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.013617309 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 281         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 243      |
|    time_elapsed    | 18021    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 244         |
|    time_elapsed         | 18087       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.014670221 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 187         |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 906         |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=2863.27 +/- 334.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.86e+03     |
| time/                   |              |
|    total_timesteps      | 4000000      |
| train/                  |              |
|    approx_kl            | 0.0148740895 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.43        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.6         |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00829     |
|    value_loss           | 134          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 245      |
|    time_elapsed    | 18165    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=2420.15 +/- 536.03
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.42e+03  |
| time/                   |           |
|    total_timesteps      | 4025000   |
| train/                  |           |
|    approx_kl            | 0.0189908 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.46     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.2      |
|    n_updates            | 2450      |
|    policy_gradient_loss | -0.00858  |
|    value_loss           | 54.8      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 246      |
|    time_elapsed    | 18243    |
|    total_timesteps | 4030464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.49e+03     |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 247          |
|    time_elapsed         | 18309        |
|    total_timesteps      | 4046848      |
| train/                  |              |
|    approx_kl            | 0.0146928895 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.43        |
|    explained_variance   | 0.92         |
|    learning_rate        | 0.0003       |
|    loss                 | 63.2         |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00833     |
|    value_loss           | 119          |
------------------------------------------
Eval num_timesteps=4050000, episode_reward=2765.40 +/- 185.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.017795313 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.5        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 248      |
|    time_elapsed    | 18387    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=2654.17 +/- 480.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.65e+03     |
| time/                   |              |
|    total_timesteps      | 4075000      |
| train/                  |              |
|    approx_kl            | 0.0073118266 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.959       |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | 71.9         |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.00488     |
|    value_loss           | 717          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 249      |
|    time_elapsed    | 18464    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.44e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 250         |
|    time_elapsed         | 18530       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.015126796 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 508         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 583         |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=2376.53 +/- 441.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.013129011 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.11        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 69.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 251      |
|    time_elapsed    | 18608    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=2409.12 +/- 343.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.012242059 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | 78          |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 519         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 252      |
|    time_elapsed    | 18686    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.38e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 253         |
|    time_elapsed         | 18751       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.011873031 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 64.5        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 732         |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=2500.62 +/- 665.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.014919786 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 63.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 254      |
|    time_elapsed    | 18829    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2355.92 +/- 311.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36e+03    |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.012431614 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 174         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 507         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 255      |
|    time_elapsed    | 18907    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.51e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 256         |
|    time_elapsed         | 18973       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.016081337 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.5        |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 70.3        |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=2227.54 +/- 687.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23e+03    |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.023730252 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.36        |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 23.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 257      |
|    time_elapsed    | 19051    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2788.78 +/- 562.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79e+03    |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.011490434 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 628         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 258      |
|    time_elapsed    | 19129    |
|    total_timesteps | 4227072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.46e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 259        |
|    time_elapsed         | 19195      |
|    total_timesteps      | 4243456    |
| train/                  |            |
|    approx_kl            | 0.01362405 |
|    clip_fraction        | 0.0959     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.1       |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.00533   |
|    value_loss           | 886        |
----------------------------------------
Eval num_timesteps=4250000, episode_reward=2432.14 +/- 423.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43e+03    |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.015061027 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.9        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 260      |
|    time_elapsed    | 19273    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=1345.35 +/- 3249.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.35e+03    |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.008856929 |
|    clip_fraction        | 0.0596      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0003      |
|    loss                 | 552         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 1.95e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 220      |
|    iterations      | 261      |
|    time_elapsed    | 19351    |
|    total_timesteps | 4276224  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.13e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 262        |
|    time_elapsed         | 19416      |
|    total_timesteps      | 4292608    |
| train/                  |            |
|    approx_kl            | 0.02092171 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.9       |
|    n_updates            | 2610       |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 123        |
----------------------------------------
Eval num_timesteps=4300000, episode_reward=2503.07 +/- 805.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.020626698 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 68.7        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0097     |
|    value_loss           | 142         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 263      |
|    time_elapsed    | 19494    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=2568.27 +/- 668.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.014921245 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 702         |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 663         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 264      |
|    time_elapsed    | 19572    |
|    total_timesteps | 4325376  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.25e+03     |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 265          |
|    time_elapsed         | 19638        |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0146195935 |
|    clip_fraction        | 0.0852       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0003       |
|    loss                 | 78.2         |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 616          |
------------------------------------------
Eval num_timesteps=4350000, episode_reward=2372.25 +/- 748.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37e+03    |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.018117394 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 266      |
|    time_elapsed    | 19716    |
|    total_timesteps | 4358144  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.29e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 267        |
|    time_elapsed         | 19781      |
|    total_timesteps      | 4374528    |
| train/                  |            |
|    approx_kl            | 0.01747775 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 55.2       |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 115        |
----------------------------------------
Eval num_timesteps=4375000, episode_reward=2406.21 +/- 418.68
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.41e+03     |
| time/                   |              |
|    total_timesteps      | 4375000      |
| train/                  |              |
|    approx_kl            | 0.0077971183 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.42        |
|    explained_variance   | 0.623        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.37e+03     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 1.72e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 268      |
|    time_elapsed    | 19859    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=2392.47 +/- 603.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.023833103 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.25        |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 75.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 269      |
|    time_elapsed    | 19937    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 270         |
|    time_elapsed         | 20003       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.013873333 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 638         |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=2668.09 +/- 408.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.017656099 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.7        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00792    |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 271      |
|    time_elapsed    | 20081    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=2762.93 +/- 346.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.010118639 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 218         |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 639         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 272      |
|    time_elapsed    | 20159    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 273         |
|    time_elapsed         | 20225       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.017916646 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.04        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 61.7        |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=2648.29 +/- 560.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.014297869 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | 741         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 1e+03       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 274      |
|    time_elapsed    | 20303    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=2975.53 +/- 465.09
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.98e+03     |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 0.0107159065 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.2         |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00756     |
|    value_loss           | 99.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 275      |
|    time_elapsed    | 20381    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.47e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 276         |
|    time_elapsed         | 20447       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.012396526 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 84          |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 174         |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=2735.09 +/- 340.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74e+03    |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.014028634 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 93.9        |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 399         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 277      |
|    time_elapsed    | 20525    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=1970.45 +/- 260.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97e+03    |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.013338817 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 461         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 278      |
|    time_elapsed    | 20603    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 279         |
|    time_elapsed         | 20668       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.010637568 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 959         |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=2091.37 +/- 439.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.09e+03   |
| time/                   |            |
|    total_timesteps      | 4575000    |
| train/                  |            |
|    approx_kl            | 0.01218039 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.14       |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0056    |
|    value_loss           | 100        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 280      |
|    time_elapsed    | 20746    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=2282.68 +/- 723.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.28e+03    |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.008884959 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 532         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 281      |
|    time_elapsed    | 20824    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.37e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 282         |
|    time_elapsed         | 20890       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.011221646 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.9        |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 465         |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=2673.21 +/- 494.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.018583482 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 735         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 283      |
|    time_elapsed    | 20968    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2576.15 +/- 516.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58e+03    |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.011053603 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 889         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 284      |
|    time_elapsed    | 21046    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.45e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 285         |
|    time_elapsed         | 21112       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.028697237 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.23        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 36.9        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=2598.81 +/- 402.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.024305306 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 52.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 286      |
|    time_elapsed    | 21190    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2702.02 +/- 626.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.007830993 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 688         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 287      |
|    time_elapsed    | 21268    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.51e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 288         |
|    time_elapsed         | 21334       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.010812452 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.5        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00502    |
|    value_loss           | 412         |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=2826.79 +/- 235.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83e+03    |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.020746037 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 64          |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 248         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 289      |
|    time_elapsed    | 21412    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=2239.80 +/- 405.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.24e+03     |
| time/                   |              |
|    total_timesteps      | 4750000      |
| train/                  |              |
|    approx_kl            | 0.0122697055 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.46        |
|    explained_variance   | 0.713        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.9         |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 697          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 290      |
|    time_elapsed    | 21490    |
|    total_timesteps | 4751360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.58e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 291        |
|    time_elapsed         | 21556      |
|    total_timesteps      | 4767744    |
| train/                  |            |
|    approx_kl            | 0.01215991 |
|    clip_fraction        | 0.0874     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 385        |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 464        |
----------------------------------------
Eval num_timesteps=4775000, episode_reward=3090.93 +/- 343.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.09e+03    |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.021017049 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.49        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 71.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 292      |
|    time_elapsed    | 21634    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=2578.65 +/- 480.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58e+03    |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.014290614 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 935         |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.12e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 293      |
|    time_elapsed    | 21712    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.28e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 294         |
|    time_elapsed         | 21778       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.016482564 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 626         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 446         |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=2821.73 +/- 503.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82e+03    |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.012551149 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.3        |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 512         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 295      |
|    time_elapsed    | 21856    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.41e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 296         |
|    time_elapsed         | 21922       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.014215477 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 266         |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=2407.10 +/- 813.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.013042423 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 397         |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 414         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 297      |
|    time_elapsed    | 22000    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=2620.04 +/- 655.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62e+03    |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.018737275 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.5        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 923         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 298      |
|    time_elapsed    | 22078    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 299         |
|    time_elapsed         | 22144       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.032469895 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 52.5        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=2585.97 +/- 520.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.021924563 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 52.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 300      |
|    time_elapsed    | 22222    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=2238.15 +/- 307.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24e+03    |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.017728124 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 40          |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00863    |
|    value_loss           | 68          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 301      |
|    time_elapsed    | 22300    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.57e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 302         |
|    time_elapsed         | 22366       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.020610642 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.2        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 273         |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=2525.78 +/- 667.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.018355794 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 919         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 303      |
|    time_elapsed    | 22444    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=2550.46 +/- 465.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.011779977 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 434         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 304      |
|    time_elapsed    | 22522    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.24e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 305         |
|    time_elapsed         | 22588       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.010295501 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 256         |
|    n_updates            | 3040        |
|    policy_gradient_loss | 0.00074     |
|    value_loss           | 910         |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=2118.12 +/- 593.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12e+03    |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.013597997 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 306      |
|    time_elapsed    | 22666    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=951.32 +/- 2911.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 951        |
| time/                   |            |
|    total_timesteps      | 5025000    |
| train/                  |            |
|    approx_kl            | 0.01342891 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 97.1       |
|    n_updates            | 3060       |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 347        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 307      |
|    time_elapsed    | 22744    |
|    total_timesteps | 5029888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.27e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 308        |
|    time_elapsed         | 22810      |
|    total_timesteps      | 5046272    |
| train/                  |            |
|    approx_kl            | 0.01695739 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 184        |
|    n_updates            | 3070       |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 161        |
----------------------------------------
Eval num_timesteps=5050000, episode_reward=2334.22 +/- 695.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33e+03    |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.018699944 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.8        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 270         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 309      |
|    time_elapsed    | 22888    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=2446.89 +/- 536.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45e+03    |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.015174413 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.46        |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 79.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 310      |
|    time_elapsed    | 22966    |
|    total_timesteps | 5079040  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.39e+03  |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 311       |
|    time_elapsed         | 23032     |
|    total_timesteps      | 5095424   |
| train/                  |           |
|    approx_kl            | 0.0202769 |
|    clip_fraction        | 0.124     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.46     |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0003    |
|    loss                 | 56.3      |
|    n_updates            | 3100      |
|    policy_gradient_loss | -0.00904  |
|    value_loss           | 185       |
---------------------------------------
Eval num_timesteps=5100000, episode_reward=2607.41 +/- 487.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.61e+03    |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.018269546 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 501         |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 234         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 312      |
|    time_elapsed    | 23110    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=2216.87 +/- 412.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22e+03    |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.014576533 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.83        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 45.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 313      |
|    time_elapsed    | 23188    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 314         |
|    time_elapsed         | 23254       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.012179937 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 29          |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 259         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=2086.86 +/- 403.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09e+03    |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.015963063 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.46        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 76.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 315      |
|    time_elapsed    | 23332    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=2549.96 +/- 412.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.012514183 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.4        |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 316      |
|    time_elapsed    | 23410    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 317         |
|    time_elapsed         | 23476       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.016330391 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.2        |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 162         |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=2944.54 +/- 721.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94e+03    |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.014794547 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.9        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 318      |
|    time_elapsed    | 23554    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=2665.27 +/- 278.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.015283091 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.8        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 319      |
|    time_elapsed    | 23632    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.32e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 320         |
|    time_elapsed         | 23698       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.012106078 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 506         |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=2552.34 +/- 756.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.020264192 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.391       |
|    learning_rate        | 0.0003      |
|    loss                 | 880         |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 2.01e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 321      |
|    time_elapsed    | 23776    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=2571.26 +/- 394.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.57e+03   |
| time/                   |            |
|    total_timesteps      | 5275000    |
| train/                  |            |
|    approx_kl            | 0.01865369 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 3210       |
|    policy_gradient_loss | -0.00624   |
|    value_loss           | 120        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 322      |
|    time_elapsed    | 23854    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 323         |
|    time_elapsed         | 23920       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.023048172 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 67.4        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=2316.18 +/- 371.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32e+03    |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.022093438 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.18        |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 324      |
|    time_elapsed    | 23998    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.67e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 325         |
|    time_elapsed         | 24064       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.019892033 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.5        |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 80.3        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=2246.82 +/- 385.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25e+03    |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.015611749 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.3        |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 326      |
|    time_elapsed    | 24142    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2453.59 +/- 543.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45e+03    |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.012991304 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.6        |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 2.65e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 327      |
|    time_elapsed    | 24220    |
|    total_timesteps | 5357568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.43e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 328        |
|    time_elapsed         | 24286      |
|    total_timesteps      | 5373952    |
| train/                  |            |
|    approx_kl            | 0.02996103 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.9        |
|    n_updates            | 3270       |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 111        |
----------------------------------------
Eval num_timesteps=5375000, episode_reward=3010.13 +/- 377.82
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.01e+03     |
| time/                   |              |
|    total_timesteps      | 5375000      |
| train/                  |              |
|    approx_kl            | 0.0154348295 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.771        |
|    learning_rate        | 0.0003       |
|    loss                 | 53.1         |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 365          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 329      |
|    time_elapsed    | 24364    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=2309.29 +/- 244.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.31e+03   |
| time/                   |            |
|    total_timesteps      | 5400000    |
| train/                  |            |
|    approx_kl            | 0.01556666 |
|    clip_fraction        | 0.094      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 3290       |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 411        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 330      |
|    time_elapsed    | 24442    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.47e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 331         |
|    time_elapsed         | 24508       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.018384963 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=2830.07 +/- 666.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83e+03    |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.010839425 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 453         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 332      |
|    time_elapsed    | 24586    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2662.92 +/- 425.78
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.66e+03     |
| time/                   |              |
|    total_timesteps      | 5450000      |
| train/                  |              |
|    approx_kl            | 0.0129076345 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.4         |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0003       |
|    loss                 | 66           |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 202          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 333      |
|    time_elapsed    | 24664    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 334         |
|    time_elapsed         | 24730       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.014828926 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=2433.65 +/- 236.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43e+03    |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.011291673 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.2        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 335      |
|    time_elapsed    | 24808    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=2294.60 +/- 362.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29e+03    |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.012579812 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 46          |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 183         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 336      |
|    time_elapsed    | 24886    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.46e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 337         |
|    time_elapsed         | 24952       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.014544017 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 469         |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=2381.26 +/- 691.79
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.38e+03     |
| time/                   |              |
|    total_timesteps      | 5525000      |
| train/                  |              |
|    approx_kl            | 0.0113037005 |
|    clip_fraction        | 0.0854       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0003       |
|    loss                 | 4.85         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00644     |
|    value_loss           | 395          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 338      |
|    time_elapsed    | 25030    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=2379.38 +/- 529.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.014543523 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.9        |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 377         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 339      |
|    time_elapsed    | 25108    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 340         |
|    time_elapsed         | 25174       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.018302057 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.1        |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 80.3        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=2975.47 +/- 344.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.98e+03   |
| time/                   |            |
|    total_timesteps      | 5575000    |
| train/                  |            |
|    approx_kl            | 0.01556606 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.64      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.96       |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.0089    |
|    value_loss           | 67.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 341      |
|    time_elapsed    | 25252    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=1993.99 +/- 502.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99e+03    |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.020168956 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.6        |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 36.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 342      |
|    time_elapsed    | 25330    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.49e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 343         |
|    time_elapsed         | 25396       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.010919463 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.6        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 132         |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=2005.49 +/- 413.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01e+03    |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.013775532 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 97.5        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 203         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 344      |
|    time_elapsed    | 25474    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=2567.77 +/- 589.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.015281696 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 124         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 345      |
|    time_elapsed    | 25552    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 346         |
|    time_elapsed         | 25618       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.013154668 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 555         |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=2453.98 +/- 307.80
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.45e+03   |
| time/                   |            |
|    total_timesteps      | 5675000    |
| train/                  |            |
|    approx_kl            | 0.01702592 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 34.6       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.00815   |
|    value_loss           | 392        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 347      |
|    time_elapsed    | 25696    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=2635.49 +/- 328.50
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.64e+03   |
| time/                   |            |
|    total_timesteps      | 5700000    |
| train/                  |            |
|    approx_kl            | 0.01373189 |
|    clip_fraction        | 0.0833     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 3470       |
|    policy_gradient_loss | -0.00759   |
|    value_loss           | 935        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 348      |
|    time_elapsed    | 25774    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.2e+03     |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 349         |
|    time_elapsed         | 25840       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.010476985 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.4        |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=2502.08 +/- 368.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.012144297 |
|    clip_fraction        | 0.0916      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | 409         |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 1.86e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 350      |
|    time_elapsed    | 25918    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2591.10 +/- 542.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.019205796 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.47        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 71.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 351      |
|    time_elapsed    | 25996    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.28e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 352         |
|    time_elapsed         | 26061       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.013608979 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=2990.14 +/- 412.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.99e+03    |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.017399725 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 127         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 353      |
|    time_elapsed    | 26139    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.27e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 354         |
|    time_elapsed         | 26205       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.010314442 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 660         |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=2499.07 +/- 671.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.025294296 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 635         |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 355      |
|    time_elapsed    | 26283    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=2194.00 +/- 784.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19e+03    |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.021020874 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.7        |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 356      |
|    time_elapsed    | 26361    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.39e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 357         |
|    time_elapsed         | 26427       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.015429101 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.5        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 661         |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=2725.22 +/- 414.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.73e+03    |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.012932135 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 149         |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 514         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 358      |
|    time_elapsed    | 26505    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=2405.50 +/- 434.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.41e+03   |
| time/                   |            |
|    total_timesteps      | 5875000    |
| train/                  |            |
|    approx_kl            | 0.02222637 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.42      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 550        |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 318        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 359      |
|    time_elapsed    | 26583    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.41e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 360         |
|    time_elapsed         | 26648       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.012765832 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 310         |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 880         |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=2823.16 +/- 895.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82e+03    |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.023147432 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.3         |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 57.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 361      |
|    time_elapsed    | 26726    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=2510.29 +/- 594.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.51e+03    |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.014571952 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.6        |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 355         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 362      |
|    time_elapsed    | 26804    |
|    total_timesteps | 5931008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.59e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 363        |
|    time_elapsed         | 26870      |
|    total_timesteps      | 5947392    |
| train/                  |            |
|    approx_kl            | 0.01600694 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 47.9       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 95.6       |
----------------------------------------
Eval num_timesteps=5950000, episode_reward=2378.17 +/- 796.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.007823568 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 441         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 364      |
|    time_elapsed    | 26948    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=1948.32 +/- 505.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95e+03    |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.014668961 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 365      |
|    time_elapsed    | 27026    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 366         |
|    time_elapsed         | 27092       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.015894888 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=2371.24 +/- 839.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.019549139 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.4        |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 307         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 367      |
|    time_elapsed    | 27170    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=-624.18 +/- 6676.70
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -624       |
| time/                   |            |
|    total_timesteps      | 6025000    |
| train/                  |            |
|    approx_kl            | 0.01315251 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.79       |
|    n_updates            | 3670       |
|    policy_gradient_loss | -0.00941   |
|    value_loss           | 38.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 368      |
|    time_elapsed    | 27248    |
|    total_timesteps | 6029312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.57e+03     |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 369          |
|    time_elapsed         | 27313        |
|    total_timesteps      | 6045696      |
| train/                  |              |
|    approx_kl            | 0.0152684655 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.0003       |
|    loss                 | 25           |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 81           |
------------------------------------------
Eval num_timesteps=6050000, episode_reward=2654.36 +/- 692.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.009226123 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.5        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 948         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 370      |
|    time_elapsed    | 27391    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=2228.00 +/- 838.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.23e+03   |
| time/                   |            |
|    total_timesteps      | 6075000    |
| train/                  |            |
|    approx_kl            | 0.01302185 |
|    clip_fraction        | 0.0914     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 754        |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.00503   |
|    value_loss           | 665        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 371      |
|    time_elapsed    | 27469    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.47e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 372         |
|    time_elapsed         | 27535       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.019673739 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.79        |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=2391.19 +/- 409.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.010976846 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 24          |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 399         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 373      |
|    time_elapsed    | 27613    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=2496.11 +/- 548.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.021980457 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 304         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 374      |
|    time_elapsed    | 27691    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 375         |
|    time_elapsed         | 27757       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.022262212 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 153         |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 712         |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=2888.42 +/- 628.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.89e+03    |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.013146597 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 457         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 376      |
|    time_elapsed    | 27835    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=2515.93 +/- 355.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.014294958 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 241         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 377      |
|    time_elapsed    | 27913    |
|    total_timesteps | 6176768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.49e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 378        |
|    time_elapsed         | 27978      |
|    total_timesteps      | 6193152    |
| train/                  |            |
|    approx_kl            | 0.01556722 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 192        |
----------------------------------------
Eval num_timesteps=6200000, episode_reward=2552.03 +/- 531.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.010731646 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 87          |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 1.22e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 379      |
|    time_elapsed    | 28056    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2215.96 +/- 289.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22e+03    |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.016020749 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 312         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 380      |
|    time_elapsed    | 28134    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.38e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 381         |
|    time_elapsed         | 28200       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.011637086 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 230         |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 356         |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=2668.49 +/- 345.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.017006256 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 190         |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 382      |
|    time_elapsed    | 28278    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=2503.84 +/- 310.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.011608124 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.3        |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 692         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 383      |
|    time_elapsed    | 28356    |
|    total_timesteps | 6275072  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.31e+03     |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 384          |
|    time_elapsed         | 28421        |
|    total_timesteps      | 6291456      |
| train/                  |              |
|    approx_kl            | 0.0088327825 |
|    clip_fraction        | 0.0774       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.0003       |
|    loss                 | 232          |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00544     |
|    value_loss           | 869          |
------------------------------------------
Eval num_timesteps=6300000, episode_reward=2403.35 +/- 673.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4e+03     |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.016867884 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.6        |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 385      |
|    time_elapsed    | 28499    |
|    total_timesteps | 6307840  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.42e+03     |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 386          |
|    time_elapsed         | 28565        |
|    total_timesteps      | 6324224      |
| train/                  |              |
|    approx_kl            | 0.0151235545 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.8         |
|    n_updates            | 3850         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 84.7         |
------------------------------------------
Eval num_timesteps=6325000, episode_reward=2163.93 +/- 317.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.16e+03    |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.012445545 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 303         |
|    n_updates            | 3860        |
|    policy_gradient_loss | 0.000912    |
|    value_loss           | 649         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 387      |
|    time_elapsed    | 28643    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=2542.18 +/- 634.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54e+03    |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.016871758 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.74        |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 21.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 388      |
|    time_elapsed    | 28721    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.66e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 389         |
|    time_elapsed         | 28787       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.014629282 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 74.8        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=3090.39 +/- 473.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.09e+03    |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.019593392 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 390      |
|    time_elapsed    | 28865    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=2539.39 +/- 32.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.54e+03   |
| time/                   |            |
|    total_timesteps      | 6400000    |
| train/                  |            |
|    approx_kl            | 0.01263874 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 117        |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0033    |
|    value_loss           | 167        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 391      |
|    time_elapsed    | 28944    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.48e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 392         |
|    time_elapsed         | 29009       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.016026633 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=2256.79 +/- 336.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.26e+03    |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.016924921 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | 550         |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.33e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 393      |
|    time_elapsed    | 29088    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=2903.39 +/- 195.28
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.9e+03    |
| time/                   |            |
|    total_timesteps      | 6450000    |
| train/                  |            |
|    approx_kl            | 0.01534356 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.4       |
|    n_updates            | 3930       |
|    policy_gradient_loss | -0.00791   |
|    value_loss           | 126        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 394      |
|    time_elapsed    | 29166    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 395         |
|    time_elapsed         | 29232       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.018907716 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.26        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 62          |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=2414.24 +/- 712.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.012157779 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 138         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 396      |
|    time_elapsed    | 29310    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2435.23 +/- 557.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.015911214 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.7        |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 89.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 397      |
|    time_elapsed    | 29388    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.7e+03     |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 398         |
|    time_elapsed         | 29454       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.014603367 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.2        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 72.7        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=2238.00 +/- 527.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24e+03    |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.011819502 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.3        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 503         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 399      |
|    time_elapsed    | 29532    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=2260.30 +/- 422.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.26e+03    |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.013382959 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 230         |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 380         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 400      |
|    time_elapsed    | 29610    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 401         |
|    time_elapsed         | 29676       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.021567333 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.77        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 40.7        |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=2890.85 +/- 231.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.89e+03    |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.024322893 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 87.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 402      |
|    time_elapsed    | 29755    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2526.87 +/- 454.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.018012501 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.9        |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 403      |
|    time_elapsed    | 29835    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 404         |
|    time_elapsed         | 29902       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.009735891 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=2741.47 +/- 580.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74e+03    |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.015723007 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.6        |
|    n_updates            | 4040        |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 405      |
|    time_elapsed    | 29982    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=2570.76 +/- 920.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.016484011 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.3        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00992    |
|    value_loss           | 883         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 406      |
|    time_elapsed    | 30061    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 407         |
|    time_elapsed         | 30128       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.016016461 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.7        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 337         |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=2643.29 +/- 588.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.012722653 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 415         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 408      |
|    time_elapsed    | 30208    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=1369.34 +/- 2484.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37e+03    |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.012975151 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | 303         |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 2.32e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 409      |
|    time_elapsed    | 30288    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.47e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 410         |
|    time_elapsed         | 30355       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.016887072 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 224         |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 738         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=2490.81 +/- 679.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49e+03    |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.026229102 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.8        |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 411      |
|    time_elapsed    | 30434    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2768.95 +/- 547.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.025391135 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 791         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 412      |
|    time_elapsed    | 30514    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.58e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 413        |
|    time_elapsed         | 30581      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.01138195 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 260        |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00334   |
|    value_loss           | 571        |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=2576.58 +/- 622.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58e+03    |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.017671444 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.7        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 414      |
|    time_elapsed    | 30660    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 415         |
|    time_elapsed         | 30727       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.018607872 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 554         |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=2394.77 +/- 149.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.017770993 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 944         |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 1e+03       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 416      |
|    time_elapsed    | 30807    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=2407.31 +/- 195.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.014408431 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 663         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 417      |
|    time_elapsed    | 30886    |
|    total_timesteps | 6832128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.52e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 418        |
|    time_elapsed         | 30953      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.00977626 |
|    clip_fraction        | 0.0994     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 329        |
|    n_updates            | 4170       |
|    policy_gradient_loss | -0.00713   |
|    value_loss           | 674        |
----------------------------------------
Eval num_timesteps=6850000, episode_reward=2397.20 +/- 670.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.4e+03    |
| time/                   |            |
|    total_timesteps      | 6850000    |
| train/                  |            |
|    approx_kl            | 0.02374952 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.07       |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 137        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 419      |
|    time_elapsed    | 31032    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=2531.42 +/- 563.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.015511174 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.1        |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 141         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 420      |
|    time_elapsed    | 31112    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.56e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 421         |
|    time_elapsed         | 31179       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.015321932 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 204         |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 198         |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=1732.37 +/- 2161.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.011410456 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 738         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 422      |
|    time_elapsed    | 31259    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=2586.95 +/- 675.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.011025564 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0003      |
|    loss                 | 112         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 870         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 423      |
|    time_elapsed    | 31338    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.48e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 424         |
|    time_elapsed         | 31406       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.016292645 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.7        |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=2504.58 +/- 165.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.025152218 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.11        |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 46.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 425      |
|    time_elapsed    | 31485    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=2631.71 +/- 212.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63e+03    |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.015436104 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.5        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 334         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 426      |
|    time_elapsed    | 31564    |
|    total_timesteps | 6979584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.4e+03      |
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 427          |
|    time_elapsed         | 31631        |
|    total_timesteps      | 6995968      |
| train/                  |              |
|    approx_kl            | 0.0030572086 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0003       |
|    loss                 | 1.12e+03     |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.0027      |
|    value_loss           | 2.57e+03     |
------------------------------------------
Eval num_timesteps=7000000, episode_reward=2546.13 +/- 158.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.015909068 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.1        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 236         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 428      |
|    time_elapsed    | 31710    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=2518.43 +/- 621.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.012861427 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.6        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 429      |
|    time_elapsed    | 31788    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 430         |
|    time_elapsed         | 31854       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.025672756 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.44        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 109         |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=2372.33 +/- 608.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37e+03    |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.018280793 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.8         |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 431      |
|    time_elapsed    | 31932    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=2506.90 +/- 389.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.51e+03    |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.017217122 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.9        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 1.39e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 432      |
|    time_elapsed    | 32011    |
|    total_timesteps | 7077888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.68e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 433        |
|    time_elapsed         | 32077      |
|    total_timesteps      | 7094272    |
| train/                  |            |
|    approx_kl            | 0.02088483 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.7       |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0074    |
|    value_loss           | 61.5       |
----------------------------------------
Eval num_timesteps=7100000, episode_reward=2220.64 +/- 780.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22e+03    |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.015484461 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 343         |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 246         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 434      |
|    time_elapsed    | 32155    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=2366.50 +/- 350.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.37e+03   |
| time/                   |            |
|    total_timesteps      | 7125000    |
| train/                  |            |
|    approx_kl            | 0.01917774 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | 202        |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 388        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 435      |
|    time_elapsed    | 32234    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.57e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 436         |
|    time_elapsed         | 32300       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.014733085 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.1        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 619         |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=1326.55 +/- 2538.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.016945757 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.6        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 690         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 437      |
|    time_elapsed    | 32378    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=1453.37 +/- 2357.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.45e+03   |
| time/                   |            |
|    total_timesteps      | 7175000    |
| train/                  |            |
|    approx_kl            | 0.01886161 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 320        |
|    n_updates            | 4370       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 581        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 438      |
|    time_elapsed    | 32456    |
|    total_timesteps | 7176192  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.57e+03   |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 439        |
|    time_elapsed         | 32522      |
|    total_timesteps      | 7192576    |
| train/                  |            |
|    approx_kl            | 0.21974067 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 82         |
----------------------------------------
Eval num_timesteps=7200000, episode_reward=2173.04 +/- 357.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.17e+03     |
| time/                   |              |
|    total_timesteps      | 7200000      |
| train/                  |              |
|    approx_kl            | 0.0134435315 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0003       |
|    loss                 | 46           |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.0064      |
|    value_loss           | 398          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 440      |
|    time_elapsed    | 32601    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=335.62 +/- 4770.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 336         |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.016463157 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.4        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 687         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 441      |
|    time_elapsed    | 32679    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 442         |
|    time_elapsed         | 32745       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.020275554 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 359         |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 887         |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=2500.28 +/- 401.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.011989322 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0003      |
|    loss                 | 648         |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 1.92e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 443      |
|    time_elapsed    | 32824    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.17e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 444         |
|    time_elapsed         | 32890       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.016932264 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0003      |
|    loss                 | 671         |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=2024.50 +/- 280.00
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.02e+03  |
| time/                   |           |
|    total_timesteps      | 7275000   |
| train/                  |           |
|    approx_kl            | 0.0205766 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.27     |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0003    |
|    loss                 | 40.4      |
|    n_updates            | 4440      |
|    policy_gradient_loss | -0.00814  |
|    value_loss           | 297       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 445      |
|    time_elapsed    | 32968    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=2717.22 +/- 379.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.72e+03   |
| time/                   |            |
|    total_timesteps      | 7300000    |
| train/                  |            |
|    approx_kl            | 0.00935256 |
|    clip_fraction        | 0.0674     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | 93.8       |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 666        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 446      |
|    time_elapsed    | 33046    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.35e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 447         |
|    time_elapsed         | 33112       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.015407272 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 285         |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=2224.00 +/- 526.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22e+03    |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.020826634 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 448      |
|    time_elapsed    | 33191    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=2384.87 +/- 680.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.023015974 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00969    |
|    value_loss           | 85.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 449      |
|    time_elapsed    | 33269    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.75e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 450         |
|    time_elapsed         | 33334       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.020462241 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.83        |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=860.14 +/- 2737.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 860         |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.020375444 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.89       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.48        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 25.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 451      |
|    time_elapsed    | 33413    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=2653.25 +/- 409.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.016364641 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 13.9        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 85.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 452      |
|    time_elapsed    | 33491    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.65e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 453         |
|    time_elapsed         | 33556       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.018425008 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.986      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 267         |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 311         |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=2724.48 +/- 406.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72e+03    |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.019038398 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 150         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 454      |
|    time_elapsed    | 33635    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=2656.75 +/- 519.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66e+03    |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.029057734 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.74        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 62.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 455      |
|    time_elapsed    | 33713    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.61e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 456         |
|    time_elapsed         | 33778       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.017655898 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.9        |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 65.3        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=2716.18 +/- 662.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72e+03    |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.009912748 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 468         |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 1.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 457      |
|    time_elapsed    | 33857    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=2207.85 +/- 715.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21e+03    |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.010548202 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 55.2        |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 1.12e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 458      |
|    time_elapsed    | 33935    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 459         |
|    time_elapsed         | 34000       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.019832462 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=2748.55 +/- 450.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.023205765 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.23        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 74.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 460      |
|    time_elapsed    | 34078    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=2926.02 +/- 492.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93e+03    |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.021375643 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.02        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 461      |
|    time_elapsed    | 34157    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.71e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 462         |
|    time_elapsed         | 34222       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.027306981 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.88        |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 99.3        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=2586.98 +/- 445.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.59e+03     |
| time/                   |              |
|    total_timesteps      | 7575000      |
| train/                  |              |
|    approx_kl            | 0.0140142385 |
|    clip_fraction        | 0.0998       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e+03     |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.00942     |
|    value_loss           | 1.06e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 463      |
|    time_elapsed    | 34300    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=2670.94 +/- 367.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67e+03    |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.019614477 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02e+03    |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 503         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 464      |
|    time_elapsed    | 34378    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.49e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 465         |
|    time_elapsed         | 34445       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.017886514 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 861         |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=2888.00 +/- 675.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.89e+03   |
| time/                   |            |
|    total_timesteps      | 7625000    |
| train/                  |            |
|    approx_kl            | 0.02337514 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.5       |
|    n_updates            | 4650       |
|    policy_gradient_loss | -0.00747   |
|    value_loss           | 47.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 466      |
|    time_elapsed    | 34525    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=2399.89 +/- 645.34
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.4e+03    |
| time/                   |            |
|    total_timesteps      | 7650000    |
| train/                  |            |
|    approx_kl            | 0.01714456 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.01       |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.00677   |
|    value_loss           | 79.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 221      |
|    iterations      | 467      |
|    time_elapsed    | 34605    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.56e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 468         |
|    time_elapsed         | 34674       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.016106773 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.1        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 155         |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=2695.81 +/- 237.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.013209154 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.8        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 469      |
|    time_elapsed    | 34757    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=2346.62 +/- 593.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35e+03    |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.015864052 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 217         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 470      |
|    time_elapsed    | 34841    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.34e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 471         |
|    time_elapsed         | 34910       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.018363636 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 112         |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 271         |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=1936.04 +/- 934.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94e+03    |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.015614841 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 252         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 472      |
|    time_elapsed    | 34993    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.51e+03    |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 473         |
|    time_elapsed         | 35063       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.016897112 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 95.1        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=2437.33 +/- 432.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.019372726 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.57        |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 69.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 474      |
|    time_elapsed    | 35146    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=2453.67 +/- 592.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45e+03    |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.013005539 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 171         |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 475      |
|    time_elapsed    | 35230    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.68e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 476         |
|    time_elapsed         | 35299       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.017270094 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 27          |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 47.2        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=2517.79 +/- 430.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.021075377 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 660         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 477      |
|    time_elapsed    | 35381    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=2651.69 +/- 488.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.017954625 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 135         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 478      |
|    time_elapsed    | 35464    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.65e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 479         |
|    time_elapsed         | 35533       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.031916827 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 24.5        |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 162         |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=2329.19 +/- 260.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33e+03    |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.011256577 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 704         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 480      |
|    time_elapsed    | 35616    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=1466.38 +/- 2653.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47e+03    |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.018276364 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 941         |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 534         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 481      |
|    time_elapsed    | 35699    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.55e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 482         |
|    time_elapsed         | 35769       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.019407887 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 151         |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 297         |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=143.78 +/- 4266.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 144         |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.016835239 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 483      |
|    time_elapsed    | 35852    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=2214.63 +/- 473.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21e+03    |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.012569222 |
|    clip_fraction        | 0.064       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 793         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 484      |
|    time_elapsed    | 35935    |
|    total_timesteps | 7929856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.44e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 485        |
|    time_elapsed         | 36004      |
|    total_timesteps      | 7946240    |
| train/                  |            |
|    approx_kl            | 0.01684967 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 55.1       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 169        |
----------------------------------------
Eval num_timesteps=7950000, episode_reward=2282.71 +/- 657.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.28e+03    |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.012542112 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 486      |
|    time_elapsed    | 36088    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=2558.12 +/- 207.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56e+03    |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.013245326 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.8        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 251         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 487      |
|    time_elapsed    | 36170    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 488         |
|    time_elapsed         | 36240       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.014714365 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.6        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 109         |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2602.28 +/- 599.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.013760081 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 255         |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 489      |
|    time_elapsed    | 36322    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=2576.40 +/- 526.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58e+03    |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.015133671 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 280         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 490      |
|    time_elapsed    | 36406    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.52e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 491         |
|    time_elapsed         | 36476       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.015834007 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.49        |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 52.1        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=2545.06 +/- 597.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.55e+03    |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.014884847 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.18        |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 412         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 492      |
|    time_elapsed    | 36559    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=2468.06 +/- 359.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.47e+03    |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.012033869 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.6        |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 187         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 493      |
|    time_elapsed    | 36643    |
|    total_timesteps | 8077312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.53e+03     |
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 494          |
|    time_elapsed         | 36713        |
|    total_timesteps      | 8093696      |
| train/                  |              |
|    approx_kl            | 0.0113691315 |
|    clip_fraction        | 0.0957       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.42        |
|    explained_variance   | 0.617        |
|    learning_rate        | 0.0003       |
|    loss                 | 168          |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.00766     |
|    value_loss           | 618          |
------------------------------------------
Eval num_timesteps=8100000, episode_reward=2745.33 +/- 267.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.024469044 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 8           |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 66.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 495      |
|    time_elapsed    | 36796    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=3030.27 +/- 372.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.03e+03    |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.020181939 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 523         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 496      |
|    time_elapsed    | 36880    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.66e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 497         |
|    time_elapsed         | 36949       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.015572342 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00862    |
|    value_loss           | 206         |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=2834.29 +/- 321.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.83e+03   |
| time/                   |            |
|    total_timesteps      | 8150000    |
| train/                  |            |
|    approx_kl            | 0.01664275 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.73       |
|    n_updates            | 4970       |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 93.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 498      |
|    time_elapsed    | 37033    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=2807.68 +/- 346.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.81e+03    |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.020055981 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.2        |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 499      |
|    time_elapsed    | 37117    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 500         |
|    time_elapsed         | 37186       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.017366102 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.76        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 74.2        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=2734.07 +/- 276.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.73e+03    |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.019102879 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 235         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 501      |
|    time_elapsed    | 37270    |
|    total_timesteps | 8208384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.41e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 502        |
|    time_elapsed         | 37340      |
|    total_timesteps      | 8224768    |
| train/                  |            |
|    approx_kl            | 0.01787512 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 5010       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 59.7       |
----------------------------------------
Eval num_timesteps=8225000, episode_reward=2612.13 +/- 334.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.61e+03    |
| time/                   |             |
|    total_timesteps      | 8225000     |
| train/                  |             |
|    approx_kl            | 0.009390427 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 329         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 503      |
|    time_elapsed    | 37423    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=2784.50 +/- 339.56
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.78e+03   |
| time/                   |            |
|    total_timesteps      | 8250000    |
| train/                  |            |
|    approx_kl            | 0.01787614 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.95       |
|    n_updates            | 5030       |
|    policy_gradient_loss | -0.00952   |
|    value_loss           | 122        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 504      |
|    time_elapsed    | 37506    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.44e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 505         |
|    time_elapsed         | 37577       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.011726247 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=2875.22 +/- 264.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.018273905 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 105         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 506      |
|    time_elapsed    | 37660    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=2846.26 +/- 304.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.85e+03    |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.025910163 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 36.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 507      |
|    time_elapsed    | 37743    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 508         |
|    time_elapsed         | 37812       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.016715093 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=338.80 +/- 4993.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 339         |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.012911694 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.1        |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 345         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 509      |
|    time_elapsed    | 37895    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=2944.42 +/- 525.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94e+03    |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.015437769 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 35.9        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 510      |
|    time_elapsed    | 37979    |
|    total_timesteps | 8355840  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.69e+03   |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 511        |
|    time_elapsed         | 38048      |
|    total_timesteps      | 8372224    |
| train/                  |            |
|    approx_kl            | 0.01860914 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.06       |
|    n_updates            | 5100       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 53.3       |
----------------------------------------
Eval num_timesteps=8375000, episode_reward=2555.13 +/- 381.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56e+03    |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.013178071 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 512      |
|    time_elapsed    | 38131    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=3017.60 +/- 411.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.02e+03    |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.023736728 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.5         |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 28.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 513      |
|    time_elapsed    | 38214    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 514         |
|    time_elapsed         | 38284       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.016014867 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.53        |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 56.5        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=2571.36 +/- 595.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.010201735 |
|    clip_fraction        | 0.0619      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04e+03    |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 2.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 515      |
|    time_elapsed    | 38366    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=2660.27 +/- 707.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66e+03    |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.024649363 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 701         |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 966         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 516      |
|    time_elapsed    | 38449    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.59e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 517         |
|    time_elapsed         | 38518       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.022702757 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.29        |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 27          |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=2636.78 +/- 631.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.019201986 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.65        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 101         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 518      |
|    time_elapsed    | 38601    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=3036.17 +/- 523.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.04e+03    |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.023991577 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.04        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 36.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 519      |
|    time_elapsed    | 38683    |
|    total_timesteps | 8503296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.51e+03     |
| time/                   |              |
|    fps                  | 219          |
|    iterations           | 520          |
|    time_elapsed         | 38752        |
|    total_timesteps      | 8519680      |
| train/                  |              |
|    approx_kl            | 0.0079258885 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | 841          |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00831     |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=8525000, episode_reward=2789.98 +/- 178.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79e+03    |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.022000726 |
|    clip_fraction        | 0.0797      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | 452         |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 1.59e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 521      |
|    time_elapsed    | 38834    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=2766.15 +/- 197.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.017005198 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 197         |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 522      |
|    time_elapsed    | 38915    |
|    total_timesteps | 8552448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.52e+03   |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 523        |
|    time_elapsed         | 38984      |
|    total_timesteps      | 8568832    |
| train/                  |            |
|    approx_kl            | 0.01870729 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.934     |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 81.7       |
|    n_updates            | 5220       |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 47.7       |
----------------------------------------
Eval num_timesteps=8575000, episode_reward=2750.00 +/- 732.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.017587159 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.19        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 77.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 524      |
|    time_elapsed    | 39066    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=2813.58 +/- 316.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.81e+03    |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.012198767 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 199         |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 388         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 525      |
|    time_elapsed    | 39149    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.64e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 526         |
|    time_elapsed         | 39218       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.029593227 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 385         |
|    n_updates            | 5250        |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 769         |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=2151.77 +/- 385.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.15e+03    |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.012768535 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.7        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 527      |
|    time_elapsed    | 39299    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=2527.00 +/- 422.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.008435443 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 699         |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 1.39e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 528      |
|    time_elapsed    | 39381    |
|    total_timesteps | 8650752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 529         |
|    time_elapsed         | 39450       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.021915417 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 385         |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=2812.33 +/- 429.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.81e+03    |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.017908223 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.12        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 174         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 530      |
|    time_elapsed    | 39532    |
|    total_timesteps | 8683520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.77e+03   |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 531        |
|    time_elapsed         | 39600      |
|    total_timesteps      | 8699904    |
| train/                  |            |
|    approx_kl            | 0.01925268 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.5       |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.0095    |
|    value_loss           | 182        |
----------------------------------------
Eval num_timesteps=8700000, episode_reward=2640.24 +/- 278.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.016226007 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.935      |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 532      |
|    time_elapsed    | 39682    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=2374.44 +/- 332.00
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.37e+03   |
| time/                   |            |
|    total_timesteps      | 8725000    |
| train/                  |            |
|    approx_kl            | 0.01764001 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.73       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.00828   |
|    value_loss           | 692        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 533      |
|    time_elapsed    | 39764    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.6e+03     |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 534         |
|    time_elapsed         | 39833       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.013782228 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.5        |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 296         |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=2536.79 +/- 587.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54e+03    |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.020298857 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | 614         |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 1.45e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 535      |
|    time_elapsed    | 39914    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=2902.60 +/- 267.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9e+03     |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.027590137 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 69.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 536      |
|    time_elapsed    | 39996    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.62e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 537         |
|    time_elapsed         | 40064       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.011414806 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.9        |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 195         |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=2678.36 +/- 330.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.68e+03   |
| time/                   |            |
|    total_timesteps      | 8800000    |
| train/                  |            |
|    approx_kl            | 0.01933745 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 420        |
|    n_updates            | 5370       |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 794        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 538      |
|    time_elapsed    | 40146    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=3102.92 +/- 316.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.1e+03     |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.016575076 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 34          |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 273         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 539      |
|    time_elapsed    | 40228    |
|    total_timesteps | 8830976  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.53e+03  |
| time/                   |           |
|    fps                  | 219       |
|    iterations           | 540       |
|    time_elapsed         | 40297     |
|    total_timesteps      | 8847360   |
| train/                  |           |
|    approx_kl            | 0.0197241 |
|    clip_fraction        | 0.152     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.55     |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | 13        |
|    n_updates            | 5390      |
|    policy_gradient_loss | -0.0116   |
|    value_loss           | 91.7      |
---------------------------------------
Eval num_timesteps=8850000, episode_reward=2196.57 +/- 385.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.017295811 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 163         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 541      |
|    time_elapsed    | 40379    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=2905.83 +/- 401.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91e+03    |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.021774469 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.71        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 21.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 542      |
|    time_elapsed    | 40461    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.71e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 543         |
|    time_elapsed         | 40529       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.013236259 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.7         |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 85.3        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=2629.45 +/- 503.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63e+03    |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.016616464 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.77        |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 335         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 544      |
|    time_elapsed    | 40611    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=2483.01 +/- 630.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48e+03    |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.019871732 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 441         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 545      |
|    time_elapsed    | 40692    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 546         |
|    time_elapsed         | 40761       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.009131823 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | 501         |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=2878.15 +/- 124.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.022851925 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.93        |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 22.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 547      |
|    time_elapsed    | 40842    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=2983.64 +/- 459.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.98e+03    |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.008434914 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05e+03    |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 548      |
|    time_elapsed    | 40924    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 549         |
|    time_elapsed         | 40993       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.018122535 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 276         |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2566.49 +/- 755.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57e+03    |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.016333576 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.68        |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00854    |
|    value_loss           | 21.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 550      |
|    time_elapsed    | 41074    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=2014.05 +/- 255.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01e+03    |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.009496633 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 633         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 551      |
|    time_elapsed    | 41156    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.73e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 552         |
|    time_elapsed         | 41224       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.016006624 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 58.6        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=2680.91 +/- 462.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68e+03    |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.015609371 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.78        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00971    |
|    value_loss           | 83.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 553      |
|    time_elapsed    | 41306    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=2630.38 +/- 391.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63e+03    |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.019674022 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.07        |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 554      |
|    time_elapsed    | 41388    |
|    total_timesteps | 9076736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.74e+03   |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 555        |
|    time_elapsed         | 41456      |
|    total_timesteps      | 9093120    |
| train/                  |            |
|    approx_kl            | 0.02634041 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.1       |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 95.2       |
----------------------------------------
Eval num_timesteps=9100000, episode_reward=2500.24 +/- 269.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5e+03     |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.015964795 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 374         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 556      |
|    time_elapsed    | 41538    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=2897.43 +/- 489.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9e+03     |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.015017359 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 273         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 557      |
|    time_elapsed    | 41620    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.66e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 558         |
|    time_elapsed         | 41688       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.023734648 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | 205         |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=2838.31 +/- 526.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.84e+03    |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.015418001 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 492         |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 665         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 559      |
|    time_elapsed    | 41770    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=2475.84 +/- 434.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.48e+03   |
| time/                   |            |
|    total_timesteps      | 9175000    |
| train/                  |            |
|    approx_kl            | 0.01756203 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 123        |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 474        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 560      |
|    time_elapsed    | 41852    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.44e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 561         |
|    time_elapsed         | 41920       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.015284354 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4e+03     |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 2.61e+03    |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=2939.44 +/- 710.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94e+03    |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.018686391 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 241         |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 189         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 562      |
|    time_elapsed    | 42002    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.45e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 563         |
|    time_elapsed         | 42071       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.018009013 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.0003      |
|    loss                 | 225         |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 1.58e+03    |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=2660.72 +/- 296.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66e+03    |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.016680732 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 22.6        |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 564      |
|    time_elapsed    | 42152    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=2661.66 +/- 533.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66e+03    |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.017452419 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.2        |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 49.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 565      |
|    time_elapsed    | 42234    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.6e+03     |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 566         |
|    time_elapsed         | 42302       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.013394803 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 407         |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=2609.07 +/- 453.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.61e+03   |
| time/                   |            |
|    total_timesteps      | 9275000    |
| train/                  |            |
|    approx_kl            | 0.01033816 |
|    clip_fraction        | 0.0776     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.0015    |
|    value_loss           | 1.04e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 567      |
|    time_elapsed    | 42384    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=2718.08 +/- 373.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72e+03    |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.025243852 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.4         |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 25.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 568      |
|    time_elapsed    | 42466    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.61e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 569         |
|    time_elapsed         | 42535       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.013775563 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=3072.42 +/- 452.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.07e+03     |
| time/                   |              |
|    total_timesteps      | 9325000      |
| train/                  |              |
|    approx_kl            | 0.0095402375 |
|    clip_fraction        | 0.0836       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.624        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.29e+03     |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.0038      |
|    value_loss           | 1.56e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 570      |
|    time_elapsed    | 42616    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=2649.56 +/- 460.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.017005721 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 460         |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 742         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 571      |
|    time_elapsed    | 42698    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.64e+03    |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 572         |
|    time_elapsed         | 42767       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.017531015 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.2        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 86.4        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=1601.90 +/- 1676.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6e+03     |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.011692912 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00989    |
|    value_loss           | 236         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 573      |
|    time_elapsed    | 42848    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=2420.70 +/- 791.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.019401278 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.86        |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 47.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 574      |
|    time_elapsed    | 42931    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.8e+03     |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 575         |
|    time_elapsed         | 43000       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.019041099 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.78        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 78.8        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=2752.58 +/- 542.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75e+03    |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.016101934 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.00854    |
|    value_loss           | 70.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 576      |
|    time_elapsed    | 43082    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=2600.77 +/- 271.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.017242849 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 5760        |
|    policy_gradient_loss | 0.00123     |
|    value_loss           | 301         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 577      |
|    time_elapsed    | 43165    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.7e+03     |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 578         |
|    time_elapsed         | 43235       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.013665851 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.6        |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=2828.66 +/- 435.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83e+03    |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.016136277 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.96        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 579      |
|    time_elapsed    | 43319    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=2963.11 +/- 465.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96e+03    |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.017497662 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.37        |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 73.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 580      |
|    time_elapsed    | 43402    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.81e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 581         |
|    time_elapsed         | 43472       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.021328252 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 37.9        |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=2654.13 +/- 187.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65e+03    |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.018346608 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 33.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 582      |
|    time_elapsed    | 43555    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=2902.56 +/- 533.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9e+03     |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.014298058 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 92.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 583      |
|    time_elapsed    | 43638    |
|    total_timesteps | 9551872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.65e+03   |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 584        |
|    time_elapsed         | 43708      |
|    total_timesteps      | 9568256    |
| train/                  |            |
|    approx_kl            | 0.01960938 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 10.5       |
|    n_updates            | 5830       |
|    policy_gradient_loss | -0.00884   |
|    value_loss           | 71.5       |
----------------------------------------
Eval num_timesteps=9575000, episode_reward=2796.23 +/- 416.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.8e+03     |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.009702602 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 682         |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00958    |
|    value_loss           | 1.92e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 585      |
|    time_elapsed    | 43791    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=2735.67 +/- 392.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74e+03    |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.019297708 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 72.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 586      |
|    time_elapsed    | 43874    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 587         |
|    time_elapsed         | 43944       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.014292775 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.8        |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 98.9        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=2642.00 +/- 192.97
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.64e+03  |
| time/                   |           |
|    total_timesteps      | 9625000   |
| train/                  |           |
|    approx_kl            | 0.0179773 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.94     |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0003    |
|    loss                 | 111       |
|    n_updates            | 5870      |
|    policy_gradient_loss | -0.01     |
|    value_loss           | 252       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 588      |
|    time_elapsed    | 44027    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=2646.46 +/- 511.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.65e+03   |
| time/                   |            |
|    total_timesteps      | 9650000    |
| train/                  |            |
|    approx_kl            | 0.02333117 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 37         |
|    n_updates            | 5880       |
|    policy_gradient_loss | 0.000869   |
|    value_loss           | 287        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 589      |
|    time_elapsed    | 44111    |
|    total_timesteps | 9650176  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.69e+03     |
| time/                   |              |
|    fps                  | 218          |
|    iterations           | 590          |
|    time_elapsed         | 44181        |
|    total_timesteps      | 9666560      |
| train/                  |              |
|    approx_kl            | 0.0147980815 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.51         |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.0083      |
|    value_loss           | 80.6         |
------------------------------------------
Eval num_timesteps=9675000, episode_reward=2744.80 +/- 439.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74e+03    |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.011785202 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 534         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 591      |
|    time_elapsed    | 44264    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.73e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 592         |
|    time_elapsed         | 44333       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.015255764 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.7        |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 205         |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=2396.41 +/- 320.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4e+03     |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.020107226 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.84        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 593      |
|    time_elapsed    | 44416    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=2559.80 +/- 467.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56e+03    |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.013205306 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.33        |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 218         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 594      |
|    time_elapsed    | 44499    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.68e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 595         |
|    time_elapsed         | 44571       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.016536716 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.6         |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00903    |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=2601.40 +/- 396.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.015480636 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.26        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 596      |
|    time_elapsed    | 44656    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=2907.87 +/- 408.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91e+03    |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.013555398 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.3        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 209         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 597      |
|    time_elapsed    | 44741    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.58e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 598         |
|    time_elapsed         | 44813       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.011936858 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=2593.09 +/- 413.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.017615499 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 65.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 599      |
|    time_elapsed    | 44897    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=2696.08 +/- 454.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.012369176 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 93.2        |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 178         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 600      |
|    time_elapsed    | 44982    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.52e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 601         |
|    time_elapsed         | 45053       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.013398451 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 442         |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=2588.94 +/- 522.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59e+03    |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.020533117 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.08        |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 58.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 602      |
|    time_elapsed    | 45138    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=3128.01 +/- 317.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.13e+03     |
| time/                   |              |
|    total_timesteps      | 9875000      |
| train/                  |              |
|    approx_kl            | 0.0126937125 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.0003       |
|    loss                 | 115          |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.00861     |
|    value_loss           | 258          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 603      |
|    time_elapsed    | 45223    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 604         |
|    time_elapsed         | 45294       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.014589812 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.7        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 57.6        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=2775.76 +/- 298.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78e+03    |
| time/                   |             |
|    total_timesteps      | 9900000     |
| train/                  |             |
|    approx_kl            | 0.013258084 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 6040        |
|    policy_gradient_loss | 0.0025      |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 605      |
|    time_elapsed    | 45378    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=3029.46 +/- 442.47
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.03e+03   |
| time/                   |            |
|    total_timesteps      | 9925000    |
| train/                  |            |
|    approx_kl            | 0.01709285 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.97       |
|    n_updates            | 6050       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 34.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 606      |
|    time_elapsed    | 45463    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.6e+03     |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 607         |
|    time_elapsed         | 45534       |
|    total_timesteps      | 9945088     |
| train/                  |             |
|    approx_kl            | 0.012012208 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | 478         |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 956         |
-----------------------------------------
Eval num_timesteps=9950000, episode_reward=2382.97 +/- 235.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38e+03    |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.016069494 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0003      |
|    loss                 | 603         |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 775         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 608      |
|    time_elapsed    | 45618    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=2435.38 +/- 501.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.014914455 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 609      |
|    time_elapsed    | 45702    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.58e+03    |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 610         |
|    time_elapsed         | 45775       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.018324662 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.5        |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 71.6        |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=2763.26 +/- 561.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.021371387 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.24        |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 20.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 611      |
|    time_elapsed    | 45859    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v11_1
