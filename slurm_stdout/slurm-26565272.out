========== uav-v7 ==========
Seed: 3406100432
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v7_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.34e+03 |
| time/              |           |
|    fps             | 399       |
|    iterations      | 1         |
|    time_elapsed    | 41        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-3533.22 +/- 3380.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.53e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007145235 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.000141    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.6e+04     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 4.44e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.01e+03 |
| time/              |           |
|    fps             | 321       |
|    iterations      | 2         |
|    time_elapsed    | 102       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.06e+03   |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 3           |
|    time_elapsed         | 152         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008261951 |
|    clip_fraction        | 0.0697      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | -1.06e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 3.96e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-7556.01 +/- 248.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -7.56e+03   |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015925366 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -1.07e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 6.44e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 1.25e+04    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.66e+03 |
| time/              |           |
|    fps             | 309       |
|    iterations      | 4         |
|    time_elapsed    | 211       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-7203.36 +/- 3932.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -7.2e+03    |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.013992916 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 8.23e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 1.55e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.5e+03 |
| time/              |          |
|    fps             | 301      |
|    iterations      | 5        |
|    time_elapsed    | 271      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.74e+03   |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 6           |
|    time_elapsed         | 321         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.011139084 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 6.65e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 9.58e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1313.51 +/- 448.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.31e+03   |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.010179549 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.98e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 4.51e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.48e+03 |
| time/              |           |
|    fps             | 300       |
|    iterations      | 7         |
|    time_elapsed    | 381       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1798.72 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.8e+03    |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.006593755 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.178       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.75e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 5.97e+03    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.94e+03 |
| time/              |           |
|    fps             | 297       |
|    iterations      | 8         |
|    time_elapsed    | 440       |
|    total_timesteps | 131072    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.39e+03   |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 9           |
|    time_elapsed         | 490         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.007435888 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 3.29e+03    |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-1559.10 +/- 294.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.009201612 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 208         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 387         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 10       |
|    time_elapsed    | 550      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-1679.13 +/- 240.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.68e+03   |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.013635322 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.4        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 163         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -617     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 11       |
|    time_elapsed    | 609      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -512        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 12          |
|    time_elapsed         | 659         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.012192103 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 86.5        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-1078.40 +/- 449.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.08e+03    |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0069376733 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0003       |
|    loss                 | 86.7         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00919     |
|    value_loss           | 852          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -501     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 13       |
|    time_elapsed    | 719      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-1075.71 +/- 452.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.011745663 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.1        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 194         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 14       |
|    time_elapsed    | 778      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -565        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 15          |
|    time_elapsed         | 828         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.009332393 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0003      |
|    loss                 | 361         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 400         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=-477.18 +/- 448.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -477        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.010129216 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -450     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 16       |
|    time_elapsed    | 888      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-238.92 +/- 294.14
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -239       |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.01731107 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.77      |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 214        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 174        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 17       |
|    time_elapsed    | 947      |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -413        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 18          |
|    time_elapsed         | 997         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.007904497 |
|    clip_fraction        | 0.0716      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.391       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 995         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-118.20 +/- 238.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.014436688 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 132         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 19       |
|    time_elapsed    | 1057     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-238.25 +/- 479.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.013511853 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 167         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -376     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 20       |
|    time_elapsed    | 1116     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -249        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 21          |
|    time_elapsed         | 1166        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.013996444 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.6        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 334         |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=0.16 +/- 2.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.165       |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.014956897 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 175         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00956    |
|    value_loss           | 231         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -230     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 22       |
|    time_elapsed    | 1226     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=1.61 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.015641153 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 180         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -229     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 23       |
|    time_elapsed    | 1285     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 24          |
|    time_elapsed         | 1335        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.015084486 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.8        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 274         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1.81 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.014040365 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | -0.382      |
|    learning_rate        | 0.0003      |
|    loss                 | 17.7        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 215         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 25       |
|    time_elapsed    | 1395     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-115.71 +/- 233.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -116       |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.01672656 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.69       |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 89         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 292      |
|    iterations      | 26       |
|    time_elapsed    | 1454     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -169        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 27          |
|    time_elapsed         | 1504        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.016332163 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=1.14 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.018163197 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 267         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 88.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 28       |
|    time_elapsed    | 1563     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.87 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.018783335 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.58        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 66          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 292      |
|    iterations      | 29       |
|    time_elapsed    | 1623     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -243        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 30          |
|    time_elapsed         | 1673        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.014173099 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | 258         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=1.88 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.008562871 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | -0.00621    |
|    learning_rate        | 0.0003      |
|    loss                 | 445         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 583         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -233     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 31       |
|    time_elapsed    | 1732     |
|    total_timesteps | 507904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -251       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 32         |
|    time_elapsed         | 1782       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.02161639 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.66      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.00703   |
|    value_loss           | 16.4       |
----------------------------------------
Eval num_timesteps=525000, episode_reward=-188.85 +/- 379.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -189        |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.014223188 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | -0.0798     |
|    learning_rate        | 0.0003      |
|    loss                 | 4.77        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 33       |
|    time_elapsed    | 1840     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-325.78 +/- 655.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -326       |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.01830875 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.56       |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.00589   |
|    value_loss           | 80.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 34       |
|    time_elapsed    | 1898     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -197        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 35          |
|    time_elapsed         | 1948        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.022330018 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.9        |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.00407     |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=-116.89 +/- 237.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.021154884 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 326         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 36       |
|    time_elapsed    | 2006     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-118.78 +/- 239.94
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -119       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.01477617 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.66      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 883        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00209   |
|    value_loss           | 1.54e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 37       |
|    time_elapsed    | 2064     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 38          |
|    time_elapsed         | 2113        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.021247983 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.83        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 4.11        |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=1.34 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.013090961 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 302         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 39       |
|    time_elapsed    | 2172     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1.17 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.020678869 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.9         |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 352         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 40       |
|    time_elapsed    | 2230     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -198        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 41          |
|    time_elapsed         | 2279        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.012975391 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.1        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 283         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=1.75 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.020324295 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 35.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 42       |
|    time_elapsed    | 2338     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=1.53 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.009852822 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | 296         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 285         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 43       |
|    time_elapsed    | 2396     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.7       |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 44          |
|    time_elapsed         | 2445        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.020001218 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.643       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=1.25 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.25       |
| time/                   |            |
|    total_timesteps      | 725000     |
| train/                  |            |
|    approx_kl            | 0.01873852 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 94.7       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.00512   |
|    value_loss           | 31.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 45       |
|    time_elapsed    | 2503     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1.34 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.018586144 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92e+03    |
|    n_updates            | 450         |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 2.36e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 46       |
|    time_elapsed    | 2562     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -154        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 47          |
|    time_elapsed         | 2611        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.020907823 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=1.88 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.88       |
| time/                   |            |
|    total_timesteps      | 775000     |
| train/                  |            |
|    approx_kl            | 0.01916271 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.1       |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00297   |
|    value_loss           | 66.4       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.1    |
| time/              |          |
|    fps             | 294      |
|    iterations      | 48       |
|    time_elapsed    | 2669     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=1.68 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.021964781 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.23        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 15.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.9    |
| time/              |          |
|    fps             | 294      |
|    iterations      | 49       |
|    time_elapsed    | 2727     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48         |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 50          |
|    time_elapsed         | 2776        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.015167416 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.91        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 52.7        |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=-12.90 +/- 30.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -12.9       |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.015837472 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | -0.0313     |
|    learning_rate        | 0.0003      |
|    loss                 | 4.51        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 72.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 51       |
|    time_elapsed    | 2835     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2.20 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.025264606 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | -0.211      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00195    |
|    value_loss           | 40          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.8    |
| time/              |          |
|    fps             | 294      |
|    iterations      | 52       |
|    time_elapsed    | 2893     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.6       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 53          |
|    time_elapsed         | 2942        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.017658636 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.8         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 24.4        |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=1.45 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.023230381 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 43.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 54       |
|    time_elapsed    | 3001     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=1.46 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.46       |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.02474896 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.6       |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0003     |
|    loss                 | 356        |
|    n_updates            | 540        |
|    policy_gradient_loss | 0.00304    |
|    value_loss           | 500        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 55       |
|    time_elapsed    | 3059     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -264        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 56          |
|    time_elapsed         | 3108        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.016971095 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 204         |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 335         |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=3.87 +/- 4.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.87        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.020146515 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.54        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 347         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -257     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 57       |
|    time_elapsed    | 3166     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1.37 +/- 0.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.37       |
| time/                   |            |
|    total_timesteps      | 950000     |
| train/                  |            |
|    approx_kl            | 0.02258699 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.526      |
|    n_updates            | 570        |
|    policy_gradient_loss | 0.00153    |
|    value_loss           | 295        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 58       |
|    time_elapsed    | 3225     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -118        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 59          |
|    time_elapsed         | 3274        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.016488517 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.8        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.000632   |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=2.25 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.017086662 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 491         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 452         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 60       |
|    time_elapsed    | 3332     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -137        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 61          |
|    time_elapsed         | 3381        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.010526587 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.6        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 414         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=1.56 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.015929949 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 193         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 62       |
|    time_elapsed    | 3440     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=1.05 +/- 0.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.05        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.014652031 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.4        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 96.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 63       |
|    time_elapsed    | 3498     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.5       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 64          |
|    time_elapsed         | 3547        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.018249797 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 63.7        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 76.1        |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=1.20 +/- 0.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 1050000    |
| train/                  |            |
|    approx_kl            | 0.01653931 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.24e+03   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.00222   |
|    value_loss           | 583        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -314     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 65       |
|    time_elapsed    | 3605     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=1.78 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.014853704 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 790         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000443   |
|    value_loss           | 2.93e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -384     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 66       |
|    time_elapsed    | 3663     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -408        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 67          |
|    time_elapsed         | 3713        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.021762008 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.252       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.000772   |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=1.36 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.023494354 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.42        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 525         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 68       |
|    time_elapsed    | 3771     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=2.21 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.020016985 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17e+03    |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 317         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -203     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 69       |
|    time_elapsed    | 3829     |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 70         |
|    time_elapsed         | 3880       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.02142644 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.0469     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.42       |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00267   |
|    value_loss           | 46.3       |
----------------------------------------
Eval num_timesteps=1150000, episode_reward=2.07 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.019407844 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.125       |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00773    |
|    value_loss           | 0.902       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 71       |
|    time_elapsed    | 3939     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=1.80 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.011768443 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 775         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 475         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 72       |
|    time_elapsed    | 3997     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69.7       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 73          |
|    time_elapsed         | 4046        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.018909704 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.33        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 19.3        |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=-1160.90 +/- 2326.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.16e+03   |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.010566268 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 35.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 74       |
|    time_elapsed    | 4105     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=2.07 +/- 0.71
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 1225000      |
| train/                  |              |
|    approx_kl            | 0.0154789025 |
|    clip_fraction        | 0.24         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.54        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.667        |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 9.13         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 75       |
|    time_elapsed    | 4163     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.7       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 76          |
|    time_elapsed         | 4213        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.013804032 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 9.3         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=1.46 +/- 0.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.46       |
| time/                   |            |
|    total_timesteps      | 1250000    |
| train/                  |            |
|    approx_kl            | 0.04647465 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 760        |
|    policy_gradient_loss | 0.00505    |
|    value_loss           | 902        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 77       |
|    time_elapsed    | 4271     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=2.63 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63        |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.008695329 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 228         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 1.36e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -265     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 78       |
|    time_elapsed    | 4329     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -272        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 79          |
|    time_elapsed         | 4378        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.016037762 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.3        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 79          |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=1.85 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.85         |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0132175535 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 126          |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00663     |
|    value_loss           | 81           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 80       |
|    time_elapsed    | 4437     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=1.75 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.016878381 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.311       |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 8.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.7    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 81       |
|    time_elapsed    | 4495     |
|    total_timesteps | 1327104  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -72.1        |
| time/                   |              |
|    fps                  | 295          |
|    iterations           | 82           |
|    time_elapsed         | 4544         |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0089062955 |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 388          |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 292          |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=2.51 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.51        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.014056639 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.7        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 42.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 83       |
|    time_elapsed    | 4603     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=1.72 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.023300398 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.000796    |
|    value_loss           | 3.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 84       |
|    time_elapsed    | 4661     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -124        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 85          |
|    time_elapsed         | 4711        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.022967942 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.85        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=1.76 +/- 0.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.005492675 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 467         |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 863         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 86       |
|    time_elapsed    | 4769     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=1.28 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.019265993 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.2        |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.000796    |
|    value_loss           | 264         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 87       |
|    time_elapsed    | 4827     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -141        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 88          |
|    time_elapsed         | 4877        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.025772262 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0947      |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 20          |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=1.41 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.007425218 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.3        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 89       |
|    time_elapsed    | 4935     |
|    total_timesteps | 1458176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -57.1      |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 90         |
|    time_elapsed         | 4984       |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 0.01538298 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.00154   |
|    value_loss           | 9.37       |
----------------------------------------
Eval num_timesteps=1475000, episode_reward=1.47 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.030958077 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.0061      |
|    value_loss           | 8.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 91       |
|    time_elapsed    | 5043     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1.06 +/- 0.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.06       |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.01220311 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 463        |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.00309   |
|    value_loss           | 189        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 92       |
|    time_elapsed    | 5101     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -68.5       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 93          |
|    time_elapsed         | 5150        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.017995492 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.897       |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 72.6        |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=1.40 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.014542771 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.54        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 86.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 94       |
|    time_elapsed    | 5209     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2.00 +/- 1.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.022689752 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 26.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 95       |
|    time_elapsed    | 5267     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -49.6       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 96          |
|    time_elapsed         | 5316        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.017316615 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.46        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 43.7        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=1.37 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.021554992 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.83        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.000431   |
|    value_loss           | 59          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 97       |
|    time_elapsed    | 5375     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1.18 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.016248973 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.23        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 84.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 98       |
|    time_elapsed    | 5433     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.5       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 99          |
|    time_elapsed         | 5482        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.014378792 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.96        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 40.4        |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=1.77 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.018561129 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 26.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 100      |
|    time_elapsed    | 5541     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=1.49 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.018587176 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.56        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 24.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 101      |
|    time_elapsed    | 5599     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.9       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 102         |
|    time_elapsed         | 5648        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.016746726 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 50.7        |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=1.88 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.020572947 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.335       |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 103      |
|    time_elapsed    | 5707     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1.16 +/- 0.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.16       |
| time/                   |            |
|    total_timesteps      | 1700000    |
| train/                  |            |
|    approx_kl            | 0.01922251 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.167      |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 2.04       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 104      |
|    time_elapsed    | 5765     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.1       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 105         |
|    time_elapsed         | 5814        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.016388677 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -8.81e-05   |
|    value_loss           | 34.4        |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=2.50 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.021260884 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0912      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 1.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 106      |
|    time_elapsed    | 5873     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2.07 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.029034127 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 88          |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.000805    |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 107      |
|    time_elapsed    | 5931     |
|    total_timesteps | 1753088  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -23.7     |
| time/                   |           |
|    fps                  | 295       |
|    iterations           | 108       |
|    time_elapsed         | 5981      |
|    total_timesteps      | 1769472   |
| train/                  |           |
|    approx_kl            | 0.0215532 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.49     |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0874    |
|    n_updates            | 1070      |
|    policy_gradient_loss | -0.00226  |
|    value_loss           | 0.396     |
---------------------------------------
Eval num_timesteps=1775000, episode_reward=2.06 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06        |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.024646979 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00255     |
|    value_loss           | 231         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 109      |
|    time_elapsed    | 6039     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2.46 +/- 0.94
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.46       |
| time/                   |            |
|    total_timesteps      | 1800000    |
| train/                  |            |
|    approx_kl            | 0.03029923 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.496      |
|    n_updates            | 1090       |
|    policy_gradient_loss | 0.00181    |
|    value_loss           | 167        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 110      |
|    time_elapsed    | 6097     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -147        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 111         |
|    time_elapsed         | 6146        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.019396152 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 2.55        |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=1.87 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.034512617 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.55        |
|    n_updates            | 1110        |
|    policy_gradient_loss | 0.00415     |
|    value_loss           | 115         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 112      |
|    time_elapsed    | 6205     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=1.63 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.028717674 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.271       |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00033    |
|    value_loss           | 41.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 113      |
|    time_elapsed    | 6263     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45.8       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 114         |
|    time_elapsed         | 6313        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.029839471 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -4.07e-05   |
|    value_loss           | 47.7        |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=3.84 +/- 1.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.84       |
| time/                   |            |
|    total_timesteps      | 1875000    |
| train/                  |            |
|    approx_kl            | 0.00847328 |
|    clip_fraction        | 0.0572     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.0003     |
|    loss                 | 25.3       |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.00494   |
|    value_loss           | 214        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 115      |
|    time_elapsed    | 6371     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=3.17 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.028306548 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.32        |
|    n_updates            | 1150        |
|    policy_gradient_loss | 0.00621     |
|    value_loss           | 134         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 116      |
|    time_elapsed    | 6429     |
|    total_timesteps | 1900544  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -142         |
| time/                   |              |
|    fps                  | 295          |
|    iterations           | 117          |
|    time_elapsed         | 6479         |
|    total_timesteps      | 1916928      |
| train/                  |              |
|    approx_kl            | 0.0070829866 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.0003       |
|    loss                 | 91.8         |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 435          |
------------------------------------------
Eval num_timesteps=1925000, episode_reward=2.96 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.019617485 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.15        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 40.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -96.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 118      |
|    time_elapsed    | 6537     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -102        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 119         |
|    time_elapsed         | 6586        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.013639271 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 89.9        |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=2.60 +/- 1.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.016978499 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.631       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 46.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 120      |
|    time_elapsed    | 6645     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=2.43 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.014418736 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.29        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 72.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 121      |
|    time_elapsed    | 6703     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -46.8       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 122         |
|    time_elapsed         | 6753        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.011662526 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 32          |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 84.5        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=3.96 +/- 1.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.017044201 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 19.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -69.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 123      |
|    time_elapsed    | 6811     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=3.93 +/- 2.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.93        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.012936126 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 90.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 124      |
|    time_elapsed    | 6869     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -42.5       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 125         |
|    time_elapsed         | 6919        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.019332077 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.54        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 32.8        |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=3.36 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.36        |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.020668201 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 14.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 126      |
|    time_elapsed    | 6977     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=4.90 +/- 2.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.016167903 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.5         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 3.47        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 127      |
|    time_elapsed    | 7036     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.86       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 128         |
|    time_elapsed         | 7085        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.019758692 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 1.87        |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=3.06 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.06        |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.012895215 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.07        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 28.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 129      |
|    time_elapsed    | 7143     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=1.99 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.026166908 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 79.5        |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.0046      |
|    value_loss           | 215         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 130      |
|    time_elapsed    | 7202     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -95         |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 131         |
|    time_elapsed         | 7251        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.016470887 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 50.5        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=2.59 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59        |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.015570315 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62        |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 40.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 132      |
|    time_elapsed    | 7309     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=3.15 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.15        |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.009489308 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 193         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 133      |
|    time_elapsed    | 7368     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -409        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 134         |
|    time_elapsed         | 7417        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.007837915 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 703         |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=4.43 +/- 2.32
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.43       |
| time/                   |            |
|    total_timesteps      | 2200000    |
| train/                  |            |
|    approx_kl            | 0.02114755 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | 7.64       |
|    n_updates            | 1340       |
|    policy_gradient_loss | 0.00219    |
|    value_loss           | 148        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 135      |
|    time_elapsed    | 7475     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=2.58 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.032020226 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 39          |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.00305     |
|    value_loss           | 111         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -359     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 136      |
|    time_elapsed    | 7534     |
|    total_timesteps | 2228224  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 137        |
|    time_elapsed         | 7583       |
|    total_timesteps      | 2244608    |
| train/                  |            |
|    approx_kl            | 0.01270186 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.000872  |
|    value_loss           | 278        |
----------------------------------------
Eval num_timesteps=2250000, episode_reward=3.15 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.15        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.017325655 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 60.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 138      |
|    time_elapsed    | 7641     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=2.90 +/- 2.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.025821118 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.4        |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.00153     |
|    value_loss           | 55.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 139      |
|    time_elapsed    | 7700     |
|    total_timesteps | 2277376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -70.7      |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 140        |
|    time_elapsed         | 7749       |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.03319705 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.51      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.4       |
|    n_updates            | 1390       |
|    policy_gradient_loss | 0.00189    |
|    value_loss           | 167        |
----------------------------------------
Eval num_timesteps=2300000, episode_reward=2.79 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.018243894 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.24        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 5.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 141      |
|    time_elapsed    | 7808     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=3.96 +/- 1.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.014928347 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.266       |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 8.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.7    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 142      |
|    time_elapsed    | 7866     |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 143         |
|    time_elapsed         | 7916        |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.017406536 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.8        |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00337     |
|    value_loss           | 243         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=3.81 +/- 1.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.81        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.031380296 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 398         |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 144      |
|    time_elapsed    | 7974     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=3.47 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.47        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.020743873 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 145      |
|    time_elapsed    | 8032     |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 146         |
|    time_elapsed         | 8082        |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.019916024 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 1450        |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 35.5        |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=2.80 +/- 0.59
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.8          |
| time/                   |              |
|    total_timesteps      | 2400000      |
| train/                  |              |
|    approx_kl            | 0.0105253495 |
|    clip_fraction        | 0.0858       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.91         |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00395     |
|    value_loss           | 153          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -55.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 147      |
|    time_elapsed    | 8140     |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 148         |
|    time_elapsed         | 8190        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.019601393 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 20.8        |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=2.32 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.016073432 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.619       |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 22.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 149      |
|    time_elapsed    | 8248     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=3.58 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.58        |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.015620656 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.7        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.000762   |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -326     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 150      |
|    time_elapsed    | 8306     |
|    total_timesteps | 2457600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -392         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 151          |
|    time_elapsed         | 8356         |
|    total_timesteps      | 2473984      |
| train/                  |              |
|    approx_kl            | 0.0062884456 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.52        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0003       |
|    loss                 | 43.5         |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00497     |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=2475000, episode_reward=3.62 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.62        |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.024886243 |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.4        |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.00658     |
|    value_loss           | 359         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 152      |
|    time_elapsed    | 8414     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2.20 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.015535924 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 18.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -308     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 153      |
|    time_elapsed    | 8473     |
|    total_timesteps | 2506752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 154        |
|    time_elapsed         | 8522       |
|    total_timesteps      | 2523136    |
| train/                  |            |
|    approx_kl            | 0.03411726 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 731        |
|    n_updates            | 1530       |
|    policy_gradient_loss | 0.00724    |
|    value_loss           | 506        |
----------------------------------------
Eval num_timesteps=2525000, episode_reward=2.25 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.014851777 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 11.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 155      |
|    time_elapsed    | 8580     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2.13 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.011777319 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.12        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 36.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 156      |
|    time_elapsed    | 8639     |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -150        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 157         |
|    time_elapsed         | 8688        |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.017168397 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.00562     |
|    value_loss           | 287         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=1.86 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.019349724 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 2.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 158      |
|    time_elapsed    | 8746     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2.25 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.018564736 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0699      |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00731    |
|    value_loss           | 0.379       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -232     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 159      |
|    time_elapsed    | 8805     |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 160         |
|    time_elapsed         | 8854        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.019803295 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.06        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 637         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=2.59 +/- 0.62
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.59         |
| time/                   |              |
|    total_timesteps      | 2625000      |
| train/                  |              |
|    approx_kl            | 0.0065151714 |
|    clip_fraction        | 0.0718       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.47        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.69         |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 85           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 161      |
|    time_elapsed    | 8913     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2.52 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.032992832 |
|    clip_fraction        | 0.406       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.9        |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.00762     |
|    value_loss           | 37          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 162      |
|    time_elapsed    | 8971     |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 163         |
|    time_elapsed         | 9021        |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.027488546 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.608       |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.00181     |
|    value_loss           | 4.37        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=2.52 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.024654455 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.788       |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.000973   |
|    value_loss           | 19.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 164      |
|    time_elapsed    | 9079     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2.21 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.016965378 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00019    |
|    value_loss           | 166         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 165      |
|    time_elapsed    | 9138     |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -40.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 166         |
|    time_elapsed         | 9187        |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.014949916 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0495      |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 13.6        |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=2.81 +/- 1.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.016841698 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.964       |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 5.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.8     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 167      |
|    time_elapsed    | 9245     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=1.79 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.019719228 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 168      |
|    time_elapsed    | 9304     |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 169         |
|    time_elapsed         | 9353        |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.018224955 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 17.9        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=1.97 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.016072363 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.817       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 18.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 170      |
|    time_elapsed    | 9412     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=1.47 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.023966562 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 1.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 171      |
|    time_elapsed    | 9470     |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 172         |
|    time_elapsed         | 9519        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.007642105 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=1.79 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.017492093 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 11.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 173      |
|    time_elapsed    | 9578     |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=1.39 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.39        |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.022234406 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.538       |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 7.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.8    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 174      |
|    time_elapsed    | 9636     |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -51         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 175         |
|    time_elapsed         | 9685        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.019842772 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.521       |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.000509    |
|    value_loss           | 159         |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=1.55 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.014036822 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 2.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.8    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 176      |
|    time_elapsed    | 9744     |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -25.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 177         |
|    time_elapsed         | 9793        |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.019275546 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 4.23        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=1.67 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.017378356 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.4        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 183         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 178      |
|    time_elapsed    | 9851     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=1.96 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.031549823 |
|    clip_fraction        | 0.426       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 1780        |
|    policy_gradient_loss | 0.0059      |
|    value_loss           | 79.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 179      |
|    time_elapsed    | 9910     |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -55.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 180         |
|    time_elapsed         | 9959        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.018711379 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.13        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 17.6        |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=2.68 +/- 1.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.018931195 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.389       |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 8.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 181      |
|    time_elapsed    | 10017    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=1.89 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.018947124 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.92        |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.004      |
|    value_loss           | 3.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.45    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 182      |
|    time_elapsed    | 10076    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.99       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 183         |
|    time_elapsed         | 10125       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.019857725 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0204      |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.528       |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=2.58 +/- 1.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.58         |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0087874215 |
|    clip_fraction        | 0.071        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.41        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.23         |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00448     |
|    value_loss           | 16.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 184      |
|    time_elapsed    | 10184    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=2.36 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.024132388 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.000577    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 185      |
|    time_elapsed    | 10242    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 186         |
|    time_elapsed         | 10291       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.020216312 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0767      |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00432    |
|    value_loss           | 14.8        |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=2.17 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.023151262 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.08        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 9.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 187      |
|    time_elapsed    | 10350    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=1.92 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.005895989 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 193         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 188      |
|    time_elapsed    | 10408    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -28.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 189         |
|    time_elapsed         | 10457       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.023039903 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0924      |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.00331     |
|    value_loss           | 16.5        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=1.82 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.016402764 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 8.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 190      |
|    time_elapsed    | 10516    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=2.38 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.033891007 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.00323     |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 191      |
|    time_elapsed    | 10574    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -46.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 192         |
|    time_elapsed         | 10624       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.017672703 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.371       |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 4.58        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=1.51 +/- 0.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.51       |
| time/                   |            |
|    total_timesteps      | 3150000    |
| train/                  |            |
|    approx_kl            | 0.01782735 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.603      |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.00695   |
|    value_loss           | 3.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 193      |
|    time_elapsed    | 10682    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=1.28 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.022505905 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.46        |
|    n_updates            | 1930        |
|    policy_gradient_loss | 0.00237     |
|    value_loss           | 59.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 194      |
|    time_elapsed    | 10741    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 195         |
|    time_elapsed         | 10790       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.019758148 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0686      |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 0.258       |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=0.99 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.986       |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.022552893 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | -0.0642     |
|    learning_rate        | 0.0003      |
|    loss                 | 74.6        |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.000289   |
|    value_loss           | 24          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 196      |
|    time_elapsed    | 10848    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=1.54 +/- 0.55
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.54         |
| time/                   |              |
|    total_timesteps      | 3225000      |
| train/                  |              |
|    approx_kl            | 0.0126774525 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0.738        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.9         |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 371          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 197      |
|    time_elapsed    | 10907    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 198         |
|    time_elapsed         | 10956       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.030381374 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 1970        |
|    policy_gradient_loss | 0.00347     |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=2.17 +/- 1.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.017688131 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 2.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 199      |
|    time_elapsed    | 11015    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=2.07 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.018915048 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 1.25        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 200      |
|    time_elapsed    | 11073    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.71       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 201         |
|    time_elapsed         | 11123       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.019128151 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0275      |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 0.571       |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=1.27 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.27         |
| time/                   |              |
|    total_timesteps      | 3300000      |
| train/                  |              |
|    approx_kl            | 0.0104559995 |
|    clip_fraction        | 0.0689       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.38        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.826        |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 22.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.09    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 202      |
|    time_elapsed    | 11181    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=1.74 +/- 0.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.015899912 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 0.834       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 203      |
|    time_elapsed    | 11240    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 204         |
|    time_elapsed         | 11289       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.014636952 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.8         |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=1.71 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.016140915 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.902       |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00634    |
|    value_loss           | 10.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.7    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 205      |
|    time_elapsed    | 11347    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=1.99 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.020327948 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | 89.3        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 206      |
|    time_elapsed    | 11406    |
|    total_timesteps | 3375104  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -57.1      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 207        |
|    time_elapsed         | 11455      |
|    total_timesteps      | 3391488    |
| train/                  |            |
|    approx_kl            | 0.01924111 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.444      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.3        |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0034    |
|    value_loss           | 65.9       |
----------------------------------------
Eval num_timesteps=3400000, episode_reward=3.17 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.012829155 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.76        |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 162         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.7    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 208      |
|    time_elapsed    | 11513    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 209         |
|    time_elapsed         | 11563       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.017944865 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 82.2        |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=1.52 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.011965189 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.17        |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 97.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 210      |
|    time_elapsed    | 11621    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=1.44 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.015568506 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.5        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 31.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 211      |
|    time_elapsed    | 11680    |
|    total_timesteps | 3457024  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -73.3      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 212        |
|    time_elapsed         | 11729      |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.01978191 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.2       |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 41.7       |
----------------------------------------
Eval num_timesteps=3475000, episode_reward=1.82 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.019801516 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.221       |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 1.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 213      |
|    time_elapsed    | 11788    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=1.18 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.18       |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.01877921 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.6       |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.00738   |
|    value_loss           | 10         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 214      |
|    time_elapsed    | 11846    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 215         |
|    time_elapsed         | 11895       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.014818631 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.403       |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 12.9        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=1.07 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.019943172 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.18        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 50.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 216      |
|    time_elapsed    | 11954    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=1.13 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13        |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.019395994 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0174      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 1.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 217      |
|    time_elapsed    | 12012    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.65       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 218         |
|    time_elapsed         | 12062       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.017278748 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.882       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=1.45 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.45       |
| time/                   |            |
|    total_timesteps      | 3575000    |
| train/                  |            |
|    approx_kl            | 0.01512684 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0166     |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.00495   |
|    value_loss           | 0.922      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.927   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 219      |
|    time_elapsed    | 12120    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=1.12 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.12        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.016425602 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.26        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 1.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.881   |
| time/              |          |
|    fps             | 295      |
|    iterations      | 220      |
|    time_elapsed    | 12179    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 221         |
|    time_elapsed         | 12228       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.022864562 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.9         |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00071     |
|    value_loss           | 72.8        |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=1.09 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.09        |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.021462899 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.364       |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 11.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 222      |
|    time_elapsed    | 12286    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=2.08 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.019886682 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0372      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 2.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 223      |
|    time_elapsed    | 12345    |
|    total_timesteps | 3653632  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -2.06        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 224          |
|    time_elapsed         | 12394        |
|    total_timesteps      | 3670016      |
| train/                  |              |
|    approx_kl            | 0.0154786175 |
|    clip_fraction        | 0.218        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | 0.853        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0579       |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00557     |
|    value_loss           | 0.0924       |
------------------------------------------
Eval num_timesteps=3675000, episode_reward=1.69 +/- 0.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 3675000    |
| train/                  |            |
|    approx_kl            | 0.01664293 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.007      |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0084    |
|    value_loss           | 0.213      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 225      |
|    time_elapsed    | 12453    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=1.16 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.007076215 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.0345      |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 49.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 226      |
|    time_elapsed    | 12511    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 227         |
|    time_elapsed         | 12560       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.017466504 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | -0.285      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.94        |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 43.3        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=1.77 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.014751639 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 350         |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.000614    |
|    value_loss           | 287         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 228      |
|    time_elapsed    | 12619    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=1.24 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.018599052 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.65        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 383         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 229      |
|    time_elapsed    | 12677    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 230         |
|    time_elapsed         | 12727       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.020042393 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.38        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=1.47 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.017397325 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.53        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 11.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.17    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 231      |
|    time_elapsed    | 12785    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=1.41 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.011442191 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 87          |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 17.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 232      |
|    time_elapsed    | 12844    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -25.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 233         |
|    time_elapsed         | 12893       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.014432009 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.88        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 73          |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=1.37 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.016827758 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.11        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 2.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 234      |
|    time_elapsed    | 12952    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=1.53 +/- 0.82
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.53       |
| time/                   |            |
|    total_timesteps      | 3850000    |
| train/                  |            |
|    approx_kl            | 0.02003313 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0126     |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.00682   |
|    value_loss           | 3.96       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 235      |
|    time_elapsed    | 13010    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.73       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 236         |
|    time_elapsed         | 13059       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.015460188 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.8         |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=1.36 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.018091477 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.307       |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 2.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.03    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 237      |
|    time_elapsed    | 13118    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.18       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 238         |
|    time_elapsed         | 13167       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.018829491 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.083       |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 0.846       |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=1.68 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.018375307 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0836      |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 2.7         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.71    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 239      |
|    time_elapsed    | 13226    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=1.19 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.019203508 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.263       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00295    |
|    value_loss           | 2.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.86    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 240      |
|    time_elapsed    | 13284    |
|    total_timesteps | 3932160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -1.49      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 241        |
|    time_elapsed         | 13333      |
|    total_timesteps      | 3948544    |
| train/                  |            |
|    approx_kl            | 0.02281719 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0141     |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 0.503      |
----------------------------------------
Eval num_timesteps=3950000, episode_reward=1.50 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.019693773 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0182      |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.494       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 242      |
|    time_elapsed    | 13392    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=1.35 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.35         |
| time/                   |              |
|    total_timesteps      | 3975000      |
| train/                  |              |
|    approx_kl            | 0.0065315077 |
|    clip_fraction        | 0.0543       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.62         |
|    learning_rate        | 0.0003       |
|    loss                 | 2.09         |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00489     |
|    value_loss           | 67.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 243      |
|    time_elapsed    | 13450    |
|    total_timesteps | 3981312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 244        |
|    time_elapsed         | 13500      |
|    total_timesteps      | 3997696    |
| train/                  |            |
|    approx_kl            | 0.01868607 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000649   |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.00558   |
|    value_loss           | 0.228      |
----------------------------------------
Eval num_timesteps=4000000, episode_reward=2.66 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.018295504 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.34        |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 245      |
|    time_elapsed    | 13558    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=1.86 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.019450895 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 5.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.83    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 246      |
|    time_elapsed    | 13617    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.93       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 247         |
|    time_elapsed         | 13666       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.016274482 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0994      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 0.379       |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=2.45 +/- 1.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45        |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.019755483 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0701      |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 0.949       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.56    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 248      |
|    time_elapsed    | 13725    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=1.63 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.017311037 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 12.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 249      |
|    time_elapsed    | 13783    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -149        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 250         |
|    time_elapsed         | 13832       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.014935872 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.23        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 193         |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=2.73 +/- 0.71
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.73      |
| time/                   |           |
|    total_timesteps      | 4100000   |
| train/                  |           |
|    approx_kl            | 0.0318829 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.18     |
|    explained_variance   | 0.0479    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.68      |
|    n_updates            | 2500      |
|    policy_gradient_loss | 0.00503   |
|    value_loss           | 210       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 251      |
|    time_elapsed    | 13891    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=2.50 +/- 1.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.021461392 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0252      |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 0.661       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 252      |
|    time_elapsed    | 13949    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -102        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 253         |
|    time_elapsed         | 13998       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.019125983 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 4.11        |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=3.05 +/- 2.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.014622286 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.547       |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 38.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 254      |
|    time_elapsed    | 14057    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2.79 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.042266533 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.919       |
|    n_updates            | 2540        |
|    policy_gradient_loss | 0.00673     |
|    value_loss           | 427         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 255      |
|    time_elapsed    | 14115    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 256         |
|    time_elapsed         | 14165       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.021373842 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0155      |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 3.48        |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=2.11 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.020164445 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0324      |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.556       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 257      |
|    time_elapsed    | 14223    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2.72 +/- 1.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.72       |
| time/                   |            |
|    total_timesteps      | 4225000    |
| train/                  |            |
|    approx_kl            | 0.01724001 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.0784     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.519      |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.00741   |
|    value_loss           | 7.92       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 258      |
|    time_elapsed    | 14282    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 259         |
|    time_elapsed         | 14331       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.030715272 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.5        |
|    n_updates            | 2580        |
|    policy_gradient_loss | 0.000414    |
|    value_loss           | 389         |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=3.65 +/- 1.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.65        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.013442183 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 185         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 260      |
|    time_elapsed    | 14389    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=2.68 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.017759541 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 2.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 261      |
|    time_elapsed    | 14448    |
|    total_timesteps | 4276224  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -178         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 262          |
|    time_elapsed         | 14497        |
|    total_timesteps      | 4292608      |
| train/                  |              |
|    approx_kl            | 0.0051143896 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.0003       |
|    loss                 | 180          |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 748          |
------------------------------------------
Eval num_timesteps=4300000, episode_reward=4.35 +/- 2.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.35        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.017134503 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.9        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 29.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 263      |
|    time_elapsed    | 14555    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=4.24 +/- 2.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.24        |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.015390742 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.4        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 65.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 264      |
|    time_elapsed    | 14614    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -96.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 265         |
|    time_elapsed         | 14663       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.015727168 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 4.05        |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=3.26 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.26        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.015033182 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.81        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 266      |
|    time_elapsed    | 14722    |
|    total_timesteps | 4358144  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -235       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 267        |
|    time_elapsed         | 14771      |
|    total_timesteps      | 4374528    |
| train/                  |            |
|    approx_kl            | 0.01598211 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 190        |
|    n_updates            | 2660       |
|    policy_gradient_loss | 0.000148   |
|    value_loss           | 233        |
----------------------------------------
Eval num_timesteps=4375000, episode_reward=3.98 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.98        |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.017293489 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 16          |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 62.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -288     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 268      |
|    time_elapsed    | 14829    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=3.91 +/- 1.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.91        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.025096316 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 2680        |
|    policy_gradient_loss | 0.000587    |
|    value_loss           | 248         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 269      |
|    time_elapsed    | 14888    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 270         |
|    time_elapsed         | 14937       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.016756084 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.585       |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 9.86        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=3.61 +/- 1.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.61        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.021607704 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.000939   |
|    value_loss           | 48.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 271      |
|    time_elapsed    | 14995    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=4.39 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.39        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.021310506 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.721       |
|    n_updates            | 2710        |
|    policy_gradient_loss | 0.000301    |
|    value_loss           | 47.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 272      |
|    time_elapsed    | 15054    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -36.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 273         |
|    time_elapsed         | 15103       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.020702256 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 73.7        |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=2.29 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.018827293 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 21.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 274      |
|    time_elapsed    | 15161    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=2.06 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.004630316 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.5        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 275      |
|    time_elapsed    | 15220    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 276         |
|    time_elapsed         | 15269       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.013290141 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 463         |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 334         |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=1.72 +/- 1.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 4525000    |
| train/                  |            |
|    approx_kl            | 0.02061427 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.636      |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.00556   |
|    value_loss           | 7.53       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 277      |
|    time_elapsed    | 15328    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=1.69 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.69        |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.013177288 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.76        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00723    |
|    value_loss           | 39.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -80.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 278      |
|    time_elapsed    | 15386    |
|    total_timesteps | 4554752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -18.1      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 279        |
|    time_elapsed         | 15435      |
|    total_timesteps      | 4571136    |
| train/                  |            |
|    approx_kl            | 0.01816662 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0744     |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.00597   |
|    value_loss           | 1.17       |
----------------------------------------
Eval num_timesteps=4575000, episode_reward=2.21 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.015295986 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.14        |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 12.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 280      |
|    time_elapsed    | 15494    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=1.62 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.020914419 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 5.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.03    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 281      |
|    time_elapsed    | 15552    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 282         |
|    time_elapsed         | 15602       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.018179202 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.618       |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00566    |
|    value_loss           | 2.3         |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=2.41 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41        |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.016271094 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.01        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 6.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 283      |
|    time_elapsed    | 15660    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2.78 +/- 1.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.017923906 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 183         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 284      |
|    time_elapsed    | 15718    |
|    total_timesteps | 4653056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -41.3      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 285        |
|    time_elapsed         | 15768      |
|    total_timesteps      | 4669440    |
| train/                  |            |
|    approx_kl            | 0.02019693 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.28      |
|    explained_variance   | 0.423      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.86       |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.00858   |
|    value_loss           | 37.5       |
----------------------------------------
Eval num_timesteps=4675000, episode_reward=1.29 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.021814505 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 5.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 286      |
|    time_elapsed    | 15826    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=1.77 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.015241146 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.9        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 4.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.6     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 287      |
|    time_elapsed    | 15885    |
|    total_timesteps | 4702208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.51      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 288        |
|    time_elapsed         | 15934      |
|    total_timesteps      | 4718592    |
| train/                  |            |
|    approx_kl            | 0.01764746 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0821     |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.00112   |
|    value_loss           | 3.71       |
----------------------------------------
Eval num_timesteps=4725000, episode_reward=1.83 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.019725136 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.617       |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 1.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.64    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 289      |
|    time_elapsed    | 15992    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=1.99 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.019372292 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 14.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.68    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 290      |
|    time_elapsed    | 16051    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.61       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 291         |
|    time_elapsed         | 16100       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.018222742 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0146      |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 0.58        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=1.34 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.023431096 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 4.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.91    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 292      |
|    time_elapsed    | 16159    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=1.42 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.022390377 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 5.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 293      |
|    time_elapsed    | 16217    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.87       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 294         |
|    time_elapsed         | 16267       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.017830024 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.404       |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00939    |
|    value_loss           | 45.7        |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=2.00 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.018976675 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.659       |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 4.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.47    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 295      |
|    time_elapsed    | 16325    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2          |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 296         |
|    time_elapsed         | 16374       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.018915325 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0754      |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=2.00 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.018326566 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 3.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.78    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 297      |
|    time_elapsed    | 16433    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=1.93 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.023610562 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.0386      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.000756   |
|    value_loss           | 46          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 298      |
|    time_elapsed    | 16491    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -42.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 299         |
|    time_elapsed         | 16540       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.019825086 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | -0.00932    |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 48.9        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=1.29 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.012208203 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.7        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 300      |
|    time_elapsed    | 16599    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=4.59 +/- 4.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.59       |
| time/                   |            |
|    total_timesteps      | 4925000    |
| train/                  |            |
|    approx_kl            | 0.01890239 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.89      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.954      |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.00556   |
|    value_loss           | 5.59       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 301      |
|    time_elapsed    | 16657    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -31.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 302         |
|    time_elapsed         | 16707       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.018988421 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.362       |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 2.14        |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=2.15 +/- 0.94
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.15       |
| time/                   |            |
|    total_timesteps      | 4950000    |
| train/                  |            |
|    approx_kl            | 0.01659115 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.00777   |
|    value_loss           | 1.85       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.28    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 303      |
|    time_elapsed    | 16765    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=1.61 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.017531842 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 17.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.47    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 304      |
|    time_elapsed    | 16824    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.65       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 305         |
|    time_elapsed         | 16873       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.022846673 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0743      |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 7.32        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=3.63 +/- 3.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.63       |
| time/                   |            |
|    total_timesteps      | 5000000    |
| train/                  |            |
|    approx_kl            | 0.01804768 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.00543   |
|    value_loss           | 1.29       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.51    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 306      |
|    time_elapsed    | 16931    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=1.27 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.018488705 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.051       |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 0.438       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.55    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 307      |
|    time_elapsed    | 16990    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.23       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 308         |
|    time_elapsed         | 17039       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.021008246 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0356      |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 14.6        |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=2.40 +/- 1.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.4        |
| time/                   |            |
|    total_timesteps      | 5050000    |
| train/                  |            |
|    approx_kl            | 0.01683525 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.283      |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.00454   |
|    value_loss           | 2.88       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 309      |
|    time_elapsed    | 17098    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=1.72 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.013575302 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 36.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.52    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 310      |
|    time_elapsed    | 17156    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.02       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 311         |
|    time_elapsed         | 17205       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.021208826 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00859    |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 0.622       |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=2.94 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.017956235 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0617      |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 3.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.86    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 312      |
|    time_elapsed    | 17264    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=2.45 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.017961476 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0189      |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.155       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 313      |
|    time_elapsed    | 17322    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 314         |
|    time_elapsed         | 17371       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.008049237 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.4        |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 64.5        |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=3.52 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.52        |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.024283126 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.399       |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.892       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 315      |
|    time_elapsed    | 17430    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=3.03 +/- 1.93
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.03       |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.01918342 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0147     |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.00382   |
|    value_loss           | 0.642      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 316      |
|    time_elapsed    | 17488    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.18        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 317         |
|    time_elapsed         | 17538       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.017531477 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.0333      |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=4.26 +/- 2.86
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.26       |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.01599978 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0206     |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.00451   |
|    value_loss           | 0.119      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 318      |
|    time_elapsed    | 17596    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=2.15 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.15        |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.017218277 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 1.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 319      |
|    time_elapsed    | 17655    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -84.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 320         |
|    time_elapsed         | 17704       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.018537156 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0011     |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 0.0258      |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=3.07 +/- 1.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.07         |
| time/                   |              |
|    total_timesteps      | 5250000      |
| train/                  |              |
|    approx_kl            | 0.0037816186 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.0003       |
|    loss                 | 44.7         |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 214          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 321      |
|    time_elapsed    | 17762    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=3.44 +/- 2.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.44       |
| time/                   |            |
|    total_timesteps      | 5275000    |
| train/                  |            |
|    approx_kl            | 0.01955974 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.78      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0124     |
|    n_updates            | 3210       |
|    policy_gradient_loss | -0.00706   |
|    value_loss           | 0.0679     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -245     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 322      |
|    time_elapsed    | 17821    |
|    total_timesteps | 5275648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 323        |
|    time_elapsed         | 17870      |
|    total_timesteps      | 5292032    |
| train/                  |            |
|    approx_kl            | 0.00697189 |
|    clip_fraction        | 0.0378     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | -0.773     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.000863  |
|    value_loss           | 186        |
----------------------------------------
Eval num_timesteps=5300000, episode_reward=2.33 +/- 1.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.018072248 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.434       |
|    n_updates            | 3230        |
|    policy_gradient_loss | 0.00486     |
|    value_loss           | 3.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 324      |
|    time_elapsed    | 17929    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -174        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 325         |
|    time_elapsed         | 17978       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.038038574 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.534       |
|    n_updates            | 3240        |
|    policy_gradient_loss | 0.00297     |
|    value_loss           | 85.5        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=3.09 +/- 1.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.020161152 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.064       |
|    n_updates            | 3250        |
|    policy_gradient_loss | 0.00571     |
|    value_loss           | 33.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 326      |
|    time_elapsed    | 18036    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2.84 +/- 1.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.84        |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.026187187 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 3260        |
|    policy_gradient_loss | 0.00612     |
|    value_loss           | 161         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 327      |
|    time_elapsed    | 18095    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 328         |
|    time_elapsed         | 18144       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.026419293 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00359     |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 0.248       |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=2.89 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.009084323 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0811      |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 1.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 329      |
|    time_elapsed    | 18202    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=1.98 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.98       |
| time/                   |            |
|    total_timesteps      | 5400000    |
| train/                  |            |
|    approx_kl            | 0.03548097 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0308     |
|    n_updates            | 3290       |
|    policy_gradient_loss | 0.00634    |
|    value_loss           | 4.49       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.3     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 330      |
|    time_elapsed    | 18261    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.38       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 331         |
|    time_elapsed         | 18310       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.015626261 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 25.6        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=5.11 +/- 2.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.11        |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.019514132 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 0.866       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.83    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 332      |
|    time_elapsed    | 18369    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2.75 +/- 1.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75        |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.017930582 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0763      |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 0.728       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 333      |
|    time_elapsed    | 18428    |
|    total_timesteps | 5455872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -14.5      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 334        |
|    time_elapsed         | 18477      |
|    total_timesteps      | 5472256    |
| train/                  |            |
|    approx_kl            | 0.01390896 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.87      |
|    explained_variance   | 0.227      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.268      |
|    n_updates            | 3330       |
|    policy_gradient_loss | -0.00253   |
|    value_loss           | 55.5       |
----------------------------------------
Eval num_timesteps=5475000, episode_reward=3.46 +/- 1.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.46       |
| time/                   |            |
|    total_timesteps      | 5475000    |
| train/                  |            |
|    approx_kl            | 0.01692047 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0575     |
|    n_updates            | 3340       |
|    policy_gradient_loss | -0.000719  |
|    value_loss           | 6.3        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 335      |
|    time_elapsed    | 18536    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=2.05 +/- 0.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.05       |
| time/                   |            |
|    total_timesteps      | 5500000    |
| train/                  |            |
|    approx_kl            | 0.02496569 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 22.9       |
|    n_updates            | 3350       |
|    policy_gradient_loss | 0.00254    |
|    value_loss           | 17.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 336      |
|    time_elapsed    | 18594    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 337         |
|    time_elapsed         | 18643       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.022663608 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 61          |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 63.5        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=1.79 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.79       |
| time/                   |            |
|    total_timesteps      | 5525000    |
| train/                  |            |
|    approx_kl            | 0.02022599 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 3370       |
|    policy_gradient_loss | -0.00498   |
|    value_loss           | 1.78       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 338      |
|    time_elapsed    | 18702    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=3.77 +/- 1.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.77        |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.019514838 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 1.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 339      |
|    time_elapsed    | 18760    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.045      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 340         |
|    time_elapsed         | 18810       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.015353206 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 2.25        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=2.88 +/- 1.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.020835899 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00206    |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.587       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.446    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 341      |
|    time_elapsed    | 18868    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=2.10 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.016435668 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0921      |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 4.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.255    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 342      |
|    time_elapsed    | 18926    |
|    total_timesteps | 5603328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 0.153      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 343        |
|    time_elapsed         | 18976      |
|    total_timesteps      | 5619712    |
| train/                  |            |
|    approx_kl            | 0.01787219 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0635     |
|    n_updates            | 3420       |
|    policy_gradient_loss | 0.000108   |
|    value_loss           | 0.242      |
----------------------------------------
Eval num_timesteps=5625000, episode_reward=2.77 +/- 1.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.018545683 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 0.0575      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.577    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 344      |
|    time_elapsed    | 19034    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=1.61 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.014370974 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0402      |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 0.642       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 345      |
|    time_elapsed    | 19093    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.16        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 346         |
|    time_elapsed         | 19142       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.018038021 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 0.0675      |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=1.74 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.011825452 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0938      |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 0.394       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 347      |
|    time_elapsed    | 19201    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=3.03 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.03        |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.019196874 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0173      |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.0292      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.362    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 348      |
|    time_elapsed    | 19259    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.61       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 349         |
|    time_elapsed         | 19308       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.016561607 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.0775      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0708      |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 7.5         |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=1.55 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.006156803 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 13.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.57    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 350      |
|    time_elapsed    | 19367    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2.19 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.021477427 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 6.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 351      |
|    time_elapsed    | 19425    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.03       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 352         |
|    time_elapsed         | 19475       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.012976864 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.25        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 32.3        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=2.77 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.020091243 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0263      |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 0.685       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.38    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 353      |
|    time_elapsed    | 19533    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.31       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 354         |
|    time_elapsed         | 19583       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.018798862 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 9.97        |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=1.81 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.017805628 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0661      |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 2.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 355      |
|    time_elapsed    | 19641    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=2.11 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.018116347 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0793      |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 1           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.6     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 356      |
|    time_elapsed    | 19700    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.796      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 357         |
|    time_elapsed         | 19749       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.017554522 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.5         |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 1.01        |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=1.24 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.021399837 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 0.143       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.279   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 358      |
|    time_elapsed    | 19807    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=1.34 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.018427184 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0522      |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 0.321       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.07    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 359      |
|    time_elapsed    | 19866    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.19       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 360         |
|    time_elapsed         | 19915       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.017704226 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.269       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 2.16        |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=1.83 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.019110113 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.37        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 0.283       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.09    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 361      |
|    time_elapsed    | 19974    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=3.22 +/- 1.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.018195042 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 7.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.7     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 362      |
|    time_elapsed    | 20032    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.3        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 363         |
|    time_elapsed         | 20082       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.017032629 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.1        |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 14.7        |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=1.50 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.018041866 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 1.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 364      |
|    time_elapsed    | 20140    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=1.68 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.019183137 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0484      |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 0.547       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.213   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 365      |
|    time_elapsed    | 20199    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.0748     |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 366         |
|    time_elapsed         | 20248       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.015025632 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.739       |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=3.09 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.021192215 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0229      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 1.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.628   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 367      |
|    time_elapsed    | 20306    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=2.03 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.018585889 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.021       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 7.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.196   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 368      |
|    time_elapsed    | 20365    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.343      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 369         |
|    time_elapsed         | 20414       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.025728907 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0686      |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 0.0736      |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=3.76 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.76        |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.021105083 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000982   |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.66    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 370      |
|    time_elapsed    | 20473    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=2.83 +/- 1.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.019286666 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.4        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 9.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.79    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 371      |
|    time_elapsed    | 20531    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.82       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 372         |
|    time_elapsed         | 20581       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.023934513 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00283    |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.55        |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=3.79 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.79        |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.018402098 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.011       |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 0.291       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 373      |
|    time_elapsed    | 20639    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=3.36 +/- 1.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.36        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.009617519 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.0259      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 66.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 374      |
|    time_elapsed    | 20698    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 375         |
|    time_elapsed         | 20747       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.022777434 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0006      |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 2.88        |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=1.97 +/- 1.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.019631658 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0331      |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00417    |
|    value_loss           | 12.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 376      |
|    time_elapsed    | 20805    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=5.66 +/- 4.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.66        |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.010820735 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.84        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 26.5        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.25    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 377      |
|    time_elapsed    | 20864    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.73       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 378         |
|    time_elapsed         | 20913       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.018591845 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.512       |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 2.87        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=1.85 +/- 0.85
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.85       |
| time/                   |            |
|    total_timesteps      | 6200000    |
| train/                  |            |
|    approx_kl            | 0.01790512 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.96      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0924     |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.00669   |
|    value_loss           | 6.19       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 379      |
|    time_elapsed    | 20972    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2.77 +/- 2.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.021146603 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.055       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 26.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.14    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 380      |
|    time_elapsed    | 21030    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.66       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 381         |
|    time_elapsed         | 21080       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.022221643 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00687    |
|    value_loss           | 0.0955      |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=2.15 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.15        |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.016507821 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0249      |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 4.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.46    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 382      |
|    time_elapsed    | 21138    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=3.17 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.018088568 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00463    |
|    value_loss           | 5.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.858   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 383      |
|    time_elapsed    | 21197    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 384         |
|    time_elapsed         | 21246       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.022000853 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.38        |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.000704   |
|    value_loss           | 1.43        |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=1.94 +/- 0.76
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.94       |
| time/                   |            |
|    total_timesteps      | 6300000    |
| train/                  |            |
|    approx_kl            | 0.02412094 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 133        |
|    n_updates            | 3840       |
|    policy_gradient_loss | 0.000548   |
|    value_loss           | 148        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 385      |
|    time_elapsed    | 21305    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -50.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 386         |
|    time_elapsed         | 21354       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.012394322 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.44        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 70.4        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=4.05 +/- 1.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.05        |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.021673841 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 387      |
|    time_elapsed    | 21413    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=2.17 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.020597206 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 7.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 388      |
|    time_elapsed    | 21471    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -68.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 389         |
|    time_elapsed         | 21521       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.019169828 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0514      |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 5.07        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=2.31 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.020992814 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 3890        |
|    policy_gradient_loss | 0.00372     |
|    value_loss           | 99.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -69.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 390      |
|    time_elapsed    | 21579    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=1.35 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.35        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.021909976 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 11.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 391      |
|    time_elapsed    | 21638    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -200        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 392         |
|    time_elapsed         | 21687       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.009862285 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.0657      |
|    learning_rate        | 0.0003      |
|    loss                 | 65.9        |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=1.76 +/- 1.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.76         |
| time/                   |              |
|    total_timesteps      | 6425000      |
| train/                  |              |
|    approx_kl            | 0.0069812546 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.74        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.6         |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.00379     |
|    value_loss           | 586          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -199     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 393      |
|    time_elapsed    | 21746    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=2.67 +/- 2.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67        |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.018462721 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 0.929       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -285     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 394      |
|    time_elapsed    | 21804    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -261        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 395         |
|    time_elapsed         | 21854       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.017096946 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    n_updates            | 3940        |
|    policy_gradient_loss | 0.000984    |
|    value_loss           | 106         |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=2.53 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.013788862 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 211         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 396      |
|    time_elapsed    | 21912    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2.46 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.019490816 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.927       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00295    |
|    value_loss           | 7.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 397      |
|    time_elapsed    | 21971    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 398         |
|    time_elapsed         | 22020       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.021612827 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.3         |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 8.76        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=3.34 +/- 1.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.022880659 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0457      |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 2.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 399      |
|    time_elapsed    | 22079    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=1.91 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.019712359 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.000518   |
|    value_loss           | 118         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 400      |
|    time_elapsed    | 22137    |
|    total_timesteps | 6553600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -9.37      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 401        |
|    time_elapsed         | 22186      |
|    total_timesteps      | 6569984    |
| train/                  |            |
|    approx_kl            | 0.02165877 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0826     |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 1.28       |
----------------------------------------
Eval num_timesteps=6575000, episode_reward=2.00 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.019324005 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0894      |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 1.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 402      |
|    time_elapsed    | 22246    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2.23 +/- 1.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.23         |
| time/                   |              |
|    total_timesteps      | 6600000      |
| train/                  |              |
|    approx_kl            | 0.0105595365 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.0003       |
|    loss                 | 472          |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 360          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 403      |
|    time_elapsed    | 22304    |
|    total_timesteps | 6602752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -359       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 404        |
|    time_elapsed         | 22353      |
|    total_timesteps      | 6619136    |
| train/                  |            |
|    approx_kl            | 0.02306955 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.67      |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.451      |
|    n_updates            | 4030       |
|    policy_gradient_loss | 0.000192   |
|    value_loss           | 393        |
----------------------------------------
Eval num_timesteps=6625000, episode_reward=1.48 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.011310424 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 126         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -367     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 405      |
|    time_elapsed    | 22412    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=1.83 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.018182207 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.87        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00511    |
|    value_loss           | 77.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 406      |
|    time_elapsed    | 22470    |
|    total_timesteps | 6651904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -10.7      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 407        |
|    time_elapsed         | 22520      |
|    total_timesteps      | 6668288    |
| train/                  |            |
|    approx_kl            | 0.02267455 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.78      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 4060       |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 0.745      |
----------------------------------------
Eval num_timesteps=6675000, episode_reward=2.04 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.013916525 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.594       |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 8.85        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.57    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 408      |
|    time_elapsed    | 22578    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=3.77 +/- 5.48
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3.77      |
| time/                   |           |
|    total_timesteps      | 6700000   |
| train/                  |           |
|    approx_kl            | 0.0223023 |
|    clip_fraction        | 0.283     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.82     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0677    |
|    n_updates            | 4080      |
|    policy_gradient_loss | -0.00312  |
|    value_loss           | 0.407     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.26    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 409      |
|    time_elapsed    | 22637    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.89       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 410         |
|    time_elapsed         | 22686       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.033196133 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.651       |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00255     |
|    value_loss           | 3.84        |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=1.42 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.023337923 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0325      |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00588    |
|    value_loss           | 3.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.06    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 411      |
|    time_elapsed    | 22745    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=1.85 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.019490309 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0429      |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 1.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.75    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 412      |
|    time_elapsed    | 22804    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.42       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 413         |
|    time_elapsed         | 22853       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.027952086 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.000783   |
|    value_loss           | 0.611       |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=1.81 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 6775000    |
| train/                  |            |
|    approx_kl            | 0.03324694 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0718     |
|    n_updates            | 4130       |
|    policy_gradient_loss | 0.00432    |
|    value_loss           | 103        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.95    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 414      |
|    time_elapsed    | 22911    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.43       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 415         |
|    time_elapsed         | 22961       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.009475974 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.01        |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 46.5        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=1.50 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.019228848 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 3.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.19    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 416      |
|    time_elapsed    | 23019    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=1.37 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.37       |
| time/                   |            |
|    total_timesteps      | 6825000    |
| train/                  |            |
|    approx_kl            | 0.02059629 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.57      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.176      |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.00119   |
|    value_loss           | 0.372      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.78    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 417      |
|    time_elapsed    | 23078    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.74       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 418         |
|    time_elapsed         | 23127       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.016854525 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.293       |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00872    |
|    value_loss           | 0.516       |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=1.41 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.021719113 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0219      |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 0.119       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.37    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 419      |
|    time_elapsed    | 23186    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=1.17 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.013370594 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 1.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.56    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 420      |
|    time_elapsed    | 23244    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 421         |
|    time_elapsed         | 23294       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.016711798 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.877       |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 3.32        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=1.75 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.017032929 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.819       |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 22.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.38    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 422      |
|    time_elapsed    | 23352    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=1.58 +/- 0.38
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.58       |
| time/                   |            |
|    total_timesteps      | 6925000    |
| train/                  |            |
|    approx_kl            | 0.02185988 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.662      |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.00645   |
|    value_loss           | 3.31       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.89    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 423      |
|    time_elapsed    | 23411    |
|    total_timesteps | 6930432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.32      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 424        |
|    time_elapsed         | 23460      |
|    total_timesteps      | 6946816    |
| train/                  |            |
|    approx_kl            | 0.01943417 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.489      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.129      |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.00259   |
|    value_loss           | 10.3       |
----------------------------------------
Eval num_timesteps=6950000, episode_reward=1.84 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.013170106 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0909      |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 0.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.7     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 425      |
|    time_elapsed    | 23519    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=1.55 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.025468461 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 4250        |
|    policy_gradient_loss | 0.000262    |
|    value_loss           | 1.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 426      |
|    time_elapsed    | 23578    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.807      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 427         |
|    time_elapsed         | 23627       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.023597755 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00809    |
|    value_loss           | 1.16        |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=3.40 +/- 1.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.020996729 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0522      |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.0993      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.44    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 428      |
|    time_elapsed    | 23686    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=2.26 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.019043155 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.214       |
|    n_updates            | 4280        |
|    policy_gradient_loss | 0.000875    |
|    value_loss           | 85.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 429      |
|    time_elapsed    | 23744    |
|    total_timesteps | 7028736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -42.2        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 430          |
|    time_elapsed         | 23793        |
|    total_timesteps      | 7045120      |
| train/                  |              |
|    approx_kl            | 0.0143593205 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.23        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.175        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 46.7         |
------------------------------------------
Eval num_timesteps=7050000, episode_reward=2.73 +/- 1.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.73       |
| time/                   |            |
|    total_timesteps      | 7050000    |
| train/                  |            |
|    approx_kl            | 0.01675228 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.45      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.75       |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.00111   |
|    value_loss           | 28.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 431      |
|    time_elapsed    | 23852    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=2.81 +/- 1.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.81       |
| time/                   |            |
|    total_timesteps      | 7075000    |
| train/                  |            |
|    approx_kl            | 0.02370703 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.226      |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.000976  |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 432      |
|    time_elapsed    | 23911    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -19.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 433         |
|    time_elapsed         | 23960       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.019381233 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0783      |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00451    |
|    value_loss           | 3.35        |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=2.46 +/- 1.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.019208672 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.6        |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 29.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.02    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 434      |
|    time_elapsed    | 24018    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=1.31 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.31       |
| time/                   |            |
|    total_timesteps      | 7125000    |
| train/                  |            |
|    approx_kl            | 0.01965575 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.48      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.294      |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.00595   |
|    value_loss           | 0.504      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.16    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 435      |
|    time_elapsed    | 24077    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.76       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 436         |
|    time_elapsed         | 24126       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.019413758 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.465       |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 0.798       |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=1.45 +/- 0.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.45       |
| time/                   |            |
|    total_timesteps      | 7150000    |
| train/                  |            |
|    approx_kl            | 0.01609054 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.393      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.351      |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 6.32       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.5     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 437      |
|    time_elapsed    | 24185    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=1.51 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.015642405 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 9.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.62    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 438      |
|    time_elapsed    | 24243    |
|    total_timesteps | 7176192  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -7.51      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 439        |
|    time_elapsed         | 24293      |
|    total_timesteps      | 7192576    |
| train/                  |            |
|    approx_kl            | 0.01625257 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.00664   |
|    value_loss           | 4.35       |
----------------------------------------
Eval num_timesteps=7200000, episode_reward=1.74 +/- 0.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.01938435 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.61      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.1        |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 6.82       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.72    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 440      |
|    time_elapsed    | 24351    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=1.30 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.019823004 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 2.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.76    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 441      |
|    time_elapsed    | 24410    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.6        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 442         |
|    time_elapsed         | 24459       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.019666268 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0159      |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.00703    |
|    value_loss           | 0.123       |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=1.10 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.020527385 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 29.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.59    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 443      |
|    time_elapsed    | 24518    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 444         |
|    time_elapsed         | 24567       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.020537065 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.283       |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 0.413       |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=1.90 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.020443918 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0312      |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 4.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.79    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 445      |
|    time_elapsed    | 24626    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=2.24 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.018454907 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0904      |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 5.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.87    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 446      |
|    time_elapsed    | 24684    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.68       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 447         |
|    time_elapsed         | 24734       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.020789083 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=1.52 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.021953214 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0291      |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.128       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.73    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 448      |
|    time_elapsed    | 24792    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=1.17 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.017106902 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0513      |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.227   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 449      |
|    time_elapsed    | 24851    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.21        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 450         |
|    time_elapsed         | 24900       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.021221787 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0752      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0789      |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=1.57 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.013357434 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0815      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 0.485       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 451      |
|    time_elapsed    | 24959    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=1.47 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.024341738 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0385      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00646    |
|    value_loss           | 15          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.92    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 452      |
|    time_elapsed    | 25017    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 453         |
|    time_elapsed         | 25066       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.015136775 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.000118   |
|    value_loss           | 12          |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=1.25 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.036337297 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 4530        |
|    policy_gradient_loss | 0.0031      |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 454      |
|    time_elapsed    | 25125    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=1.52 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.022204654 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.3         |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.559       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 455      |
|    time_elapsed    | 25184    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -182        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 456         |
|    time_elapsed         | 25233       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.014893118 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 1.35        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=2.19 +/- 0.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.19       |
| time/                   |            |
|    total_timesteps      | 7475000    |
| train/                  |            |
|    approx_kl            | 0.01805289 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.981      |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 160        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 457      |
|    time_elapsed    | 25291    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=1.81 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.021443883 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.721       |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00364    |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 458      |
|    time_elapsed    | 25350    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -165        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 459         |
|    time_elapsed         | 25399       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.014607357 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.951       |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 3.38        |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=1.80 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.018626597 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.93        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 56.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 460      |
|    time_elapsed    | 25458    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=1.78 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.029314686 |
|    clip_fraction        | 0.428       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.642       |
|    n_updates            | 4600        |
|    policy_gradient_loss | 0.00538     |
|    value_loss           | 17.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 461      |
|    time_elapsed    | 25516    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 462         |
|    time_elapsed         | 25566       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.017712343 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.786       |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 26.8        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=1.91 +/- 0.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.91       |
| time/                   |            |
|    total_timesteps      | 7575000    |
| train/                  |            |
|    approx_kl            | 0.01827194 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.449      |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.00605   |
|    value_loss           | 2.99       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 463      |
|    time_elapsed    | 25624    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=2.03 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.016435139 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.51        |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 3.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.83    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 464      |
|    time_elapsed    | 25683    |
|    total_timesteps | 7602176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.8       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 465        |
|    time_elapsed         | 25732      |
|    total_timesteps      | 7618560    |
| train/                  |            |
|    approx_kl            | 0.02565212 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.59      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0924     |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.00549   |
|    value_loss           | 1.7        |
----------------------------------------
Eval num_timesteps=7625000, episode_reward=2.09 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.012573466 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.06        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 27.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.99    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 466      |
|    time_elapsed    | 25791    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=1.95 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.019787291 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.89        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.000872   |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 467      |
|    time_elapsed    | 25849    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.6        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 468         |
|    time_elapsed         | 25898       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.021701448 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.69        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 5.96        |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=1.26 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.025557984 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 10.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.69    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 469      |
|    time_elapsed    | 25957    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=-22.60 +/- 48.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -22.6       |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.021674734 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.000136   |
|    value_loss           | 11.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.27    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 470      |
|    time_elapsed    | 26015    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.77       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 471         |
|    time_elapsed         | 26065       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.019357115 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.26        |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 3.63        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=1.13 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.13       |
| time/                   |            |
|    total_timesteps      | 7725000    |
| train/                  |            |
|    approx_kl            | 0.02293629 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.78      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 4710       |
|    policy_gradient_loss | -0.00709   |
|    value_loss           | 7.08       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 472      |
|    time_elapsed    | 26123    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.82       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 473         |
|    time_elapsed         | 26173       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.012792715 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.26        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 13          |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=1.34 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.016782805 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.42        |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 5.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.42    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 474      |
|    time_elapsed    | 26231    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=1.66 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.020257328 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 1.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.39    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 475      |
|    time_elapsed    | 26290    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.4        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 476         |
|    time_elapsed         | 26339       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.017432239 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.244       |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 1.77        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=0.71 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.71        |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.009215355 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 22.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 477      |
|    time_elapsed    | 26398    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=1.57 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.017390367 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00417    |
|    value_loss           | 18.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 478      |
|    time_elapsed    | 26456    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -25.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 479         |
|    time_elapsed         | 26506       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.017961377 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0003      |
|    loss                 | 306         |
|    n_updates            | 4780        |
|    policy_gradient_loss | 0.000892    |
|    value_loss           | 150         |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=2.07 +/- 0.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.07       |
| time/                   |            |
|    total_timesteps      | 7850000    |
| train/                  |            |
|    approx_kl            | 0.02118593 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.97      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0775     |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.00152   |
|    value_loss           | 3.53       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 480      |
|    time_elapsed    | 26564    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=1.14 +/- 0.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.14       |
| time/                   |            |
|    total_timesteps      | 7875000    |
| train/                  |            |
|    approx_kl            | 0.01849664 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.83      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.00416   |
|    value_loss           | 2.36       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -71.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 481      |
|    time_elapsed    | 26623    |
|    total_timesteps | 7880704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -55        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 482        |
|    time_elapsed         | 26672      |
|    total_timesteps      | 7897088    |
| train/                  |            |
|    approx_kl            | 0.03360227 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | -0.489     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.09       |
|    n_updates            | 4810       |
|    policy_gradient_loss | 0.0092     |
|    value_loss           | 254        |
----------------------------------------
Eval num_timesteps=7900000, episode_reward=1.50 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.022495564 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.181       |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 0.746       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -52.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 483      |
|    time_elapsed    | 26730    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=0.90 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.899       |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.022635888 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0442      |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.000723   |
|    value_loss           | 0.401       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 484      |
|    time_elapsed    | 26789    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.22       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 485         |
|    time_elapsed         | 26838       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.016511856 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 3.74        |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=1.49 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.023826202 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.89        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.000744   |
|    value_loss           | 17.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.29    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 486      |
|    time_elapsed    | 26897    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=2.74 +/- 2.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.012329968 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.04        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 9.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.12    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 487      |
|    time_elapsed    | 26955    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 488         |
|    time_elapsed         | 27005       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.022231627 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.495       |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 3.22        |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=1.27 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.013390841 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.99        |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 206         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 489      |
|    time_elapsed    | 27063    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=1.73 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.014972133 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 68.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 490      |
|    time_elapsed    | 27122    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -132        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 491         |
|    time_elapsed         | 27171       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.025827255 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 2.11        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=2.01 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.020085882 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0678      |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.000241   |
|    value_loss           | 3.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 492      |
|    time_elapsed    | 27230    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=1.16 +/- 0.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.023517407 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 2.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.65    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 493      |
|    time_elapsed    | 27289    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.32       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 494         |
|    time_elapsed         | 27338       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.020968093 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 0.423       |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=1.46 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.014628723 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 0.884       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.1     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 495      |
|    time_elapsed    | 27397    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=1.82 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.018601509 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.445       |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 3.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 496      |
|    time_elapsed    | 27455    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.38       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 497         |
|    time_elapsed         | 27505       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.018395439 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.531       |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 21.3        |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=1.75 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.019351503 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.193       |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 14.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 498      |
|    time_elapsed    | 27563    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=1.37 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.018502556 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0936      |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 1.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.72    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 499      |
|    time_elapsed    | 27622    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.41       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 500         |
|    time_elapsed         | 27671       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.021592835 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.391       |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 0.453       |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=1.33 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33        |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.018669363 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0643      |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 0.411       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.76    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 501      |
|    time_elapsed    | 27730    |
|    total_timesteps | 8208384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.11      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 502        |
|    time_elapsed         | 27779      |
|    total_timesteps      | 8224768    |
| train/                  |            |
|    approx_kl            | 0.01090847 |
|    clip_fraction        | 0.0939     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.73      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0995     |
|    n_updates            | 5010       |
|    policy_gradient_loss | -0.0041    |
|    value_loss           | 1.67       |
----------------------------------------
Eval num_timesteps=8225000, episode_reward=1.64 +/- 0.67
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.64       |
| time/                   |            |
|    total_timesteps      | 8225000    |
| train/                  |            |
|    approx_kl            | 0.01940343 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.077      |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.00647   |
|    value_loss           | 0.298      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.29    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 503      |
|    time_elapsed    | 27838    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=1.66 +/- 1.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.016689148 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 2.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.11    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 504      |
|    time_elapsed    | 27896    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 505         |
|    time_elapsed         | 27945       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.020508006 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0249      |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 2.5         |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=1.20 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.016973134 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0608      |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 3.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 506      |
|    time_elapsed    | 28004    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=1.17 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.012593326 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.27        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 158         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 507      |
|    time_elapsed    | 28062    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -19.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 508         |
|    time_elapsed         | 28112       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.019719934 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0548      |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 1.96        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=1.30 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.023663146 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.0453      |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 5080        |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 357         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 509      |
|    time_elapsed    | 28170    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=1.47 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.010765536 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 321         |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 510      |
|    time_elapsed    | 28229    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -127        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 511         |
|    time_elapsed         | 28278       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.019978836 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.8         |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.000958   |
|    value_loss           | 181         |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=1.44 +/- 0.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.44       |
| time/                   |            |
|    total_timesteps      | 8375000    |
| train/                  |            |
|    approx_kl            | 0.02492952 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.462      |
|    n_updates            | 5110       |
|    policy_gradient_loss | -0.00175   |
|    value_loss           | 24.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 512      |
|    time_elapsed    | 28337    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=2.17 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.007984294 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 24          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.18    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 513      |
|    time_elapsed    | 28395    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 514         |
|    time_elapsed         | 28445       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.021322843 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0438      |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 0.191       |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=1.67 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.009455875 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17        |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 60.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 515      |
|    time_elapsed    | 28503    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=2.00 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.018747447 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 1.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.57    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 516      |
|    time_elapsed    | 28562    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.19       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 517         |
|    time_elapsed         | 28611       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.018680612 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0557      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 1.17        |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=1.67 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.019077076 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 2.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.83    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 518      |
|    time_elapsed    | 28670    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=1.73 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.009163672 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.31        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 15.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.1     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 519      |
|    time_elapsed    | 28728    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.04       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 520         |
|    time_elapsed         | 28778       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.021653956 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 32.8        |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=2.13 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.014889286 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.536       |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 6.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 521      |
|    time_elapsed    | 28836    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=2.09 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.018811539 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.6        |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.000346   |
|    value_loss           | 198         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 522      |
|    time_elapsed    | 28895    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -152        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 523         |
|    time_elapsed         | 28944       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.019341934 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0845      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 0.354       |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=1.09 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.09        |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.016852232 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.21        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.000478   |
|    value_loss           | 31.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 524      |
|    time_elapsed    | 29003    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=2.00 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.017693203 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0531      |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 0.313       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.37    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 525      |
|    time_elapsed    | 29061    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.52       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 526         |
|    time_elapsed         | 29110       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.011732701 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.95        |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 25.8        |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=1.37 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.019401006 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.951       |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 3.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.27    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 527      |
|    time_elapsed    | 29169    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=1.41 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.023982171 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 13          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5       |
| time/              |          |
|    fps             | 295      |
|    iterations      | 528      |
|    time_elapsed    | 29228    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -3.88      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 529        |
|    time_elapsed         | 29277      |
|    total_timesteps      | 8667136    |
| train/                  |            |
|    approx_kl            | 0.01578367 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.226      |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.000294  |
|    value_loss           | 2.6        |
----------------------------------------
Eval num_timesteps=8675000, episode_reward=1.90 +/- 0.78
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.9        |
| time/                   |            |
|    total_timesteps      | 8675000    |
| train/                  |            |
|    approx_kl            | 0.02163795 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.00564   |
|    value_loss           | 1.87       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.49    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 530      |
|    time_elapsed    | 29335    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.68       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 531         |
|    time_elapsed         | 29385       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.018576032 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 4.61        |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=1.31 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.31        |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.021038331 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 0.899       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.03    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 532      |
|    time_elapsed    | 29443    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=1.56 +/- 0.72
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.56      |
| time/                   |           |
|    total_timesteps      | 8725000   |
| train/                  |           |
|    approx_kl            | 0.0192192 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.23     |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0246    |
|    n_updates            | 5320      |
|    policy_gradient_loss | -0.00654  |
|    value_loss           | 0.282     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.06    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 533      |
|    time_elapsed    | 29502    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.6        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 534         |
|    time_elapsed         | 29551       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.013818224 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 6.04        |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=1.43 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.016108591 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 2.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.31    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 535      |
|    time_elapsed    | 29610    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=1.36 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.011355521 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.24        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 24.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 536      |
|    time_elapsed    | 29668    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 537         |
|    time_elapsed         | 29718       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.021675214 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 5.42        |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=1.04 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.04        |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.020583805 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.55    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 538      |
|    time_elapsed    | 29776    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=1.11 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.11        |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.017798018 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.256       |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 3           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.7     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 539      |
|    time_elapsed    | 29835    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -129        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 540         |
|    time_elapsed         | 29884       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.021217465 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.071       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 20.6        |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=2.25 +/- 2.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.025447117 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.95        |
|    n_updates            | 5400        |
|    policy_gradient_loss | 0.0054      |
|    value_loss           | 269         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -233     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 541      |
|    time_elapsed    | 29942    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=2.12 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.013887467 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.28        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.000881   |
|    value_loss           | 418         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -234     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 542      |
|    time_elapsed    | 30001    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -233        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 543         |
|    time_elapsed         | 30050       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.021261074 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.66        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.0036     |
|    value_loss           | 15.6        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=1.35 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.35        |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.020773798 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.759       |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 16.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 544      |
|    time_elapsed    | 30109    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=2.12 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.017202126 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 3.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 545      |
|    time_elapsed    | 30167    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.97       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 546         |
|    time_elapsed         | 30217       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.022078266 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.6        |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 26.5        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=1.86 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.014174021 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.208       |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 1.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 547      |
|    time_elapsed    | 30275    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=1.89 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.019212896 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 16.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 548      |
|    time_elapsed    | 30334    |
|    total_timesteps | 8978432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.37      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 549        |
|    time_elapsed         | 30383      |
|    total_timesteps      | 8994816    |
| train/                  |            |
|    approx_kl            | 0.02196347 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.207      |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.00641   |
|    value_loss           | 6.36       |
----------------------------------------
Eval num_timesteps=9000000, episode_reward=1.92 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.016293364 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0813      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 1.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.09    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 550      |
|    time_elapsed    | 30442    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=1.81 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.016953787 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.229       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 4.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.98    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 551      |
|    time_elapsed    | 30500    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.14       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 552         |
|    time_elapsed         | 30549       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.017964829 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 29.6        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 13.7        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=2.31 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.016875802 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.168       |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 0.454       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.04    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 553      |
|    time_elapsed    | 30608    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=1.82 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.019845586 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 6.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 554      |
|    time_elapsed    | 30667    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 555         |
|    time_elapsed         | 30716       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.012572732 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.44        |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 52          |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=1.87 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 9100000    |
| train/                  |            |
|    approx_kl            | 0.01715641 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.395      |
|    n_updates            | 5550       |
|    policy_gradient_loss | -0.00525   |
|    value_loss           | 1.05       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 556      |
|    time_elapsed    | 30774    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=3.38 +/- 2.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.38        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.013290361 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.9        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 98          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 557      |
|    time_elapsed    | 30833    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 558         |
|    time_elapsed         | 30882       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.019969165 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.747       |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 53.8        |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=1.79 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.017264374 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 3.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 559      |
|    time_elapsed    | 30941    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=2.35 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35        |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.013698404 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 24          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 560      |
|    time_elapsed    | 30999    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 561         |
|    time_elapsed         | 31049       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.012274726 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 209         |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 159         |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=1.90 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.020286132 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 1.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 562      |
|    time_elapsed    | 31107    |
|    total_timesteps | 9207808  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -13.8        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 563          |
|    time_elapsed         | 31157        |
|    total_timesteps      | 9224192      |
| train/                  |              |
|    approx_kl            | 0.0137988385 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.408        |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00427     |
|    value_loss           | 3.3          |
------------------------------------------
Eval num_timesteps=9225000, episode_reward=1.86 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.018155482 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.76        |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 3.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.66    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 564      |
|    time_elapsed    | 31215    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=2.97 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.020784404 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.271       |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 5.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 565      |
|    time_elapsed    | 31274    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 566         |
|    time_elapsed         | 31323       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.019021783 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.314       |
|    n_updates            | 5650        |
|    policy_gradient_loss | 0.00444     |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=2.04 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.019155394 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 3.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 567      |
|    time_elapsed    | 31382    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=1.39 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.39        |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.017704438 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 0.995       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -117     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 568      |
|    time_elapsed    | 31440    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.08       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 569         |
|    time_elapsed         | 31490       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.011301408 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.898       |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 23.7        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=2.21 +/- 0.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.21       |
| time/                   |            |
|    total_timesteps      | 9325000    |
| train/                  |            |
|    approx_kl            | 0.01809468 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 5690       |
|    policy_gradient_loss | -0.00683   |
|    value_loss           | 0.631      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.44    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 570      |
|    time_elapsed    | 31548    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=2.40 +/- 0.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.4        |
| time/                   |            |
|    total_timesteps      | 9350000    |
| train/                  |            |
|    approx_kl            | 0.01724605 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 26         |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 3.7        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.78    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 571      |
|    time_elapsed    | 31607    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.79       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 572         |
|    time_elapsed         | 31656       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.013988486 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.63        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 5.54        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=2.34 +/- 1.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.016142383 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 6.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.9     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 573      |
|    time_elapsed    | 31715    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=1.52 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.52       |
| time/                   |            |
|    total_timesteps      | 9400000    |
| train/                  |            |
|    approx_kl            | 0.02929343 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.28      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.217      |
|    n_updates            | 5730       |
|    policy_gradient_loss | 0.0018     |
|    value_loss           | 3.75       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.56    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 574      |
|    time_elapsed    | 31773    |
|    total_timesteps | 9404416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -10.7      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 575        |
|    time_elapsed         | 31822      |
|    total_timesteps      | 9420800    |
| train/                  |            |
|    approx_kl            | 0.01586317 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.162      |
|    n_updates            | 5740       |
|    policy_gradient_loss | -0.000308  |
|    value_loss           | 18         |
----------------------------------------
Eval num_timesteps=9425000, episode_reward=3.07 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.019953879 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.891       |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 25.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 576      |
|    time_elapsed    | 31881    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=1.60 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.018258248 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 2.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 577      |
|    time_elapsed    | 31939    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 578         |
|    time_elapsed         | 31989       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.015360031 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.218       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 3.26        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=1.95 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.016232917 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.35    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 579      |
|    time_elapsed    | 32047    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=-0.54 +/- 3.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -0.542      |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.017544407 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0818      |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 6.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.9     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 580      |
|    time_elapsed    | 32106    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.02       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 581         |
|    time_elapsed         | 32155       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.016069781 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 0.586       |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=2.08 +/- 0.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.08       |
| time/                   |            |
|    total_timesteps      | 9525000    |
| train/                  |            |
|    approx_kl            | 0.01563692 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.54       |
|    n_updates            | 5810       |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 17.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.95    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 582      |
|    time_elapsed    | 32214    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=1.26 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.016611775 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.648       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00696    |
|    value_loss           | 3.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.68    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 583      |
|    time_elapsed    | 32272    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 584         |
|    time_elapsed         | 32321       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.017160749 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.339       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 2.53        |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=1.23 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.23       |
| time/                   |            |
|    total_timesteps      | 9575000    |
| train/                  |            |
|    approx_kl            | 0.01858278 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.68       |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.00674   |
|    value_loss           | 1.75       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 585      |
|    time_elapsed    | 32380    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=1.25 +/- 0.14
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.25       |
| time/                   |            |
|    total_timesteps      | 9600000    |
| train/                  |            |
|    approx_kl            | 0.01816771 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.04       |
|    n_updates            | 5850       |
|    policy_gradient_loss | -0.0054    |
|    value_loss           | 15.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 586      |
|    time_elapsed    | 32439    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 587         |
|    time_elapsed         | 32489       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.018274643 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.35        |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 9.44        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=1.87 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.019764334 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 1.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 588      |
|    time_elapsed    | 32547    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=1.95 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.016418561 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 0.312       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.68    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 589      |
|    time_elapsed    | 32606    |
|    total_timesteps | 9650176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.98      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 590        |
|    time_elapsed         | 32655      |
|    total_timesteps      | 9666560    |
| train/                  |            |
|    approx_kl            | 0.01654543 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.419      |
|    n_updates            | 5890       |
|    policy_gradient_loss | -0.00791   |
|    value_loss           | 1.51       |
----------------------------------------
Eval num_timesteps=9675000, episode_reward=1.68 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.016904727 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.31        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 1.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.35    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 591      |
|    time_elapsed    | 32714    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.58       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 592         |
|    time_elapsed         | 32763       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.019477181 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 0.699       |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=1.40 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.016111005 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.26        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 0.362       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.8     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 593      |
|    time_elapsed    | 32822    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=2.16 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.019924728 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 0.272       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.24    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 594      |
|    time_elapsed    | 32880    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.67       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 595         |
|    time_elapsed         | 32929       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.020289999 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 0.557       |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=2.43 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.018675178 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.087       |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.993   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 596      |
|    time_elapsed    | 32988    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=1.49 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.012123853 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.349       |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 3.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.3     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 597      |
|    time_elapsed    | 33046    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.45       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 598         |
|    time_elapsed         | 33096       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.019676132 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0412      |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 3.08        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=1.96 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.017347898 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0976      |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 1.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.83    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 599      |
|    time_elapsed    | 33154    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=1.19 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.018439924 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0814      |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 0.302       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.35    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 600      |
|    time_elapsed    | 33213    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.24       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 601         |
|    time_elapsed         | 33262       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.014989279 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0777      |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=2.42 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.016367752 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0276      |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.307   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 602      |
|    time_elapsed    | 33321    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=1.45 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.016339434 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 1.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.37    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 603      |
|    time_elapsed    | 33379    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1          |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 604         |
|    time_elapsed         | 33429       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.021137038 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 1.42        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=2.36 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 9900000     |
| train/                  |             |
|    approx_kl            | 0.022498932 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 0.562       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 605      |
|    time_elapsed    | 33487    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=1.66 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.019689009 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0417      |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 0.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.67    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 606      |
|    time_elapsed    | 33547    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 607         |
|    time_elapsed         | 33597       |
|    total_timesteps      | 9945088     |
| train/                  |             |
|    approx_kl            | 0.016420852 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.63        |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 4.83        |
-----------------------------------------
Eval num_timesteps=9950000, episode_reward=2.23 +/- 2.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.017413393 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.647       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 14.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.51    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 608      |
|    time_elapsed    | 33656    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=2.07 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.016576132 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.49        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 8.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.04    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 609      |
|    time_elapsed    | 33714    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.6        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 610         |
|    time_elapsed         | 33766       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.018559989 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0051     |
|    value_loss           | 0.886       |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=2.10 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.027168907 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 6100        |
|    policy_gradient_loss | 0.000738    |
|    value_loss           | 2.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.75    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 611      |
|    time_elapsed    | 33825    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v7_2
