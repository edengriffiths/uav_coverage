========== uav-v5 ==========
Seed: 3099198586
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v5_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.83e+03 |
| time/              |           |
|    fps             | 401       |
|    iterations      | 1         |
|    time_elapsed    | 40        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-4757.63 +/- 4735.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -4.76e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007833123 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | -0.000135   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 4.1e+04     |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.63e+03 |
| time/              |           |
|    fps             | 321       |
|    iterations      | 2         |
|    time_elapsed    | 101       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.39e+03   |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 3           |
|    time_elapsed         | 151         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008150348 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -9.42e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 3.3e+04     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-2616.32 +/- 1955.01
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -2.62e+03    |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0118828565 |
|    clip_fraction        | 0.182        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | -1.19e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 8.53e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.9e+03 |
| time/              |          |
|    fps             | 310      |
|    iterations      | 4        |
|    time_elapsed    | 211      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-1796.90 +/- 1.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.8e+03     |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0057393797 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.42e+03     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00387     |
|    value_loss           | 9.58e+03     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.64e+03 |
| time/              |           |
|    fps             | 303       |
|    iterations      | 5         |
|    time_elapsed    | 270       |
|    total_timesteps | 81920     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.78e+03    |
| time/                   |              |
|    fps                  | 307          |
|    iterations           | 6            |
|    time_elapsed         | 320          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0061599733 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.48e+03     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00776     |
|    value_loss           | 3.79e+03     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=-1677.57 +/- 239.87
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.68e+03    |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0076995995 |
|    clip_fraction        | 0.0881       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.439        |
|    learning_rate        | 0.0003       |
|    loss                 | 332          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0096      |
|    value_loss           | 1.22e+03     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.36e+03 |
| time/              |           |
|    fps             | 302       |
|    iterations      | 7         |
|    time_elapsed    | 379       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1557.41 +/- 294.00
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.56e+03  |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.01117082 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.76      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0003     |
|    loss                 | 88.4       |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 134        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -748     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 8        |
|    time_elapsed    | 438      |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -532         |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 9            |
|    time_elapsed         | 488          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0111878365 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0003       |
|    loss                 | 40.4         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-1317.42 +/- 239.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.32e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.014323529 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 72.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 299      |
|    iterations      | 10       |
|    time_elapsed    | 547      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-837.33 +/- 293.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -837        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.009660123 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0003      |
|    loss                 | 60.2        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 212         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 11       |
|    time_elapsed    | 607      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -385        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 12          |
|    time_elapsed         | 657         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.010442414 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-357.75 +/- 293.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -358        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.013368469 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 248         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 13       |
|    time_elapsed    | 716      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-474.89 +/- 448.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -475       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.01110721 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.77      |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.0003     |
|    loss                 | 43.1       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 359        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 14       |
|    time_elapsed    | 775      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -359        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 15          |
|    time_elapsed         | 825         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.013672459 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 41.1        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 527         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=-115.91 +/- 240.50
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -116      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0141753 |
|    clip_fraction        | 0.185     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.76     |
|    explained_variance   | 0.327     |
|    learning_rate        | 0.0003    |
|    loss                 | 89.1      |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.00948  |
|    value_loss           | 432       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -350     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 16       |
|    time_elapsed    | 884      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=3.12 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.12        |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.013369718 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 382         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 17       |
|    time_elapsed    | 943      |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -302        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 18          |
|    time_elapsed         | 993         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.013914546 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 426         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-113.07 +/- 234.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -113        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.015190644 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.0003      |
|    loss                 | 192         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 359         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -236     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 19       |
|    time_elapsed    | 1053     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=3.86 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.86        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.016128125 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 57.3        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 20       |
|    time_elapsed    | 1112     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -342        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 21          |
|    time_elapsed         | 1162        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.014304538 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | -0.0153     |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 487         |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=3.34 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.012701492 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 1.38e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -304     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 22       |
|    time_elapsed    | 1221     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=5.32 +/- 2.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.32        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.019676112 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.1        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 24.9        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 23       |
|    time_elapsed    | 1280     |
|    total_timesteps | 376832   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -532         |
| time/                   |              |
|    fps                  | 295          |
|    iterations           | 24           |
|    time_elapsed         | 1330         |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0077071036 |
|    clip_fraction        | 0.0541       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.703        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.06e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 4.84e+03     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=4.46 +/- 1.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.46        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.018512692 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 25       |
|    time_elapsed    | 1389     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=3.53 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.53        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.018950148 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00959    |
|    value_loss           | 80.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -456     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 26       |
|    time_elapsed    | 1448     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -306        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 27          |
|    time_elapsed         | 1498        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.013072783 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 300         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 589         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=4.35 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.35        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.013194516 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 280         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 2.5e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -291     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 28       |
|    time_elapsed    | 1557     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=5.17 +/- 1.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.17        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.017220858 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 220         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -383     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 29       |
|    time_elapsed    | 1617     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -341        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 30          |
|    time_elapsed         | 1666        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.017528197 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.19        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 439         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=4.18 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.18        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.020368926 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.7        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 50.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 31       |
|    time_elapsed    | 1726     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -239        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 32          |
|    time_elapsed         | 1775        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.017551623 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.72        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 56.5        |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=3.20 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.2         |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.014577903 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 201         |
|    n_updates            | 320         |
|    policy_gradient_loss | 5.43e-05    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 33       |
|    time_elapsed    | 1835     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=4.02 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.014486186 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.9        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 160         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 34       |
|    time_elapsed    | 1893     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 35          |
|    time_elapsed         | 1942        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.025077019 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.71        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 89.4        |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=-115.21 +/- 239.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.016363747 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.7        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 124         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -96.3    |
| time/              |          |
|    fps             | 294      |
|    iterations      | 36       |
|    time_elapsed    | 2000     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=4.80 +/- 2.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.8        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.01730916 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.0874     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.27       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.000965  |
|    value_loss           | 260        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 37       |
|    time_elapsed    | 2058     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -131        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 38          |
|    time_elapsed         | 2107        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.018519972 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.2        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 70.3        |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=5.13 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.13        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.007420148 |
|    clip_fraction        | 0.0538      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.9        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 539         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 39       |
|    time_elapsed    | 2164     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=4.85 +/- 2.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.85         |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0146587845 |
|    clip_fraction        | 0.195        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 0.848        |
|    learning_rate        | 0.0003       |
|    loss                 | 16           |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00917     |
|    value_loss           | 298          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 40       |
|    time_elapsed    | 2222     |
|    total_timesteps | 655360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -235       |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 41         |
|    time_elapsed         | 2271       |
|    total_timesteps      | 671744     |
| train/                  |            |
|    approx_kl            | 0.01638814 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.0003     |
|    loss                 | 40.3       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 69.6       |
----------------------------------------
Eval num_timesteps=675000, episode_reward=3.46 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.46        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.021161024 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 410         |
|    policy_gradient_loss | 0.00147     |
|    value_loss           | 209         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 42       |
|    time_elapsed    | 2329     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=3.41 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.018409614 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.43        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 80.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 43       |
|    time_elapsed    | 2386     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -138        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 44          |
|    time_elapsed         | 2435        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.022566594 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.727       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00862    |
|    value_loss           | 11.1        |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=4.70 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.7         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.009633115 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.48        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 160         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 45       |
|    time_elapsed    | 2493     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=3.54 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.54        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.020995412 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.16        |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 29.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 46       |
|    time_elapsed    | 2551     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 47          |
|    time_elapsed         | 2599        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.021881808 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.574       |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 6.9         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=4.12 +/- 0.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.12        |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.020582218 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.225       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 5.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 48       |
|    time_elapsed    | 2657     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=4.09 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.09        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.018793609 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00892    |
|    value_loss           | 3.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 49       |
|    time_elapsed    | 2715     |
|    total_timesteps | 802816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11.1      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 50         |
|    time_elapsed         | 2764       |
|    total_timesteps      | 819200     |
| train/                  |            |
|    approx_kl            | 0.02018745 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.23       |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.00852   |
|    value_loss           | 4.34       |
----------------------------------------
Eval num_timesteps=825000, episode_reward=5.66 +/- 3.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.66        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.008631061 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.6        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 51       |
|    time_elapsed    | 2821     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=7.49 +/- 4.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 7.49         |
| time/                   |              |
|    total_timesteps      | 850000       |
| train/                  |              |
|    approx_kl            | 0.0056623616 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.61        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.0003       |
|    loss                 | 64.4         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.0081      |
|    value_loss           | 1.36e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 52       |
|    time_elapsed    | 2879     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -192        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 53          |
|    time_elapsed         | 2928        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.019794049 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.35        |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=4.96 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.96        |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.022691697 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.38        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.000812   |
|    value_loss           | 22.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 54       |
|    time_elapsed    | 2986     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=5.84 +/- 1.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.84        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.024010172 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.000475   |
|    value_loss           | 128         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 55       |
|    time_elapsed    | 3043     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -88.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 56          |
|    time_elapsed         | 3092        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.020790018 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 44.1        |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=5.38 +/- 1.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.013116135 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.1        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00917    |
|    value_loss           | 276         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 57       |
|    time_elapsed    | 3150     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=3.86 +/- 0.72
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.86       |
| time/                   |            |
|    total_timesteps      | 950000     |
| train/                  |            |
|    approx_kl            | 0.01656876 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.0003     |
|    loss                 | 219        |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 106        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 58       |
|    time_elapsed    | 3208     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -140        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 59          |
|    time_elapsed         | 3256        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.021252092 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.1        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 212         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=4.27 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.27        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.011839243 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.9        |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 456         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 60       |
|    time_elapsed    | 3314     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -213        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 61          |
|    time_elapsed         | 3363        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.020487744 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.2        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=3.95 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.95        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.025508367 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 610         |
|    policy_gradient_loss | 0.00659     |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -242     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 62       |
|    time_elapsed    | 3421     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=3.79 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.79        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.009284063 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 386         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -297     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 63       |
|    time_elapsed    | 3478     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -493        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 64          |
|    time_elapsed         | 3527        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.019749178 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.7        |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 670         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=3.97 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.97        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.011569385 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 806         |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.000887    |
|    value_loss           | 2.76e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 65       |
|    time_elapsed    | 3585     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=3.49 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.49        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.022138696 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.283       |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 4.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -223     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 66       |
|    time_elapsed    | 3643     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -132        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 67          |
|    time_elapsed         | 3691        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.014381131 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.1        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=4.84 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.014460292 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.66        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 118         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.8    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 68       |
|    time_elapsed    | 3749     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=4.31 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.31        |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.018142512 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.831       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 1.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -82.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 69       |
|    time_elapsed    | 3807     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -92.2       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 70          |
|    time_elapsed         | 3858        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.017459823 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 413         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 419         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=4.19 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.19        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.013423007 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 80.9        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -87.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 71       |
|    time_elapsed    | 3917     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=4.75 +/- 1.57
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.75       |
| time/                   |            |
|    total_timesteps      | 1175000    |
| train/                  |            |
|    approx_kl            | 0.01851173 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.64       |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.00636   |
|    value_loss           | 43.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 72       |
|    time_elapsed    | 3975     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -59.9       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 73          |
|    time_elapsed         | 4023        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.016690638 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.5         |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 24.8        |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=4.49 +/- 0.93
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 4.49      |
| time/                   |           |
|    total_timesteps      | 1200000   |
| train/                  |           |
|    approx_kl            | 0.0112209 |
|    clip_fraction        | 0.124     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.52     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | 99.7      |
|    n_updates            | 730       |
|    policy_gradient_loss | -0.00335  |
|    value_loss           | 443       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.8    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 74       |
|    time_elapsed    | 4081     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=4.83 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.83        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.016432358 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 35.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 75       |
|    time_elapsed    | 4139     |
|    total_timesteps | 1228800  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -55.9      |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 76         |
|    time_elapsed         | 4188       |
|    total_timesteps      | 1245184    |
| train/                  |            |
|    approx_kl            | 0.01899152 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.159      |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.00125   |
|    value_loss           | 2.8        |
----------------------------------------
Eval num_timesteps=1250000, episode_reward=5.89 +/- 3.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.89        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.021242727 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 0.714       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 77       |
|    time_elapsed    | 4246     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=5.70 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.7         |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.018862661 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.27        |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 3.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 78       |
|    time_elapsed    | 4304     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 79          |
|    time_elapsed         | 4353        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.014421137 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 99.6        |
|    n_updates            | 780         |
|    policy_gradient_loss | 0.000805    |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=5.74 +/- 1.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.74        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.021665338 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.0476      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 70.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -82.9    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 80       |
|    time_elapsed    | 4411     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=9.27 +/- 6.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.27        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.009145481 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 68.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 81       |
|    time_elapsed    | 4469     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.9       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 82          |
|    time_elapsed         | 4517        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.020079892 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.29        |
|    n_updates            | 810         |
|    policy_gradient_loss | 0.00293     |
|    value_loss           | 17.6        |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=4.61 +/- 0.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.61       |
| time/                   |            |
|    total_timesteps      | 1350000    |
| train/                  |            |
|    approx_kl            | 0.02116295 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.00236   |
|    value_loss           | 125        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 83       |
|    time_elapsed    | 4575     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=5.98 +/- 1.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.98        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.021737125 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 5.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 84       |
|    time_elapsed    | 4633     |
|    total_timesteps | 1376256  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -14.1      |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 85         |
|    time_elapsed         | 4682       |
|    total_timesteps      | 1392640    |
| train/                  |            |
|    approx_kl            | 0.01586592 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.61       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.00481   |
|    value_loss           | 10.1       |
----------------------------------------
Eval num_timesteps=1400000, episode_reward=5.49 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.49        |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.022777043 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.308       |
|    n_updates            | 850         |
|    policy_gradient_loss | 0.000927    |
|    value_loss           | 3.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -236     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 86       |
|    time_elapsed    | 4740     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=6.97 +/- 2.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.97        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.005292371 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 677         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -243     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 87       |
|    time_elapsed    | 4798     |
|    total_timesteps | 1425408  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -462       |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 88         |
|    time_elapsed         | 4847       |
|    total_timesteps      | 1441792    |
| train/                  |            |
|    approx_kl            | 0.01098975 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 31         |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.00184   |
|    value_loss           | 813        |
----------------------------------------
Eval num_timesteps=1450000, episode_reward=5.63 +/- 1.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.63        |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.011715664 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37e+03    |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 1.44e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 89       |
|    time_elapsed    | 4905     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -313        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 90          |
|    time_elapsed         | 4954        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.017197054 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15e+03    |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.000731   |
|    value_loss           | 704         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=7.25 +/- 2.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.25       |
| time/                   |            |
|    total_timesteps      | 1475000    |
| train/                  |            |
|    approx_kl            | 0.01862334 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.9        |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.00613   |
|    value_loss           | 4.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 91       |
|    time_elapsed    | 5012     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=6.19 +/- 1.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.19        |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.020714734 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 603         |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.000173   |
|    value_loss           | 1.92e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 92       |
|    time_elapsed    | 5069     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -160        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 93          |
|    time_elapsed         | 5118        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.014187712 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.53        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 16.1        |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=7.01 +/- 2.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.01        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.032419942 |
|    clip_fraction        | 0.389       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 366         |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.00284     |
|    value_loss           | 181         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 94       |
|    time_elapsed    | 5176     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=10.12 +/- 4.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.023612473 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 593         |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00011     |
|    value_loss           | 1.13e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 95       |
|    time_elapsed    | 5234     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -153        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 96          |
|    time_elapsed         | 5283        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.018517412 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.849       |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 45.6        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=7.22 +/- 4.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.22        |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.013052085 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 43.6        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00633    |
|    value_loss           | 145         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 97       |
|    time_elapsed    | 5341     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=9.20 +/- 1.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.018560536 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.8        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 43.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.1    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 98       |
|    time_elapsed    | 5399     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -93.8      |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 99         |
|    time_elapsed         | 5447       |
|    total_timesteps      | 1622016    |
| train/                  |            |
|    approx_kl            | 0.01630279 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 65.3       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.00453   |
|    value_loss           | 129        |
----------------------------------------
Eval num_timesteps=1625000, episode_reward=9.72 +/- 3.80
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 9.72         |
| time/                   |              |
|    total_timesteps      | 1625000      |
| train/                  |              |
|    approx_kl            | 0.0154031515 |
|    clip_fraction        | 0.208        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 43.4         |
|    n_updates            | 990          |
|    policy_gradient_loss | 0.00219      |
|    value_loss           | 201          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -76.3    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 100      |
|    time_elapsed    | 5505     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=7.44 +/- 2.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.44       |
| time/                   |            |
|    total_timesteps      | 1650000    |
| train/                  |            |
|    approx_kl            | 0.01938597 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.00383   |
|    value_loss           | 41.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 101      |
|    time_elapsed    | 5563     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -96.9       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 102         |
|    time_elapsed         | 5612        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.019441064 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 380         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=7.75 +/- 2.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.75        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.019563999 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 6.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 103      |
|    time_elapsed    | 5670     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=8.41 +/- 1.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 8.41       |
| time/                   |            |
|    total_timesteps      | 1700000    |
| train/                  |            |
|    approx_kl            | 0.01780812 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.00101   |
|    value_loss           | 139        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -97.5    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 104      |
|    time_elapsed    | 5728     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -40.2       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 105         |
|    time_elapsed         | 5777        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.025260607 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 1.27        |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=9.48 +/- 5.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.48        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.022569392 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 3.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.9    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 106      |
|    time_elapsed    | 5834     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=6.45 +/- 1.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.45        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.015830025 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.506       |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 11.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.4    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 107      |
|    time_elapsed    | 5892     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.9       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 108         |
|    time_elapsed         | 5941        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.019303866 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.38        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 69          |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=8.42 +/- 4.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.42        |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.023710757 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.34        |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 2.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.4    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 109      |
|    time_elapsed    | 5999     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=8.69 +/- 2.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.69        |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.012356957 |
|    clip_fraction        | 0.0935      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.98        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 94.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.6    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 110      |
|    time_elapsed    | 6057     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -40.7       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 111         |
|    time_elapsed         | 6105        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.019984249 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 65.1        |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=6.50 +/- 1.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 1825000    |
| train/                  |            |
|    approx_kl            | 0.02949719 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 1110       |
|    policy_gradient_loss | 0.00751    |
|    value_loss           | 47.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 112      |
|    time_elapsed    | 6163     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=6.30 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.3         |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.030997556 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.48        |
|    n_updates            | 1120        |
|    policy_gradient_loss | 0.00354     |
|    value_loss           | 69.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 113      |
|    time_elapsed    | 6221     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -241        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 114         |
|    time_elapsed         | 6270        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.004384084 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.28        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 215         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=6.86 +/- 1.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.86        |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.012380179 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 151         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 509         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 115      |
|    time_elapsed    | 6328     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=6.14 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.14        |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.018282715 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.000858   |
|    value_loss           | 2.11e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 116      |
|    time_elapsed    | 6386     |
|    total_timesteps | 1900544  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -337         |
| time/                   |              |
|    fps                  | 297          |
|    iterations           | 117          |
|    time_elapsed         | 6435         |
|    total_timesteps      | 1916928      |
| train/                  |              |
|    approx_kl            | 0.0124035925 |
|    clip_fraction        | 0.156        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.01e+03     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 1.72e+03     |
------------------------------------------
Eval num_timesteps=1925000, episode_reward=7.91 +/- 2.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.91        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.018216705 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 34.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 118      |
|    time_elapsed    | 6493     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -201        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 119         |
|    time_elapsed         | 6541        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.024714075 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.3        |
|    n_updates            | 1180        |
|    policy_gradient_loss | 0.000386    |
|    value_loss           | 323         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=5.74 +/- 2.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.74        |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.021319693 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.558       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 8.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.2    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 120      |
|    time_elapsed    | 6599     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=7.35 +/- 4.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.35        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.013001997 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.4        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00787    |
|    value_loss           | 19.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.6    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 121      |
|    time_elapsed    | 6657     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -85         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 122         |
|    time_elapsed         | 6706        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.008769669 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 221         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 360         |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=7.06 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.06        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.019375488 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 83.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -94.3    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 123      |
|    time_elapsed    | 6764     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=8.34 +/- 3.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.34        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.014506671 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 124      |
|    time_elapsed    | 6822     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -116        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 125         |
|    time_elapsed         | 6871        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.020860884 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | 28          |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=7.56 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.56        |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.022436576 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -5.63e-05   |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 126      |
|    time_elapsed    | 6928     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=8.20 +/- 3.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.2         |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.016575718 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.735       |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 8.9         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 127      |
|    time_elapsed    | 6986     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -62.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 128         |
|    time_elapsed         | 7035        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.016531289 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 5.33        |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=8.02 +/- 3.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.02        |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.019439723 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.429       |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 1.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.75    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 129      |
|    time_elapsed    | 7093     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=6.33 +/- 2.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.33        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.016285408 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.601       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00731    |
|    value_loss           | 4.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 130      |
|    time_elapsed    | 7151     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 131         |
|    time_elapsed         | 7200        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.023278754 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | -0.275      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0926      |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.000731   |
|    value_loss           | 54.6        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=4.99 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.99        |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.019557184 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.32        |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 5.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 132      |
|    time_elapsed    | 7257     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=7.71 +/- 1.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.71       |
| time/                   |            |
|    total_timesteps      | 2175000    |
| train/                  |            |
|    approx_kl            | 0.01779499 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.671      |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.00549   |
|    value_loss           | 3.8        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 133      |
|    time_elapsed    | 7315     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.36       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 134         |
|    time_elapsed         | 7364        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.020400345 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.8         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 11.8        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=5.96 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.96        |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.016159113 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.406       |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 6.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.55    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 135      |
|    time_elapsed    | 7422     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=5.30 +/- 2.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.3         |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.016765617 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.263       |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.76    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 136      |
|    time_elapsed    | 7480     |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.04       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 137         |
|    time_elapsed         | 7528        |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.018923175 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0972      |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 7.62        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=6.78 +/- 2.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.78        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.018476356 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.784       |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 1.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 138      |
|    time_elapsed    | 7586     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=7.30 +/- 3.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.012688009 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.08        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.7    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 139      |
|    time_elapsed    | 7644     |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -71.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 140         |
|    time_elapsed         | 7693        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.008426631 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.0343      |
|    learning_rate        | 0.0003      |
|    loss                 | 246         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 296         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=7.09 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.09        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.023170901 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.17        |
|    n_updates            | 1400        |
|    policy_gradient_loss | 0.00265     |
|    value_loss           | 19.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 141      |
|    time_elapsed    | 7751     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=5.84 +/- 1.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.84        |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.012658138 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.22        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 14.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -71.2    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 142      |
|    time_elapsed    | 7809     |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 143         |
|    time_elapsed         | 7858        |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.011410471 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 72.9        |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=6.19 +/- 2.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.19        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.021108536 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.18        |
|    n_updates            | 1430        |
|    policy_gradient_loss | 9.27e-05    |
|    value_loss           | 51.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 144      |
|    time_elapsed    | 7916     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=6.68 +/- 2.46
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.68       |
| time/                   |            |
|    total_timesteps      | 2375000    |
| train/                  |            |
|    approx_kl            | 0.01812965 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 368        |
|    n_updates            | 1440       |
|    policy_gradient_loss | 0.000549   |
|    value_loss           | 371        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 145      |
|    time_elapsed    | 7974     |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -291        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 146         |
|    time_elapsed         | 8023        |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.021044116 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.969       |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 15.2        |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=10.56 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.001928034 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.000928   |
|    value_loss           | 512         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -296     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 147      |
|    time_elapsed    | 8080     |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -231        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 148         |
|    time_elapsed         | 8129        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.014761854 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.38        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 211         |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=8.90 +/- 4.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.9         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.026782159 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 1480        |
|    policy_gradient_loss | 0.00538     |
|    value_loss           | 367         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 149      |
|    time_elapsed    | 8187     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=7.14 +/- 2.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.14        |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.015435854 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 1490        |
|    policy_gradient_loss | 0.000347    |
|    value_loss           | 420         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 150      |
|    time_elapsed    | 8245     |
|    total_timesteps | 2457600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 151        |
|    time_elapsed         | 8294       |
|    total_timesteps      | 2473984    |
| train/                  |            |
|    approx_kl            | 0.01815814 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.817      |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.00152   |
|    value_loss           | 12.1       |
----------------------------------------
Eval num_timesteps=2475000, episode_reward=4.59 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.59        |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.014857849 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.23        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 9.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -89.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 152      |
|    time_elapsed    | 8352     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=5.73 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.73        |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.022416115 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000562   |
|    value_loss           | 5.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 153      |
|    time_elapsed    | 8409     |
|    total_timesteps | 2506752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -73.3        |
| time/                   |              |
|    fps                  | 298          |
|    iterations           | 154          |
|    time_elapsed         | 8458         |
|    total_timesteps      | 2523136      |
| train/                  |              |
|    approx_kl            | 0.0053497944 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 269          |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00483     |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=2525000, episode_reward=7.24 +/- 2.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.24        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.018469919 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 15.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -96.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 155      |
|    time_elapsed    | 8516     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=4.26 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.26        |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.009346053 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.9        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 114         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -99.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 156      |
|    time_elapsed    | 8574     |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 157         |
|    time_elapsed         | 8623        |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.018911304 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.3        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 14.3        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=4.17 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.17        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.027164882 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.6        |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 20          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 158      |
|    time_elapsed    | 8681     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=5.44 +/- 1.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.44        |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.012345446 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 323         |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 159      |
|    time_elapsed    | 8739     |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.5       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 160         |
|    time_elapsed         | 8787        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.016235806 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.786       |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 8.44        |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=5.22 +/- 2.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.22        |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.018333385 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.99        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 12.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 161      |
|    time_elapsed    | 8845     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=4.52 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.52        |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.025132634 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.68        |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.00293     |
|    value_loss           | 91.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 162      |
|    time_elapsed    | 8903     |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -156        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 163         |
|    time_elapsed         | 8952        |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.015683971 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.1        |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 52.7        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=5.10 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.017670022 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 20.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 164      |
|    time_elapsed    | 9010     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=4.47 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.47        |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.018820908 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0003      |
|    loss                 | 229         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 50.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 165      |
|    time_elapsed    | 9068     |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.4       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 166         |
|    time_elapsed         | 9117        |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.013451797 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.783       |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 18.8        |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=3.73 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.73        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.020724848 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 16.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 167      |
|    time_elapsed    | 9175     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=3.88 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.88        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.021646317 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 22.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 168      |
|    time_elapsed    | 9232     |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.4       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 169         |
|    time_elapsed         | 9281        |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.013204349 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.518       |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 53.2        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=5.19 +/- 1.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.19        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.022647373 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.85        |
|    n_updates            | 1690        |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 154         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 170      |
|    time_elapsed    | 9339     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=4.07 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.07        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.021167256 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 99.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 171      |
|    time_elapsed    | 9397     |
|    total_timesteps | 2801664  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -77.9      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 172        |
|    time_elapsed         | 9446       |
|    total_timesteps      | 2818048    |
| train/                  |            |
|    approx_kl            | 0.03076768 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.0656     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.45       |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.000236  |
|    value_loss           | 99.8       |
----------------------------------------
Eval num_timesteps=2825000, episode_reward=3.82 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.82        |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.013967134 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.59        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 173      |
|    time_elapsed    | 9504     |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=3.36 +/- 0.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.36       |
| time/                   |            |
|    total_timesteps      | 2850000    |
| train/                  |            |
|    approx_kl            | 0.03620574 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 159        |
|    n_updates            | 1730       |
|    policy_gradient_loss | 0.00251    |
|    value_loss           | 239        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 174      |
|    time_elapsed    | 9562     |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -197        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 175         |
|    time_elapsed         | 9611        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.017435301 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 57.7        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=5.96 +/- 3.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.96        |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.017706137 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.24        |
|    n_updates            | 1750        |
|    policy_gradient_loss | -6.59e-05   |
|    value_loss           | 262         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 176      |
|    time_elapsed    | 9669     |
|    total_timesteps | 2883584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -116       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 177        |
|    time_elapsed         | 9717       |
|    total_timesteps      | 2899968    |
| train/                  |            |
|    approx_kl            | 0.01975653 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.00997   |
|    value_loss           | 11.3       |
----------------------------------------
Eval num_timesteps=2900000, episode_reward=4.05 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.05        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.018427957 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.296       |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 2.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 178      |
|    time_elapsed    | 9775     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=6.85 +/- 4.12
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.85       |
| time/                   |            |
|    total_timesteps      | 2925000    |
| train/                  |            |
|    approx_kl            | 0.01646145 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.15      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.906      |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.00547   |
|    value_loss           | 8.41       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.45    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 179      |
|    time_elapsed    | 9833     |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.51       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 180         |
|    time_elapsed         | 9882        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.018799782 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0451      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 6.2         |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=3.86 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.86        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.015706908 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.266       |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 181      |
|    time_elapsed    | 9940     |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=5.28 +/- 1.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.28         |
| time/                   |              |
|    total_timesteps      | 2975000      |
| train/                  |              |
|    approx_kl            | 0.0045886384 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.749        |
|    learning_rate        | 0.0003       |
|    loss                 | 147          |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 334          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -326     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 182      |
|    time_elapsed    | 9998     |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -324        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 183         |
|    time_elapsed         | 10046       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.015776262 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.0011      |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=4.20 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.022253081 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 1.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -334     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 184      |
|    time_elapsed    | 10104    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=5.26 +/- 2.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.26        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.021491501 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.000884    |
|    value_loss           | 40.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 185      |
|    time_elapsed    | 10162    |
|    total_timesteps | 3031040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -23.6      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 186        |
|    time_elapsed         | 10211      |
|    total_timesteps      | 3047424    |
| train/                  |            |
|    approx_kl            | 0.02982085 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.32      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.8       |
|    n_updates            | 1850       |
|    policy_gradient_loss | 0.000811   |
|    value_loss           | 56.3       |
----------------------------------------
Eval num_timesteps=3050000, episode_reward=6.38 +/- 4.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.38        |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.013424768 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.757       |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 2.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 187      |
|    time_elapsed    | 10269    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=4.17 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.17        |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.019960485 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.79        |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 188      |
|    time_elapsed    | 10327    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -207        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 189         |
|    time_elapsed         | 10376       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.018916108 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 471         |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.00157     |
|    value_loss           | 441         |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=3.56 +/- 0.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.56       |
| time/                   |            |
|    total_timesteps      | 3100000    |
| train/                  |            |
|    approx_kl            | 0.03370039 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 918        |
|    n_updates            | 1890       |
|    policy_gradient_loss | 0.000257   |
|    value_loss           | 1.75e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 190      |
|    time_elapsed    | 10433    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=4.70 +/- 1.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.7         |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.014299893 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -371     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 191      |
|    time_elapsed    | 10491    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 192         |
|    time_elapsed         | 10540       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.011020238 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 369         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 463         |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=4.02 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.022157773 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.285       |
|    n_updates            | 1920        |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 1.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -234     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 193      |
|    time_elapsed    | 10598    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=3.40 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.4        |
| time/                   |            |
|    total_timesteps      | 3175000    |
| train/                  |            |
|    approx_kl            | 0.01971417 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.13       |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.00416   |
|    value_loss           | 550        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 194      |
|    time_elapsed    | 10656    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -96.9       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 195         |
|    time_elapsed         | 10705       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.021111432 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.346       |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 44.2        |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=4.60 +/- 1.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.010692192 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 20.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 196      |
|    time_elapsed    | 10763    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=4.74 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.74        |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.022088625 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.3        |
|    n_updates            | 1960        |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 32.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 197      |
|    time_elapsed    | 10821    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -95.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 198         |
|    time_elapsed         | 10869       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.015177159 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 42.2        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=3.56 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.56        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.014500322 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 1980        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 71.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -95.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 199      |
|    time_elapsed    | 10927    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=3.14 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.14        |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.018538585 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 7.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 200      |
|    time_elapsed    | 10985    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -32.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 201         |
|    time_elapsed         | 11034       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.008584575 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 198         |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=-1168.14 +/- 2349.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.17e+03  |
| time/                   |            |
|    total_timesteps      | 3300000    |
| train/                  |            |
|    approx_kl            | 0.03139118 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.1        |
|    n_updates            | 2010       |
|    policy_gradient_loss | 0.00641    |
|    value_loss           | 10.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 202      |
|    time_elapsed    | 11092    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=3.60 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.6         |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.022895055 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.1         |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0049     |
|    value_loss           | 13.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 203      |
|    time_elapsed    | 11150    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.16       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 204         |
|    time_elapsed         | 11198       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.020305526 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00916     |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 2.53        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=4.86 +/- 2.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.86        |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.009038791 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 26.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.26    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 205      |
|    time_elapsed    | 11256    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=3.62 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.62        |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.016174192 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 6.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.57    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 206      |
|    time_elapsed    | 11314    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 207         |
|    time_elapsed         | 11363       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.021551415 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 5.97        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=4.70 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.7         |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.015615042 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 76.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 208      |
|    time_elapsed    | 11421    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 209         |
|    time_elapsed         | 11470       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.019452684 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.482       |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=5.43 +/- 1.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.43        |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.016049227 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.309       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 22.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 210      |
|    time_elapsed    | 11528    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=5.37 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.37        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.015623529 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.681       |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 9.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 211      |
|    time_elapsed    | 11586    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 212         |
|    time_elapsed         | 11635       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.011945194 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 55.4        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=3.48 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.019154329 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0385      |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00696    |
|    value_loss           | 1.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 213      |
|    time_elapsed    | 11693    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=2.92 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.92         |
| time/                   |              |
|    total_timesteps      | 3500000      |
| train/                  |              |
|    approx_kl            | 0.0150043145 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.804        |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00811     |
|    value_loss           | 5.81         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 214      |
|    time_elapsed    | 11751    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.85       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 215         |
|    time_elapsed         | 11800       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.014497941 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.55        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 64.1        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=3.85 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.85        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.019301053 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.373       |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 1.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 216      |
|    time_elapsed    | 11858    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=4.66 +/- 2.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.66       |
| time/                   |            |
|    total_timesteps      | 3550000    |
| train/                  |            |
|    approx_kl            | 0.02132895 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.469      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.45       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.00396   |
|    value_loss           | 9.43       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 217      |
|    time_elapsed    | 11915    |
|    total_timesteps | 3555328  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -120         |
| time/                   |              |
|    fps                  | 298          |
|    iterations           | 218          |
|    time_elapsed         | 11964        |
|    total_timesteps      | 3571712      |
| train/                  |              |
|    approx_kl            | 0.0067948895 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.0003       |
|    loss                 | 412          |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 669          |
------------------------------------------
Eval num_timesteps=3575000, episode_reward=3.54 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.54        |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.017819881 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.735       |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 22          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 219      |
|    time_elapsed    | 12022    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=2.65 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.65        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.012111886 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.1        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 224         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 220      |
|    time_elapsed    | 12080    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -74.1      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 221        |
|    time_elapsed         | 12129      |
|    total_timesteps      | 3620864    |
| train/                  |            |
|    approx_kl            | 0.01867726 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.1       |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.11       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.00433   |
|    value_loss           | 55.5       |
----------------------------------------
Eval num_timesteps=3625000, episode_reward=3.31 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.31        |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.019108715 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.83        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 19.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 222      |
|    time_elapsed    | 12187    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=7.50 +/- 4.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.5         |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.016074646 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 2.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 223      |
|    time_elapsed    | 12245    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 224         |
|    time_elapsed         | 12294       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.016150026 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.22        |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 23          |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=4.64 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.64        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.016765626 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.7        |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 53.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 225      |
|    time_elapsed    | 12352    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=3.73 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.73        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.018167028 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 12.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 226      |
|    time_elapsed    | 12409    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -192        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 227         |
|    time_elapsed         | 12458       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.012864524 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00342    |
|    value_loss           | 302         |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=2.96 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.013808901 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 228      |
|    time_elapsed    | 12516    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=4.02 +/- 2.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.019002154 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.01        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 81.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 229      |
|    time_elapsed    | 12574    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -94         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 230         |
|    time_elapsed         | 12623       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.018188974 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 1.71        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=4.88 +/- 2.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.88        |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.008294819 |
|    clip_fraction        | 0.0619      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0003      |
|    loss                 | 351         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 439         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 231      |
|    time_elapsed    | 12681    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=3.64 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.64        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.017306678 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 16.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 232      |
|    time_elapsed    | 12739    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 233         |
|    time_elapsed         | 12788       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.019435557 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.89        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 5.57        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=4.43 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.43        |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.016309783 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.67        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 12.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 234      |
|    time_elapsed    | 12845    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2.96 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.017574629 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 6.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.72    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 235      |
|    time_elapsed    | 12903    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.53       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 236         |
|    time_elapsed         | 12952       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.017737977 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.36        |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 3.43        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=4.28 +/- 1.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.28       |
| time/                   |            |
|    total_timesteps      | 3875000    |
| train/                  |            |
|    approx_kl            | 0.01995628 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.26       |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.00879   |
|    value_loss           | 6.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.45    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 237      |
|    time_elapsed    | 13010    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.35       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 238         |
|    time_elapsed         | 13059       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.017730221 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.4         |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 8.91        |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=5.29 +/- 1.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.29        |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.021357441 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0644      |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.691       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.3     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 239      |
|    time_elapsed    | 13117    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=3.63 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.63        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.017399587 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0385      |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 1.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.83    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 240      |
|    time_elapsed    | 13175    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.14       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 241         |
|    time_elapsed         | 13224       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.015582301 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 16.2        |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=6.13 +/- 1.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.13        |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.017974367 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 1.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.0297  |
| time/              |          |
|    fps             | 298      |
|    iterations      | 242      |
|    time_elapsed    | 13282    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=4.87 +/- 1.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.87        |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.017283078 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.655       |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 2.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 243      |
|    time_elapsed    | 13339    |
|    total_timesteps | 3981312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -31.4     |
| time/                   |           |
|    fps                  | 298       |
|    iterations           | 244       |
|    time_elapsed         | 13388     |
|    total_timesteps      | 3997696   |
| train/                  |           |
|    approx_kl            | 0.0180518 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.04     |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.167     |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.00363  |
|    value_loss           | 1.52      |
---------------------------------------
Eval num_timesteps=4000000, episode_reward=3.90 +/- 0.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.9        |
| time/                   |            |
|    total_timesteps      | 4000000    |
| train/                  |            |
|    approx_kl            | 0.00975005 |
|    clip_fraction        | 0.069      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.02       |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0013    |
|    value_loss           | 337        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 245      |
|    time_elapsed    | 13446    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=6.98 +/- 3.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.98         |
| time/                   |              |
|    total_timesteps      | 4025000      |
| train/                  |              |
|    approx_kl            | 0.0053406674 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.0003       |
|    loss                 | 322          |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00243     |
|    value_loss           | 388          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 246      |
|    time_elapsed    | 13504    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -196        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 247         |
|    time_elapsed         | 13553       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.015468981 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.7        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 106         |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=3.75 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.75        |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.016821623 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.4        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00295    |
|    value_loss           | 229         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 248      |
|    time_elapsed    | 13611    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=5.78 +/- 2.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.78        |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.020939993 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.248       |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.000215   |
|    value_loss           | 6.15        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 249      |
|    time_elapsed    | 13668    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -99.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 250         |
|    time_elapsed         | 13717       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.018324217 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+03     |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=7.92 +/- 4.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.92        |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.017869161 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0803      |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 1.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 251      |
|    time_elapsed    | 13775    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=4.75 +/- 1.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.75        |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.023684591 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00085    |
|    value_loss           | 99.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 252      |
|    time_elapsed    | 13833    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 253         |
|    time_elapsed         | 13882       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.030709054 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 26.6        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 65.3        |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=5.11 +/- 1.94
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.11       |
| time/                   |            |
|    total_timesteps      | 4150000    |
| train/                  |            |
|    approx_kl            | 0.02101367 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.24       |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 1.91       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 254      |
|    time_elapsed    | 13940    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=7.00 +/- 2.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7           |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.012106739 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.7        |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 91.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 255      |
|    time_elapsed    | 13998    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.9       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 256         |
|    time_elapsed         | 14046       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.021497753 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 36.9        |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=8.07 +/- 4.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.07        |
| time/                   |             |
|    total_timesteps      | 4200000     |
| train/                  |             |
|    approx_kl            | 0.018363481 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.57        |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 7.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 257      |
|    time_elapsed    | 14104    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=7.14 +/- 3.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.14        |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.017149182 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 69.4        |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 49.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 258      |
|    time_elapsed    | 14162    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -61.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 259         |
|    time_elapsed         | 14211       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.017581398 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | -0.096      |
|    learning_rate        | 0.0003      |
|    loss                 | 70.1        |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=5.21 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.21        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.012886135 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0003      |
|    loss                 | 612         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 226         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 260      |
|    time_elapsed    | 14269    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=4.65 +/- 1.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.65        |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.017857816 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 21.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 261      |
|    time_elapsed    | 14327    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -152        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 262         |
|    time_elapsed         | 14376       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.028328735 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.5        |
|    n_updates            | 2610        |
|    policy_gradient_loss | 0.000754    |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=9.35 +/- 5.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.35        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.021652287 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.0451      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.66        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 25.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -240     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 263      |
|    time_elapsed    | 14434    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=7.79 +/- 3.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.79        |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.016106274 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.2        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 264      |
|    time_elapsed    | 14492    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -210        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 265         |
|    time_elapsed         | 14540       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.016472274 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.4        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 112         |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=5.84 +/- 2.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.84        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.012149245 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 199         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 266      |
|    time_elapsed    | 14598    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -76.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 267         |
|    time_elapsed         | 14647       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.018961344 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 8.51        |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=6.13 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.13        |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.017136266 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.17        |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 9.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 268      |
|    time_elapsed    | 14705    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=1.87 +/- 3.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 4400000    |
| train/                  |            |
|    approx_kl            | 0.01717323 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.3       |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.000612  |
|    value_loss           | 114        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -76.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 269      |
|    time_elapsed    | 14763    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -77.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 270         |
|    time_elapsed         | 14812       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.024225231 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 49.1        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=5.09 +/- 2.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.09        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.022177022 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 9.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 271      |
|    time_elapsed    | 14869    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=6.55 +/- 3.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.55        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.016004976 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 2.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.24    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 272      |
|    time_elapsed    | 14927    |
|    total_timesteps | 4456448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11        |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 273        |
|    time_elapsed         | 14976      |
|    total_timesteps      | 4472832    |
| train/                  |            |
|    approx_kl            | 0.01206653 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.00586   |
|    value_loss           | 7.45       |
----------------------------------------
Eval num_timesteps=4475000, episode_reward=-107.56 +/- 229.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -108        |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.014037887 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.21        |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 274      |
|    time_elapsed    | 15034    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=4.91 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.91        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.020863514 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.75        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 46.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 275      |
|    time_elapsed    | 15092    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 276         |
|    time_elapsed         | 15141       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.018065441 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.38        |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 5.36        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=6.68 +/- 2.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.68        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.011909341 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 61.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 277      |
|    time_elapsed    | 15199    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=5.58 +/- 1.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.58        |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.017990278 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 26.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 278      |
|    time_elapsed    | 15257    |
|    total_timesteps | 4554752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -32.3      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 279        |
|    time_elapsed         | 15305      |
|    total_timesteps      | 4571136    |
| train/                  |            |
|    approx_kl            | 0.01742637 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 48.6       |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.00102   |
|    value_loss           | 109        |
----------------------------------------
Eval num_timesteps=4575000, episode_reward=7.72 +/- 4.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.72        |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.019752627 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 32.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 280      |
|    time_elapsed    | 15363    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=6.32 +/- 2.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.32        |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.019029029 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 7.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 281      |
|    time_elapsed    | 15421    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 282         |
|    time_elapsed         | 15470       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.016832173 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.556       |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 14.4        |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=6.88 +/- 2.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.88        |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.020083673 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 2.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.69    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 283      |
|    time_elapsed    | 15528    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=7.36 +/- 3.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.36        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.020787481 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.05        |
|    n_updates            | 2830        |
|    policy_gradient_loss | 4.57e-05    |
|    value_loss           | 2.42        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 284      |
|    time_elapsed    | 15586    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -141        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 285         |
|    time_elapsed         | 15635       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.024253309 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.567       |
|    n_updates            | 2840        |
|    policy_gradient_loss | 0.000536    |
|    value_loss           | 64.5        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=7.63 +/- 4.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.63        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.018856935 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 0.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 286      |
|    time_elapsed    | 15693    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=7.13 +/- 3.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.13        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.010919498 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 287      |
|    time_elapsed    | 15751    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.2        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 288         |
|    time_elapsed         | 15800       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.018211886 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 4.43        |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=5.74 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.74        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.017414283 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 21.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.33    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 289      |
|    time_elapsed    | 15857    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=6.61 +/- 2.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.61        |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.022238327 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.904       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 6.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 290      |
|    time_elapsed    | 15915    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 291         |
|    time_elapsed         | 15964       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.016035419 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.7        |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 45.6        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=7.54 +/- 1.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.54        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.017258637 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 9.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 292      |
|    time_elapsed    | 16022    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=7.27 +/- 2.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.27       |
| time/                   |            |
|    total_timesteps      | 4800000    |
| train/                  |            |
|    approx_kl            | 0.01675513 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.62       |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.00422   |
|    value_loss           | 94.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -96.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 293      |
|    time_elapsed    | 16080    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -87.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 294         |
|    time_elapsed         | 16129       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.020769723 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.000743   |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=6.66 +/- 2.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.66        |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.023198146 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 57.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 295      |
|    time_elapsed    | 16187    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.47       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 296         |
|    time_elapsed         | 16236       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.017354615 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 3.03        |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=7.41 +/- 3.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.41       |
| time/                   |            |
|    total_timesteps      | 4850000    |
| train/                  |            |
|    approx_kl            | 0.02043483 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.137      |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.000765  |
|    value_loss           | 1.08       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.42    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 297      |
|    time_elapsed    | 16294    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=6.85 +/- 2.70
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 6.85      |
| time/                   |           |
|    total_timesteps      | 4875000   |
| train/                  |           |
|    approx_kl            | 0.0163637 |
|    clip_fraction        | 0.103     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.63     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 2970      |
|    policy_gradient_loss | -0.00229  |
|    value_loss           | 23.3      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.436    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 298      |
|    time_elapsed    | 16351    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.064      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 299         |
|    time_elapsed         | 16400       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.021136805 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.000561   |
|    value_loss           | 8.1         |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=6.99 +/- 2.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.99        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.019105773 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 6.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.61    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 300      |
|    time_elapsed    | 16458    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=7.22 +/- 3.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.22        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.018062111 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.1        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 15.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 301      |
|    time_elapsed    | 16516    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -63.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 302         |
|    time_elapsed         | 16565       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.020921491 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.38        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 441         |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=6.52 +/- 2.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.52        |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.018828603 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.65        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 3.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 303      |
|    time_elapsed    | 16623    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=6.85 +/- 1.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.85        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.017987173 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.759       |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 30          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 304      |
|    time_elapsed    | 16681    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.17       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 305         |
|    time_elapsed         | 16729       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.019517604 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.762       |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 16.7        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=-2251.96 +/- 4515.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.25e+03   |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.015039515 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.058       |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 15          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 306      |
|    time_elapsed    | 16787    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=6.19 +/- 3.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.19        |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.009962717 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 307      |
|    time_elapsed    | 16845    |
|    total_timesteps | 5029888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -22.9      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 308        |
|    time_elapsed         | 16894      |
|    total_timesteps      | 5046272    |
| train/                  |            |
|    approx_kl            | 0.01908696 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.56      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.01       |
|    n_updates            | 3070       |
|    policy_gradient_loss | -0.00625   |
|    value_loss           | 12.4       |
----------------------------------------
Eval num_timesteps=5050000, episode_reward=6.18 +/- 2.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.18       |
| time/                   |            |
|    total_timesteps      | 5050000    |
| train/                  |            |
|    approx_kl            | 0.01701805 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.66      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.12       |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.00612   |
|    value_loss           | 7.3        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 309      |
|    time_elapsed    | 16952    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=7.87 +/- 2.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.87       |
| time/                   |            |
|    total_timesteps      | 5075000    |
| train/                  |            |
|    approx_kl            | 0.01796962 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.07       |
|    n_updates            | 3090       |
|    policy_gradient_loss | -0.000964  |
|    value_loss           | 13.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 310      |
|    time_elapsed    | 17010    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.98       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 311         |
|    time_elapsed         | 17059       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.011700087 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 44.8        |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=6.01 +/- 1.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.01       |
| time/                   |            |
|    total_timesteps      | 5100000    |
| train/                  |            |
|    approx_kl            | 0.02004395 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.11      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.208      |
|    n_updates            | 3110       |
|    policy_gradient_loss | -0.00462   |
|    value_loss           | 5.12       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.49    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 312      |
|    time_elapsed    | 17117    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=4.31 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.31        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.019631444 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0568      |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0073     |
|    value_loss           | 0.412       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 313      |
|    time_elapsed    | 17175    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 314         |
|    time_elapsed         | 17224       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.019744352 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.0032      |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=3.71 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.71        |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.019193111 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.373       |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 315      |
|    time_elapsed    | 17282    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=3.58 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.58        |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.019437416 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.00787    |
|    value_loss           | 2.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 316      |
|    time_elapsed    | 17340    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 317         |
|    time_elapsed         | 17388       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.009118348 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 37          |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 29.7        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=5.55 +/- 1.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.55        |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.021801312 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.2        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.00511    |
|    value_loss           | 55.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 318      |
|    time_elapsed    | 17446    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=3.10 +/- 0.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.1        |
| time/                   |            |
|    total_timesteps      | 5225000    |
| train/                  |            |
|    approx_kl            | 0.02011723 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.261      |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.00624   |
|    value_loss           | 13.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 319      |
|    time_elapsed    | 17504    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.12       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 320         |
|    time_elapsed         | 17553       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.015837844 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.82        |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 25.9        |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=5.38 +/- 1.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.019095797 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 1.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.37    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 321      |
|    time_elapsed    | 17611    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=4.59 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.59        |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.017371472 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 1.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.66    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 322      |
|    time_elapsed    | 17669    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.95        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 323         |
|    time_elapsed         | 17718       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.019807229 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 1.86        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=5.45 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.45        |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.017933689 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0277      |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.183       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 324      |
|    time_elapsed    | 17776    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.4         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 325         |
|    time_elapsed         | 17825       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.018071325 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 5.65        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=7.23 +/- 3.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.23       |
| time/                   |            |
|    total_timesteps      | 5325000    |
| train/                  |            |
|    approx_kl            | 0.01773212 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0393     |
|    n_updates            | 3250       |
|    policy_gradient_loss | -0.00807   |
|    value_loss           | 0.351      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.47    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 326      |
|    time_elapsed    | 17883    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=8.32 +/- 4.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 8.32         |
| time/                   |              |
|    total_timesteps      | 5350000      |
| train/                  |              |
|    approx_kl            | 0.0071721803 |
|    clip_fraction        | 0.0716       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.98         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 38.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.9     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 327      |
|    time_elapsed    | 17940    |
|    total_timesteps | 5357568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.01      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 328        |
|    time_elapsed         | 17989      |
|    total_timesteps      | 5373952    |
| train/                  |            |
|    approx_kl            | 0.02069465 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0388     |
|    n_updates            | 3270       |
|    policy_gradient_loss | 9.25e-05   |
|    value_loss           | 5.59       |
----------------------------------------
Eval num_timesteps=5375000, episode_reward=4.19 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.19        |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.019407634 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0939      |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 10.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.03    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 329      |
|    time_elapsed    | 18047    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=6.05 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.05        |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.019731551 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.83        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 0.411       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.46    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 330      |
|    time_elapsed    | 18105    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.79       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 331         |
|    time_elapsed         | 18154       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.019932231 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.0495      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0481      |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 65.4        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=4.58 +/- 1.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.58        |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.023789283 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.56        |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 3.75        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.82    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 332      |
|    time_elapsed    | 18212    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=5.16 +/- 3.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.16        |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.020768333 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 2.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.36    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 333      |
|    time_elapsed    | 18270    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.3         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 334         |
|    time_elapsed         | 18318       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.021157037 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0312      |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 0.556       |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=6.08 +/- 1.72
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.08       |
| time/                   |            |
|    total_timesteps      | 5475000    |
| train/                  |            |
|    approx_kl            | 0.01683671 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.97      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.148      |
|    n_updates            | 3340       |
|    policy_gradient_loss | -0.0024    |
|    value_loss           | 0.283      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 6.76     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 335      |
|    time_elapsed    | 18376    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=5.45 +/- 2.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.45        |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.018735602 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0283      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 0.0787      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.81     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 336      |
|    time_elapsed    | 18434    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.58        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 337         |
|    time_elapsed         | 18483       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.016072858 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0951      |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 16.2        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=6.58 +/- 2.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.58        |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.015204899 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.9        |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 10.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.27    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 338      |
|    time_elapsed    | 18541    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=5.23 +/- 1.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.23        |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.019330569 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 339      |
|    time_elapsed    | 18599    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.5       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 340         |
|    time_elapsed         | 18648       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.009535707 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 113         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=5.34 +/- 1.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.34        |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.018904913 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 1.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 341      |
|    time_elapsed    | 18706    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=4.97 +/- 1.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.97        |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.018262947 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0764      |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 1.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 342      |
|    time_elapsed    | 18764    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -34.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 343         |
|    time_elapsed         | 18813       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.007506597 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.73        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -3.53e-05   |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=5.52 +/- 2.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.52       |
| time/                   |            |
|    total_timesteps      | 5625000    |
| train/                  |            |
|    approx_kl            | 0.02156428 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.45      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.69       |
|    n_updates            | 3430       |
|    policy_gradient_loss | -0.0028    |
|    value_loss           | 109        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 344      |
|    time_elapsed    | 18871    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=8.08 +/- 4.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.08        |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.020590607 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 345      |
|    time_elapsed    | 18929    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -94.5       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 346         |
|    time_elapsed         | 18977       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.023911815 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.2        |
|    n_updates            | 3450        |
|    policy_gradient_loss | 0.00366     |
|    value_loss           | 345         |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=5.56 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.56        |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.021351118 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.75        |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 19.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 347      |
|    time_elapsed    | 19035    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=5.64 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.64        |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.019431923 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.327       |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 2.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 348      |
|    time_elapsed    | 19093    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -128        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 349         |
|    time_elapsed         | 19142       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.021878164 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.0638      |
|    learning_rate        | 0.0003      |
|    loss                 | 189         |
|    n_updates            | 3480        |
|    policy_gradient_loss | 0.00224     |
|    value_loss           | 390         |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=7.44 +/- 1.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.44        |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.023852069 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 261         |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.000842   |
|    value_loss           | 249         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 350      |
|    time_elapsed    | 19200    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=6.55 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.55        |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.014638459 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 57.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -99      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 351      |
|    time_elapsed    | 19258    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -80.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 352         |
|    time_elapsed         | 19307       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.024456853 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 50.8        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=6.04 +/- 3.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.04        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.018989563 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.3         |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 353      |
|    time_elapsed    | 19365    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.15       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 354         |
|    time_elapsed         | 19414       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.018023008 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.412       |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 11          |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=6.03 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.03        |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.018191513 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0823      |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.43    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 355      |
|    time_elapsed    | 19472    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=8.76 +/- 2.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.76        |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.016016841 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 5.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.27    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 356      |
|    time_elapsed    | 19530    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.23       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 357         |
|    time_elapsed         | 19579       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.017098684 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 9.09        |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=7.74 +/- 4.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.74        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.017963827 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 0.408       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.3     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 358      |
|    time_elapsed    | 19637    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=5.92 +/- 3.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.92        |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.012957986 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.216       |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 52          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.77    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 359      |
|    time_elapsed    | 19695    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.99       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 360         |
|    time_elapsed         | 19744       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.018242488 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 1.51        |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=10.81 +/- 2.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.013899401 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 302         |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 47.3        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.14    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 361      |
|    time_elapsed    | 19801    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=9.29 +/- 3.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.29        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.018144764 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0368      |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 0.861       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.68    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 362      |
|    time_elapsed    | 19859    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.46       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 363         |
|    time_elapsed         | 19908       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.012201723 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.147       |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=11.04 +/- 2.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 11          |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.024258796 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0425      |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 2.49        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.04    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 364      |
|    time_elapsed    | 19966    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=8.26 +/- 3.76
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 8.26       |
| time/                   |            |
|    total_timesteps      | 5975000    |
| train/                  |            |
|    approx_kl            | 0.02001999 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.7       |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 25.6       |
|    n_updates            | 3640       |
|    policy_gradient_loss | 0.000654   |
|    value_loss           | 4.95       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.34    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 365      |
|    time_elapsed    | 20024    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.18       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 366         |
|    time_elapsed         | 20073       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.019026037 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.475       |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 19.4        |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=10.86 +/- 4.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 10.9       |
| time/                   |            |
|    total_timesteps      | 6000000    |
| train/                  |            |
|    approx_kl            | 0.02016495 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.54      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.739      |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.00656   |
|    value_loss           | 5.43       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.17    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 367      |
|    time_elapsed    | 20131    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=6.36 +/- 3.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.36        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.014657928 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.857       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 19.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.08    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 368      |
|    time_elapsed    | 20189    |
|    total_timesteps | 6029312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -4.15     |
| time/                   |           |
|    fps                  | 298       |
|    iterations           | 369       |
|    time_elapsed         | 20238     |
|    total_timesteps      | 6045696   |
| train/                  |           |
|    approx_kl            | 0.0205832 |
|    clip_fraction        | 0.251     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.44     |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0549    |
|    n_updates            | 3680      |
|    policy_gradient_loss | -0.00945  |
|    value_loss           | 0.151     |
---------------------------------------
Eval num_timesteps=6050000, episode_reward=5.80 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.016808134 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.197       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 2.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.22    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 370      |
|    time_elapsed    | 20296    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=6.14 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.14        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.017715266 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 0.971       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 371      |
|    time_elapsed    | 20354    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.78        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 372         |
|    time_elapsed         | 20403       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.020388555 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 2.62        |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=5.01 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.01        |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.019367237 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00738    |
|    value_loss           | 0.817       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 6.8      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 373      |
|    time_elapsed    | 20461    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=6.17 +/- 1.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.17        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.018091993 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0102      |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 1.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -61.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 374      |
|    time_elapsed    | 20519    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 375         |
|    time_elapsed         | 20568       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.007254581 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=6.00 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6           |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.019421268 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 64.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -71.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 376      |
|    time_elapsed    | 20625    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=4.24 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.24        |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.023643361 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 3760        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 5.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 377      |
|    time_elapsed    | 20683    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -47.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 378         |
|    time_elapsed         | 20732       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.010425148 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 29.3        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=5.43 +/- 2.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.43        |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.009944891 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.3        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 526         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 379      |
|    time_elapsed    | 20790    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=5.56 +/- 2.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.56        |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.031717204 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.4        |
|    n_updates            | 3790        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 380      |
|    time_elapsed    | 20848    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -71.4       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 381         |
|    time_elapsed         | 20897       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.020927921 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.234       |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 15.4        |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=4.16 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.16        |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.019089244 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.52        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 382      |
|    time_elapsed    | 20955    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=3.72 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.72        |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.021728609 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.4         |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 4.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.0573  |
| time/              |          |
|    fps             | 298      |
|    iterations      | 383      |
|    time_elapsed    | 21013    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.68       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 384         |
|    time_elapsed         | 21062       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.019942496 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.03       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.52        |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 6.42        |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=3.42 +/- 1.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.42        |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.020111661 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.572       |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 33.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 385      |
|    time_elapsed    | 21120    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.5       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 386         |
|    time_elapsed         | 21168       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.013621504 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.36        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00637    |
|    value_loss           | 27.1        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=5.30 +/- 3.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.3         |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.019738635 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0709      |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 9.15        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 387      |
|    time_elapsed    | 21226    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=3.90 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.9         |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.016046494 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.363       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 18.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 388      |
|    time_elapsed    | 21284    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.9        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 389         |
|    time_elapsed         | 21333       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.020619186 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.717       |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 28.3        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=3.77 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.77        |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.018766388 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.378       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 4.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.88    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 390      |
|    time_elapsed    | 21391    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=4.90 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.023567855 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.58        |
|    n_updates            | 3900        |
|    policy_gradient_loss | 0.000401    |
|    value_loss           | 5.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.83    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 391      |
|    time_elapsed    | 21449    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.186       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 392         |
|    time_elapsed         | 21498       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.017112952 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.391       |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 3.73        |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=9.88 +/- 4.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.88        |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.017303616 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 3.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 393      |
|    time_elapsed    | 21556    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=4.25 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.25        |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.010028457 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 70.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 394      |
|    time_elapsed    | 21614    |
|    total_timesteps | 6455296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.2      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 395        |
|    time_elapsed         | 21663      |
|    total_timesteps      | 6471680    |
| train/                  |            |
|    approx_kl            | 0.01983963 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.46      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.22       |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00934   |
|    value_loss           | 4.8        |
----------------------------------------
Eval num_timesteps=6475000, episode_reward=4.03 +/- 1.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.03       |
| time/                   |            |
|    total_timesteps      | 6475000    |
| train/                  |            |
|    approx_kl            | 0.02018135 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 3950       |
|    policy_gradient_loss | -0.00915   |
|    value_loss           | 0.584      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 396      |
|    time_elapsed    | 21721    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=5.78 +/- 3.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.78        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.021558646 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00448    |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.151       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.458   |
| time/              |          |
|    fps             | 298      |
|    iterations      | 397      |
|    time_elapsed    | 21779    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.472      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 398         |
|    time_elapsed         | 21827       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.014630647 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.159       |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 8.24        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=6.45 +/- 2.24
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 6.45      |
| time/                   |           |
|    total_timesteps      | 6525000   |
| train/                  |           |
|    approx_kl            | 0.0203658 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.95     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.233     |
|    n_updates            | 3980      |
|    policy_gradient_loss | -0.00966  |
|    value_loss           | 7.88      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3       |
| time/              |          |
|    fps             | 298      |
|    iterations      | 399      |
|    time_elapsed    | 21885    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=8.25 +/- 2.41
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 8.25      |
| time/                   |           |
|    total_timesteps      | 6550000   |
| train/                  |           |
|    approx_kl            | 0.0241124 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.58     |
|    explained_variance   | 0.125     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0786    |
|    n_updates            | 3990      |
|    policy_gradient_loss | -0.00497  |
|    value_loss           | 33.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 400      |
|    time_elapsed    | 21943    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -21.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 401         |
|    time_elapsed         | 21992       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.014081511 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | -0.298      |
|    learning_rate        | 0.0003      |
|    loss                 | 35.1        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 157         |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=8.94 +/- 5.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.94        |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.021968285 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.977       |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 37.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 402      |
|    time_elapsed    | 22050    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=7.18 +/- 2.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.18        |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.018805414 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.2        |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 1.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 403      |
|    time_elapsed    | 22108    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.903       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 404         |
|    time_elapsed         | 22157       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.020278739 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.000108   |
|    value_loss           | 6.64        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=7.60 +/- 3.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 6625000    |
| train/                  |            |
|    approx_kl            | 0.01958182 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.24      |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.587      |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.00469   |
|    value_loss           | 13.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 405      |
|    time_elapsed    | 22215    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=8.75 +/- 4.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.75        |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.019661022 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.6        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 24          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 406      |
|    time_elapsed    | 22273    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.791      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 407         |
|    time_elapsed         | 22322       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.017580982 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 1.57        |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=5.41 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.41        |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.013429729 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.81        |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 30.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.11    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 408      |
|    time_elapsed    | 22380    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=5.49 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.49        |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.021126617 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.44        |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 16.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.78    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 409      |
|    time_elapsed    | 22438    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.71       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 410         |
|    time_elapsed         | 22487       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.018059118 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 10.2        |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=5.89 +/- 1.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.89        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.021587007 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.31       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 4100        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 64.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.2     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 411      |
|    time_elapsed    | 22545    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=5.72 +/- 3.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.72        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.019854218 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.192       |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 412      |
|    time_elapsed    | 22603    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 413         |
|    time_elapsed         | 22652       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.009620313 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | 516         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=10.08 +/- 6.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.027968487 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 2.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 414      |
|    time_elapsed    | 22710    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 415         |
|    time_elapsed         | 22759       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.025975857 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 31.8        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=5.74 +/- 3.93
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.74       |
| time/                   |            |
|    total_timesteps      | 6800000    |
| train/                  |            |
|    approx_kl            | 0.01876583 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.08      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.243      |
|    n_updates            | 4150       |
|    policy_gradient_loss | -0.00973   |
|    value_loss           | 5.94       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 416      |
|    time_elapsed    | 22817    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=7.27 +/- 5.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.27        |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.020661078 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 387         |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.000782   |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 417      |
|    time_elapsed    | 22875    |
|    total_timesteps | 6832128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 418        |
|    time_elapsed         | 22923      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.02376898 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.59      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 4170       |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 2.56       |
----------------------------------------
Eval num_timesteps=6850000, episode_reward=6.59 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.59        |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.019472415 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 419      |
|    time_elapsed    | 22981    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=8.32 +/- 2.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.32        |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.018685956 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.0976      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0309      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 420      |
|    time_elapsed    | 23039    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -74.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 421         |
|    time_elapsed         | 23088       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.010087615 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | -0.0146     |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=7.99 +/- 3.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.99       |
| time/                   |            |
|    total_timesteps      | 6900000    |
| train/                  |            |
|    approx_kl            | 0.02049267 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.96      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 130        |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.00794   |
|    value_loss           | 59.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 422      |
|    time_elapsed    | 23146    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=7.45 +/- 2.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.45        |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.019747179 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 16.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 423      |
|    time_elapsed    | 23204    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.5       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 424         |
|    time_elapsed         | 23253       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.013185205 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0003      |
|    loss                 | 287         |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 326         |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=1.71 +/- 8.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.020614397 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.949       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 425      |
|    time_elapsed    | 23311    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=6.03 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.03        |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.017432017 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 1.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 426      |
|    time_elapsed    | 23369    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 427         |
|    time_elapsed         | 23418       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.019793676 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.332       |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 2.79        |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=4.28 +/- 1.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.28        |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.012596013 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.6        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.000168   |
|    value_loss           | 208         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 428      |
|    time_elapsed    | 23476    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=7.32 +/- 2.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.32        |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.025759963 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 1.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -55.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 429      |
|    time_elapsed    | 23534    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 430         |
|    time_elapsed         | 23583       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.016694523 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.91       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.124       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 2.62        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=6.70 +/- 1.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.7         |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.026938137 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.851       |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 53.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.67    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 431      |
|    time_elapsed    | 23641    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=5.86 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.86        |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.020229738 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00485    |
|    value_loss           | 1.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 432      |
|    time_elapsed    | 23699    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 433         |
|    time_elapsed         | 23747       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.032983318 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.227       |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 12          |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=7.21 +/- 3.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.21        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.018244855 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.333       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 35.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.183   |
| time/              |          |
|    fps             | 298      |
|    iterations      | 434      |
|    time_elapsed    | 23805    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=8.93 +/- 2.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.93        |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.024145804 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.201       |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 4.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.66    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 435      |
|    time_elapsed    | 23863    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -59.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 436         |
|    time_elapsed         | 23912       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.027920961 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 2.11        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=6.24 +/- 2.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.24        |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.008476354 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.17       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.8        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 341         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 437      |
|    time_elapsed    | 23970    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=6.01 +/- 1.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.01        |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.023255613 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    n_updates            | 4370        |
|    policy_gradient_loss | 0.000549    |
|    value_loss           | 8.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 438      |
|    time_elapsed    | 24028    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 439         |
|    time_elapsed         | 24077       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.010785593 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 969         |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=7.69 +/- 2.38
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.69       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.03376926 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.7       |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.00303   |
|    value_loss           | 217        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 440      |
|    time_elapsed    | 24135    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=5.77 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.77        |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.023131639 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0003      |
|    loss                 | 461         |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 137         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 441      |
|    time_elapsed    | 24192    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44.9       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 442         |
|    time_elapsed         | 24241       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.014140585 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 15.9        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=9.40 +/- 5.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.022239923 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 4.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 443      |
|    time_elapsed    | 24299    |
|    total_timesteps | 7258112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -138       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 444        |
|    time_elapsed         | 24348      |
|    total_timesteps      | 7274496    |
| train/                  |            |
|    approx_kl            | 0.02956833 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.39      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 4430       |
|    policy_gradient_loss | -0.00792   |
|    value_loss           | 133        |
----------------------------------------
Eval num_timesteps=7275000, episode_reward=6.08 +/- 2.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.08        |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.018814938 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.000377   |
|    value_loss           | 300         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 445      |
|    time_elapsed    | 24406    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=7.28 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.28        |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.016526531 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 6.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 446      |
|    time_elapsed    | 24464    |
|    total_timesteps | 7307264  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -36.9      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 447        |
|    time_elapsed         | 24513      |
|    total_timesteps      | 7323648    |
| train/                  |            |
|    approx_kl            | 0.02111634 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.38      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.409      |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.00582   |
|    value_loss           | 5.96       |
----------------------------------------
Eval num_timesteps=7325000, episode_reward=5.76 +/- 3.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.76        |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.026915371 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.000751   |
|    value_loss           | 89.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 448      |
|    time_elapsed    | 24570    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=7.31 +/- 1.80
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 7.31       |
| time/                   |            |
|    total_timesteps      | 7350000    |
| train/                  |            |
|    approx_kl            | 0.02101548 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.37      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.00285   |
|    value_loss           | 37.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 449      |
|    time_elapsed    | 24628    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -168        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 450         |
|    time_elapsed         | 24677       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.017242957 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.702       |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 17.9        |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=11.61 +/- 11.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.008919768 |
|    clip_fraction        | 0.064       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.000452   |
|    value_loss           | 110         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 451      |
|    time_elapsed    | 24735    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=5.18 +/- 6.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.18       |
| time/                   |            |
|    total_timesteps      | 7400000    |
| train/                  |            |
|    approx_kl            | 0.02203893 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.44      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 40.6       |
|    n_updates            | 4510       |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 45.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -249     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 452      |
|    time_elapsed    | 24793    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -264        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 453         |
|    time_elapsed         | 24842       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.013765938 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.27        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=3.63 +/- 0.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.63       |
| time/                   |            |
|    total_timesteps      | 7425000    |
| train/                  |            |
|    approx_kl            | 0.01921641 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 4530       |
|    policy_gradient_loss | -0.00431   |
|    value_loss           | 111        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 454      |
|    time_elapsed    | 24900    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=7.33 +/- 4.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.33        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.014014559 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.264       |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 30.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -94.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 455      |
|    time_elapsed    | 24958    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 456         |
|    time_elapsed         | 25007       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.016416267 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 275         |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 118         |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=8.50 +/- 1.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.5         |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.019575888 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.365       |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 77.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 457      |
|    time_elapsed    | 25065    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=6.19 +/- 1.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.19        |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.012577999 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.04       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.21        |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 58.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 458      |
|    time_elapsed    | 25123    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -29.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 459         |
|    time_elapsed         | 25172       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.017383542 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 42.8        |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 32.3        |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=5.81 +/- 2.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.81        |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.017950293 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.69        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 486         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 460      |
|    time_elapsed    | 25230    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=8.07 +/- 5.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.07        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.014957154 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00901    |
|    value_loss           | 44.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 461      |
|    time_elapsed    | 25288    |
|    total_timesteps | 7553024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -44.1        |
| time/                   |              |
|    fps                  | 298          |
|    iterations           | 462          |
|    time_elapsed         | 25336        |
|    total_timesteps      | 7569408      |
| train/                  |              |
|    approx_kl            | 0.0134381335 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.19        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 3.32         |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00416     |
|    value_loss           | 17.1         |
------------------------------------------
Eval num_timesteps=7575000, episode_reward=4.13 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.13        |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.016833164 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.7        |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 11.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.79    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 463      |
|    time_elapsed    | 25394    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=4.77 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.77        |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.016395211 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.652       |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 5.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.1     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 464      |
|    time_elapsed    | 25452    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 465         |
|    time_elapsed         | 25501       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.014438132 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 13.9        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=4.27 +/- 1.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.27        |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.018376816 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 27.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 466      |
|    time_elapsed    | 25559    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=8.96 +/- 10.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.96        |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.016162686 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.518       |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 3.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 467      |
|    time_elapsed    | 25617    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.98       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 468         |
|    time_elapsed         | 25666       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.014620015 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.402       |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 19.4        |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=3.34 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.016325656 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 8.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.89    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 469      |
|    time_elapsed    | 25724    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=5.00 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5           |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.019058153 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.713       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 18.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 470      |
|    time_elapsed    | 25782    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 471         |
|    time_elapsed         | 25830       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.013020406 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 23.8        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=3.46 +/- 2.06
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3.46      |
| time/                   |           |
|    total_timesteps      | 7725000   |
| train/                  |           |
|    approx_kl            | 0.0185852 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.68     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 4710      |
|    policy_gradient_loss | -0.00385  |
|    value_loss           | 6.78      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 472      |
|    time_elapsed    | 25888    |
|    total_timesteps | 7733248  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -3.1       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 473        |
|    time_elapsed         | 25937      |
|    total_timesteps      | 7749632    |
| train/                  |            |
|    approx_kl            | 0.02006407 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.364      |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.00463   |
|    value_loss           | 1.39       |
----------------------------------------
Eval num_timesteps=7750000, episode_reward=2.56 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.019160947 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.14        |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 2.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.88    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 474      |
|    time_elapsed    | 25995    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=3.15 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.15       |
| time/                   |            |
|    total_timesteps      | 7775000    |
| train/                  |            |
|    approx_kl            | 0.01590336 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.176      |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.00497   |
|    value_loss           | 4.27       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.16    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 475      |
|    time_elapsed    | 26053    |
|    total_timesteps | 7782400  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.84      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 476        |
|    time_elapsed         | 26102      |
|    total_timesteps      | 7798784    |
| train/                  |            |
|    approx_kl            | 0.02187457 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.58      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.00394   |
|    value_loss           | 2.93       |
----------------------------------------
Eval num_timesteps=7800000, episode_reward=3.43 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.019421585 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.3         |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 1.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.38    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 477      |
|    time_elapsed    | 26160    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=3.43 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.009564119 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 2.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.17    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 478      |
|    time_elapsed    | 26218    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.18       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 479         |
|    time_elapsed         | 26267       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.018341701 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 16          |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 7.47        |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=4.40 +/- 1.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.018088106 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.53        |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 13.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.36    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 480      |
|    time_elapsed    | 26325    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=-1173.05 +/- 2362.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.17e+03   |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.012558372 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.356       |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 14.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.11    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 481      |
|    time_elapsed    | 26383    |
|    total_timesteps | 7880704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.12      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 482        |
|    time_elapsed         | 26431      |
|    total_timesteps      | 7897088    |
| train/                  |            |
|    approx_kl            | 0.02163457 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.151      |
|    n_updates            | 4810       |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 1.72       |
----------------------------------------
Eval num_timesteps=7900000, episode_reward=4.92 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.92        |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.012372306 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 19.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.86    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 483      |
|    time_elapsed    | 26489    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=5.56 +/- 2.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.56        |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.021905985 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.361       |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 5.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.41    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 484      |
|    time_elapsed    | 26547    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 485         |
|    time_elapsed         | 26596       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.020747062 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.285       |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 26.7        |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=6.15 +/- 3.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.15        |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.010649237 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 486      |
|    time_elapsed    | 26654    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=4.62 +/- 1.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.62        |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.020557784 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.692       |
|    n_updates            | 4860        |
|    policy_gradient_loss | 0.00228     |
|    value_loss           | 22.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 487      |
|    time_elapsed    | 26712    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -41.4       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 488         |
|    time_elapsed         | 26761       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.019570993 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 0.567       |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=3.80 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.014228486 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.01        |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 18.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.28    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 489      |
|    time_elapsed    | 26819    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=4.96 +/- 2.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.96       |
| time/                   |            |
|    total_timesteps      | 8025000    |
| train/                  |            |
|    approx_kl            | 0.01672057 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.734      |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.00426   |
|    value_loss           | 25.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.29    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 490      |
|    time_elapsed    | 26877    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.71       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 491         |
|    time_elapsed         | 26925       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.028205812 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 2.53        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=3.67 +/- 1.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.67        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.016283182 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 2.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.48    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 492      |
|    time_elapsed    | 26983    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=6.08 +/- 5.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.08        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.016960919 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 0.974       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 493      |
|    time_elapsed    | 27041    |
|    total_timesteps | 8077312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.36       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 494        |
|    time_elapsed         | 27090      |
|    total_timesteps      | 8093696    |
| train/                  |            |
|    approx_kl            | 0.01822704 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0106     |
|    n_updates            | 4930       |
|    policy_gradient_loss | -0.00708   |
|    value_loss           | 0.23       |
----------------------------------------
Eval num_timesteps=8100000, episode_reward=4.38 +/- 2.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.38        |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.009161942 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.24        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 5.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 495      |
|    time_elapsed    | 27148    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=4.27 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.27        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.021883324 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.9        |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 4.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 496      |
|    time_elapsed    | 27206    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.49        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 497         |
|    time_elapsed         | 27255       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.015075078 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 4.22        |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=5.22 +/- 3.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.22        |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.021636717 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0466      |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 2.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.388    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 498      |
|    time_elapsed    | 27313    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=4.35 +/- 2.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.35       |
| time/                   |            |
|    total_timesteps      | 8175000    |
| train/                  |            |
|    approx_kl            | 0.01720571 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.68      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.7       |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.00573   |
|    value_loss           | 16.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.00994 |
| time/              |          |
|    fps             | 298      |
|    iterations      | 499      |
|    time_elapsed    | 27371    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.644      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 500         |
|    time_elapsed         | 27420       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.017998466 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.15        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 6.17        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=6.99 +/- 3.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.99       |
| time/                   |            |
|    total_timesteps      | 8200000    |
| train/                  |            |
|    approx_kl            | 0.02453062 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.347      |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.00748   |
|    value_loss           | 4.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.73    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 501      |
|    time_elapsed    | 27478    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.827      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 502         |
|    time_elapsed         | 27527       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.024951816 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.3        |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 13.3        |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=9.59 +/- 5.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.59        |
| time/                   |             |
|    total_timesteps      | 8225000     |
| train/                  |             |
|    approx_kl            | 0.017916974 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.485       |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 1.9         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.564    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 503      |
|    time_elapsed    | 27585    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=6.53 +/- 2.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.53        |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.021378525 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.997       |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 1.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 504      |
|    time_elapsed    | 27643    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.9       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 505         |
|    time_elapsed         | 27692       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.010717927 |
|    clip_fraction        | 0.0985      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 25.1        |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 55.9        |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=11.94 +/- 2.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.025499359 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.249       |
|    n_updates            | 5050        |
|    policy_gradient_loss | 4.4e-05     |
|    value_loss           | 2.03        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 506      |
|    time_elapsed    | 27750    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=9.66 +/- 3.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.66        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.017356994 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.865       |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 7.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 507      |
|    time_elapsed    | 27807    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.2       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 508         |
|    time_elapsed         | 27856       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.017871143 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 20.7        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=7.35 +/- 2.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.35        |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.009760326 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 205         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 509      |
|    time_elapsed    | 27914    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=9.29 +/- 5.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.29        |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.023142895 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.8        |
|    n_updates            | 5090        |
|    policy_gradient_loss | 0.000632    |
|    value_loss           | 47.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 510      |
|    time_elapsed    | 27972    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.03       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 511         |
|    time_elapsed         | 28021       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.028704176 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.714       |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 1.47        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=6.09 +/- 4.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.09        |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.016208572 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.828       |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 15.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.17    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 512      |
|    time_elapsed    | 28079    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=8.75 +/- 6.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.75        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.018120855 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.366       |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 2.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 513      |
|    time_elapsed    | 28137    |
|    total_timesteps | 8404992  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -5.87     |
| time/                   |           |
|    fps                  | 298       |
|    iterations           | 514       |
|    time_elapsed         | 28186     |
|    total_timesteps      | 8421376   |
| train/                  |           |
|    approx_kl            | 0.0246385 |
|    clip_fraction        | 0.356     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.1      |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.9       |
|    n_updates            | 5130      |
|    policy_gradient_loss | 0.00192   |
|    value_loss           | 7.8       |
---------------------------------------
Eval num_timesteps=8425000, episode_reward=6.47 +/- 2.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.47        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.010095732 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 66.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.71    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 515      |
|    time_elapsed    | 28244    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=9.33 +/- 5.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.33        |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.025641005 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 2.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.74    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 516      |
|    time_elapsed    | 28302    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -28.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 517         |
|    time_elapsed         | 28351       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.017336708 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.713       |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 3.41        |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=7.07 +/- 3.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.07        |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.010671195 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 282         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 518      |
|    time_elapsed    | 28409    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=5.46 +/- 2.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.46        |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.018349316 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.41        |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 35.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 519      |
|    time_elapsed    | 28467    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 520         |
|    time_elapsed         | 28516       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.016162679 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.48        |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 3.9         |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=5.28 +/- 4.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.28        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.020720055 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.528       |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 4.95        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.68    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 521      |
|    time_elapsed    | 28574    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=7.05 +/- 4.85
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 7.05     |
| time/                   |          |
|    total_timesteps      | 8550000  |
| train/                  |          |
|    approx_kl            | 0.019922 |
|    clip_fraction        | 0.22     |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.91    |
|    explained_variance   | 0.93     |
|    learning_rate        | 0.0003   |
|    loss                 | 49.7     |
|    n_updates            | 5210     |
|    policy_gradient_loss | -0.00734 |
|    value_loss           | 20.3     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 522      |
|    time_elapsed    | 28632    |
|    total_timesteps | 8552448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -84.9      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 523        |
|    time_elapsed         | 28680      |
|    total_timesteps      | 8568832    |
| train/                  |            |
|    approx_kl            | 0.01943512 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.372      |
|    n_updates            | 5220       |
|    policy_gradient_loss | 0.00162    |
|    value_loss           | 66.8       |
----------------------------------------
Eval num_timesteps=8575000, episode_reward=4.89 +/- 1.91
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 4.89     |
| time/                   |          |
|    total_timesteps      | 8575000  |
| train/                  |          |
|    approx_kl            | 0.023174 |
|    clip_fraction        | 0.236    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.67    |
|    explained_variance   | 0.91     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.58     |
|    n_updates            | 5230     |
|    policy_gradient_loss | -0.011   |
|    value_loss           | 6.5      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 524      |
|    time_elapsed    | 28738    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=9.68 +/- 9.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 9.68        |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.018515797 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 51.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 525      |
|    time_elapsed    | 28796    |
|    total_timesteps | 8601600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11.1      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 526        |
|    time_elapsed         | 28845      |
|    total_timesteps      | 8617984    |
| train/                  |            |
|    approx_kl            | 0.02359641 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.35      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28       |
|    n_updates            | 5250       |
|    policy_gradient_loss | -0.00693   |
|    value_loss           | 41.4       |
----------------------------------------
Eval num_timesteps=8625000, episode_reward=5.49 +/- 3.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.49        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.019628517 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 3.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 527      |
|    time_elapsed    | 28903    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=7.66 +/- 3.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.66        |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.014309607 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.56        |
|    n_updates            | 5270        |
|    policy_gradient_loss | 0.000824    |
|    value_loss           | 41.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 528      |
|    time_elapsed    | 28961    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -28.8      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 529        |
|    time_elapsed         | 29010      |
|    total_timesteps      | 8667136    |
| train/                  |            |
|    approx_kl            | 0.02234909 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.268      |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.00636   |
|    value_loss           | 1.15       |
----------------------------------------
Eval num_timesteps=8675000, episode_reward=8.88 +/- 2.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.88        |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.015906045 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.78        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 4.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.06    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 530      |
|    time_elapsed    | 29068    |
|    total_timesteps | 8683520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -15.6      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 531        |
|    time_elapsed         | 29117      |
|    total_timesteps      | 8699904    |
| train/                  |            |
|    approx_kl            | 0.02158787 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0003     |
|    loss                 | 18.5       |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.00342   |
|    value_loss           | 36.5       |
----------------------------------------
Eval num_timesteps=8700000, episode_reward=6.82 +/- 2.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6.82       |
| time/                   |            |
|    total_timesteps      | 8700000    |
| train/                  |            |
|    approx_kl            | 0.02925297 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 5310       |
|    policy_gradient_loss | 0.00237    |
|    value_loss           | 51.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 532      |
|    time_elapsed    | 29175    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=5.20 +/- 2.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.2         |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.018210586 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 8.25        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 533      |
|    time_elapsed    | 29233    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 534         |
|    time_elapsed         | 29282       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.020585895 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 6.96        |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=4.14 +/- 1.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.14        |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.020937558 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 2.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.8     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 535      |
|    time_elapsed    | 29340    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=4.94 +/- 2.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.94        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.015729375 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 2.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.63    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 536      |
|    time_elapsed    | 29398    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.2        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 537         |
|    time_elapsed         | 29446       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.014136361 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.2        |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 12.9        |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=6.40 +/- 3.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.4         |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.021006484 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 6.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.68    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 538      |
|    time_elapsed    | 29504    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=5.72 +/- 2.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.72        |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.021072093 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.07        |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 9           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.9     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 539      |
|    time_elapsed    | 29562    |
|    total_timesteps | 8830976  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -29.8      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 540        |
|    time_elapsed         | 29611      |
|    total_timesteps      | 8847360    |
| train/                  |            |
|    approx_kl            | 0.02177947 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.5       |
|    n_updates            | 5390       |
|    policy_gradient_loss | -0.00165   |
|    value_loss           | 31.5       |
----------------------------------------
Eval num_timesteps=8850000, episode_reward=6.53 +/- 5.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.53        |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.020380992 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 23.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 541      |
|    time_elapsed    | 29669    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=8.48 +/- 3.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.48        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.014436686 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 895         |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 256         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 542      |
|    time_elapsed    | 29727    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.8       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 543         |
|    time_elapsed         | 29776       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.019422634 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 7.5         |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=6.12 +/- 3.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.12        |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.022099387 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61        |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 4.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 544      |
|    time_elapsed    | 29834    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=7.83 +/- 3.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.83        |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.018027011 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.617       |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 60.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.58    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 545      |
|    time_elapsed    | 29892    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 546         |
|    time_elapsed         | 29941       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.021381829 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 3.06        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=3.82 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.82        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.012121422 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 150         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 547      |
|    time_elapsed    | 29999    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=4.58 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.58        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.023157515 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.224       |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 1.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 548      |
|    time_elapsed    | 30057    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 549         |
|    time_elapsed         | 30105       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.014304303 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.627       |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 1.4         |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=3.48 +/- 2.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.022506984 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.267       |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 3.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 550      |
|    time_elapsed    | 30164    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=6.33 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.33        |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.026134659 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.376       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 5.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.201    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 551      |
|    time_elapsed    | 30222    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.59       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 552         |
|    time_elapsed         | 30270       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.009510522 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 16.5        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=-113.64 +/- 236.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -114        |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.020541145 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 18.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.94    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 553      |
|    time_elapsed    | 30328    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=5.23 +/- 3.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.23        |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.011916226 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 29.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 554      |
|    time_elapsed    | 30386    |
|    total_timesteps | 9076736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -48        |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 555        |
|    time_elapsed         | 30435      |
|    total_timesteps      | 9093120    |
| train/                  |            |
|    approx_kl            | 0.01245179 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.0003     |
|    loss                 | 227        |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.00287   |
|    value_loss           | 179        |
----------------------------------------
Eval num_timesteps=9100000, episode_reward=3.84 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.84        |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.019453358 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 15.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 556      |
|    time_elapsed    | 30493    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=5.88 +/- 1.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.019927695 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.318       |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00962    |
|    value_loss           | 3.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.8    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 557      |
|    time_elapsed    | 30551    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.96        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 558         |
|    time_elapsed         | 30600       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.016866548 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.17        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 4.72        |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=4.31 +/- 0.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.31        |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.017636638 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 12.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 559      |
|    time_elapsed    | 30658    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=4.85 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.85        |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.014110433 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.43        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 52.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 560      |
|    time_elapsed    | 30716    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.1       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 561         |
|    time_elapsed         | 30765       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.021511627 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.514       |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 11.4        |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=5.35 +/- 2.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.35        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.016714979 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.568       |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 23.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 562      |
|    time_elapsed    | 30823    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.6       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 563         |
|    time_elapsed         | 30871       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.016944364 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.85        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 17.7        |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=3.77 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.77        |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.017104229 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.665       |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 1.75        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 564      |
|    time_elapsed    | 30929    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=-115.46 +/- 238.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.017770559 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17        |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00989    |
|    value_loss           | 3.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.4     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 565      |
|    time_elapsed    | 30987    |
|    total_timesteps | 9256960  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -2.35      |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 566        |
|    time_elapsed         | 31036      |
|    total_timesteps      | 9273344    |
| train/                  |            |
|    approx_kl            | 0.01586341 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.207      |
|    n_updates            | 5650       |
|    policy_gradient_loss | -0.00733   |
|    value_loss           | 1.15       |
----------------------------------------
Eval num_timesteps=9275000, episode_reward=3.56 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.56        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.013848818 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 15.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.76    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 567      |
|    time_elapsed    | 31094    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=3.01 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.01        |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.020589404 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 6.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.84    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 568      |
|    time_elapsed    | 31152    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.44       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 569         |
|    time_elapsed         | 31201       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.019217063 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.256       |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 18.5        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=5.37 +/- 4.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.37        |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.021022432 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.94        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.677   |
| time/              |          |
|    fps             | 298      |
|    iterations      | 570      |
|    time_elapsed    | 31259    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=-115.97 +/- 238.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.016757453 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 1.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.126   |
| time/              |          |
|    fps             | 298      |
|    iterations      | 571      |
|    time_elapsed    | 31317    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.909       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 572         |
|    time_elapsed         | 31366       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.015741948 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.429       |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 3.48        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=6.88 +/- 4.56
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 6.88      |
| time/                   |           |
|    total_timesteps      | 9375000   |
| train/                  |           |
|    approx_kl            | 0.0204623 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.13     |
|    explained_variance   | 0.808     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.58      |
|    n_updates            | 5720      |
|    policy_gradient_loss | -0.00984  |
|    value_loss           | 8.49      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.952    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 573      |
|    time_elapsed    | 31424    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=6.34 +/- 3.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.34        |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.020413939 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 1.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 574      |
|    time_elapsed    | 31482    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.3       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 575         |
|    time_elapsed         | 31531       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.013369459 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 28.5        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=4.39 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.39        |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.023090996 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.0908      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42        |
|    n_updates            | 5750        |
|    policy_gradient_loss | 0.000113    |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -87.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 576      |
|    time_elapsed    | 31589    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=5.99 +/- 1.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.99        |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.018822907 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.7        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 124         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 577      |
|    time_elapsed    | 31647    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 578         |
|    time_elapsed         | 31696       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.012073519 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=6.68 +/- 1.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.68        |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.028384157 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 5780        |
|    policy_gradient_loss | 0.00165     |
|    value_loss           | 9.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.7    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 579      |
|    time_elapsed    | 31754    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=6.32 +/- 1.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.32        |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.018533405 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 48.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 580      |
|    time_elapsed    | 31812    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.4       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 581         |
|    time_elapsed         | 31860       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.018959332 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=5.68 +/- 2.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.68        |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.013028976 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 554         |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 582      |
|    time_elapsed    | 31918    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=3.61 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.61        |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.019840334 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.257       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 1.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 583      |
|    time_elapsed    | 31976    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -150        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 584         |
|    time_elapsed         | 32025       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.013996296 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.764       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 38.4        |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=6.18 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.18        |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.016614232 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.37        |
|    n_updates            | 5840        |
|    policy_gradient_loss | 0.00222     |
|    value_loss           | 134         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 585      |
|    time_elapsed    | 32083    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=3.95 +/- 3.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.95        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.016152143 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.85        |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 19.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 586      |
|    time_elapsed    | 32141    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -144        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 587         |
|    time_elapsed         | 32190       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.017648477 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.518       |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 2.14        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=4.93 +/- 0.61
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.93       |
| time/                   |            |
|    total_timesteps      | 9625000    |
| train/                  |            |
|    approx_kl            | 0.01673051 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.92      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.691      |
|    n_updates            | 5870       |
|    policy_gradient_loss | -0.00984   |
|    value_loss           | 5.41       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 588      |
|    time_elapsed    | 32248    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=3.47 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.47        |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.015088469 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.93        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 72.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 589      |
|    time_elapsed    | 32306    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.7       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 590         |
|    time_elapsed         | 32355       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.019224457 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.591       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 11.3        |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=-114.74 +/- 237.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.021374686 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 5.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 591      |
|    time_elapsed    | 32413    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.22       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 592         |
|    time_elapsed         | 32462       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.012876325 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.886       |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 13.7        |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=4.22 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.22        |
| time/                   |             |
|    total_timesteps      | 9700000     |
| train/                  |             |
|    approx_kl            | 0.016036069 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 16          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 593      |
|    time_elapsed    | 32520    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=-117.13 +/- 238.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.016315809 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.562       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 5.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 594      |
|    time_elapsed    | 32578    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.87       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 595         |
|    time_elapsed         | 32627       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.016706202 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.292       |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 4.1         |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=3.55 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.55        |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.015406919 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.96        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 2           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 596      |
|    time_elapsed    | 32685    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=4.46 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.46        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.017723246 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0938      |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 0.718       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.52    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 597      |
|    time_elapsed    | 32743    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.22       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 598         |
|    time_elapsed         | 32792       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.016013253 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.46        |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 3.04        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=-55.05 +/- 117.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -55        |
| time/                   |            |
|    total_timesteps      | 9800000    |
| train/                  |            |
|    approx_kl            | 0.01634432 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.9        |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.00879   |
|    value_loss           | 2.8        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.01    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 599      |
|    time_elapsed    | 32850    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=4.08 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.08        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.017949887 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 10.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.62    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 600      |
|    time_elapsed    | 32908    |
|    total_timesteps | 9830400  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6         |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 601        |
|    time_elapsed         | 32957      |
|    total_timesteps      | 9846784    |
| train/                  |            |
|    approx_kl            | 0.01773977 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.99      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.369      |
|    n_updates            | 6000       |
|    policy_gradient_loss | -0.00216   |
|    value_loss           | 1.45       |
----------------------------------------
Eval num_timesteps=9850000, episode_reward=4.37 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.37        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.017312933 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.704       |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 2.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.64    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 602      |
|    time_elapsed    | 33014    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=5.01 +/- 1.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.01       |
| time/                   |            |
|    total_timesteps      | 9875000    |
| train/                  |            |
|    approx_kl            | 0.02219607 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.00679   |
|    value_loss           | 1.43       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.11    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 603      |
|    time_elapsed    | 33072    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.69       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 604         |
|    time_elapsed         | 33121       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.017009974 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 0.955       |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=-115.36 +/- 237.37
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -115      |
| time/                   |           |
|    total_timesteps      | 9900000   |
| train/                  |           |
|    approx_kl            | 0.0193538 |
|    clip_fraction        | 0.24      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.08     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.203     |
|    n_updates            | 6040      |
|    policy_gradient_loss | -0.00262  |
|    value_loss           | 2.65      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.124   |
| time/              |          |
|    fps             | 298      |
|    iterations      | 605      |
|    time_elapsed    | 33179    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=3.94 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.94        |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.018272735 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 3.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.801    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 606      |
|    time_elapsed    | 33237    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.172       |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 607         |
|    time_elapsed         | 33286       |
|    total_timesteps      | 9945088     |
| train/                  |             |
|    approx_kl            | 0.018462528 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 1.1         |
-----------------------------------------
Eval num_timesteps=9950000, episode_reward=3.80 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.019927673 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0654      |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.69     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 608      |
|    time_elapsed    | 33344    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=3.64 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.64        |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.009548743 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.000789   |
|    value_loss           | 7.34        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.45    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 609      |
|    time_elapsed    | 33402    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.741      |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 610         |
|    time_elapsed         | 33451       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.015246769 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.17        |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.000788   |
|    value_loss           | 19          |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=3.07 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.021823287 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 6100        |
|    policy_gradient_loss | 0.000669    |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.67    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 611      |
|    time_elapsed    | 33509    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v5_2
