========== uav-v8 ==========
Seed: 1746086347
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v8_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.74e+03 |
| time/              |           |
|    fps             | 359       |
|    iterations      | 1         |
|    time_elapsed    | 45        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-910.28 +/- 3084.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -910        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008696152 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | -0.000437   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.07e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 3.98e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 287       |
|    iterations      | 2         |
|    time_elapsed    | 114       |
|    total_timesteps | 32768     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -3.74e+03    |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 3            |
|    time_elapsed         | 171          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0103071155 |
|    clip_fraction        | 0.0945       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.81        |
|    explained_variance   | 0.000225     |
|    learning_rate        | 0.0003       |
|    loss                 | 9.46e+03     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00966     |
|    value_loss           | 1.78e+04     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-1579.32 +/- 3221.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.58e+03   |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008413338 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.0237      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.91e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 1.21e+04    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.14e+03 |
| time/              |           |
|    fps             | 273       |
|    iterations      | 4         |
|    time_elapsed    | 239       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=1202.52 +/- 0.52
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.2e+03      |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0090449415 |
|    clip_fraction        | 0.0928       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.38e+03     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 6.1e+03      |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.69e+03 |
| time/              |           |
|    fps             | 265       |
|    iterations      | 5         |
|    time_elapsed    | 308       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -915        |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 6           |
|    time_elapsed         | 365         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009494271 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 3.94e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=1322.31 +/- 239.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.009717722 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 1.34e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -285     |
| time/              |          |
|    fps             | 264      |
|    iterations      | 7        |
|    time_elapsed    | 433      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=125000, episode_reward=1322.57 +/- 239.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.009547869 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 584         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 261      |
|    iterations      | 8        |
|    time_elapsed    | 501      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 357         |
| time/                   |             |
|    fps                  | 263         |
|    iterations           | 9           |
|    time_elapsed         | 558         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.011026083 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 350         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=1562.38 +/- 293.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.010205526 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 439         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 261      |
|    iterations      | 10       |
|    time_elapsed    | 626      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=1562.64 +/- 294.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.010168264 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 171         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 267         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 11       |
|    time_elapsed    | 695      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 401         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 12          |
|    time_elapsed         | 752         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.009680297 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 387         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=864.83 +/- 1002.72
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 865          |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0076810606 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.0003       |
|    loss                 | 345          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0074      |
|    value_loss           | 1.18e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 13       |
|    time_elapsed    | 820      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=1442.44 +/- 293.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44e+03    |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.009183517 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.5        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 327         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 14       |
|    time_elapsed    | 888      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 509         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 15          |
|    time_elapsed         | 944         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.009722881 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 172         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 426         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1442.50 +/- 294.08
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.44e+03   |
| time/                   |            |
|    total_timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.00983488 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.72      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 125        |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0097    |
|    value_loss           | 313        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 657      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 16       |
|    time_elapsed    | 1012     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=1562.50 +/- 479.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.010396399 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 420         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 621      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 17       |
|    time_elapsed    | 1080     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 588         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 18          |
|    time_elapsed         | 1136        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.009683274 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 319         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 414         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=1682.68 +/- 588.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.68e+03    |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.007671051 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 193         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00876    |
|    value_loss           | 780         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 633      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 19       |
|    time_elapsed    | 1204     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=2042.31 +/- 720.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04e+03    |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.009665015 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 155         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 399         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 653      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 20       |
|    time_elapsed    | 1272     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 21          |
|    time_elapsed         | 1328        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.009334973 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 349         |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=983.63 +/- 1072.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 984        |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.00832901 |
|    clip_fraction        | 0.0774     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.72      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 349        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00917   |
|    value_loss           | 661        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 22       |
|    time_elapsed    | 1397     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=1461.00 +/- 780.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46e+03    |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.010102611 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 443         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 667      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 23       |
|    time_elapsed    | 1464     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 695         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 24          |
|    time_elapsed         | 1521        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.010626996 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1602.00 +/- 738.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6e+03     |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.008449248 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 430         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 25       |
|    time_elapsed    | 1588     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=1342.68 +/- 1352.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34e+03    |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.009188055 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 421         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 26       |
|    time_elapsed    | 1656     |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 728        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 27         |
|    time_elapsed         | 1713       |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.01005184 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.68      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 96.9       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00889   |
|    value_loss           | 273        |
----------------------------------------
Eval num_timesteps=450000, episode_reward=1462.76 +/- 1076.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46e+03    |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.010022195 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.7        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 28       |
|    time_elapsed    | 1781     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1664.93 +/- 1313.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66e+03    |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.008915894 |
|    clip_fraction        | 0.0796      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 256         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 29       |
|    time_elapsed    | 1849     |
|    total_timesteps | 475136   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 750          |
| time/                   |              |
|    fps                  | 257          |
|    iterations           | 30           |
|    time_elapsed         | 1906         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0081528425 |
|    clip_fraction        | 0.0814       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0003       |
|    loss                 | 209          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00895     |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=500000, episode_reward=1862.65 +/- 480.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86e+03    |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.008141991 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 249         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 585         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 31       |
|    time_elapsed    | 1975     |
|    total_timesteps | 507904   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 654          |
| time/                   |              |
|    fps                  | 257          |
|    iterations           | 32           |
|    time_elapsed         | 2032         |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0110120475 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0003       |
|    loss                 | 130          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 261          |
------------------------------------------
Eval num_timesteps=525000, episode_reward=1492.52 +/- 620.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49e+03    |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.009818694 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 516         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 528         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 33       |
|    time_elapsed    | 2100     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=1761.69 +/- 732.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.009042367 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.8        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 262         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 697      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 34       |
|    time_elapsed    | 2168     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 656         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 35          |
|    time_elapsed         | 2225        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.009747978 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 265         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=1662.37 +/- 1182.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66e+03    |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.010059055 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.4        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 333         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 739      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 36       |
|    time_elapsed    | 2293     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2402.47 +/- 379.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4e+03     |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.012013973 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 225         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 704      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 37       |
|    time_elapsed    | 2361     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 749         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 38          |
|    time_elapsed         | 2418        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.008688301 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 648         |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=1584.95 +/- 939.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58e+03    |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.011846561 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.4        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 729      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 39       |
|    time_elapsed    | 2486     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1702.70 +/- 1186.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.7e+03     |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.009482158 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 201         |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 387         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 40       |
|    time_elapsed    | 2553     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 41          |
|    time_elapsed         | 2610        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.008284929 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 434         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=1378.67 +/- 1033.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.009328779 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 491         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 42       |
|    time_elapsed    | 2676     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=2762.59 +/- 293.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.008827932 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.3        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 558         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 905      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 43       |
|    time_elapsed    | 2743     |
|    total_timesteps | 704512   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 917        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 44         |
|    time_elapsed         | 2800       |
|    total_timesteps      | 720896     |
| train/                  |            |
|    approx_kl            | 0.01117456 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 169        |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.00985   |
|    value_loss           | 198        |
----------------------------------------
Eval num_timesteps=725000, episode_reward=2067.56 +/- 997.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07e+03    |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.009102032 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.3        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00816    |
|    value_loss           | 660         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 980      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 45       |
|    time_elapsed    | 2867     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2522.56 +/- 587.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52e+03    |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.011823585 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00994    |
|    value_loss           | 354         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 971      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 46       |
|    time_elapsed    | 2933     |
|    total_timesteps | 753664   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.06e+03     |
| time/                   |              |
|    fps                  | 257          |
|    iterations           | 47           |
|    time_elapsed         | 2990         |
|    total_timesteps      | 770048       |
| train/                  |              |
|    approx_kl            | 0.0130549725 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 63.2         |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00649     |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=775000, episode_reward=1585.59 +/- 1069.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.59e+03    |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.008270491 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.8        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 345         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 976      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 48       |
|    time_elapsed    | 3057     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2282.08 +/- 239.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.28e+03    |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.011422552 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.8        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 187         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 967      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 49       |
|    time_elapsed    | 3124     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 962         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 50          |
|    time_elapsed         | 3179        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.010276537 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00946    |
|    value_loss           | 215         |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=1988.29 +/- 827.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99e+03    |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.011076977 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.9        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 314         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 996      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 51       |
|    time_elapsed    | 3245     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=1634.91 +/- 1206.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63e+03    |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.010955105 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 80.7        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 276         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 257      |
|    iterations      | 52       |
|    time_elapsed    | 3311     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 993         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 53          |
|    time_elapsed         | 3367        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.013073253 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 235         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=2249.94 +/- 1227.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.25e+03    |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.009071361 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.6        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 384         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 998      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 54       |
|    time_elapsed    | 3433     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=2642.31 +/- 480.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.64e+03     |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0126389265 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.62        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 83.5         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.0075      |
|    value_loss           | 147          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 991      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 55       |
|    time_elapsed    | 3499     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 966         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 56          |
|    time_elapsed         | 3554        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.011667519 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 141         |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 290         |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=2642.41 +/- 294.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.010630182 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 270         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 463         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 992      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 57       |
|    time_elapsed    | 3620     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2303.30 +/- 1124.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.009807164 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.7        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00951    |
|    value_loss           | 586         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 58       |
|    time_elapsed    | 3686     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 916         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 59          |
|    time_elapsed         | 3741        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.013337664 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.3        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=3002.56 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.013182432 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.8        |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 166         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 927      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 60       |
|    time_elapsed    | 3806     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 814        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 61         |
|    time_elapsed         | 3861       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.01115931 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 210        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.00804   |
|    value_loss           | 235        |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=2199.71 +/- 1040.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.008443797 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 164         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 758         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 62       |
|    time_elapsed    | 3926     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=2202.85 +/- 1034.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.012912588 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.6        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 180         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 63       |
|    time_elapsed    | 3990     |
|    total_timesteps | 1032192  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 944          |
| time/                   |              |
|    fps                  | 259          |
|    iterations           | 64           |
|    time_elapsed         | 4044         |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0076127527 |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.6         |
|    explained_variance   | 0.831        |
|    learning_rate        | 0.0003       |
|    loss                 | 65.9         |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 523          |
------------------------------------------
Eval num_timesteps=1050000, episode_reward=2762.35 +/- 293.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.007895732 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 378         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 259      |
|    iterations      | 65       |
|    time_elapsed    | 4108     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=2762.93 +/- 293.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.011700591 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.6        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 135         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 259      |
|    iterations      | 66       |
|    time_elapsed    | 4171     |
|    total_timesteps | 1081344  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.1e+03      |
| time/                   |              |
|    fps                  | 259          |
|    iterations           | 67           |
|    time_elapsed         | 4223         |
|    total_timesteps      | 1097728      |
| train/                  |              |
|    approx_kl            | 0.0077811955 |
|    clip_fraction        | 0.0662       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0003       |
|    loss                 | 186          |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00611     |
|    value_loss           | 367          |
------------------------------------------
Eval num_timesteps=1100000, episode_reward=2763.15 +/- 480.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.010391683 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 351         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 989      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 68       |
|    time_elapsed    | 4286     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=2642.73 +/- 479.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.008800184 |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 339         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 977      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 69       |
|    time_elapsed    | 4348     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 70          |
|    time_elapsed         | 4400        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.009417567 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 91          |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00773    |
|    value_loss           | 313         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=3002.93 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.013808758 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 87.9        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 260      |
|    iterations      | 71       |
|    time_elapsed    | 4462     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=3002.34 +/- 0.23
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 1175000    |
| train/                  |            |
|    approx_kl            | 0.01240938 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 40.1       |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.00869   |
|    value_loss           | 141        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 260      |
|    iterations      | 72       |
|    time_elapsed    | 4523     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.2e+03     |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 73          |
|    time_elapsed         | 4575        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.009884473 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.8        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 457         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=2522.53 +/- 699.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.52e+03   |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.01099561 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.5       |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 317        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 261      |
|    iterations      | 74       |
|    time_elapsed    | 4636     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=2882.60 +/- 240.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.009967964 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 87.9        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 479         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 962      |
| time/              |          |
|    fps             | 261      |
|    iterations      | 75       |
|    time_elapsed    | 4697     |
|    total_timesteps | 1228800  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1e+03        |
| time/                   |              |
|    fps                  | 262          |
|    iterations           | 76           |
|    time_elapsed         | 4749         |
|    total_timesteps      | 1245184      |
| train/                  |              |
|    approx_kl            | 0.0138057005 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0003       |
|    loss                 | 23.7         |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 181          |
------------------------------------------
Eval num_timesteps=1250000, episode_reward=2882.58 +/- 240.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.011156356 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.8        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00994    |
|    value_loss           | 307         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 262      |
|    iterations      | 77       |
|    time_elapsed    | 4810     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=2203.73 +/- 1318.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.011530861 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.3        |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 981      |
| time/              |          |
|    fps             | 262      |
|    iterations      | 78       |
|    time_elapsed    | 4870     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 982         |
| time/                   |             |
|    fps                  | 262         |
|    iterations           | 79          |
|    time_elapsed         | 4921        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.011507401 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.2        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 294         |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=2305.35 +/- 1119.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.31e+03    |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.011262016 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.6        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 921      |
| time/              |          |
|    fps             | 263      |
|    iterations      | 80       |
|    time_elapsed    | 4981     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=2681.13 +/- 643.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68e+03    |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.011725515 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 98          |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 285         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 991      |
| time/              |          |
|    fps             | 263      |
|    iterations      | 81       |
|    time_elapsed    | 5041     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.02e+03    |
| time/                   |             |
|    fps                  | 263         |
|    iterations           | 82          |
|    time_elapsed         | 5092        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.011753956 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.8        |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 290         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=2762.65 +/- 293.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.010901876 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 321         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 263      |
|    iterations      | 83       |
|    time_elapsed    | 5152     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=3002.77 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.008645123 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 315         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 264      |
|    iterations      | 84       |
|    time_elapsed    | 5211     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.21e+03    |
| time/                   |             |
|    fps                  | 264         |
|    iterations           | 85          |
|    time_elapsed         | 5261        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.010317939 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 45.8        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 226         |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=3002.42 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.014089813 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.2        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00826    |
|    value_loss           | 112         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 264      |
|    iterations      | 86       |
|    time_elapsed    | 5320     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=2701.15 +/- 381.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.008435883 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 509         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 264      |
|    iterations      | 87       |
|    time_elapsed    | 5380     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 265         |
|    iterations           | 88          |
|    time_elapsed         | 5430        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.013276053 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 94.6        |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=2882.33 +/- 240.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.011864785 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.7        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 246         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 994      |
| time/              |          |
|    fps             | 265      |
|    iterations      | 89       |
|    time_elapsed    | 5489     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.05e+03    |
| time/                   |             |
|    fps                  | 266         |
|    iterations           | 90          |
|    time_elapsed         | 5539        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.012071288 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.4        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 252         |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=2125.12 +/- 963.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13e+03    |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.011492077 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 254         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 358         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 266      |
|    iterations      | 91       |
|    time_elapsed    | 5599     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2444.84 +/- 1115.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.44e+03   |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.01065009 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.36      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.00698   |
|    value_loss           | 191        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 266      |
|    iterations      | 92       |
|    time_elapsed    | 5658     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.14e+03    |
| time/                   |             |
|    fps                  | 266         |
|    iterations           | 93          |
|    time_elapsed         | 5708        |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.010291023 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.7        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 269         |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=2304.64 +/- 1120.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.011970609 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 219         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 267      |
|    iterations      | 94       |
|    time_elapsed    | 5767     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2424.95 +/- 1155.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.009711083 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 120         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00975    |
|    value_loss           | 379         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 267      |
|    iterations      | 95       |
|    time_elapsed    | 5827     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.21e+03    |
| time/                   |             |
|    fps                  | 267         |
|    iterations           | 96          |
|    time_elapsed         | 5877        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.011036573 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.6        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 226         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=3002.45 +/- 0.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 1575000    |
| train/                  |            |
|    approx_kl            | 0.00942381 |
|    clip_fraction        | 0.0969     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.7       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00761   |
|    value_loss           | 265        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 267      |
|    iterations      | 97       |
|    time_elapsed    | 5936     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=2291.13 +/- 1146.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29e+03    |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.011900673 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 160         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 267      |
|    iterations      | 98       |
|    time_elapsed    | 5995     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.22e+03    |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 99          |
|    time_elapsed         | 6045        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.010804059 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.8        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=3002.46 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.010785121 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 127         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 323         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 268      |
|    iterations      | 100      |
|    time_elapsed    | 6104     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2882.42 +/- 240.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.011371424 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.7        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 268      |
|    iterations      | 101      |
|    time_elapsed    | 6163     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.29e+03    |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 102         |
|    time_elapsed         | 6213        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.010062995 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 218         |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 589         |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=3002.50 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.009912894 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.8        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 390         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 269      |
|    iterations      | 103      |
|    time_elapsed    | 6272     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2582.65 +/- 588.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58e+03    |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.011800239 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00881    |
|    value_loss           | 307         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 269      |
|    iterations      | 104      |
|    time_elapsed    | 6330     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 105         |
|    time_elapsed         | 6379        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.010205335 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 235         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00979    |
|    value_loss           | 724         |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=2882.49 +/- 240.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.011480404 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 61.2        |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 161         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 269      |
|    iterations      | 106      |
|    time_elapsed    | 6437     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=3002.60 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.010877764 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00998    |
|    value_loss           | 337         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 269      |
|    iterations      | 107      |
|    time_elapsed    | 6495     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.39e+03    |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 108         |
|    time_elapsed         | 6544        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.014095609 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.7        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=2422.82 +/- 1159.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.011171668 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 387         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 270      |
|    iterations      | 109      |
|    time_elapsed    | 6602     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2423.00 +/- 1159.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.012056251 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 84          |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 221         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 270      |
|    iterations      | 110      |
|    time_elapsed    | 6660     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.39e+03    |
| time/                   |             |
|    fps                  | 271         |
|    iterations           | 111         |
|    time_elapsed         | 6709        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.010946579 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.6        |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 177         |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=3002.40 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.009659046 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 589         |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 709         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 271      |
|    iterations      | 112      |
|    time_elapsed    | 6767     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=1937.94 +/- 1130.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.94e+03    |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.009620165 |
|    clip_fraction        | 0.0976      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 297         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 271      |
|    iterations      | 113      |
|    time_elapsed    | 6825     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.36e+03    |
| time/                   |             |
|    fps                  | 271         |
|    iterations           | 114         |
|    time_elapsed         | 6874        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.014576293 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 188         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=2476.48 +/- 1052.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48e+03    |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.011433059 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.3        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 271      |
|    iterations      | 115      |
|    time_elapsed    | 6932     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2422.93 +/- 1159.59
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 1900000    |
| train/                  |            |
|    approx_kl            | 0.01218154 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.3       |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 293        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 271      |
|    iterations      | 116      |
|    time_elapsed    | 6990     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 272         |
|    iterations           | 117         |
|    time_elapsed         | 7038        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.011664129 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.5        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=3002.55 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.010541855 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 288         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 272      |
|    iterations      | 118      |
|    time_elapsed    | 7096     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 272         |
|    iterations           | 119         |
|    time_elapsed         | 7145        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.010500383 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 296         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=2762.55 +/- 293.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.009931965 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 130         |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 423         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 272      |
|    iterations      | 120      |
|    time_elapsed    | 7203     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=3002.71 +/- 0.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 1975000      |
| train/                  |              |
|    approx_kl            | 0.0140568055 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0003       |
|    loss                 | 56.6         |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 298          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 273      |
|    iterations      | 121      |
|    time_elapsed    | 7261     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 273         |
|    iterations           | 122         |
|    time_elapsed         | 7310        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.012096685 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 166         |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=2882.91 +/- 240.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.012977313 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.9        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 273      |
|    iterations      | 123      |
|    time_elapsed    | 7368     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=1846.96 +/- 2310.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.011520913 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.6        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 185         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 273      |
|    iterations      | 124      |
|    time_elapsed    | 7426     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 273         |
|    iterations           | 125         |
|    time_elapsed         | 7475        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.010349823 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.8        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 287         |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=2620.17 +/- 764.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62e+03    |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.012333521 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.6        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 274      |
|    iterations      | 126      |
|    time_elapsed    | 7533     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=1310.30 +/- 2298.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.31e+03    |
| time/                   |             |
|    total_timesteps      | 2075000     |
| train/                  |             |
|    approx_kl            | 0.012410293 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 201         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 274      |
|    iterations      | 127      |
|    time_elapsed    | 7591     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 274         |
|    iterations           | 128         |
|    time_elapsed         | 7640        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.011847484 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 636         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 432         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=2621.69 +/- 762.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62e+03    |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.017313758 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.9        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 274      |
|    iterations      | 129      |
|    time_elapsed    | 7698     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=3002.39 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.008905781 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 127         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00695    |
|    value_loss           | 283         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 274      |
|    iterations      | 130      |
|    time_elapsed    | 7756     |
|    total_timesteps | 2129920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.62e+03   |
| time/                   |            |
|    fps                  | 274        |
|    iterations           | 131        |
|    time_elapsed         | 7805       |
|    total_timesteps      | 2146304    |
| train/                  |            |
|    approx_kl            | 0.01004724 |
|    clip_fraction        | 0.0998     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.72      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.00855   |
|    value_loss           | 331        |
----------------------------------------
Eval num_timesteps=2150000, episode_reward=3002.92 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.013236852 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.1        |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 185         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 132      |
|    time_elapsed    | 7863     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=3002.79 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.009290198 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.7        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 214         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 133      |
|    time_elapsed    | 7921     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.61e+03    |
| time/                   |             |
|    fps                  | 275         |
|    iterations           | 134         |
|    time_elapsed         | 7970        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.011117971 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.2        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=2785.62 +/- 433.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.79e+03    |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.012835756 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 355         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 381         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 135      |
|    time_elapsed    | 8028     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=2422.77 +/- 1159.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.010337843 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.6        |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 202         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 136      |
|    time_elapsed    | 8086     |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.65e+03    |
| time/                   |             |
|    fps                  | 275         |
|    iterations           | 137         |
|    time_elapsed         | 8134        |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.014931168 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.6        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=3002.59 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.010613106 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.4        |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 138      |
|    time_elapsed    | 8192     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=2422.30 +/- 1159.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.012371436 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.3        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 168         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    fps             | 276      |
|    iterations      | 139      |
|    time_elapsed    | 8250     |
|    total_timesteps | 2277376  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.76e+03     |
| time/                   |              |
|    fps                  | 276          |
|    iterations           | 140          |
|    time_elapsed         | 8299         |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0116302185 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.2         |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 89.9         |
------------------------------------------
Eval num_timesteps=2300000, episode_reward=2529.76 +/- 945.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53e+03    |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.014299399 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.6        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 276      |
|    iterations      | 141      |
|    time_elapsed    | 8357     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=2304.87 +/- 1119.97
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.3e+03      |
| time/                   |              |
|    total_timesteps      | 2325000      |
| train/                  |              |
|    approx_kl            | 0.0094978595 |
|    clip_fraction        | 0.0897       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.16        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0003       |
|    loss                 | 54.2         |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.0061      |
|    value_loss           | 182          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    fps             | 276      |
|    iterations      | 142      |
|    time_elapsed    | 8415     |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 276         |
|    iterations           | 143         |
|    time_elapsed         | 8464        |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.014531491 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.3        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 114         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=2302.61 +/- 1123.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.008899453 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.6        |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 188         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    fps             | 276      |
|    iterations      | 144      |
|    time_elapsed    | 8521     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=3002.65 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.016136333 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.7        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 115         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    fps             | 276      |
|    iterations      | 145      |
|    time_elapsed    | 8579     |
|    total_timesteps | 2375680  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 1.92e+03  |
| time/                   |           |
|    fps                  | 277       |
|    iterations           | 146       |
|    time_elapsed         | 8628      |
|    total_timesteps      | 2392064   |
| train/                  |           |
|    approx_kl            | 0.0096028 |
|    clip_fraction        | 0.0944    |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.68     |
|    explained_variance   | 0.868     |
|    learning_rate        | 0.0003    |
|    loss                 | 159       |
|    n_updates            | 1450      |
|    policy_gradient_loss | -0.00379  |
|    value_loss           | 376       |
---------------------------------------
Eval num_timesteps=2400000, episode_reward=3002.58 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.013670365 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.4        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 166         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    fps             | 277      |
|    iterations      | 147      |
|    time_elapsed    | 8686     |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 277         |
|    iterations           | 148         |
|    time_elapsed         | 8735        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.009043166 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.5        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 431         |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=3002.95 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.010841811 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.17       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.3        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 158         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    fps             | 277      |
|    iterations      | 149      |
|    time_elapsed    | 8793     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=3002.48 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.009655466 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.04       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00697    |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    fps             | 277      |
|    iterations      | 150      |
|    time_elapsed    | 8851     |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.82e+03    |
| time/                   |             |
|    fps                  | 277         |
|    iterations           | 151         |
|    time_elapsed         | 8899        |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.010473888 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.07       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.6        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00902    |
|    value_loss           | 402         |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=3002.59 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.011624478 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.2        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 175         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 152      |
|    time_elapsed    | 8957     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=3002.56 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.008452421 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 401         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 153      |
|    time_elapsed    | 9015     |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.98e+03    |
| time/                   |             |
|    fps                  | 278         |
|    iterations           | 154         |
|    time_elapsed         | 9064        |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.010012301 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.3        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00588    |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=3002.35 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.009873804 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.2        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 382         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 155      |
|    time_elapsed    | 9122     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2464.25 +/- 1076.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46e+03    |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.010588479 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 41.1        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 126         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 156      |
|    time_elapsed    | 9180     |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 278         |
|    iterations           | 157         |
|    time_elapsed         | 9228        |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.016690701 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 45          |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 102         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=3002.52 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.010223625 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 88          |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 206         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 158      |
|    time_elapsed    | 9286     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=3002.22 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.009209061 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.9        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 167         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 159      |
|    time_elapsed    | 9344     |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 279         |
|    iterations           | 160         |
|    time_elapsed         | 9393        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.011872428 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 21          |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=3002.55 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.008807022 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.7        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 161      |
|    time_elapsed    | 9451     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=3002.52 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2650000      |
| train/                  |              |
|    approx_kl            | 0.0077632507 |
|    clip_fraction        | 0.076        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.26        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.8         |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00417     |
|    value_loss           | 185          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 162      |
|    time_elapsed    | 9509     |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 279         |
|    iterations           | 163         |
|    time_elapsed         | 9557        |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.008415509 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 216         |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=3002.45 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.013110064 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 72.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 164      |
|    time_elapsed    | 9615     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=3002.39 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.009314405 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.7        |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 165      |
|    time_elapsed    | 9673     |
|    total_timesteps | 2703360  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.29e+03  |
| time/                   |           |
|    fps                  | 279       |
|    iterations           | 166       |
|    time_elapsed         | 9722      |
|    total_timesteps      | 2719744   |
| train/                  |           |
|    approx_kl            | 0.0099622 |
|    clip_fraction        | 0.0938    |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.01     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 76.7      |
|    n_updates            | 1650      |
|    policy_gradient_loss | -0.00364  |
|    value_loss           | 120       |
---------------------------------------
Eval num_timesteps=2725000, episode_reward=2882.70 +/- 239.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.88e+03   |
| time/                   |            |
|    total_timesteps      | 2725000    |
| train/                  |            |
|    approx_kl            | 0.01435145 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.00356   |
|    value_loss           | 86.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 167      |
|    time_elapsed    | 9780     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=3002.28 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.013280755 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.1        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 168      |
|    time_elapsed    | 9838     |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.23e+03    |
| time/                   |             |
|    fps                  | 280         |
|    iterations           | 169         |
|    time_elapsed         | 9887        |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.007929098 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.5        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 188         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=3002.54 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.010301588 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 170      |
|    time_elapsed    | 9944     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=3002.52 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.009672586 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 81.6        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00431    |
|    value_loss           | 135         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 171      |
|    time_elapsed    | 10002    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.17e+03    |
| time/                   |             |
|    fps                  | 280         |
|    iterations           | 172         |
|    time_elapsed         | 10051       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.014194556 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.5        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 73.2        |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=3002.25 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.008270663 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.4        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    fps             | 280      |
|    iterations      | 173      |
|    time_elapsed    | 10109    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=3002.46 +/- 0.47
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2850000    |
| train/                  |            |
|    approx_kl            | 0.01387229 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 88.8       |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.00696   |
|    value_loss           | 169        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 174      |
|    time_elapsed    | 10167    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.32e+03    |
| time/                   |             |
|    fps                  | 280         |
|    iterations           | 175         |
|    time_elapsed         | 10216       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.011447493 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=3002.74 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.013066163 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 66.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 176      |
|    time_elapsed    | 10274    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.4e+03     |
| time/                   |             |
|    fps                  | 280         |
|    iterations           | 177         |
|    time_elapsed         | 10322       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.012007305 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 60.8        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 158         |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=3002.64 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.007017149 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.3        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00364    |
|    value_loss           | 144         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 178      |
|    time_elapsed    | 10380    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=3002.17 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 2925000      |
| train/                  |              |
|    approx_kl            | 0.0081366375 |
|    clip_fraction        | 0.0943       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.52        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 37.1         |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00532     |
|    value_loss           | 178          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 179      |
|    time_elapsed    | 10438    |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.37e+03    |
| time/                   |             |
|    fps                  | 281         |
|    iterations           | 180         |
|    time_elapsed         | 10487       |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.010256281 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.57        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=3002.58 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 2950000    |
| train/                  |            |
|    approx_kl            | 0.00823765 |
|    clip_fraction        | 0.0706     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 29         |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.00335   |
|    value_loss           | 85.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 181      |
|    time_elapsed    | 10545    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=2423.07 +/- 1159.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.007816376 |
|    clip_fraction        | 0.0628      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.6        |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 182      |
|    time_elapsed    | 10603    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 281         |
|    iterations           | 183         |
|    time_elapsed         | 10652       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.015415813 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.13        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=3002.72 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.008497092 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.4        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.000519   |
|    value_loss           | 136         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 184      |
|    time_elapsed    | 10710    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=3002.69 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3025000      |
| train/                  |              |
|    approx_kl            | 0.0065799877 |
|    clip_fraction        | 0.0674       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 30.9         |
|    n_updates            | 1840         |
|    policy_gradient_loss | 0.000683     |
|    value_loss           | 116          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 185      |
|    time_elapsed    | 10769    |
|    total_timesteps | 3031040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.42e+03     |
| time/                   |              |
|    fps                  | 281          |
|    iterations           | 186          |
|    time_elapsed         | 10817        |
|    total_timesteps      | 3047424      |
| train/                  |              |
|    approx_kl            | 0.0058295685 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 99.7         |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 181          |
------------------------------------------
Eval num_timesteps=3050000, episode_reward=3002.45 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3050000    |
| train/                  |            |
|    approx_kl            | 0.01335235 |
|    clip_fraction        | 0.0856     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.2       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.00342   |
|    value_loss           | 101        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 187      |
|    time_elapsed    | 10875    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=3002.12 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3075000      |
| train/                  |              |
|    approx_kl            | 0.0044552954 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.78        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.6         |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 111          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 188      |
|    time_elapsed    | 10934    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.58e+03    |
| time/                   |             |
|    fps                  | 281         |
|    iterations           | 189         |
|    time_elapsed         | 10982       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.009046884 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 62.6        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 86.9        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=3002.50 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3100000      |
| train/                  |              |
|    approx_kl            | 0.0076023834 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.944       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.3         |
|    n_updates            | 1890         |
|    policy_gradient_loss | 0.000624     |
|    value_loss           | 58.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 190      |
|    time_elapsed    | 11040    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=3002.47 +/- 0.32
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3125000    |
| train/                  |            |
|    approx_kl            | 0.01787691 |
|    clip_fraction        | 0.0684     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.583     |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.64       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0043    |
|    value_loss           | 66.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 191      |
|    time_elapsed    | 11099    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.68e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 192         |
|    time_elapsed         | 11148       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.009255301 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.00425     |
|    value_loss           | 55.1        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=3002.45 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.014549578 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.48        |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.000699   |
|    value_loss           | 43.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 193      |
|    time_elapsed    | 11207    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=3002.42 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3175000      |
| train/                  |              |
|    approx_kl            | 0.0077731293 |
|    clip_fraction        | 0.076        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.825       |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.88         |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 90.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 194      |
|    time_elapsed    | 11265    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 195         |
|    time_elapsed         | 11314       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.008162393 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 67.2        |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=3002.36 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.011768693 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.3        |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 92.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 196      |
|    time_elapsed    | 11373    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=3002.62 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.012149192 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.5        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 60.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 197      |
|    time_elapsed    | 11431    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 198         |
|    time_elapsed         | 11480       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.011404226 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.2        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=3002.28 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.005547164 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.5        |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 199      |
|    time_elapsed    | 11538    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=1706.88 +/- 2591.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.018572764 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 42.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 200      |
|    time_elapsed    | 11597    |
|    total_timesteps | 3276800  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.46e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 201          |
|    time_elapsed         | 11645        |
|    total_timesteps      | 3293184      |
| train/                  |              |
|    approx_kl            | 0.0049531627 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.0003       |
|    loss                 | 31.5         |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.0048      |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=3300000, episode_reward=3002.51 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3300000      |
| train/                  |              |
|    approx_kl            | 0.0066814586 |
|    clip_fraction        | 0.0683       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 60.9         |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 162          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 202      |
|    time_elapsed    | 11704    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=3002.36 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.014330888 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 55.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 203      |
|    time_elapsed    | 11762    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.6e+03     |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 204         |
|    time_elapsed         | 11811       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.016201058 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 33.9        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=3002.62 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.006728745 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 264         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 205      |
|    time_elapsed    | 11869    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=3002.39 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.010085203 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.8        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 206      |
|    time_elapsed    | 11927    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.48e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 207         |
|    time_elapsed         | 11976       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.009398942 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.88        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00064    |
|    value_loss           | 53.6        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=3002.48 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3400000      |
| train/                  |              |
|    approx_kl            | 0.0064349053 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0003       |
|    loss                 | 39.2         |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00551     |
|    value_loss           | 100          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 208      |
|    time_elapsed    | 12034    |
|    total_timesteps | 3407872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.69e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 209          |
|    time_elapsed         | 12082        |
|    total_timesteps      | 3424256      |
| train/                  |              |
|    approx_kl            | 0.0073907864 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.3         |
|    n_updates            | 2080         |
|    policy_gradient_loss | -6.87e-05    |
|    value_loss           | 66.1         |
------------------------------------------
Eval num_timesteps=3425000, episode_reward=3002.63 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.013785904 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.631      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.000579   |
|    value_loss           | 30.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 210      |
|    time_elapsed    | 12140    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=3002.20 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3450000      |
| train/                  |              |
|    approx_kl            | 0.0062521715 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 90.8         |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 82.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 211      |
|    time_elapsed    | 12200    |
|    total_timesteps | 3457024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.7e+03      |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 212          |
|    time_elapsed         | 12251        |
|    total_timesteps      | 3473408      |
| train/                  |              |
|    approx_kl            | 0.0090743005 |
|    clip_fraction        | 0.0687       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.809       |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 11           |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00533     |
|    value_loss           | 69.4         |
------------------------------------------
Eval num_timesteps=3475000, episode_reward=2422.88 +/- 1159.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 3475000      |
| train/                  |              |
|    approx_kl            | 0.0071226344 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.01         |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 97.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 213      |
|    time_elapsed    | 12311    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3002.53 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.011368988 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.000954   |
|    value_loss           | 34.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 214      |
|    time_elapsed    | 12371    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.8e+03     |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 215         |
|    time_elapsed         | 12422       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.009143019 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.79        |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 33.1        |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=3002.67 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.010569184 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 7.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 216      |
|    time_elapsed    | 12482    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=3002.59 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.007831704 |
|    clip_fraction        | 0.0509      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.506      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.69        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 20.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 217      |
|    time_elapsed    | 12541    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.85e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 218         |
|    time_elapsed         | 12592       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.016957793 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.94        |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 25.1        |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=3002.53 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3575000      |
| train/                  |              |
|    approx_kl            | 0.0067340326 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.442       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.5          |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.000589    |
|    value_loss           | 33.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 219      |
|    time_elapsed    | 12651    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=3002.51 +/- 0.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.01245261 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.554     |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.5       |
|    n_updates            | 2190       |
|    policy_gradient_loss | 0.000392   |
|    value_loss           | 56.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 220      |
|    time_elapsed    | 12711    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.8e+03    |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 221        |
|    time_elapsed         | 12760      |
|    total_timesteps      | 3620864    |
| train/                  |            |
|    approx_kl            | 0.02050862 |
|    clip_fraction        | 0.0671     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.1       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.00278   |
|    value_loss           | 20.2       |
----------------------------------------
Eval num_timesteps=3625000, episode_reward=3002.50 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3625000      |
| train/                  |              |
|    approx_kl            | 0.0039513763 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.386       |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.23         |
|    n_updates            | 2210         |
|    policy_gradient_loss | 0.000854     |
|    value_loss           | 92           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 222      |
|    time_elapsed    | 12819    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=3002.60 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0045424867 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.199       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11         |
|    n_updates            | 2220         |
|    policy_gradient_loss | 0.000722     |
|    value_loss           | 21           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 223      |
|    time_elapsed    | 12878    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.79e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 224         |
|    time_elapsed         | 12932       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.003414088 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.71        |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.000322    |
|    value_loss           | 57.8        |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=3002.51 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.014414913 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.243       |
|    n_updates            | 2240        |
|    policy_gradient_loss | 0.00308     |
|    value_loss           | 19.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 225      |
|    time_elapsed    | 12996    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=3002.50 +/- 0.14
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3700000    |
| train/                  |            |
|    approx_kl            | 0.01371629 |
|    clip_fraction        | 0.0474     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 202        |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.000353  |
|    value_loss           | 88.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 226      |
|    time_elapsed    | 13059    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.9e+03     |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 227         |
|    time_elapsed         | 13112       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.009738236 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.198       |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 1.43        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=2422.78 +/- 1159.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.010642062 |
|    clip_fraction        | 0.0383      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.091      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.434       |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 11          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 228      |
|    time_elapsed    | 13174    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=3002.95 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.030963857 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.36        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 61.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 229      |
|    time_elapsed    | 13235    |
|    total_timesteps | 3751936  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.53e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 230          |
|    time_elapsed         | 13287        |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 0.0068299198 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.848        |
|    learning_rate        | 0.0003       |
|    loss                 | 60.3         |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 239          |
------------------------------------------
Eval num_timesteps=3775000, episode_reward=3002.50 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.010768578 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.3        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 199         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 231      |
|    time_elapsed    | 13348    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=3002.48 +/- 0.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 3800000    |
| train/                  |            |
|    approx_kl            | 0.01823226 |
|    clip_fraction        | 0.0313     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.232     |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.00496   |
|    value_loss           | 107        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 232      |
|    time_elapsed    | 13410    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.59e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 233         |
|    time_elapsed         | 13461       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.012076583 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.9        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=3002.52 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3825000      |
| train/                  |              |
|    approx_kl            | 0.0026245068 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.646        |
|    n_updates            | 2330         |
|    policy_gradient_loss | 0.000629     |
|    value_loss           | 38.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 234      |
|    time_elapsed    | 13522    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=3002.70 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.008867698 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.8         |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 65.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 235      |
|    time_elapsed    | 13583    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.71e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 236         |
|    time_elapsed         | 13634       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.016562099 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.5        |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.000174   |
|    value_loss           | 44.3        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=3002.23 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.009114578 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 39          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 237      |
|    time_elapsed    | 13695    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.83e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 238         |
|    time_elapsed         | 13746       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.010023743 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.68        |
|    n_updates            | 2370        |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 17.8        |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=3002.23 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.007225424 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.000181   |
|    value_loss           | 96.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 239      |
|    time_elapsed    | 13807    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=3002.38 +/- 0.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 3925000      |
| train/                  |              |
|    approx_kl            | 0.0080173435 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 59.8         |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 67.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 240      |
|    time_elapsed    | 13867    |
|    total_timesteps | 3932160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.67e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 241          |
|    time_elapsed         | 13920        |
|    total_timesteps      | 3948544      |
| train/                  |              |
|    approx_kl            | 0.0073364954 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.2         |
|    n_updates            | 2400         |
|    policy_gradient_loss | 7.12e-05     |
|    value_loss           | 88.6         |
------------------------------------------
Eval num_timesteps=3950000, episode_reward=2422.56 +/- 1159.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.008816336 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.2        |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 99.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 242      |
|    time_elapsed    | 13985    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=3002.40 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.018748945 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 47.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 243      |
|    time_elapsed    | 14049    |
|    total_timesteps | 3981312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 1.8e+03   |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 244       |
|    time_elapsed         | 14101     |
|    total_timesteps      | 3997696   |
| train/                  |           |
|    approx_kl            | 0.2737951 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.352    |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.039     |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.0354   |
|    value_loss           | 6.96      |
---------------------------------------
Eval num_timesteps=4000000, episode_reward=1244.49 +/- 3515.79
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.24e+03     |
| time/                   |              |
|    total_timesteps      | 4000000      |
| train/                  |              |
|    approx_kl            | 0.0022077453 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.257       |
|    explained_variance   | 0.61         |
|    learning_rate        | 0.0003       |
|    loss                 | 448          |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 1.1e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 640      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 245      |
|    time_elapsed    | 14163    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=1706.44 +/- 2591.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 4025000      |
| train/                  |              |
|    approx_kl            | 0.0024805812 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.251       |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0003       |
|    loss                 | 99           |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 624          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 246      |
|    time_elapsed    | 14225    |
|    total_timesteps | 4030464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.07e+03    |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 247          |
|    time_elapsed         | 14277        |
|    total_timesteps      | 4046848      |
| train/                  |              |
|    approx_kl            | 0.0017078777 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.0003       |
|    loss                 | 79.1         |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 570          |
------------------------------------------
Eval num_timesteps=4050000, episode_reward=3002.63 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4050000      |
| train/                  |              |
|    approx_kl            | 0.0021345736 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | 262          |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 518          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -747     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 248      |
|    time_elapsed    | 14338    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=3002.40 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.007384136 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.6        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00304    |
|    value_loss           | 361         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -563     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 249      |
|    time_elapsed    | 14399    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 368         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 250         |
|    time_elapsed         | 14451       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.010139821 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.365      |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.8        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 337         |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=3002.48 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4100000      |
| train/                  |              |
|    approx_kl            | 0.0073001147 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 55           |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00457     |
|    value_loss           | 313          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 251      |
|    time_elapsed    | 14511    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=1706.32 +/- 2592.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.007842216 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.243      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.9        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.0047     |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 252      |
|    time_elapsed    | 14572    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 253         |
|    time_elapsed         | 14623       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.008044349 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 80.8        |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=3002.59 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.003608292 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0749     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.93        |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 16.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 254      |
|    time_elapsed    | 14684    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=3002.25 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 4175000    |
| train/                  |            |
|    approx_kl            | 0.00463473 |
|    clip_fraction        | 0.0263     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0999    |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.97       |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.00524   |
|    value_loss           | 32.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 255      |
|    time_elapsed    | 14745    |
|    total_timesteps | 4177920  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.82e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 256          |
|    time_elapsed         | 14796        |
|    total_timesteps      | 4194304      |
| train/                  |              |
|    approx_kl            | 0.0075807073 |
|    clip_fraction        | 0.00963      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0633      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.44         |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00266     |
|    value_loss           | 49.6         |
------------------------------------------
Eval num_timesteps=4200000, episode_reward=3002.46 +/- 0.17
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 4200000       |
| train/                  |               |
|    approx_kl            | 0.00060223293 |
|    clip_fraction        | 0.0023        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0253       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.2          |
|    n_updates            | 2560          |
|    policy_gradient_loss | -0.000699     |
|    value_loss           | 24.6          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 257      |
|    time_elapsed    | 14856    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=3002.22 +/- 0.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 4225000    |
| train/                  |            |
|    approx_kl            | 0.07181899 |
|    clip_fraction        | 0.0551     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.4        |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.00729   |
|    value_loss           | 24.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 258      |
|    time_elapsed    | 14917    |
|    total_timesteps | 4227072  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.49e+03  |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 259       |
|    time_elapsed         | 14971     |
|    total_timesteps      | 4243456   |
| train/                  |           |
|    approx_kl            | 0.0164168 |
|    clip_fraction        | 0.0696    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.374    |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.16      |
|    n_updates            | 2580      |
|    policy_gradient_loss | -0.00222  |
|    value_loss           | 15.3      |
---------------------------------------
Eval num_timesteps=4250000, episode_reward=3002.32 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4250000      |
| train/                  |              |
|    approx_kl            | 0.0017518781 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.422       |
|    explained_variance   | 0.542        |
|    learning_rate        | 0.0003       |
|    loss                 | 332          |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 397          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 260      |
|    time_elapsed    | 15035    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=3002.65 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.015419055 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 205         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 261      |
|    time_elapsed    | 15098    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 262         |
|    time_elapsed         | 15150       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.015543351 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 207         |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=1843.01 +/- 1420.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.008257705 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.39        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 263      |
|    time_elapsed    | 15212    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=3002.48 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.008406362 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.65        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 105         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 264      |
|    time_elapsed    | 15274    |
|    total_timesteps | 4325376  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.47e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 265          |
|    time_elapsed         | 15325        |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0031366833 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.5         |
|    n_updates            | 2640         |
|    policy_gradient_loss | 0.00362      |
|    value_loss           | 131          |
------------------------------------------
Eval num_timesteps=4350000, episode_reward=2422.51 +/- 1159.57
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 4350000      |
| train/                  |              |
|    approx_kl            | 0.0072479337 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.3         |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 118          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 266      |
|    time_elapsed    | 15386    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.6e+03     |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 267         |
|    time_elapsed         | 15438       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.007978615 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 66          |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=3002.57 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.008302867 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 2670        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 6.75        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 268      |
|    time_elapsed    | 15498    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=3002.98 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.005562795 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.8        |
|    n_updates            | 2680        |
|    policy_gradient_loss | 0.00178     |
|    value_loss           | 48.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 269      |
|    time_elapsed    | 15559    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.82e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 270         |
|    time_elapsed         | 15610       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.014294003 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.41        |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 20.2        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=3002.51 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.012584669 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0908     |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.47        |
|    n_updates            | 2700        |
|    policy_gradient_loss | 0.000591    |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 271      |
|    time_elapsed    | 15671    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=2422.97 +/- 1159.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.008554316 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.57        |
|    n_updates            | 2710        |
|    policy_gradient_loss | 0.000579    |
|    value_loss           | 55.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 272      |
|    time_elapsed    | 15731    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.77e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 273         |
|    time_elapsed         | 15783       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.004724034 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.000942   |
|    value_loss           | 2.47        |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=2422.72 +/- 1159.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.009813898 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    n_updates            | 2730        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 74.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 274      |
|    time_elapsed    | 15843    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=3003.21 +/- 0.70
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 0.0002665873 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0464      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.8         |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.000418    |
|    value_loss           | 21           |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 275      |
|    time_elapsed    | 15904    |
|    total_timesteps | 4505600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.84e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 276          |
|    time_elapsed         | 15957        |
|    total_timesteps      | 4521984      |
| train/                  |              |
|    approx_kl            | 0.0013845353 |
|    clip_fraction        | 0.00316      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0377      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.148        |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.000208    |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=4525000, episode_reward=3002.55 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4525000      |
| train/                  |              |
|    approx_kl            | 0.0117631005 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0608      |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.196        |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 16.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 277      |
|    time_elapsed    | 16022    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=3002.44 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.009944574 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0547     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.417       |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 24.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 278      |
|    time_elapsed    | 16085    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 279         |
|    time_elapsed         | 16138       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.010957954 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0973     |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 36.9        |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=3002.43 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 4575000    |
| train/                  |            |
|    approx_kl            | 0.00249397 |
|    clip_fraction        | 0.0059     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.028     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0473     |
|    n_updates            | 2790       |
|    policy_gradient_loss | 0.00106    |
|    value_loss           | 31         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 280      |
|    time_elapsed    | 16200    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=3002.89 +/- 0.55
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4600000      |
| train/                  |              |
|    approx_kl            | 0.0038813972 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.233        |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00428     |
|    value_loss           | 1.1          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 281      |
|    time_elapsed    | 16261    |
|    total_timesteps | 4603904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.89e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 282          |
|    time_elapsed         | 16313        |
|    total_timesteps      | 4620288      |
| train/                  |              |
|    approx_kl            | 0.0035819146 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0493      |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0951       |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.0197      |
|    value_loss           | 1.19         |
------------------------------------------
Eval num_timesteps=4625000, episode_reward=2302.60 +/- 1124.12
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.3e+03    |
| time/                   |            |
|    total_timesteps      | 4625000    |
| train/                  |            |
|    approx_kl            | 0.11668607 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.229      |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.00354   |
|    value_loss           | 1.41       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 283      |
|    time_elapsed    | 16374    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2422.60 +/- 1159.28
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 4650000    |
| train/                  |            |
|    approx_kl            | 0.10307876 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 2830       |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 2.73       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 284      |
|    time_elapsed    | 16435    |
|    total_timesteps | 4653056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.63e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 285          |
|    time_elapsed         | 16487        |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0032528695 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.334       |
|    explained_variance   | 0.614        |
|    learning_rate        | 0.0003       |
|    loss                 | 644          |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 478          |
------------------------------------------
Eval num_timesteps=4675000, episode_reward=3002.45 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.001315304 |
|    clip_fraction        | 0.00851     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 238         |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.000856   |
|    value_loss           | 860         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 286      |
|    time_elapsed    | 16548    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2882.55 +/- 240.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 4700000      |
| train/                  |              |
|    approx_kl            | 0.0020168303 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.491       |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.0003       |
|    loss                 | 127          |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 687          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -221     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 287      |
|    time_elapsed    | 16608    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -422        |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 288         |
|    time_elapsed         | 16660       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.003802545 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.1        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 413         |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=3002.41 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4725000      |
| train/                  |              |
|    approx_kl            | 0.0025597042 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.388       |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0003       |
|    loss                 | 155          |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 564          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 289      |
|    time_elapsed    | 16720    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=2322.89 +/- 1083.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32e+03    |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.010923328 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 270         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 290      |
|    time_elapsed    | 16781    |
|    total_timesteps | 4751360  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 929          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 291          |
|    time_elapsed         | 16832        |
|    total_timesteps      | 4767744      |
| train/                  |              |
|    approx_kl            | 0.0066939937 |
|    clip_fraction        | 0.0611       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.492       |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 99.6         |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=4775000, episode_reward=3002.31 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4775000      |
| train/                  |              |
|    approx_kl            | 0.0062932344 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.524       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 700          |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.00657     |
|    value_loss           | 670          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 292      |
|    time_elapsed    | 16893    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=2882.38 +/- 240.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.013757654 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.24e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 293      |
|    time_elapsed    | 16953    |
|    total_timesteps | 4800512  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.55e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 294          |
|    time_elapsed         | 17007        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 0.0033424548 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 104          |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.000161    |
|    value_loss           | 194          |
------------------------------------------
Eval num_timesteps=4825000, episode_reward=3002.46 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.008567229 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 32.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 295      |
|    time_elapsed    | 17071    |
|    total_timesteps | 4833280  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 2.64e+03  |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 296       |
|    time_elapsed         | 17124     |
|    total_timesteps      | 4849664   |
| train/                  |           |
|    approx_kl            | 0.0411733 |
|    clip_fraction        | 0.0954    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.192    |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.26      |
|    n_updates            | 2950      |
|    policy_gradient_loss | -0.0156   |
|    value_loss           | 18.3      |
---------------------------------------
Eval num_timesteps=4850000, episode_reward=3002.65 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.024309928 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 10.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 297      |
|    time_elapsed    | 17186    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=3002.79 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.009498885 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.535       |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 12.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 298      |
|    time_elapsed    | 17248    |
|    total_timesteps | 4882432  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.86e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 299          |
|    time_elapsed         | 17300        |
|    total_timesteps      | 4898816      |
| train/                  |              |
|    approx_kl            | 0.0033343784 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.44         |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.000178    |
|    value_loss           | 20.2         |
------------------------------------------
Eval num_timesteps=4900000, episode_reward=3002.58 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.030942561 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 44.9        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 16.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 300      |
|    time_elapsed    | 17361    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=3002.64 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 4925000      |
| train/                  |              |
|    approx_kl            | 0.0031179183 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0917      |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.552        |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.000353    |
|    value_loss           | 25.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 301      |
|    time_elapsed    | 17423    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.83e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 302         |
|    time_elapsed         | 17474       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.019919192 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 19.5        |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=3002.46 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.035315953 |
|    clip_fraction        | 0.0512      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.719       |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00922    |
|    value_loss           | 17.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 303      |
|    time_elapsed    | 17535    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=3002.60 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.009493796 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.868       |
|    n_updates            | 3030        |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 23.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 304      |
|    time_elapsed    | 17596    |
|    total_timesteps | 4980736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.86e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 305          |
|    time_elapsed         | 17647        |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0123316515 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.604        |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.000832    |
|    value_loss           | 2.02         |
------------------------------------------
Eval num_timesteps=5000000, episode_reward=3002.92 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.015779834 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 25.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 306      |
|    time_elapsed    | 17708    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=3002.77 +/- 0.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 5025000    |
| train/                  |            |
|    approx_kl            | 0.08713861 |
|    clip_fraction        | 0.0437     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 3060       |
|    policy_gradient_loss | -0.0021    |
|    value_loss           | 9.48       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 307      |
|    time_elapsed    | 17768    |
|    total_timesteps | 5029888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.16e+03   |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 308        |
|    time_elapsed         | 17820      |
|    total_timesteps      | 5046272    |
| train/                  |            |
|    approx_kl            | 0.08195279 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.484     |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.92       |
|    n_updates            | 3070       |
|    policy_gradient_loss | -0.0284    |
|    value_loss           | 6.46       |
----------------------------------------
Eval num_timesteps=5050000, episode_reward=3002.58 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5050000      |
| train/                  |              |
|    approx_kl            | 0.0036602332 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.0003       |
|    loss                 | 262          |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 513          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 309      |
|    time_elapsed    | 17881    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=-48.49 +/- 3893.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -48.5       |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.017886288 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 99.5        |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 476         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 310      |
|    time_elapsed    | 17941    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -341        |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 311         |
|    time_elapsed         | 17993       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.032613657 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.4        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 533         |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=3002.47 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5100000      |
| train/                  |              |
|    approx_kl            | 0.0135673545 |
|    clip_fraction        | 0.0466       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.663       |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | 65           |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00411     |
|    value_loss           | 322          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 312      |
|    time_elapsed    | 18060    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=1822.73 +/- 2359.56
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.82e+03   |
| time/                   |            |
|    total_timesteps      | 5125000    |
| train/                  |            |
|    approx_kl            | 0.02453252 |
|    clip_fraction        | 0.0617     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 61.1       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.00481   |
|    value_loss           | 293        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 313      |
|    time_elapsed    | 18123    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 782         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 314         |
|    time_elapsed         | 18175       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.005192464 |
|    clip_fraction        | 0.0538      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 282         |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 392         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=3002.28 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.006353235 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 413         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 315      |
|    time_elapsed    | 18238    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=2422.49 +/- 1159.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.002858764 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 284         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 316      |
|    time_elapsed    | 18299    |
|    total_timesteps | 5177344  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.37e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 317          |
|    time_elapsed         | 18351        |
|    total_timesteps      | 5193728      |
| train/                  |              |
|    approx_kl            | 0.0071778083 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 27.4         |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=5200000, episode_reward=2443.11 +/- 1119.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 5200000      |
| train/                  |              |
|    approx_kl            | 0.0035572355 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.5         |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 326          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 318      |
|    time_elapsed    | 18412    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=3002.13 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.005320404 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.6         |
|    n_updates            | 3180        |
|    policy_gradient_loss | 0.000435    |
|    value_loss           | 68.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 319      |
|    time_elapsed    | 18474    |
|    total_timesteps | 5226496  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.64e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 320          |
|    time_elapsed         | 18525        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0059985416 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.4         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000243    |
|    value_loss           | 112          |
------------------------------------------
Eval num_timesteps=5250000, episode_reward=3002.71 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.006345961 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 54.7        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.000182   |
|    value_loss           | 77.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 321      |
|    time_elapsed    | 18586    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=3002.60 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.010091836 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00073    |
|    value_loss           | 8.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 322      |
|    time_elapsed    | 18647    |
|    total_timesteps | 5275648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.83e+03   |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 323        |
|    time_elapsed         | 18698      |
|    total_timesteps      | 5292032    |
| train/                  |            |
|    approx_kl            | 0.00667644 |
|    clip_fraction        | 0.0246     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 158        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.000184  |
|    value_loss           | 20.2       |
----------------------------------------
Eval num_timesteps=5300000, episode_reward=3002.37 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.012686372 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 18.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 324      |
|    time_elapsed    | 18759    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.84e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 325         |
|    time_elapsed         | 18810       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.003399216 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0832     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 3240        |
|    policy_gradient_loss | 0.00013     |
|    value_loss           | 19.1        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=3002.94 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.013680398 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 31.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 326      |
|    time_elapsed    | 18871    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2422.38 +/- 1159.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 5350000    |
| train/                  |            |
|    approx_kl            | 0.05808855 |
|    clip_fraction        | 0.0519     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.37       |
|    n_updates            | 3260       |
|    policy_gradient_loss | -0.00251   |
|    value_loss           | 28.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 327      |
|    time_elapsed    | 18932    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 328         |
|    time_elapsed         | 18983       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.004011498 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 327         |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=3002.85 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.011391334 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 269         |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 329      |
|    time_elapsed    | 19046    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=3002.78 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.012471536 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 58.2        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 271         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 330      |
|    time_elapsed    | 19111    |
|    total_timesteps | 5406720  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.18e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 331          |
|    time_elapsed         | 19164        |
|    total_timesteps      | 5423104      |
| train/                  |              |
|    approx_kl            | 0.0055812374 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.392       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.18         |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.000729    |
|    value_loss           | 41.2         |
------------------------------------------
Eval num_timesteps=5425000, episode_reward=3002.31 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.010834429 |
|    clip_fraction        | 0.0512      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.77        |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 4.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 332      |
|    time_elapsed    | 19226    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2422.85 +/- 1159.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.002671292 |
|    clip_fraction        | 0.0214      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 25.7        |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 38.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 333      |
|    time_elapsed    | 19288    |
|    total_timesteps | 5455872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.81e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 334          |
|    time_elapsed         | 19340        |
|    total_timesteps      | 5472256      |
| train/                  |              |
|    approx_kl            | 0.0065900655 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.478       |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0003       |
|    loss                 | 25.8         |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=5475000, episode_reward=2423.05 +/- 1159.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.020321194 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 78          |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 57.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 335      |
|    time_elapsed    | 19402    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=3002.77 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.004814883 |
|    clip_fraction        | 0.0554      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.6        |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 62.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 336      |
|    time_elapsed    | 19463    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 337         |
|    time_elapsed         | 19515       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.032167986 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.75        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 30.7        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=3002.55 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5525000      |
| train/                  |              |
|    approx_kl            | 0.0068686623 |
|    clip_fraction        | 0.0678       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.3         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 59.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 282      |
|    iterations      | 338      |
|    time_elapsed    | 19576    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=3002.35 +/- 0.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 5550000    |
| train/                  |            |
|    approx_kl            | 0.02611818 |
|    clip_fraction        | 0.0755     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.00056   |
|    value_loss           | 22         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 339      |
|    time_elapsed    | 19637    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.52e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 340         |
|    time_elapsed         | 19688       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.009426376 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 254         |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 757         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=3002.64 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.012374109 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 294         |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 905         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 341      |
|    time_elapsed    | 19749    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=3002.37 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.017280232 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 91          |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 393         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 342      |
|    time_elapsed    | 19809    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 343         |
|    time_elapsed         | 19861       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.005061312 |
|    clip_fraction        | 0.0416      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.343      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 67.4        |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=3002.74 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.008311349 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.27        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 11.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 344      |
|    time_elapsed    | 19921    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=3003.04 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5650000      |
| train/                  |              |
|    approx_kl            | 0.0054364027 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.332       |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.13         |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 9.99         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 345      |
|    time_elapsed    | 19982    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.9e+03     |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 346         |
|    time_elapsed         | 20034       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.014604207 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.000816   |
|    value_loss           | 20          |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=2443.26 +/- 1118.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44e+03    |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.017121768 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.288      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 23.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 347      |
|    time_elapsed    | 20099    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=3002.54 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.016682645 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.98        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.000552   |
|    value_loss           | 28.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 348      |
|    time_elapsed    | 20162    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.65e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 349         |
|    time_elapsed         | 20215       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.041140094 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.848       |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=3002.57 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.012647821 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 206         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    fps             | 282      |
|    iterations      | 350      |
|    time_elapsed    | 20277    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=3002.38 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5750000      |
| train/                  |              |
|    approx_kl            | 0.0062820483 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35         |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 74           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 351      |
|    time_elapsed    | 20339    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 352         |
|    time_elapsed         | 20390       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.012676641 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0718     |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.187       |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 1.52        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=3002.52 +/- 0.56
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3e+03     |
| time/                   |           |
|    total_timesteps      | 5775000   |
| train/                  |           |
|    approx_kl            | 0.0063213 |
|    clip_fraction        | 0.0439    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.197    |
|    explained_variance   | 0.964     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.594     |
|    n_updates            | 3520      |
|    policy_gradient_loss | -0.00194  |
|    value_loss           | 2.74      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 353      |
|    time_elapsed    | 20452    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.97e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 354         |
|    time_elapsed         | 20504       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.056036655 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 0.607       |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=3002.63 +/- 0.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 5800000    |
| train/                  |            |
|    approx_kl            | 0.04340475 |
|    clip_fraction        | 0.0897     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.262      |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 1.28       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 355      |
|    time_elapsed    | 20565    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=2882.70 +/- 240.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.88e+03   |
| time/                   |            |
|    total_timesteps      | 5825000    |
| train/                  |            |
|    approx_kl            | 0.12475498 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.358      |
|    n_updates            | 3550       |
|    policy_gradient_loss | -0.0305    |
|    value_loss           | 0.872      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 356      |
|    time_elapsed    | 20626    |
|    total_timesteps | 5832704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.66e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 357        |
|    time_elapsed         | 20677      |
|    total_timesteps      | 5849088    |
| train/                  |            |
|    approx_kl            | 0.02244056 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.392      |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 24.6       |
----------------------------------------
Eval num_timesteps=5850000, episode_reward=2422.70 +/- 1159.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.005688936 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.7        |
|    n_updates            | 3570        |
|    policy_gradient_loss | 0.000524    |
|    value_loss           | 160         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 358      |
|    time_elapsed    | 20738    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=3002.63 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5875000      |
| train/                  |              |
|    approx_kl            | 0.0045141103 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.383       |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.3         |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 232          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 359      |
|    time_elapsed    | 20798    |
|    total_timesteps | 5881856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.75e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 360        |
|    time_elapsed         | 20850      |
|    total_timesteps      | 5898240    |
| train/                  |            |
|    approx_kl            | 0.03444802 |
|    clip_fraction        | 0.0477     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.6       |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.000659  |
|    value_loss           | 91.5       |
----------------------------------------
Eval num_timesteps=5900000, episode_reward=2884.23 +/- 236.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.008595327 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.423      |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.9        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00651    |
|    value_loss           | 435         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 361      |
|    time_elapsed    | 20911    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=3002.59 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 5925000      |
| train/                  |              |
|    approx_kl            | 0.0045414157 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.0003       |
|    loss                 | 281          |
|    n_updates            | 3610         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 490          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 362      |
|    time_elapsed    | 20971    |
|    total_timesteps | 5931008  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.51e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 363          |
|    time_elapsed         | 21023        |
|    total_timesteps      | 5947392      |
| train/                  |              |
|    approx_kl            | 0.0044181985 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.2         |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=5950000, episode_reward=2422.94 +/- 1158.85
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 5950000      |
| train/                  |              |
|    approx_kl            | 0.0054292306 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 385          |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 570          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 364      |
|    time_elapsed    | 21083    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=3002.59 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.039687492 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 808         |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 2.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 365      |
|    time_elapsed    | 21149    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.33e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 366         |
|    time_elapsed         | 21202       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.008423865 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.346      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 429         |
|    n_updates            | 3650        |
|    policy_gradient_loss | 0.00228     |
|    value_loss           | 670         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=3002.77 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6000000      |
| train/                  |              |
|    approx_kl            | 0.0075237737 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.084       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 17.2         |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 56           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 367      |
|    time_elapsed    | 21265    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=2422.96 +/- 1159.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.029819723 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 237         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 368      |
|    time_elapsed    | 21327    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 369         |
|    time_elapsed         | 21379       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.039019346 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0705     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.41        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 77.9        |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=3002.95 +/- 0.76
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6050000      |
| train/                  |              |
|    approx_kl            | 0.0090006925 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.262       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 146          |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00579     |
|    value_loss           | 354          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 370      |
|    time_elapsed    | 21440    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=3002.46 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6075000      |
| train/                  |              |
|    approx_kl            | 0.0013465332 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.9         |
|    n_updates            | 3700         |
|    policy_gradient_loss | 0.000631     |
|    value_loss           | 50.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 371      |
|    time_elapsed    | 21502    |
|    total_timesteps | 6078464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.6e+03      |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 372          |
|    time_elapsed         | 21553        |
|    total_timesteps      | 6094848      |
| train/                  |              |
|    approx_kl            | 0.0005710413 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0463      |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62         |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.000469    |
|    value_loss           | 16.9         |
------------------------------------------
Eval num_timesteps=6100000, episode_reward=3003.03 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6100000      |
| train/                  |              |
|    approx_kl            | 0.0011875894 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0363      |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.17         |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000469    |
|    value_loss           | 8.15         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 373      |
|    time_elapsed    | 21615    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=3002.41 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6125000      |
| train/                  |              |
|    approx_kl            | 0.0021012032 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 75.3         |
|    n_updates            | 3730         |
|    policy_gradient_loss | 0.00702      |
|    value_loss           | 301          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 374      |
|    time_elapsed    | 21675    |
|    total_timesteps | 6127616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.73e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 375          |
|    time_elapsed         | 21727        |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0005027241 |
|    clip_fraction        | 0.00217      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0174      |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.691        |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000623    |
|    value_loss           | 9.24         |
------------------------------------------
Eval num_timesteps=6150000, episode_reward=3002.40 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6150000    |
| train/                  |            |
|    approx_kl            | 0.01061429 |
|    clip_fraction        | 0.00558    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0194    |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.44       |
|    n_updates            | 3750       |
|    policy_gradient_loss | -0.00368   |
|    value_loss           | 109        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 376      |
|    time_elapsed    | 21788    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=2422.69 +/- 1159.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.039404076 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0556     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.402       |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 377      |
|    time_elapsed    | 21849    |
|    total_timesteps | 6176768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.89e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 378        |
|    time_elapsed         | 21900      |
|    total_timesteps      | 6193152    |
| train/                  |            |
|    approx_kl            | 0.08719883 |
|    clip_fraction        | 0.0411     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.611      |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.000784  |
|    value_loss           | 9.29       |
----------------------------------------
Eval num_timesteps=6200000, episode_reward=2422.83 +/- 1159.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 6200000      |
| train/                  |              |
|    approx_kl            | 0.0054272953 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.75         |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 6.3          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 379      |
|    time_elapsed    | 21961    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2422.57 +/- 1159.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.009100423 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.539       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 2.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 380      |
|    time_elapsed    | 22022    |
|    total_timesteps | 6225920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.88e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 381        |
|    time_elapsed         | 22073      |
|    total_timesteps      | 6242304    |
| train/                  |            |
|    approx_kl            | 0.06449363 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.09       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0308    |
|    value_loss           | 19.5       |
----------------------------------------
Eval num_timesteps=6250000, episode_reward=3002.49 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6250000      |
| train/                  |              |
|    approx_kl            | 0.0016455352 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.343       |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.0003       |
|    loss                 | 99.9         |
|    n_updates            | 3810         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 809          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 382      |
|    time_elapsed    | 22133    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=3002.32 +/- 0.07
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6275000      |
| train/                  |              |
|    approx_kl            | 0.0027809283 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.0003       |
|    loss                 | 81.3         |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 524          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 383      |
|    time_elapsed    | 22194    |
|    total_timesteps | 6275072  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -974         |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 384          |
|    time_elapsed         | 22245        |
|    total_timesteps      | 6291456      |
| train/                  |              |
|    approx_kl            | 0.0017908657 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | 161          |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=6300000, episode_reward=1842.99 +/- 1420.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.004089414 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 42.4        |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 350         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 385      |
|    time_elapsed    | 22306    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 15.1        |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 386         |
|    time_elapsed         | 22356       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.005862503 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.5        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 373         |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=3002.80 +/- 0.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6325000      |
| train/                  |              |
|    approx_kl            | 0.0027492663 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 63.1         |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 285          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 387      |
|    time_elapsed    | 22417    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=3002.40 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6350000      |
| train/                  |              |
|    approx_kl            | 0.0033644694 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.5         |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 245          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 388      |
|    time_elapsed    | 22478    |
|    total_timesteps | 6356992  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 1.16e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 389        |
|    time_elapsed         | 22529      |
|    total_timesteps      | 6373376    |
| train/                  |            |
|    approx_kl            | 0.00739195 |
|    clip_fraction        | 0.04       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.3       |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.00253   |
|    value_loss           | 186        |
----------------------------------------
Eval num_timesteps=6375000, episode_reward=3002.47 +/- 0.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6375000      |
| train/                  |              |
|    approx_kl            | 0.0032725583 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 39.8         |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 149          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 390      |
|    time_elapsed    | 22589    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=3002.79 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6400000      |
| train/                  |              |
|    approx_kl            | 0.0029633304 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.7         |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.000621    |
|    value_loss           | 88.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 391      |
|    time_elapsed    | 22650    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 392         |
|    time_elapsed         | 22703       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.013744632 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.4        |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 88.5        |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=3002.18 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.003274271 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0873     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.1        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 393      |
|    time_elapsed    | 22765    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=2422.60 +/- 1159.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.002296643 |
|    clip_fraction        | 0.00773     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0543     |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.41        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.000956   |
|    value_loss           | 17.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 394      |
|    time_elapsed    | 22826    |
|    total_timesteps | 6455296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.94e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 395          |
|    time_elapsed         | 22878        |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 0.0013550485 |
|    clip_fraction        | 0.00656      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0518      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.28         |
|    n_updates            | 3940         |
|    policy_gradient_loss | -9.88e-05    |
|    value_loss           | 13           |
------------------------------------------
Eval num_timesteps=6475000, episode_reward=3002.90 +/- 0.44
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6475000    |
| train/                  |            |
|    approx_kl            | 0.02001189 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.85       |
|    n_updates            | 3950       |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 8.55       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 396      |
|    time_elapsed    | 22940    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=3002.41 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6500000    |
| train/                  |            |
|    approx_kl            | 0.04975211 |
|    clip_fraction        | 0.0841     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.243     |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.00479   |
|    value_loss           | 4.66       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 397      |
|    time_elapsed    | 23003    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.87e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 398         |
|    time_elapsed         | 23055       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.006433224 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.374      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.65        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 9.32        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=2762.32 +/- 480.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76e+03    |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.021511722 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.429      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.81        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 8.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 399      |
|    time_elapsed    | 23117    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=3002.80 +/- 0.85
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6550000    |
| train/                  |            |
|    approx_kl            | 0.12905848 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53       |
|    n_updates            | 3990       |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 4.18       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 400      |
|    time_elapsed    | 23179    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.02e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 401         |
|    time_elapsed         | 23231       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.008346295 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 995         |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 2.44e+03    |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=3002.53 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.008538875 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 434         |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00687    |
|    value_loss           | 721         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 95.4     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 402      |
|    time_elapsed    | 23292    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2422.45 +/- 1159.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.007915203 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00485    |
|    value_loss           | 968         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 60.4     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 403      |
|    time_elapsed    | 23354    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 744         |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 404         |
|    time_elapsed         | 23406       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.005848589 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 215         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 621         |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=3002.26 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6625000      |
| train/                  |              |
|    approx_kl            | 0.0048150546 |
|    clip_fraction        | 0.0821       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 295          |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 1.02e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 405      |
|    time_elapsed    | 23467    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=3002.59 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.005315654 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 237         |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 430         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 406      |
|    time_elapsed    | 23528    |
|    total_timesteps | 6651904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.37e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 407          |
|    time_elapsed         | 23580        |
|    total_timesteps      | 6668288      |
| train/                  |              |
|    approx_kl            | 0.0035850634 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.9         |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 135          |
------------------------------------------
Eval num_timesteps=6675000, episode_reward=3002.42 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6675000      |
| train/                  |              |
|    approx_kl            | 0.0111370385 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.66         |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00606     |
|    value_loss           | 193          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 408      |
|    time_elapsed    | 23641    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=3002.27 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.007965694 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.74        |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.000571   |
|    value_loss           | 17.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 409      |
|    time_elapsed    | 23704    |
|    total_timesteps | 6701056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.81e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 410          |
|    time_elapsed         | 23756        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0031572392 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.34         |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.00066     |
|    value_loss           | 20.1         |
------------------------------------------
Eval num_timesteps=6725000, episode_reward=3002.22 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.008555602 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.6         |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 82.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 411      |
|    time_elapsed    | 23818    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2882.35 +/- 240.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.88e+03   |
| time/                   |            |
|    total_timesteps      | 6750000    |
| train/                  |            |
|    approx_kl            | 0.03978846 |
|    clip_fraction        | 0.0527     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 4110       |
|    policy_gradient_loss | -0.0053    |
|    value_loss           | 9.39       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 412      |
|    time_elapsed    | 23879    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.86e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 413         |
|    time_elapsed         | 23931       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.016838033 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0682     |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.95        |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 21.3        |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=1842.92 +/- 1420.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.010889796 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.16        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.000717   |
|    value_loss           | 8.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 414      |
|    time_elapsed    | 23993    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.86e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 415         |
|    time_elapsed         | 24045       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.018963158 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.533       |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 9.63        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=3002.70 +/- 0.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 6800000    |
| train/                  |            |
|    approx_kl            | 0.00743404 |
|    clip_fraction        | 0.0326     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 4150       |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 51.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 416      |
|    time_elapsed    | 24107    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=3002.78 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 6825000      |
| train/                  |              |
|    approx_kl            | 0.0071343654 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.81         |
|    n_updates            | 4160         |
|    policy_gradient_loss | 0.000555     |
|    value_loss           | 25.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 282      |
|    iterations      | 417      |
|    time_elapsed    | 24170    |
|    total_timesteps | 6832128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.07e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 418        |
|    time_elapsed         | 24222      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.16526735 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 4170       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 12         |
----------------------------------------
Eval num_timesteps=6850000, episode_reward=1706.45 +/- 2592.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.009148479 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 615         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 419      |
|    time_elapsed    | 24284    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=3002.76 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.018661812 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 549         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 505      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 420      |
|    time_elapsed    | 24346    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 165         |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 421         |
|    time_elapsed         | 24398       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.008968551 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.9        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 478         |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=3002.95 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.011203835 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.6        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 517         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 647      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 422      |
|    time_elapsed    | 24460    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=1706.53 +/- 2592.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.014281389 |
|    clip_fraction        | 0.0827      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 40          |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00606    |
|    value_loss           | 351         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 423      |
|    time_elapsed    | 24521    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 424         |
|    time_elapsed         | 24572       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.009041778 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.15        |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 48.1        |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=3002.70 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.014204685 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 425      |
|    time_elapsed    | 24633    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=3002.34 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.004620952 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.466      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.4        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 373         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 426      |
|    time_elapsed    | 24694    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.69e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 427         |
|    time_elapsed         | 24744       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.014738105 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.9        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.000922   |
|    value_loss           | 144         |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=2182.36 +/- 1073.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18e+03    |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.010836922 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.51        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 91.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 428      |
|    time_elapsed    | 24804    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=2882.10 +/- 240.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.041166574 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 39.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 429      |
|    time_elapsed    | 24865    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.67e+03    |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 430         |
|    time_elapsed         | 24915       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.017674068 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.3        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=2303.02 +/- 1123.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.013666155 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34e+03    |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 15.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 431      |
|    time_elapsed    | 24974    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=3002.49 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7075000      |
| train/                  |              |
|    approx_kl            | 0.0111932615 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.2          |
|    n_updates            | 4310         |
|    policy_gradient_loss | -0.000489    |
|    value_loss           | 6.39         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 432      |
|    time_elapsed    | 25033    |
|    total_timesteps | 7077888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.79e+03     |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 433          |
|    time_elapsed         | 25082        |
|    total_timesteps      | 7094272      |
| train/                  |              |
|    approx_kl            | 0.0044349786 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0947      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 157          |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.000793    |
|    value_loss           | 60.9         |
------------------------------------------
Eval num_timesteps=7100000, episode_reward=2422.71 +/- 1159.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.012220798 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.403       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 79.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 282      |
|    iterations      | 434      |
|    time_elapsed    | 25140    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=2762.34 +/- 294.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.76e+03   |
| time/                   |            |
|    total_timesteps      | 7125000    |
| train/                  |            |
|    approx_kl            | 0.07220218 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.221     |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 1.64       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 435      |
|    time_elapsed    | 25199    |
|    total_timesteps | 7127040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.65e+03   |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 436        |
|    time_elapsed         | 25247      |
|    total_timesteps      | 7143424    |
| train/                  |            |
|    approx_kl            | 0.07621435 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.42      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.67       |
|    n_updates            | 4350       |
|    policy_gradient_loss | -0.00312   |
|    value_loss           | 34.3       |
----------------------------------------
Eval num_timesteps=7150000, episode_reward=2642.38 +/- 294.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64e+03    |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.019800534 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.36        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 24.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 437      |
|    time_elapsed    | 25305    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=2882.36 +/- 240.14
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.88e+03  |
| time/                   |           |
|    total_timesteps      | 7175000   |
| train/                  |           |
|    approx_kl            | 0.0375112 |
|    clip_fraction        | 0.101     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.423    |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.11      |
|    n_updates            | 4370      |
|    policy_gradient_loss | -0.00343  |
|    value_loss           | 30.7      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 438      |
|    time_elapsed    | 25364    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.64e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 439         |
|    time_elapsed         | 25412       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.038783714 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.494       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 12.8        |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=3002.80 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.010628847 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.663       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.000586   |
|    value_loss           | 10.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 440      |
|    time_elapsed    | 25470    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=3002.39 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.007065428 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.463       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.000404   |
|    value_loss           | 6.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 441      |
|    time_elapsed    | 25528    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.53e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 442         |
|    time_elapsed         | 25577       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.025535375 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.466      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.937       |
|    n_updates            | 4410        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 2.65        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=3002.71 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7250000      |
| train/                  |              |
|    approx_kl            | 0.0025453519 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.1         |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 236          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 443      |
|    time_elapsed    | 25635    |
|    total_timesteps | 7258112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.65e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 444          |
|    time_elapsed         | 25684        |
|    total_timesteps      | 7274496      |
| train/                  |              |
|    approx_kl            | 0.0026079295 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.0003       |
|    loss                 | 65.5         |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.000975    |
|    value_loss           | 319          |
------------------------------------------
Eval num_timesteps=7275000, episode_reward=2882.57 +/- 240.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.88e+03     |
| time/                   |              |
|    total_timesteps      | 7275000      |
| train/                  |              |
|    approx_kl            | 0.0029341842 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 126          |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 271          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 445      |
|    time_elapsed    | 25742    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=3002.41 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7300000      |
| train/                  |              |
|    approx_kl            | 0.0036262828 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 40.5         |
|    n_updates            | 4450         |
|    policy_gradient_loss | 8.6e-05      |
|    value_loss           | 139          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 446      |
|    time_elapsed    | 25800    |
|    total_timesteps | 7307264  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.67e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 447          |
|    time_elapsed         | 25849        |
|    total_timesteps      | 7323648      |
| train/                  |              |
|    approx_kl            | 0.0070781363 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 205          |
------------------------------------------
Eval num_timesteps=7325000, episode_reward=3002.72 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.019741151 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.9        |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 448      |
|    time_elapsed    | 25907    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=2882.67 +/- 240.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.88e+03    |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.009293553 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.6        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 298         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 449      |
|    time_elapsed    | 25965    |
|    total_timesteps | 7356416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.45e+03   |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 450        |
|    time_elapsed         | 26014      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.00813062 |
|    clip_fraction        | 0.0377     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.00116   |
|    value_loss           | 51.6       |
----------------------------------------
Eval num_timesteps=7375000, episode_reward=3002.58 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7375000      |
| train/                  |              |
|    approx_kl            | 0.0045985696 |
|    clip_fraction        | 0.0555       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.2         |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 43           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 451      |
|    time_elapsed    | 26072    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=3002.43 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.009324634 |
|    clip_fraction        | 0.0501      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.58        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 17          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 452      |
|    time_elapsed    | 26130    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.8e+03     |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 453         |
|    time_elapsed         | 26179       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.009671824 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.429      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 6.68        |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=3002.29 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.017199397 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.373      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 30          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 454      |
|    time_elapsed    | 26237    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=3002.60 +/- 0.66
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7450000      |
| train/                  |              |
|    approx_kl            | 0.0067900415 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.14         |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 27.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 455      |
|    time_elapsed    | 26295    |
|    total_timesteps | 7454720  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.77e+03     |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 456          |
|    time_elapsed         | 26344        |
|    total_timesteps      | 7471104      |
| train/                  |              |
|    approx_kl            | 0.0127978055 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.431        |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 8.81         |
------------------------------------------
Eval num_timesteps=7475000, episode_reward=2423.28 +/- 1159.61
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 7475000      |
| train/                  |              |
|    approx_kl            | 0.0056183757 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.43         |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 3.42         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 457      |
|    time_elapsed    | 26402    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=3002.57 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.014984748 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.354       |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 2.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 458      |
|    time_elapsed    | 26461    |
|    total_timesteps | 7503872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.94e+03   |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 459        |
|    time_elapsed         | 26510      |
|    total_timesteps      | 7520256    |
| train/                  |            |
|    approx_kl            | 0.04118721 |
|    clip_fraction        | 0.051      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.757      |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.00406   |
|    value_loss           | 5.82       |
----------------------------------------
Eval num_timesteps=7525000, episode_reward=3002.65 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.021616876 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 2.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 460      |
|    time_elapsed    | 26568    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=3002.52 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7550000      |
| train/                  |              |
|    approx_kl            | 0.0045478074 |
|    clip_fraction        | 0.00577      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.019       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.07         |
|    n_updates            | 4600         |
|    policy_gradient_loss | 0.000146     |
|    value_loss           | 13.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 283      |
|    iterations      | 461      |
|    time_elapsed    | 26626    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.83e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 462         |
|    time_elapsed         | 26675       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.062528536 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0488     |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 1.08        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=3002.48 +/- 0.31
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3e+03     |
| time/                   |           |
|    total_timesteps      | 7575000   |
| train/                  |           |
|    approx_kl            | 0.0044742 |
|    clip_fraction        | 0.0272    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.18     |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | 15        |
|    n_updates            | 4620      |
|    policy_gradient_loss | -0.000663 |
|    value_loss           | 53.8      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 463      |
|    time_elapsed    | 26733    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=3002.56 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.005598836 |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 8           |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 107         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 464      |
|    time_elapsed    | 26791    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.76e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 465         |
|    time_elapsed         | 26840       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.021841291 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.81        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 39.6        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=3002.64 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.009798244 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00038    |
|    value_loss           | 20.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 466      |
|    time_elapsed    | 26898    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=3002.33 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.026028436 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 26.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 467      |
|    time_elapsed    | 26956    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.73e+03    |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 468         |
|    time_elapsed         | 27005       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.005831712 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.644       |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 8.57        |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=3002.47 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.005237867 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.6        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 178         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 469      |
|    time_elapsed    | 27063    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=3002.82 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.036114268 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.47        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 27.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 470      |
|    time_elapsed    | 27121    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 471         |
|    time_elapsed         | 27170       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.044457212 |
|    clip_fraction        | 0.0681      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.2         |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 8.32        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=3002.74 +/- 0.58
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7725000      |
| train/                  |              |
|    approx_kl            | 0.0112766195 |
|    clip_fraction        | 0.0532       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.312       |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.13         |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00407     |
|    value_loss           | 54.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 472      |
|    time_elapsed    | 27228    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.57e+03    |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 473         |
|    time_elapsed         | 27277       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.078924395 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.853       |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 12.5        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=3002.52 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.010128267 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.3        |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.000855   |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 474      |
|    time_elapsed    | 27335    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=3002.54 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7775000      |
| train/                  |              |
|    approx_kl            | 0.0041518975 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 133          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 475      |
|    time_elapsed    | 27393    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 383         |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 476         |
|    time_elapsed         | 27441       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.011029853 |
|    clip_fraction        | 0.032       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 792         |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=3002.62 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7800000      |
| train/                  |              |
|    approx_kl            | 0.0073162233 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 382          |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 738          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 44.1     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 477      |
|    time_elapsed    | 27499    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=1144.00 +/- 3717.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14e+03    |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.004207253 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.281      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 193         |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00083    |
|    value_loss           | 368         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 284      |
|    iterations      | 478      |
|    time_elapsed    | 27557    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.1e+03     |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 479         |
|    time_elapsed         | 27606       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.013751572 |
|    clip_fraction        | 0.0343      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 204         |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 494         |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=-644.97 +/- 4469.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -645         |
| time/                   |              |
|    total_timesteps      | 7850000      |
| train/                  |              |
|    approx_kl            | 0.0040421914 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.361       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 564          |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00172     |
|    value_loss           | 217          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 480      |
|    time_elapsed    | 27664    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=2423.07 +/- 1159.32
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 7875000    |
| train/                  |            |
|    approx_kl            | 0.00787759 |
|    clip_fraction        | 0.0414     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 37.8       |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.00347   |
|    value_loss           | 207        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 481      |
|    time_elapsed    | 27722    |
|    total_timesteps | 7880704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.04e+03     |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 482          |
|    time_elapsed         | 27771        |
|    total_timesteps      | 7897088      |
| train/                  |              |
|    approx_kl            | 0.0060442286 |
|    clip_fraction        | 0.0671       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 240          |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00768     |
|    value_loss           | 488          |
------------------------------------------
Eval num_timesteps=7900000, episode_reward=2422.84 +/- 1159.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.004957755 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 168         |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 305         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 483      |
|    time_elapsed    | 27829    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=3002.46 +/- 0.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7925000      |
| train/                  |              |
|    approx_kl            | 0.0067017856 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 25.2         |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00465     |
|    value_loss           | 155          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 484      |
|    time_elapsed    | 27887    |
|    total_timesteps | 7929856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.51e+03     |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 485          |
|    time_elapsed         | 27936        |
|    total_timesteps      | 7946240      |
| train/                  |              |
|    approx_kl            | 0.0030831434 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.59         |
|    n_updates            | 4840         |
|    policy_gradient_loss | 0.0009       |
|    value_loss           | 44.9         |
------------------------------------------
Eval num_timesteps=7950000, episode_reward=3002.55 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.003574246 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -4.99e-05   |
|    value_loss           | 49.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 486      |
|    time_elapsed    | 27994    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=3002.56 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 7975000      |
| train/                  |              |
|    approx_kl            | 0.0029440871 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.16        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.1         |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.000694    |
|    value_loss           | 154          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 487      |
|    time_elapsed    | 28052    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.84e+03    |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 488         |
|    time_elapsed         | 28101       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.007654026 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.091      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00079    |
|    value_loss           | 38.3        |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=3002.57 +/- 0.75
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8000000      |
| train/                  |              |
|    approx_kl            | 0.0031669838 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.2          |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.000875    |
|    value_loss           | 12.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 489      |
|    time_elapsed    | 28159    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=3002.42 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.001842189 |
|    clip_fraction        | 0.00612     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0481     |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.27        |
|    n_updates            | 4890        |
|    policy_gradient_loss | 0.00023     |
|    value_loss           | 5.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 490      |
|    time_elapsed    | 28217    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.87e+03    |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 491         |
|    time_elapsed         | 28266       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.010782874 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0611     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 26.8        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=3002.31 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8050000      |
| train/                  |              |
|    approx_kl            | 0.0010905324 |
|    clip_fraction        | 0.00624      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0506      |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.168        |
|    n_updates            | 4910         |
|    policy_gradient_loss | 0.000251     |
|    value_loss           | 15           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 492      |
|    time_elapsed    | 28324    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=3002.63 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8075000      |
| train/                  |              |
|    approx_kl            | 0.0010375035 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0475      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.54         |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.000252    |
|    value_loss           | 16           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 493      |
|    time_elapsed    | 28382    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.8e+03     |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 494         |
|    time_elapsed         | 28430       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.005784806 |
|    clip_fraction        | 0.00901     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0397     |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 2.91        |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=3002.25 +/- 0.49
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8100000       |
| train/                  |               |
|    approx_kl            | 0.00015924829 |
|    clip_fraction        | 0.00139       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.022        |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.3           |
|    n_updates            | 4940          |
|    policy_gradient_loss | -0.000235     |
|    value_loss           | 26.6          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 495      |
|    time_elapsed    | 28488    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=3002.43 +/- 0.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8125000      |
| train/                  |              |
|    approx_kl            | 0.0038505634 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0503      |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.248        |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00854     |
|    value_loss           | 10.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 496      |
|    time_elapsed    | 28546    |
|    total_timesteps | 8126464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.78e+03     |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 497          |
|    time_elapsed         | 28595        |
|    total_timesteps      | 8142848      |
| train/                  |              |
|    approx_kl            | 0.0022889518 |
|    clip_fraction        | 0.00946      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0522      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.217        |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 33.1         |
------------------------------------------
Eval num_timesteps=8150000, episode_reward=3002.40 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.000273642 |
|    clip_fraction        | 0.00211     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0271     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.533       |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.000242   |
|    value_loss           | 11.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 498      |
|    time_elapsed    | 28653    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=3002.62 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8175000      |
| train/                  |              |
|    approx_kl            | 9.635652e-05 |
|    clip_fraction        | 0.000647     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0157      |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.253        |
|    n_updates            | 4980         |
|    policy_gradient_loss | -8.16e-05    |
|    value_loss           | 16.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 499      |
|    time_elapsed    | 28711    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.83e+03    |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 500         |
|    time_elapsed         | 28760       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.082971275 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0846      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 10.2        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=3002.66 +/- 0.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8200000      |
| train/                  |              |
|    approx_kl            | 0.0040963693 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.382       |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.0003       |
|    loss                 | 172          |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 752          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 656      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 501      |
|    time_elapsed    | 28819    |
|    total_timesteps | 8208384  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -502         |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 502          |
|    time_elapsed         | 28868        |
|    total_timesteps      | 8224768      |
| train/                  |              |
|    approx_kl            | 0.0045311954 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 128          |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 468          |
------------------------------------------
Eval num_timesteps=8225000, episode_reward=3002.41 +/- 0.26
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8225000      |
| train/                  |              |
|    approx_kl            | 0.0036534565 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.417       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 20.5         |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 361          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.38e+03 |
| time/              |           |
|    fps             | 284       |
|    iterations      | 503       |
|    time_elapsed    | 28927     |
|    total_timesteps | 8241152   |
----------------------------------
Eval num_timesteps=8250000, episode_reward=1863.19 +/- 1395.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 8250000      |
| train/                  |              |
|    approx_kl            | 0.0035076581 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.297       |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 66.2         |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 287          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.29e+03 |
| time/              |           |
|    fps             | 284       |
|    iterations      | 504       |
|    time_elapsed    | 28986     |
|    total_timesteps | 8257536   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -999        |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 505         |
|    time_elapsed         | 29035       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.004717897 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.36       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.7        |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 329         |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=3002.76 +/- 0.64
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8275000      |
| train/                  |              |
|    approx_kl            | 0.0070714448 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.266       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 20.2         |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 178          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 83.2     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 506      |
|    time_elapsed    | 29094    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=3002.40 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.002701409 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.9        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 205         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 507      |
|    time_elapsed    | 29152    |
|    total_timesteps | 8306688  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 769          |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 508          |
|    time_elapsed         | 29202        |
|    total_timesteps      | 8323072      |
| train/                  |              |
|    approx_kl            | 0.0021015885 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 101          |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=8325000, episode_reward=2422.69 +/- 1159.68
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 8325000      |
| train/                  |              |
|    approx_kl            | 0.0025640433 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 132          |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 272          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 509      |
|    time_elapsed    | 29260    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=3002.89 +/- 0.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.002732353 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 27.5        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 510      |
|    time_elapsed    | 29319    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 511         |
|    time_elapsed         | 29368       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.001056424 |
|    clip_fraction        | 0.00665     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0798     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.8        |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 128         |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=3002.33 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8375000      |
| train/                  |              |
|    approx_kl            | 0.0019547394 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0584      |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.8         |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.000888    |
|    value_loss           | 71.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 512      |
|    time_elapsed    | 29427    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=3002.61 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.001025653 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0251     |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 71.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 513      |
|    time_elapsed    | 29486    |
|    total_timesteps | 8404992  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.76e+03      |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 514           |
|    time_elapsed         | 29535         |
|    total_timesteps      | 8421376       |
| train/                  |               |
|    approx_kl            | 0.00014336656 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.63          |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000138     |
|    value_loss           | 22.6          |
-------------------------------------------
Eval num_timesteps=8425000, episode_reward=3002.60 +/- 0.30
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8425000       |
| train/                  |               |
|    approx_kl            | 0.00013048675 |
|    clip_fraction        | 0.000659      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00658      |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.0003        |
|    loss                 | 4.75          |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000127     |
|    value_loss           | 32            |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 515      |
|    time_elapsed    | 29594    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=3002.48 +/- 0.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8450000      |
| train/                  |              |
|    approx_kl            | 0.0009352549 |
|    clip_fraction        | 0.00235      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.14         |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.000444    |
|    value_loss           | 26.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 516      |
|    time_elapsed    | 29652    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.91e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 517         |
|    time_elapsed         | 29702       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.000857664 |
|    clip_fraction        | 0.00247     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0222     |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00046    |
|    value_loss           | 74          |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=3002.32 +/- 0.27
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8475000       |
| train/                  |               |
|    approx_kl            | 0.00016708666 |
|    clip_fraction        | 0.000623      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00704      |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.0003        |
|    loss                 | 6.93          |
|    n_updates            | 5170          |
|    policy_gradient_loss | -0.000125     |
|    value_loss           | 11.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 518      |
|    time_elapsed    | 29762    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=3002.60 +/- 0.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8500000      |
| train/                  |              |
|    approx_kl            | 0.0031038448 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0324      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.494        |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 21           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 519      |
|    time_elapsed    | 29821    |
|    total_timesteps | 8503296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.94e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 520          |
|    time_elapsed         | 29871        |
|    total_timesteps      | 8519680      |
| train/                  |              |
|    approx_kl            | 4.317478e-05 |
|    clip_fraction        | 0.000482     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00947     |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.48         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 9.21         |
------------------------------------------
Eval num_timesteps=8525000, episode_reward=3002.41 +/- 0.42
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8525000       |
| train/                  |               |
|    approx_kl            | 0.00026637598 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00694      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.218         |
|    n_updates            | 5200          |
|    policy_gradient_loss | -0.000181     |
|    value_loss           | 4.51          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 521      |
|    time_elapsed    | 29931    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=2422.64 +/- 1159.40
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 2.42e+03      |
| time/                   |               |
|    total_timesteps      | 8550000       |
| train/                  |               |
|    approx_kl            | 0.00011746531 |
|    clip_fraction        | 0.000665      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00925      |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.0003        |
|    loss                 | 4.75          |
|    n_updates            | 5210          |
|    policy_gradient_loss | 8.22e-06      |
|    value_loss           | 11.9          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 522      |
|    time_elapsed    | 29992    |
|    total_timesteps | 8552448  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.91e+03      |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 523           |
|    time_elapsed         | 30042         |
|    total_timesteps      | 8568832       |
| train/                  |               |
|    approx_kl            | 4.2486954e-05 |
|    clip_fraction        | 0.000287      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.239         |
|    n_updates            | 5220          |
|    policy_gradient_loss | -2.79e-06     |
|    value_loss           | 16.9          |
-------------------------------------------
Eval num_timesteps=8575000, episode_reward=-639.76 +/- 4460.84
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -640       |
| time/                   |            |
|    total_timesteps      | 8575000    |
| train/                  |            |
|    approx_kl            | 0.21970987 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.458      |
|    n_updates            | 5230       |
|    policy_gradient_loss | -0.044     |
|    value_loss           | 10.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 524      |
|    time_elapsed    | 30103    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=3002.37 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.005351741 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 229         |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 699         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -87.5    |
| time/              |          |
|    fps             | 285      |
|    iterations      | 525      |
|    time_elapsed    | 30163    |
|    total_timesteps | 8601600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -1.21e+03    |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 526          |
|    time_elapsed         | 30214        |
|    total_timesteps      | 8617984      |
| train/                  |              |
|    approx_kl            | 0.0022001066 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.0003       |
|    loss                 | 65.3         |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.000241    |
|    value_loss           | 562          |
------------------------------------------
Eval num_timesteps=8625000, episode_reward=-703.76 +/- 4539.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -704         |
| time/                   |              |
|    total_timesteps      | 8625000      |
| train/                  |              |
|    approx_kl            | 0.0041991337 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.294       |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 69.3         |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000356    |
|    value_loss           | 358          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.37e+03 |
| time/              |           |
|    fps             | 285       |
|    iterations      | 527       |
|    time_elapsed    | 30274     |
|    total_timesteps | 8634368   |
----------------------------------
Eval num_timesteps=8650000, episode_reward=3002.35 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 8650000    |
| train/                  |            |
|    approx_kl            | 0.00911949 |
|    clip_fraction        | 0.0543     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 35         |
|    n_updates            | 5270       |
|    policy_gradient_loss | -0.00229   |
|    value_loss           | 321        |
----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.88e+03 |
| time/              |           |
|    fps             | 285       |
|    iterations      | 528       |
|    time_elapsed    | 30334     |
|    total_timesteps | 8650752   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.59e+03   |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 529         |
|    time_elapsed         | 30385       |
|    total_timesteps      | 8667136     |
| train/                  |             |
|    approx_kl            | 0.014740803 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 351         |
-----------------------------------------
Eval num_timesteps=8675000, episode_reward=1706.48 +/- 2591.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.011121634 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.5        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 250         |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.32e+03 |
| time/              |           |
|    fps             | 285       |
|    iterations      | 530       |
|    time_elapsed    | 30446     |
|    total_timesteps | 8683520   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.11e+03   |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 531         |
|    time_elapsed         | 30496       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.025214713 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.6        |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 255         |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=3002.44 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.013784909 |
|    clip_fraction        | 0.0628      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.7        |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -438     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 532      |
|    time_elapsed    | 30556    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=3002.49 +/- 0.09
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8725000      |
| train/                  |              |
|    approx_kl            | 0.0035688854 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 101          |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 172          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 533      |
|    time_elapsed    | 30616    |
|    total_timesteps | 8732672  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 580          |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 534          |
|    time_elapsed         | 30667        |
|    total_timesteps      | 8749056      |
| train/                  |              |
|    approx_kl            | 0.0035730698 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 254          |
|    n_updates            | 5330         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 328          |
------------------------------------------
Eval num_timesteps=8750000, episode_reward=3002.55 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8750000      |
| train/                  |              |
|    approx_kl            | 0.0049311705 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.377       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 55.3         |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 204          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 927      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 535      |
|    time_elapsed    | 30726    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=2422.98 +/- 1159.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.42e+03     |
| time/                   |              |
|    total_timesteps      | 8775000      |
| train/                  |              |
|    approx_kl            | 0.0065560015 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.306       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.7         |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 203          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 536      |
|    time_elapsed    | 30787    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.54e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 537         |
|    time_elapsed         | 30837       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.004089673 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=3002.61 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.026446521 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 58          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 538      |
|    time_elapsed    | 30897    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=3002.46 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 8825000      |
| train/                  |              |
|    approx_kl            | 0.0010193137 |
|    clip_fraction        | 0.00212      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0203      |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.7         |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.000919    |
|    value_loss           | 106          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 539      |
|    time_elapsed    | 30957    |
|    total_timesteps | 8830976  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.66e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 540          |
|    time_elapsed         | 31007        |
|    total_timesteps      | 8847360      |
| train/                  |              |
|    approx_kl            | 0.0001681291 |
|    clip_fraction        | 0.000983     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0138      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 6            |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.000288    |
|    value_loss           | 50           |
------------------------------------------
Eval num_timesteps=8850000, episode_reward=3002.50 +/- 0.17
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8850000       |
| train/                  |               |
|    approx_kl            | 4.2694395e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.0003        |
|    loss                 | 16.3          |
|    n_updates            | 5400          |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 31            |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 541      |
|    time_elapsed    | 31069    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=3002.46 +/- 0.63
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8875000       |
| train/                  |               |
|    approx_kl            | 0.00012933533 |
|    clip_fraction        | 0.000671      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00856      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.93          |
|    n_updates            | 5410          |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 31.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 542      |
|    time_elapsed    | 31129    |
|    total_timesteps | 8880128  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.83e+03      |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 543           |
|    time_elapsed         | 31179         |
|    total_timesteps      | 8896512       |
| train/                  |               |
|    approx_kl            | 4.3902597e-05 |
|    clip_fraction        | 0.000696      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0098       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.6           |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 27.6          |
-------------------------------------------
Eval num_timesteps=8900000, episode_reward=3002.65 +/- 0.46
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 8900000    |
| train/                  |            |
|    approx_kl            | 0.01823187 |
|    clip_fraction        | 0.0191     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0779    |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.17       |
|    n_updates            | 5430       |
|    policy_gradient_loss | -0.00835   |
|    value_loss           | 72.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 544      |
|    time_elapsed    | 31239    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=3002.14 +/- 0.29
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 8925000       |
| train/                  |               |
|    approx_kl            | 0.00030646805 |
|    clip_fraction        | 0.000513      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00633      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.527         |
|    n_updates            | 5440          |
|    policy_gradient_loss | -4.98e-05     |
|    value_loss           | 5             |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 545      |
|    time_elapsed    | 31299    |
|    total_timesteps | 8929280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.75e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 546          |
|    time_elapsed         | 31350        |
|    total_timesteps      | 8945664      |
| train/                  |              |
|    approx_kl            | 5.128783e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00548     |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56         |
|    n_updates            | 5450         |
|    policy_gradient_loss | -7.21e-05    |
|    value_loss           | 17.5         |
------------------------------------------
Eval num_timesteps=8950000, episode_reward=3002.17 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.001741053 |
|    clip_fraction        | 0.000806    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00535    |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.556       |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.000253   |
|    value_loss           | 9.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 547      |
|    time_elapsed    | 31410    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=3002.29 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.020786496 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.056      |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.27        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 19.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 548      |
|    time_elapsed    | 31470    |
|    total_timesteps | 8978432  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.9e+03      |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 549          |
|    time_elapsed         | 31520        |
|    total_timesteps      | 8994816      |
| train/                  |              |
|    approx_kl            | 6.333385e-05 |
|    clip_fraction        | 0.000854     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00932     |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.07         |
|    n_updates            | 5480         |
|    policy_gradient_loss | -5.84e-05    |
|    value_loss           | 10.9         |
------------------------------------------
Eval num_timesteps=9000000, episode_reward=1135.70 +/- 3734.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.14e+03   |
| time/                   |            |
|    total_timesteps      | 9000000    |
| train/                  |            |
|    approx_kl            | 0.11901968 |
|    clip_fraction        | 0.0903     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0861    |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.31       |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 3.3        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 550      |
|    time_elapsed    | 31579    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=410.63 +/- 3174.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 411         |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.006769059 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 252         |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 553         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 551      |
|    time_elapsed    | 31638    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 396         |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 552         |
|    time_elapsed         | 31688       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.007882186 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=3002.31 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.003217118 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 154         |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00196    |
|    value_loss           | 378         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 553      |
|    time_elapsed    | 31746    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=3002.52 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.008196319 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -212     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 554      |
|    time_elapsed    | 31804    |
|    total_timesteps | 9076736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 188          |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 555          |
|    time_elapsed         | 31853        |
|    total_timesteps      | 9093120      |
| train/                  |              |
|    approx_kl            | 0.0076562134 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.346       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.6         |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=9100000, episode_reward=3002.54 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9100000      |
| train/                  |              |
|    approx_kl            | 0.0044156155 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 130          |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000776    |
|    value_loss           | 172          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 260      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 556      |
|    time_elapsed    | 31912    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=3002.73 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.004869355 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.4        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 557      |
|    time_elapsed    | 31970    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.05e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 558         |
|    time_elapsed         | 32019       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.030121174 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.8        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=3002.90 +/- 0.45
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9150000      |
| train/                  |              |
|    approx_kl            | 0.0045365826 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.303       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.4         |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 89.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 559      |
|    time_elapsed    | 32077    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=3002.59 +/- 0.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3e+03      |
| time/                   |            |
|    total_timesteps      | 9175000    |
| train/                  |            |
|    approx_kl            | 0.01206694 |
|    clip_fraction        | 0.0336     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.321     |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.36       |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.00543   |
|    value_loss           | 67.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 560      |
|    time_elapsed    | 32136    |
|    total_timesteps | 9175040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.17e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 561          |
|    time_elapsed         | 32185        |
|    total_timesteps      | 9191424      |
| train/                  |              |
|    approx_kl            | 0.0030983388 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.8         |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 47.8         |
------------------------------------------
Eval num_timesteps=9200000, episode_reward=3002.72 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9200000      |
| train/                  |              |
|    approx_kl            | 0.0036486266 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.2         |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 122          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 562      |
|    time_elapsed    | 32243    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.72e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 563         |
|    time_elapsed         | 32292       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.002706198 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 66.3        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 70.2        |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=3002.20 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9225000      |
| train/                  |              |
|    approx_kl            | 0.0039367992 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.11         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.000303    |
|    value_loss           | 48.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 564      |
|    time_elapsed    | 32351    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=3002.67 +/- 0.44
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9250000       |
| train/                  |               |
|    approx_kl            | 0.00048767898 |
|    clip_fraction        | 0.00321       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0543       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.87          |
|    n_updates            | 5640          |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 60            |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 565      |
|    time_elapsed    | 32409    |
|    total_timesteps | 9256960  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.88e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 566          |
|    time_elapsed         | 32458        |
|    total_timesteps      | 9273344      |
| train/                  |              |
|    approx_kl            | 0.0008008557 |
|    clip_fraction        | 0.00486      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.055       |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.33         |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.000129    |
|    value_loss           | 13.7         |
------------------------------------------
Eval num_timesteps=9275000, episode_reward=2422.72 +/- 1159.92
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 2.42e+03      |
| time/                   |               |
|    total_timesteps      | 9275000       |
| train/                  |               |
|    approx_kl            | 0.00089721684 |
|    clip_fraction        | 0.00321       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0413       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.479         |
|    n_updates            | 5660          |
|    policy_gradient_loss | -4.44e-05     |
|    value_loss           | 3.33          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 567      |
|    time_elapsed    | 32517    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=3002.32 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9300000      |
| train/                  |              |
|    approx_kl            | 7.450079e-05 |
|    clip_fraction        | 0.000885     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0257      |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.12         |
|    n_updates            | 5670         |
|    policy_gradient_loss | -7.4e-05     |
|    value_loss           | 11.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 568      |
|    time_elapsed    | 32577    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.97e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 569         |
|    time_elapsed         | 32629       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.007619368 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0432     |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 5.79        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=3002.70 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9325000      |
| train/                  |              |
|    approx_kl            | 0.0002819547 |
|    clip_fraction        | 0.00219      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0231      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.71         |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.000198    |
|    value_loss           | 4.91         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 570      |
|    time_elapsed    | 32689    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=3002.62 +/- 0.30
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9350000       |
| train/                  |               |
|    approx_kl            | 0.00010701294 |
|    clip_fraction        | 0.000952      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.424         |
|    n_updates            | 5700          |
|    policy_gradient_loss | -0.000201     |
|    value_loss           | 6.49          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 571      |
|    time_elapsed    | 32749    |
|    total_timesteps | 9355264  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 2.86e+03      |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 572           |
|    time_elapsed         | 32799         |
|    total_timesteps      | 9371648       |
| train/                  |               |
|    approx_kl            | 0.00094841496 |
|    clip_fraction        | 0.00198       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0162       |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.304         |
|    n_updates            | 5710          |
|    policy_gradient_loss | -0.000278     |
|    value_loss           | 6.04          |
-------------------------------------------
Eval num_timesteps=9375000, episode_reward=3002.56 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9375000      |
| train/                  |              |
|    approx_kl            | 0.0034099864 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.4         |
|    n_updates            | 5720         |
|    policy_gradient_loss | 0.0035       |
|    value_loss           | 109          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 573      |
|    time_elapsed    | 32859    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=3002.73 +/- 0.62
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 9400000       |
| train/                  |               |
|    approx_kl            | 0.00045061694 |
|    clip_fraction        | 0.00441       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.036        |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.71          |
|    n_updates            | 5730          |
|    policy_gradient_loss | 0.000973      |
|    value_loss           | 13.9          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 574      |
|    time_elapsed    | 32918    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.85e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 575         |
|    time_elapsed         | 32968       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.011366744 |
|    clip_fraction        | 0.0222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0609     |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=3002.29 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.023660555 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.204       |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 0.861       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 576      |
|    time_elapsed    | 33028    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=3002.49 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.030572914 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.99        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 37.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 577      |
|    time_elapsed    | 33087    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.82e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 578         |
|    time_elapsed         | 33137       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.057680175 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.658       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 17.2        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=3002.41 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.024600767 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 9.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 579      |
|    time_elapsed    | 33196    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=3002.73 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3e+03        |
| time/                   |              |
|    total_timesteps      | 9500000      |
| train/                  |              |
|    approx_kl            | 0.0026259888 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.0003       |
|    loss                 | 91.3         |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 471          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 580      |
|    time_elapsed    | 33255    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 581         |
|    time_elapsed         | 33305       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.010320643 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 365         |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=3002.50 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.003931266 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 127         |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 351         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 582      |
|    time_elapsed    | 33363    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=3002.58 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.004305043 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00196    |
|    value_loss           | 518         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 583      |
|    time_elapsed    | 33421    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.74e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 584         |
|    time_elapsed         | 33470       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.008744738 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.9        |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=1743.10 +/- 1550.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.009095605 |
|    clip_fraction        | 0.0546      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.8        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00275    |
|    value_loss           | 218         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 285      |
|    iterations      | 585      |
|    time_elapsed    | 33528    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=2302.88 +/- 1399.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.3e+03     |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.004413549 |
|    clip_fraction        | 0.0366      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.3        |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 131         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 586      |
|    time_elapsed    | 33586    |
|    total_timesteps | 9601024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.63e+03     |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 587          |
|    time_elapsed         | 33635        |
|    total_timesteps      | 9617408      |
| train/                  |              |
|    approx_kl            | 0.0048019383 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 33.5         |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.000502    |
|    value_loss           | 15.6         |
------------------------------------------
Eval num_timesteps=9625000, episode_reward=2422.35 +/- 1159.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42e+03    |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.003485614 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.429      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 37.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 588      |
|    time_elapsed    | 33693    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=3003.15 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.008539423 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 5880        |
|    policy_gradient_loss | 0.000588    |
|    value_loss           | 59.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 589      |
|    time_elapsed    | 33751    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.61e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 590         |
|    time_elapsed         | 33800       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.008504663 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.416      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 47.4        |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=3002.53 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.003711599 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -7.14e-05   |
|    value_loss           | 17.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 591      |
|    time_elapsed    | 33858    |
|    total_timesteps | 9682944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.78e+03   |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 592        |
|    time_elapsed         | 33907      |
|    total_timesteps      | 9699328    |
| train/                  |            |
|    approx_kl            | 0.00623833 |
|    clip_fraction        | 0.0362     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.63       |
|    n_updates            | 5910       |
|    policy_gradient_loss | -0.000543  |
|    value_loss           | 13.6       |
----------------------------------------
Eval num_timesteps=9700000, episode_reward=2463.03 +/- 1079.08
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.46e+03     |
| time/                   |              |
|    total_timesteps      | 9700000      |
| train/                  |              |
|    approx_kl            | 0.0075082853 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.79         |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 18.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 593      |
|    time_elapsed    | 33965    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=3002.50 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.009879654 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.648       |
|    n_updates            | 5930        |
|    policy_gradient_loss | 0.000497    |
|    value_loss           | 36.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 594      |
|    time_elapsed    | 34023    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.85e+03    |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 595         |
|    time_elapsed         | 34072       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.026946424 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 37.2        |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=3002.56 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.012193171 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.04        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 12.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 596      |
|    time_elapsed    | 34130    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=3002.45 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.023551146 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 13.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 597      |
|    time_elapsed    | 34188    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.64e+03    |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 598         |
|    time_elapsed         | 34237       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.045285076 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 6.39        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=3002.31 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.013104514 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 380         |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 249         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 599      |
|    time_elapsed    | 34295    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=3002.54 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.009217844 |
|    clip_fraction        | 0.00889     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0559     |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.996       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.000732   |
|    value_loss           | 6.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 600      |
|    time_elapsed    | 34353    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.58e+03    |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 601         |
|    time_elapsed         | 34402       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.007395473 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.528       |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 4.85        |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=3002.39 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.020594615 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.489       |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 27.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 602      |
|    time_elapsed    | 34463    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=3002.72 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.081826344 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 2.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 603      |
|    time_elapsed    | 34527    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.78e+03    |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 604         |
|    time_elapsed         | 34579       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.040779628 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 27.7        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=2483.47 +/- 1037.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.48e+03   |
| time/                   |            |
|    total_timesteps      | 9900000    |
| train/                  |            |
|    approx_kl            | 0.13692513 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.638     |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.946      |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 3.51       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 286      |
|    iterations      | 605      |
|    time_elapsed    | 34645    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=3002.48 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.016246974 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.785       |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 13.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 606      |
|    time_elapsed    | 34713    |
|    total_timesteps | 9928704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.71e+03     |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 607          |
|    time_elapsed         | 34771        |
|    total_timesteps      | 9945088      |
| train/                  |              |
|    approx_kl            | 0.0057784957 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 101          |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.000883    |
|    value_loss           | 16           |
------------------------------------------
Eval num_timesteps=9950000, episode_reward=3002.50 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.048768047 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.573       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 3.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 608      |
|    time_elapsed    | 34840    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=3002.55 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3e+03       |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.011898115 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.000974   |
|    value_loss           | 1.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 609      |
|    time_elapsed    | 34910    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.74e+03    |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 610         |
|    time_elapsed         | 34968       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.021322351 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 32.1        |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=2422.69 +/- 1159.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42e+03   |
| time/                   |            |
|    total_timesteps      | 10000000   |
| train/                  |            |
|    approx_kl            | 0.01937662 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.305     |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.566      |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.000923  |
|    value_loss           | 3.72       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 611      |
|    time_elapsed    | 35038    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v8_2
