========== uav-v2 ==========
Seed: 3508964360
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v2_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.45e+03 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 1         |
|    time_elapsed    | 145       |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-4264.39 +/- 3570.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -4.26e+03   |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008165155 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.00217     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.95e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 4.64e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.94e+03 |
| time/              |           |
|    fps             | 94        |
|    iterations      | 2         |
|    time_elapsed    | 346       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.73e+03   |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 3           |
|    time_elapsed         | 516         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009037937 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 4.73e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 2.1e+04     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 3.5e+04     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-3026.47 +/- 2391.83
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -3.03e+03 |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0120648 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.8      |
|    explained_variance   | -8.34e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 4.84e+03  |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.00388  |
|    value_loss           | 1.19e+04  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.84e+03 |
| time/              |           |
|    fps             | 91        |
|    iterations      | 4         |
|    time_elapsed    | 719       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-2025.64 +/- 1099.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.03e+03   |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.010517741 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -3.58e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.34e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 5.51e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.62e+03 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 5         |
|    time_elapsed    | 921       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.01e+03   |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 6           |
|    time_elapsed         | 1092        |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.005123531 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.38e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 7.71e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1559.13 +/- 294.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.007913553 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 3.36e+03    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.48e+03 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 7         |
|    time_elapsed    | 1294      |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1438.39 +/- 480.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.44e+03   |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.011567075 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 348         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 371         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.1e+03 |
| time/              |          |
|    fps             | 87       |
|    iterations      | 8        |
|    time_elapsed    | 1495     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -631        |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 9           |
|    time_elapsed         | 1664        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.014038108 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 46          |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-959.07 +/- 293.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -959        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.011091173 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 236         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -469     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 10       |
|    time_elapsed    | 1866     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-837.17 +/- 291.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -837        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.010657345 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 483         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -481     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 11       |
|    time_elapsed    | 2067     |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -529       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 12         |
|    time_elapsed         | 2236       |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.00927589 |
|    clip_fraction        | 0.0926     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.77      |
|    explained_variance   | 0.307      |
|    learning_rate        | 0.0003     |
|    loss                 | 274        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00983   |
|    value_loss           | 1.14e+03   |
----------------------------------------
Eval num_timesteps=200000, episode_reward=-717.57 +/- 448.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -718        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.011546379 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.4        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 369         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -491     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 13       |
|    time_elapsed    | 2437     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-358.63 +/- 479.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -359       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.01196185 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.77      |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.6       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 222        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 14       |
|    time_elapsed    | 2640     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -301        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 15          |
|    time_elapsed         | 2809        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.015450285 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 69.1        |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=-334.87 +/- 277.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -335        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.014499536 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.1        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 132         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -245     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 16       |
|    time_elapsed    | 3011     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=0.75 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.754       |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.017009532 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.5        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 115         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -244     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 17       |
|    time_elapsed    | 3212     |
|    total_timesteps | 278528   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -252       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 18         |
|    time_elapsed         | 3381       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.01210986 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.75      |
|    explained_variance   | 0.112      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 447        |
----------------------------------------
Eval num_timesteps=300000, episode_reward=3.83 +/- 4.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.83        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.012728318 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 132         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 390         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 19       |
|    time_elapsed    | 3582     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-117.22 +/- 239.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.016728377 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.1        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 82.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -270     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 20       |
|    time_elapsed    | 3784     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -208        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 21          |
|    time_elapsed         | 3955        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.014371682 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 127         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=-3082.97 +/- 6168.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.08e+03   |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.018675543 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 59.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 22       |
|    time_elapsed    | 4156     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-112.38 +/- 229.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.012837173 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 348         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 1.09e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -217     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 23       |
|    time_elapsed    | 4358     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -249        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 24          |
|    time_elapsed         | 4527        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.014284772 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 316         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00559    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=3.51 +/- 4.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.51        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.012797764 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 232         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 706         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -266     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 25       |
|    time_elapsed    | 4728     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=8.32 +/- 6.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.32        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.019142382 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.59        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00966    |
|    value_loss           | 59.8        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 26       |
|    time_elapsed    | 4930     |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -202       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 27         |
|    time_elapsed         | 5098       |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.01745598 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.7       |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.51       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 33.5       |
----------------------------------------
Eval num_timesteps=450000, episode_reward=12.23 +/- 16.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 12.2        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.012755221 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | -0.11       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 327         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 28       |
|    time_elapsed    | 5300     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=3.06 +/- 1.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.06        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.013365023 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 200         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 29       |
|    time_elapsed    | 5501     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -217        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 30          |
|    time_elapsed         | 5670        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.019183487 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 99.7        |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=8.11 +/- 12.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.11        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.012366829 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.2        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 834         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 31       |
|    time_elapsed    | 5871     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -218        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 32          |
|    time_elapsed         | 6040        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.012489398 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 508         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=50.78 +/- 80.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 50.8        |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.009614927 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.8        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 624         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -324     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 33       |
|    time_elapsed    | 6242     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-514.47 +/- 1070.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -514        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.012434055 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33e+03    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 1.55e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 34       |
|    time_elapsed    | 6444     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -272        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 35          |
|    time_elapsed         | 6611        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.013396523 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0003      |
|    loss                 | 410         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 393         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=42.62 +/- 52.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 42.6        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.011040677 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 273         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 446         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 36       |
|    time_elapsed    | 6809     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=1.38 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.015700087 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 888         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 273         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.5    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 37       |
|    time_elapsed    | 7006     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -91.5       |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 38          |
|    time_elapsed         | 7173        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.016356006 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.1        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 111         |
-----------------------------------------
Eval num_timesteps=625000, episode_reward=-1053.38 +/- 2164.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.05e+03   |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.014716652 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.5        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 270         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 39       |
|    time_elapsed    | 7371     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-187.09 +/- 416.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.011495557 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 715         |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 689         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -97.2    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 40       |
|    time_elapsed    | 7568     |
|    total_timesteps | 655360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 41         |
|    time_elapsed         | 7735       |
|    total_timesteps      | 671744     |
| train/                  |            |
|    approx_kl            | 0.01839259 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 12.9       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00846   |
|    value_loss           | 127        |
----------------------------------------
Eval num_timesteps=675000, episode_reward=84.33 +/- 163.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 84.3        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.015135525 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.5        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 287         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.68    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 42       |
|    time_elapsed    | 7934     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=7.55 +/- 4.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.016959976 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.3        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 115         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.88    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 43       |
|    time_elapsed    | 8131     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.27        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 44          |
|    time_elapsed         | 8297        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.018753389 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00888    |
|    value_loss           | 85.8        |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=77.53 +/- 89.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 77.5        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.019841883 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00847    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.2    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 45       |
|    time_elapsed    | 8495     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=66.34 +/- 46.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 66.3        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.012574457 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.0364      |
|    learning_rate        | 0.0003      |
|    loss                 | 228         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 319         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.9    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 46       |
|    time_elapsed    | 8693     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -84.4       |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 47          |
|    time_elapsed         | 8859        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.014794316 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 453         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 364         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=47.97 +/- 24.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 48          |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.018466223 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.54        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 48       |
|    time_elapsed    | 9057     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=187.22 +/- 274.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.019136144 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.1        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 213         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.9    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 49       |
|    time_elapsed    | 9255     |
|    total_timesteps | 802816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -35       |
| time/                   |           |
|    fps                  | 86        |
|    iterations           | 50        |
|    time_elapsed         | 9421      |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.0197324 |
|    clip_fraction        | 0.283     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.62     |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.51      |
|    n_updates            | 490       |
|    policy_gradient_loss | 0.000141  |
|    value_loss           | 122       |
---------------------------------------
Eval num_timesteps=825000, episode_reward=25.13 +/- 14.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 25.1        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.015679112 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 243         |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 325         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -42.7    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 51       |
|    time_elapsed    | 9619     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-458.71 +/- 1133.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -459        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.017352574 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.53        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 90.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.8    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 52       |
|    time_elapsed    | 9817     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.72       |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 53          |
|    time_elapsed         | 9984        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.013672288 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0003      |
|    loss                 | 120         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=172.71 +/- 175.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.016681787 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 31          |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 93.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 54       |
|    time_elapsed    | 10182    |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=16.92 +/- 18.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.01760408 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.6       |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | 126        |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00852   |
|    value_loss           | 53.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 55       |
|    time_elapsed    | 10380    |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 37.1        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 56          |
|    time_elapsed         | 10547       |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.016587157 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.92        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 86.6        |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=114.70 +/- 75.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 115         |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.018163303 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.8         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 28.8     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 57       |
|    time_elapsed    | 10746    |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=31.78 +/- 40.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 31.8        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.017921457 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 162         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 58       |
|    time_elapsed    | 10944    |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 41.1        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 59          |
|    time_elapsed         | 11110       |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.018648665 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 20.7        |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=140.52 +/- 108.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 141         |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.017600317 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 44.5        |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 66.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 49.7     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 60       |
|    time_elapsed    | 11308    |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 48.9        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 61          |
|    time_elapsed         | 11474       |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.018633518 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.3        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0072     |
|    value_loss           | 158         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=32.28 +/- 38.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 32.3        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.020607326 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.71        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 65          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 30.3     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 62       |
|    time_elapsed    | 11672    |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=72.38 +/- 82.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 72.4        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.017421558 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 62.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 29.6     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 63       |
|    time_elapsed    | 11871    |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 39.8        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 64          |
|    time_elapsed         | 12038       |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.017868094 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 54.2        |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=30.17 +/- 30.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 30.2        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.017537517 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 197         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 33.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 65       |
|    time_elapsed    | 12236    |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=78.78 +/- 32.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 78.8        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.015756423 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.7        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 31       |
| time/              |          |
|    fps             | 86       |
|    iterations      | 66       |
|    time_elapsed    | 12434    |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 38.2        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 67          |
|    time_elapsed         | 12601       |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.018084811 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.44        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 84.2        |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=76.79 +/- 47.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 76.8        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.021768745 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 158         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.11    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 68       |
|    time_elapsed    | 12799    |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=119.78 +/- 148.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 120         |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.015277339 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 236         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 69       |
|    time_elapsed    | 12997    |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.4      |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 70         |
|    time_elapsed         | 13163      |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.02014982 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 77.7       |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 76.1       |
----------------------------------------
Eval num_timesteps=1150000, episode_reward=78.36 +/- 94.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 78.4       |
| time/                   |            |
|    total_timesteps      | 1150000    |
| train/                  |            |
|    approx_kl            | 0.01809556 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.56      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.8       |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 48.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 36.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 71       |
|    time_elapsed    | 13362    |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=190.66 +/- 120.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 191        |
| time/                   |            |
|    total_timesteps      | 1175000    |
| train/                  |            |
|    approx_kl            | 0.01836168 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.88       |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.00921   |
|    value_loss           | 78.4       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 47.9     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 72       |
|    time_elapsed    | 13561    |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 71.6        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 73          |
|    time_elapsed         | 13728       |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.017496508 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.56        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 43.7        |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=105.62 +/- 126.67
Episode length: 600.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 600      |
|    mean_reward          | 106      |
| time/                   |          |
|    total_timesteps      | 1200000  |
| train/                  |          |
|    approx_kl            | 0.017583 |
|    clip_fraction        | 0.217    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.52    |
|    explained_variance   | 0.71     |
|    learning_rate        | 0.0003   |
|    loss                 | 5.16     |
|    n_updates            | 730      |
|    policy_gradient_loss | -0.00977 |
|    value_loss           | 39.1     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 56.4     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 74       |
|    time_elapsed    | 13926    |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=56.94 +/- 34.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 56.9       |
| time/                   |            |
|    total_timesteps      | 1225000    |
| train/                  |            |
|    approx_kl            | 0.01376387 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.1       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 146        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 80.2     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 75       |
|    time_elapsed    | 14124    |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 80.5        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 76          |
|    time_elapsed         | 14290       |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.019872382 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.45        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00634    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=46.56 +/- 36.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 46.6        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.015254462 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.7        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 66.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 87.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 77       |
|    time_elapsed    | 14488    |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=43.05 +/- 55.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 43         |
| time/                   |            |
|    total_timesteps      | 1275000    |
| train/                  |            |
|    approx_kl            | 0.01932712 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 38         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 78       |
|    time_elapsed    | 14686    |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 79          |
|    time_elapsed         | 14852       |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.016423125 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.47        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 57.8        |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=171.27 +/- 73.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.018328138 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00868    |
|    value_loss           | 18.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 80       |
|    time_elapsed    | 15050    |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=129.77 +/- 92.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.015226964 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 58.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 91.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 81       |
|    time_elapsed    | 15248    |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 79.1        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 82          |
|    time_elapsed         | 15414       |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.015094811 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=63.18 +/- 72.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 63.2        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.017597377 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.89        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 55.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 69.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 83       |
|    time_elapsed    | 15612    |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=213.45 +/- 113.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.019163387 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.7        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 36.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 69.2     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 84       |
|    time_elapsed    | 15811    |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 73.6        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 85          |
|    time_elapsed         | 15977       |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.017494975 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 69          |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=129.99 +/- 80.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.013421314 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0003      |
|    loss                 | 96.3        |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 138         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 73.3     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 86       |
|    time_elapsed    | 16175    |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=79.54 +/- 60.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 79.5        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.018176671 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.6        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00915    |
|    value_loss           | 61.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 68.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 87       |
|    time_elapsed    | 16373    |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 17.1        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 88          |
|    time_elapsed         | 16539       |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.018260531 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.3        |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 70.1        |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=196.48 +/- 192.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.021161133 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 315         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 65.7     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 89       |
|    time_elapsed    | 16737    |
|    total_timesteps | 1458176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 58         |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 90         |
|    time_elapsed         | 16903      |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 0.01994431 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.1        |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.00761   |
|    value_loss           | 40.6       |
----------------------------------------
Eval num_timesteps=1475000, episode_reward=-2136.93 +/- 4682.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.14e+03   |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.020117743 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.887       |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 20.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 74.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 91       |
|    time_elapsed    | 17101    |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=96.33 +/- 60.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 96.3        |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.016758624 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.9        |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 72          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 92       |
|    time_elapsed    | 17298    |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 115         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 93          |
|    time_elapsed         | 17464       |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.018497765 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.66        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 70.2        |
-----------------------------------------
Eval num_timesteps=1525000, episode_reward=221.67 +/- 169.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.019357007 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.7        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 24.1        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 94       |
|    time_elapsed    | 17662    |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=116.97 +/- 37.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 117         |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.016043156 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 96.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 71.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 95       |
|    time_elapsed    | 17860    |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 73          |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 96          |
|    time_elapsed         | 18026       |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.018277746 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.8         |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 85.8        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=184.01 +/- 89.43
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 184       |
| time/                   |           |
|    total_timesteps      | 1575000   |
| train/                  |           |
|    approx_kl            | 0.0181625 |
|    clip_fraction        | 0.24      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.42     |
|    explained_variance   | 0.761     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.2      |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.00922  |
|    value_loss           | 70.4      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 86.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 97       |
|    time_elapsed    | 18224    |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=68.83 +/- 52.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 68.8       |
| time/                   |            |
|    total_timesteps      | 1600000    |
| train/                  |            |
|    approx_kl            | 0.01976087 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.00934   |
|    value_loss           | 11.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 99       |
| time/              |          |
|    fps             | 87       |
|    iterations      | 98       |
|    time_elapsed    | 18422    |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 72.5       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 99         |
|    time_elapsed         | 18588      |
|    total_timesteps      | 1622016    |
| train/                  |            |
|    approx_kl            | 0.01637541 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 44.6       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 32.2       |
----------------------------------------
Eval num_timesteps=1625000, episode_reward=194.96 +/- 149.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.010127546 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.1        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 211         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 35.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 100      |
|    time_elapsed    | 18786    |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=210.69 +/- 84.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.018358532 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.09        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00731    |
|    value_loss           | 121         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 101      |
|    time_elapsed    | 18983    |
|    total_timesteps | 1654784  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 31.4       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 102        |
|    time_elapsed         | 19149      |
|    total_timesteps      | 1671168    |
| train/                  |            |
|    approx_kl            | 0.01786258 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.00511   |
|    value_loss           | 119        |
----------------------------------------
Eval num_timesteps=1675000, episode_reward=147.88 +/- 100.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 148         |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.017210325 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.3        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 144         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 58.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 103      |
|    time_elapsed    | 19347    |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=189.53 +/- 225.01
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 190       |
| time/                   |           |
|    total_timesteps      | 1700000   |
| train/                  |           |
|    approx_kl            | 0.0186544 |
|    clip_fraction        | 0.246     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.4      |
|    explained_variance   | 0.732     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.05      |
|    n_updates            | 1030      |
|    policy_gradient_loss | -0.00583  |
|    value_loss           | 79.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 74.6     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 104      |
|    time_elapsed    | 19544    |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 93.5       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 105        |
|    time_elapsed         | 19711      |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 0.01989339 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 117        |
----------------------------------------
Eval num_timesteps=1725000, episode_reward=225.70 +/- 202.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.020923655 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.37        |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 86.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 89.2     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 106      |
|    time_elapsed    | 19909    |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=240.72 +/- 214.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.021570139 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 115         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.8    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 107      |
|    time_elapsed    | 20107    |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -51.9       |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 108         |
|    time_elapsed         | 20274       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.014012817 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | -0.0992     |
|    learning_rate        | 0.0003      |
|    loss                 | 290         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 340         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=101.18 +/- 67.39
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 101        |
| time/                   |            |
|    total_timesteps      | 1775000    |
| train/                  |            |
|    approx_kl            | 0.01874798 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00869   |
|    value_loss           | 76.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.5    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 109      |
|    time_elapsed    | 20472    |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=150.48 +/- 61.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.018776488 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 48.2        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 56.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 110      |
|    time_elapsed    | 20670    |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 130         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 111         |
|    time_elapsed         | 20837       |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.018160146 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.73        |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 34.7        |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=175.29 +/- 150.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.018261207 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 103         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 112      |
|    time_elapsed    | 21036    |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=226.33 +/- 175.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.017401058 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.5         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00787    |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 82.4     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 113      |
|    time_elapsed    | 21234    |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 77.1        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 114         |
|    time_elapsed         | 21400       |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.017323926 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.87        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=75.93 +/- 54.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 75.9        |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.018052693 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 47.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.9    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 115      |
|    time_elapsed    | 21598    |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=-2217.01 +/- 4721.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.22e+03   |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.015177808 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 319         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.000879   |
|    value_loss           | 457         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.1    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 116      |
|    time_elapsed    | 21796    |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 27.9        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 117         |
|    time_elapsed         | 21962       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.019037204 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.66        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 22.1        |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=191.14 +/- 78.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.016473908 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.99        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 16.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 31.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 118      |
|    time_elapsed    | 22160    |
|    total_timesteps | 1933312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 119        |
|    time_elapsed         | 22326      |
|    total_timesteps      | 1949696    |
| train/                  |            |
|    approx_kl            | 0.02007066 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.58       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 15.4       |
----------------------------------------
Eval num_timesteps=1950000, episode_reward=127.59 +/- 75.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 128         |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.018762872 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 52.2        |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 120      |
|    time_elapsed    | 22524    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=161.27 +/- 85.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.021139044 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.33        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 134         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 121      |
|    time_elapsed    | 22722    |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 140         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 122         |
|    time_elapsed         | 22888       |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.017187076 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.1        |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 85.6        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=169.93 +/- 94.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.020796662 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.431       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 123      |
|    time_elapsed    | 23086    |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=114.35 +/- 80.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 114        |
| time/                   |            |
|    total_timesteps      | 2025000    |
| train/                  |            |
|    approx_kl            | 0.01878541 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.28      |
|    explained_variance   | 0.443      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.6       |
|    n_updates            | 1230       |
|    policy_gradient_loss | 0.000195   |
|    value_loss           | 138        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 124      |
|    time_elapsed    | 23284    |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 118         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 125         |
|    time_elapsed         | 23450       |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.019308949 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00787    |
|    value_loss           | 76.4        |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=286.00 +/- 243.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.019686218 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.02        |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 76.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 126      |
|    time_elapsed    | 23650    |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=250.48 +/- 163.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 250        |
| time/                   |            |
|    total_timesteps      | 2075000    |
| train/                  |            |
|    approx_kl            | 0.01997046 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.03       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 40.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 127      |
|    time_elapsed    | 23848    |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 150         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 128         |
|    time_elapsed         | 24014       |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.016767938 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=146.37 +/- 93.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.020734437 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.99        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 14.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 129      |
|    time_elapsed    | 24212    |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=228.08 +/- 81.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.016256846 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0003      |
|    loss                 | 234         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 188         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 130      |
|    time_elapsed    | 24411    |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 136         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 131         |
|    time_elapsed         | 24577       |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.016100887 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 98.2        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=284.98 +/- 124.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.019596042 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 45.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 132      |
|    time_elapsed    | 24774    |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=182.11 +/- 79.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.022198807 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.2        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 76.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 133      |
|    time_elapsed    | 24973    |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 134         |
|    time_elapsed         | 25139       |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.020668201 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.63        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 48          |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=299.98 +/- 118.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.010867698 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.0003      |
|    loss                 | 190         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 412         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 135      |
|    time_elapsed    | 25337    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=209.79 +/- 62.44
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 210        |
| time/                   |            |
|    total_timesteps      | 2225000    |
| train/                  |            |
|    approx_kl            | 0.02021471 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.17       |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 129        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 98       |
| time/              |          |
|    fps             | 87       |
|    iterations      | 136      |
|    time_elapsed    | 25535    |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 87.6        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 137         |
|    time_elapsed         | 25702       |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.016483398 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 92.2        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=251.18 +/- 162.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 251         |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.018013941 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 192         |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 155         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 138      |
|    time_elapsed    | 25900    |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=291.69 +/- 53.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 292         |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.019102393 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.75        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00943    |
|    value_loss           | 38.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 139      |
|    time_elapsed    | 26099    |
|    total_timesteps | 2277376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 140        |
|    time_elapsed         | 26265      |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.02008392 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.49       |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.00814   |
|    value_loss           | 47.6       |
----------------------------------------
Eval num_timesteps=2300000, episode_reward=271.27 +/- 116.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.018041443 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 233         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 141      |
|    time_elapsed    | 26463    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=101.55 +/- 106.74
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 102        |
| time/                   |            |
|    total_timesteps      | 2325000    |
| train/                  |            |
|    approx_kl            | 0.01753296 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.74       |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.00426   |
|    value_loss           | 79.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 142      |
|    time_elapsed    | 26661    |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 180         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 143         |
|    time_elapsed         | 26827       |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.019524094 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 29.4        |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=305.90 +/- 216.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.019419352 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 4           |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 19          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 144      |
|    time_elapsed    | 27025    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=94.33 +/- 46.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 94.3        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.017806455 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 93.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 202      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 145      |
|    time_elapsed    | 27223    |
|    total_timesteps | 2375680  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 214        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 146        |
|    time_elapsed         | 27389      |
|    total_timesteps      | 2392064    |
| train/                  |            |
|    approx_kl            | 0.01934751 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.67       |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.00627   |
|    value_loss           | 68.4       |
----------------------------------------
Eval num_timesteps=2400000, episode_reward=311.75 +/- 109.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.021662664 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00782    |
|    value_loss           | 13.5        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 147      |
|    time_elapsed    | 27587    |
|    total_timesteps | 2408448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 204          |
| time/                   |              |
|    fps                  | 87           |
|    iterations           | 148          |
|    time_elapsed         | 27754        |
|    total_timesteps      | 2424832      |
| train/                  |              |
|    approx_kl            | 0.0148059465 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.75         |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 45.6         |
------------------------------------------
Eval num_timesteps=2425000, episode_reward=382.17 +/- 54.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 382         |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.021971986 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.39        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 125         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 149      |
|    time_elapsed    | 27953    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=215.86 +/- 170.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.020978259 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 26.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 150      |
|    time_elapsed    | 28152    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 195         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 151         |
|    time_elapsed         | 28319       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.017130636 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.97        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 64.9        |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=315.31 +/- 165.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.019793669 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.33        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 18.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 152      |
|    time_elapsed    | 28517    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=188.88 +/- 71.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.022210903 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 16          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 219      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 153      |
|    time_elapsed    | 28715    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 216         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 154         |
|    time_elapsed         | 28882       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.019482354 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00752    |
|    value_loss           | 97.4        |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=280.28 +/- 65.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 280        |
| time/                   |            |
|    total_timesteps      | 2525000    |
| train/                  |            |
|    approx_kl            | 0.01850548 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.89      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.3       |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.00907   |
|    value_loss           | 60.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 155      |
|    time_elapsed    | 29080    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=239.73 +/- 113.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.018979816 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.7        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00919    |
|    value_loss           | 153         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 156      |
|    time_elapsed    | 29279    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 170         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 157         |
|    time_elapsed         | 29446       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.020965585 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.6        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=109.29 +/- 70.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 109        |
| time/                   |            |
|    total_timesteps      | 2575000    |
| train/                  |            |
|    approx_kl            | 0.01997625 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.12       |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 48.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 158      |
|    time_elapsed    | 29645    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=129.20 +/- 88.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.019111188 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 49          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 200      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 159      |
|    time_elapsed    | 29844    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 205         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 160         |
|    time_elapsed         | 30012       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.017647538 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.7        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=217.16 +/- 135.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.018728983 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 33          |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 64.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 161      |
|    time_elapsed    | 30211    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=262.19 +/- 84.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.022924861 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.7         |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 98.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 162      |
|    time_elapsed    | 30410    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 198         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 163         |
|    time_elapsed         | 30577       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.016830407 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 75.3        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=-87.37 +/- 927.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -87.4       |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.023007931 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 108         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 164      |
|    time_elapsed    | 30775    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=274.80 +/- 174.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 275        |
| time/                   |            |
|    total_timesteps      | 2700000    |
| train/                  |            |
|    approx_kl            | 0.01841826 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.19       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.00879   |
|    value_loss           | 84.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 165      |
|    time_elapsed    | 30974    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 190         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 166         |
|    time_elapsed         | 31140       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.021789234 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 40.2        |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=160.60 +/- 89.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.015618663 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.2        |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 98.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 167      |
|    time_elapsed    | 31339    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=219.80 +/- 162.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.019844214 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.36        |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 77.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 168      |
|    time_elapsed    | 31538    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 131         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 169         |
|    time_elapsed         | 31706       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.020179925 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.32        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00637    |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=202.38 +/- 151.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 202         |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.016656106 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.5        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 336         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 170      |
|    time_elapsed    | 31905    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=189.37 +/- 159.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.019481678 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.882       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 7.85        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 200      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 171      |
|    time_elapsed    | 32103    |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 185         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 172         |
|    time_elapsed         | 32270       |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.019264843 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.82        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00815    |
|    value_loss           | 54          |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=384.45 +/- 188.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 384         |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.024302274 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 340         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 267         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 173      |
|    time_elapsed    | 32468    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=129.18 +/- 165.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.014074565 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.6        |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 162         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 174      |
|    time_elapsed    | 32666    |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 163         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 175         |
|    time_elapsed         | 32833       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.020134185 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.48        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 61.8        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=189.51 +/- 100.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.022701487 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 22          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 176      |
|    time_elapsed    | 33032    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 230         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 177         |
|    time_elapsed         | 33198       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.021881556 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.28        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 21.2        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=341.88 +/- 180.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.024795886 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 6.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 178      |
|    time_elapsed    | 33397    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=225.86 +/- 110.62
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 226        |
| time/                   |            |
|    total_timesteps      | 2925000    |
| train/                  |            |
|    approx_kl            | 0.01859371 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.69       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.00917   |
|    value_loss           | 138        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 179      |
|    time_elapsed    | 33596    |
|    total_timesteps | 2932736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 200        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 180        |
|    time_elapsed         | 33762      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.02568468 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.69      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 148        |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.000737  |
|    value_loss           | 259        |
----------------------------------------
Eval num_timesteps=2950000, episode_reward=208.16 +/- 103.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.024918323 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 53          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 181      |
|    time_elapsed    | 33961    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=272.75 +/- 147.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.022356689 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.53        |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 27.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 203      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 182      |
|    time_elapsed    | 34159    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 197         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 183         |
|    time_elapsed         | 34325       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.024199763 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.9        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00913    |
|    value_loss           | 49.9        |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=158.14 +/- 114.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.017865157 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.7        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 43.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 184      |
|    time_elapsed    | 34524    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=310.25 +/- 79.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.020768259 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.78        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 15.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 220      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 185      |
|    time_elapsed    | 34723    |
|    total_timesteps | 3031040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 240        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 186        |
|    time_elapsed         | 34889      |
|    total_timesteps      | 3047424    |
| train/                  |            |
|    approx_kl            | 0.01767892 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.67      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.00938   |
|    value_loss           | 44.9       |
----------------------------------------
Eval num_timesteps=3050000, episode_reward=199.23 +/- 182.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.023544544 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 23.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 187      |
|    time_elapsed    | 35087    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=365.17 +/- 203.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 3075000     |
| train/                  |             |
|    approx_kl            | 0.015351558 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 94          |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 260      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 188      |
|    time_elapsed    | 35286    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 233         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 189         |
|    time_elapsed         | 35452       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.020758068 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 37.8        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=176.01 +/- 99.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.021020083 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 53.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 190      |
|    time_elapsed    | 35650    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=389.66 +/- 104.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 390         |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.030427754 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0003      |
|    loss                 | 161         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.00348     |
|    value_loss           | 512         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 65.4     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 191      |
|    time_elapsed    | 35848    |
|    total_timesteps | 3129344  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 79.1      |
| time/                   |           |
|    fps                  | 87        |
|    iterations           | 192       |
|    time_elapsed         | 36014     |
|    total_timesteps      | 3145728   |
| train/                  |           |
|    approx_kl            | 0.0214508 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.66     |
|    explained_variance   | 0.623     |
|    learning_rate        | 0.0003    |
|    loss                 | 49.6      |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.000592 |
|    value_loss           | 161       |
---------------------------------------
Eval num_timesteps=3150000, episode_reward=192.00 +/- 83.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 3150000    |
| train/                  |            |
|    approx_kl            | 0.02199817 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.61      |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 157        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.00792   |
|    value_loss           | 157        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 75.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 193      |
|    time_elapsed    | 36213    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=184.87 +/- 76.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.019275714 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.5        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 60.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 194      |
|    time_elapsed    | 36411    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 142         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 195         |
|    time_elapsed         | 36577       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.018981287 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 67          |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=224.67 +/- 146.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 3200000      |
| train/                  |              |
|    approx_kl            | 0.0130922925 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.0003       |
|    loss                 | 223          |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 355          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 196      |
|    time_elapsed    | 36775    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=275.19 +/- 165.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.024177179 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 49.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 200      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 197      |
|    time_elapsed    | 36973    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 198         |
|    time_elapsed         | 37140       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.021045858 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.98        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 13.8        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=224.65 +/- 123.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 225        |
| time/                   |            |
|    total_timesteps      | 3250000    |
| train/                  |            |
|    approx_kl            | 0.02233594 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.57      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.4       |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.00941   |
|    value_loss           | 26.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 261      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 199      |
|    time_elapsed    | 37339    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=317.76 +/- 120.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 318        |
| time/                   |            |
|    total_timesteps      | 3275000    |
| train/                  |            |
|    approx_kl            | 0.02095728 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.67      |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.6       |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.00835   |
|    value_loss           | 122        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 200      |
|    time_elapsed    | 37538    |
|    total_timesteps | 3276800  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 201        |
|    time_elapsed         | 37704      |
|    total_timesteps      | 3293184    |
| train/                  |            |
|    approx_kl            | 0.02053874 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.68      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 66.4       |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0086    |
|    value_loss           | 87.5       |
----------------------------------------
Eval num_timesteps=3300000, episode_reward=182.04 +/- 95.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.023147738 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.55        |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 13.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 202      |
|    time_elapsed    | 37902    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=240.56 +/- 135.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.019627392 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.33        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 18.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 203      |
|    time_elapsed    | 38100    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 283         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 204         |
|    time_elapsed         | 38266       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.018783394 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.51        |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=203.64 +/- 127.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.020181295 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.32        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 43.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 205      |
|    time_elapsed    | 38464    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=204.90 +/- 88.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.023186736 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.9         |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 39.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 206      |
|    time_elapsed    | 38664    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 209         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 207         |
|    time_elapsed         | 38830       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.021309868 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.2        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00945    |
|    value_loss           | 69          |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=256.13 +/- 99.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.022514865 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.32        |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 221      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 208      |
|    time_elapsed    | 39029    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 223         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 209         |
|    time_elapsed         | 39196       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.021530941 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | 84          |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 152         |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=317.05 +/- 199.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.023928586 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.24        |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 40.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 249      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 210      |
|    time_elapsed    | 39394    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=249.15 +/- 82.79
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 249       |
| time/                   |           |
|    total_timesteps      | 3450000   |
| train/                  |           |
|    approx_kl            | 0.0206745 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.41     |
|    explained_variance   | 0.814     |
|    learning_rate        | 0.0003    |
|    loss                 | 39.8      |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.00747  |
|    value_loss           | 41.4      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 211      |
|    time_elapsed    | 39592    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 261         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 212         |
|    time_elapsed         | 39758       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.018531654 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.25        |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 91.1        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=166.08 +/- 434.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 166         |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.026664212 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.31       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00365     |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 213      |
|    time_elapsed    | 39957    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=100.26 +/- 60.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 100         |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.029875198 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 201         |
|    n_updates            | 2130        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 875         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 214      |
|    time_elapsed    | 40155    |
|    total_timesteps | 3506176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 177        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 215        |
|    time_elapsed         | 40322      |
|    total_timesteps      | 3522560    |
| train/                  |            |
|    approx_kl            | 0.02431096 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.28      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.26       |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0051    |
|    value_loss           | 13.8       |
----------------------------------------
Eval num_timesteps=3525000, episode_reward=382.89 +/- 142.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 383         |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.015708426 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 216      |
|    time_elapsed    | 40521    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=518.68 +/- 135.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 519         |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.021607773 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.95        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 39.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 217      |
|    time_elapsed    | 40720    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 257         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 218         |
|    time_elapsed         | 40887       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.013486672 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 601         |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=241.12 +/- 122.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.023919877 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.37        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 75.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 219      |
|    time_elapsed    | 41085    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=308.96 +/- 157.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 309         |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.026120456 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.96        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 33.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 220      |
|    time_elapsed    | 41284    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 235         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 221         |
|    time_elapsed         | 41451       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.014152801 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | 206         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=311.62 +/- 61.81
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 312        |
| time/                   |            |
|    total_timesteps      | 3625000    |
| train/                  |            |
|    approx_kl            | 0.02301339 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.04      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 177        |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.00434   |
|    value_loss           | 55.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 222      |
|    time_elapsed    | 41649    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=282.37 +/- 139.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.017816797 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 38.8        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 95.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 223      |
|    time_elapsed    | 41848    |
|    total_timesteps | 3653632  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 293       |
| time/                   |           |
|    fps                  | 87        |
|    iterations           | 224       |
|    time_elapsed         | 42015     |
|    total_timesteps      | 3670016   |
| train/                  |           |
|    approx_kl            | 0.0218437 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.26     |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | 21        |
|    n_updates            | 2230      |
|    policy_gradient_loss | -0.00505  |
|    value_loss           | 35        |
---------------------------------------
Eval num_timesteps=3675000, episode_reward=209.23 +/- 96.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.023256874 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.6         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 258      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 225      |
|    time_elapsed    | 42213    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=307.94 +/- 136.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 308         |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.014956577 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.03       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 151         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 249      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 226      |
|    time_elapsed    | 42412    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 253         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 227         |
|    time_elapsed         | 42579       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.021052293 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 67.2        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=234.26 +/- 111.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.024631876 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.86        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00876    |
|    value_loss           | 18.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 248      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 228      |
|    time_elapsed    | 42778    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=232.28 +/- 127.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.014173216 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.17       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0003      |
|    loss                 | 89          |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 563         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 263      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 229      |
|    time_elapsed    | 42977    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 228         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 230         |
|    time_elapsed         | 43144       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.022836896 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.93       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.62        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 43.3        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=343.82 +/- 89.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 344         |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.023444079 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 485         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 231      |
|    time_elapsed    | 43342    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=284.40 +/- 114.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 3800000    |
| train/                  |            |
|    approx_kl            | 0.02201616 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.95      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.76       |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.00489   |
|    value_loss           | 109        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 228      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 232      |
|    time_elapsed    | 43541    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 203         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 233         |
|    time_elapsed         | 43707       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.016943384 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.7        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 529         |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=379.54 +/- 134.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 380         |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.024252413 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.7        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 80.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 234      |
|    time_elapsed    | 43906    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=-1942.93 +/- 4557.70
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -1.94e+03 |
| time/                   |           |
|    total_timesteps      | 3850000   |
| train/                  |           |
|    approx_kl            | 0.0187365 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.99     |
|    explained_variance   | 0.762     |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 2340      |
|    policy_gradient_loss | -0.00511  |
|    value_loss           | 95.1      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 235      |
|    time_elapsed    | 44105    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 209         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 236         |
|    time_elapsed         | 44272       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.025419781 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.18       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.3        |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=227.60 +/- 182.54
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 228       |
| time/                   |           |
|    total_timesteps      | 3875000   |
| train/                  |           |
|    approx_kl            | 0.0295691 |
|    clip_fraction        | 0.272     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.08     |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.99      |
|    n_updates            | 2360      |
|    policy_gradient_loss | -0.0113   |
|    value_loss           | 552       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 237      |
|    time_elapsed    | 44470    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 167         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 238         |
|    time_elapsed         | 44637       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.017476365 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.99       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.3        |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 439         |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=355.67 +/- 176.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 356         |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.022522472 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.99       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.78        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 258         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 239      |
|    time_elapsed    | 44835    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=309.39 +/- 93.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 309         |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.022091936 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.98        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 87.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 240      |
|    time_elapsed    | 45034    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 263         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 241         |
|    time_elapsed         | 45201       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.020411883 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.76        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=242.60 +/- 85.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.022450149 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.0003      |
|    loss                 | 177         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 217         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 242      |
|    time_elapsed    | 45401    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=277.90 +/- 112.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.037083127 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.92        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 135         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 247      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 243      |
|    time_elapsed    | 45600    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 244         |
|    time_elapsed         | 45767       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.023776168 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.6        |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 17.4        |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=187.78 +/- 68.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.010658778 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 352         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 245      |
|    time_elapsed    | 45966    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=451.26 +/- 79.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 451         |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.020921499 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.8        |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 128         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 246      |
|    time_elapsed    | 46164    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 246         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 247         |
|    time_elapsed         | 46331       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.028486738 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.95        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 27.1        |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=262.38 +/- 47.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.023338225 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.64        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00815    |
|    value_loss           | 21.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 248      |
|    time_elapsed    | 46529    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=219.50 +/- 135.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 219        |
| time/                   |            |
|    total_timesteps      | 4075000    |
| train/                  |            |
|    approx_kl            | 0.02384968 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.41      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.815      |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.00826   |
|    value_loss           | 46.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 249      |
|    time_elapsed    | 46728    |
|    total_timesteps | 4079616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 271        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 250        |
|    time_elapsed         | 46896      |
|    total_timesteps      | 4096000    |
| train/                  |            |
|    approx_kl            | 0.01670062 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.62      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.3       |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0095    |
|    value_loss           | 320        |
----------------------------------------
Eval num_timesteps=4100000, episode_reward=398.67 +/- 229.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 399         |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.024823628 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 893         |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 227      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 251      |
|    time_elapsed    | 47095    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=270.05 +/- 219.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.012517662 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0003      |
|    loss                 | 254         |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 252      |
|    time_elapsed    | 47294    |
|    total_timesteps | 4128768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 31.6       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 253        |
|    time_elapsed         | 47461      |
|    total_timesteps      | 4145152    |
| train/                  |            |
|    approx_kl            | 0.02450658 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.59      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23e+03   |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.00172   |
|    value_loss           | 767        |
----------------------------------------
Eval num_timesteps=4150000, episode_reward=310.34 +/- 230.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.018199861 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.0541      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.05e+03    |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 1.99e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 34.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 254      |
|    time_elapsed    | 47660    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=302.20 +/- 73.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 302         |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.026017804 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 18.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 60.6     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 255      |
|    time_elapsed    | 47859    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 211         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 256         |
|    time_elapsed         | 48025       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.021532107 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.91        |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=-3183.28 +/- 6870.17
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -3.18e+03  |
| time/                   |            |
|    total_timesteps      | 4200000    |
| train/                  |            |
|    approx_kl            | 0.02209499 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.6       |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.31       |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.00916   |
|    value_loss           | 80.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 257      |
|    time_elapsed    | 48224    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=197.76 +/- 61.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.020766918 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 352         |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 347         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 258      |
|    time_elapsed    | 48424    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 244         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 259         |
|    time_elapsed         | 48591       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.021735962 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.47        |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 64          |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=348.99 +/- 107.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 349         |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.034136258 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 260      |
|    time_elapsed    | 48791    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=221.01 +/- 109.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.016532838 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 377         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 255      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 261      |
|    time_elapsed    | 48991    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 245         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 262         |
|    time_elapsed         | 49159       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.020985454 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 246         |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=262.43 +/- 127.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 262         |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.022236712 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 10.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 259      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 263      |
|    time_elapsed    | 49358    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=333.13 +/- 135.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 333         |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.018708985 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.1        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 97.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 264      |
|    time_elapsed    | 49558    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 277         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 265         |
|    time_elapsed         | 49725       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.028160855 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 318         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=300.23 +/- 186.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.020377997 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.8        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 266      |
|    time_elapsed    | 49923    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 94.9        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 267         |
|    time_elapsed         | 50091       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.017433085 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.2        |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00616    |
|    value_loss           | 549         |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=-3222.54 +/- 6839.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -3.22e+03  |
| time/                   |            |
|    total_timesteps      | 4375000    |
| train/                  |            |
|    approx_kl            | 0.01877281 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.21      |
|    explained_variance   | 0.117      |
|    learning_rate        | 0.0003     |
|    loss                 | 373        |
|    n_updates            | 2670       |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 677        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 98.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 268      |
|    time_elapsed    | 50290    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=230.34 +/- 154.93
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 230       |
| time/                   |           |
|    total_timesteps      | 4400000   |
| train/                  |           |
|    approx_kl            | 0.0284301 |
|    clip_fraction        | 0.245     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.35     |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | 20.6      |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.0115   |
|    value_loss           | 54.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 269      |
|    time_elapsed    | 50489    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 270         |
|    time_elapsed         | 50657       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.024128057 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.62        |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 27.9        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=352.99 +/- 102.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 353         |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.015199244 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 271      |
|    time_elapsed    | 50855    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=368.64 +/- 240.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.026280273 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 327         |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 77.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 251      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 272      |
|    time_elapsed    | 51055    |
|    total_timesteps | 4456448  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 256        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 273        |
|    time_elapsed         | 51222      |
|    total_timesteps      | 4472832    |
| train/                  |            |
|    approx_kl            | 0.02296305 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.7       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 186        |
----------------------------------------
Eval num_timesteps=4475000, episode_reward=411.47 +/- 147.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 411         |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.029100323 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 35.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 272      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 274      |
|    time_elapsed    | 51420    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=355.27 +/- 142.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 355         |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.019577902 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 275      |
|    time_elapsed    | 51619    |
|    total_timesteps | 4505600  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 241        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 276        |
|    time_elapsed         | 51787      |
|    total_timesteps      | 4521984    |
| train/                  |            |
|    approx_kl            | 0.02109474 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.56      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 114        |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.00601   |
|    value_loss           | 158        |
----------------------------------------
Eval num_timesteps=4525000, episode_reward=-1874.79 +/- 4783.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.87e+03   |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.027158929 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 73          |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 203         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 263      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 277      |
|    time_elapsed    | 51986    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=365.26 +/- 169.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.022584818 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 68          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 278      |
|    time_elapsed    | 52186    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 292         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 279         |
|    time_elapsed         | 52353       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.023822442 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.24        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 40.7        |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=358.75 +/- 106.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 359         |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.024235912 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 50.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 280      |
|    time_elapsed    | 52552    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=335.90 +/- 167.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 336         |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.019867688 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.5        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 74.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 281      |
|    time_elapsed    | 52751    |
|    total_timesteps | 4603904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 261        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 282        |
|    time_elapsed         | 52919      |
|    total_timesteps      | 4620288    |
| train/                  |            |
|    approx_kl            | 0.01928891 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.38      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 124        |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.00689   |
|    value_loss           | 224        |
----------------------------------------
Eval num_timesteps=4625000, episode_reward=281.50 +/- 92.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.026877148 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.4        |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 46.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 283      |
|    time_elapsed    | 53118    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=270.72 +/- 73.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.016621139 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 457         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 405         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 284      |
|    time_elapsed    | 53317    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 285         |
|    time_elapsed         | 53484       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.030396238 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 15.1        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=320.72 +/- 97.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.009803452 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.0003      |
|    loss                 | 68.4        |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 474         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 286      |
|    time_elapsed    | 53683    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=198.61 +/- 118.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.026403848 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.1         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 107         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 287      |
|    time_elapsed    | 53881    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 168         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 288         |
|    time_elapsed         | 54048       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.023092771 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 251         |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=296.47 +/- 187.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 296         |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.022920178 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.0374      |
|    learning_rate        | 0.0003      |
|    loss                 | 19.7        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 292         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 196      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 289      |
|    time_elapsed    | 54247    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=290.87 +/- 182.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.021964587 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.29        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 232         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 290      |
|    time_elapsed    | 54446    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 143         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 291         |
|    time_elapsed         | 54613       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.027091606 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 77.6        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=218.56 +/- 88.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.013628048 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.0003      |
|    loss                 | 491         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 533         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 292      |
|    time_elapsed    | 54813    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=370.89 +/- 150.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.025016747 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.3        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 19.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 241      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 293      |
|    time_elapsed    | 55012    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 181         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 294         |
|    time_elapsed         | 55179       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.022260718 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.4        |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 33          |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=324.44 +/- 79.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 324         |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.008598598 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 290         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 213      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 295      |
|    time_elapsed    | 55378    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 186         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 296         |
|    time_elapsed         | 55545       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.027372044 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 256         |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=247.01 +/- 128.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.018455513 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.8        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 119         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 297      |
|    time_elapsed    | 55744    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=299.63 +/- 410.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.027322827 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 276         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 298      |
|    time_elapsed    | 55943    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 207         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 299         |
|    time_elapsed         | 56109       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.015341517 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.9        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 395         |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=-1201.49 +/- 2348.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.024339609 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 42.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 300      |
|    time_elapsed    | 56308    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=240.33 +/- 100.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.018879509 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 449         |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 278         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 301      |
|    time_elapsed    | 56507    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 201         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 302         |
|    time_elapsed         | 56674       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.023330994 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.4        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 93.1        |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=297.45 +/- 148.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.012108775 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 514         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 303      |
|    time_elapsed    | 56874    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=259.91 +/- 151.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.024342427 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.66        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 19.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 304      |
|    time_elapsed    | 57072    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 135         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 305         |
|    time_elapsed         | 57239       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.010411631 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 202         |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 308         |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=284.29 +/- 124.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.016765516 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.9        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 196         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 306      |
|    time_elapsed    | 57438    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=-2060.56 +/- 4657.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.06e+03   |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.016493004 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0003      |
|    loss                 | 710         |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 871         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.566    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 307      |
|    time_elapsed    | 57637    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 64.8        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 308         |
|    time_elapsed         | 57804       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.019675968 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 83.3        |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=173.88 +/- 30.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.024430428 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 24.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 78.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 309      |
|    time_elapsed    | 58003    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=259.14 +/- 134.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.015011696 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 390         |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 437         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 310      |
|    time_elapsed    | 58201    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 311         |
|    time_elapsed         | 58368       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.028172329 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.78        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 52.7        |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=209.59 +/- 206.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.025134366 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.7        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 47.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 312      |
|    time_elapsed    | 58567    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=304.37 +/- 104.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.019686123 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0003      |
|    loss                 | 171         |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 338         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 313      |
|    time_elapsed    | 58767    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 88.9        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 314         |
|    time_elapsed         | 58934       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.025800882 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.5        |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 76.4        |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=351.58 +/- 240.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 352         |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.016528692 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0003      |
|    loss                 | 225         |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 1.2e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 315      |
|    time_elapsed    | 59133    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=198.45 +/- 62.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.023018584 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.87        |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 33.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 316      |
|    time_elapsed    | 59332    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 317         |
|    time_elapsed         | 59499       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.024642484 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.5        |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 677         |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=207.71 +/- 108.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 208        |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.01896391 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.155      |
|    learning_rate        | 0.0003     |
|    loss                 | 244        |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 1.17e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 318      |
|    time_elapsed    | 59698    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=134.30 +/- 88.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.022246618 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.4        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 258         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 319      |
|    time_elapsed    | 59896    |
|    total_timesteps | 5226496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 320        |
|    time_elapsed         | 60063      |
|    total_timesteps      | 5242880    |
| train/                  |            |
|    approx_kl            | 0.02647185 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 293        |
|    n_updates            | 3190       |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 137        |
----------------------------------------
Eval num_timesteps=5250000, episode_reward=208.35 +/- 104.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 208        |
| time/                   |            |
|    total_timesteps      | 5250000    |
| train/                  |            |
|    approx_kl            | 0.02571734 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.96       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.00504   |
|    value_loss           | 73.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 321      |
|    time_elapsed    | 60262    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=339.48 +/- 51.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 339         |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.024919815 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.19        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 88.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 322      |
|    time_elapsed    | 60461    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 189         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 323         |
|    time_elapsed         | 60628       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.012268121 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0003      |
|    loss                 | 306         |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=328.15 +/- 84.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 328         |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.025341868 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 20.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 217      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 324      |
|    time_elapsed    | 60828    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 175         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 325         |
|    time_elapsed         | 60995       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.025184836 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.1        |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=265.96 +/- 147.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 266         |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.021505613 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 7           |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00959    |
|    value_loss           | 66.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 272      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 326      |
|    time_elapsed    | 61194    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=203.94 +/- 65.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.020840058 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.73        |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 20.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 327      |
|    time_elapsed    | 61393    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 260         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 328         |
|    time_elapsed         | 61561       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.019476596 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 65.5        |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=199.34 +/- 129.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.021092454 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.74        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 268         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 260      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 329      |
|    time_elapsed    | 61760    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=333.24 +/- 79.56
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 333        |
| time/                   |            |
|    total_timesteps      | 5400000    |
| train/                  |            |
|    approx_kl            | 0.02410563 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.11      |
|    explained_variance   | 0.0568     |
|    learning_rate        | 0.0003     |
|    loss                 | 313        |
|    n_updates            | 3290       |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 1.36e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 330      |
|    time_elapsed    | 61960    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 223         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 331         |
|    time_elapsed         | 62127       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.026603194 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 300         |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=317.00 +/- 211.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.022167463 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.694       |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 7.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 332      |
|    time_elapsed    | 62327    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=306.50 +/- 118.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.014797207 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.4        |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00953    |
|    value_loss           | 686         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 333      |
|    time_elapsed    | 62526    |
|    total_timesteps | 5455872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 334        |
|    time_elapsed         | 62693      |
|    total_timesteps      | 5472256    |
| train/                  |            |
|    approx_kl            | 0.02431592 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.42      |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.0003     |
|    loss                 | 70.4       |
|    n_updates            | 3330       |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 580        |
----------------------------------------
Eval num_timesteps=5475000, episode_reward=232.23 +/- 109.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.023947895 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.69        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 131         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 335      |
|    time_elapsed    | 62892    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=344.88 +/- 135.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 345        |
| time/                   |            |
|    total_timesteps      | 5500000    |
| train/                  |            |
|    approx_kl            | 0.01800384 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.16      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 138        |
|    n_updates            | 3350       |
|    policy_gradient_loss | -0.00622   |
|    value_loss           | 140        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 336      |
|    time_elapsed    | 63091    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 242         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 337         |
|    time_elapsed         | 63258       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.024030369 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 55.7        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=284.93 +/- 179.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.017296119 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 223         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 338      |
|    time_elapsed    | 63457    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=287.89 +/- 116.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.018469755 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 12          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 339      |
|    time_elapsed    | 63656    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 289         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 340         |
|    time_elapsed         | 63824       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.023452744 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=273.95 +/- 73.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.024133136 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.09        |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 31.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 341      |
|    time_elapsed    | 64023    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=177.88 +/- 107.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.022620091 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.6        |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 55.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 342      |
|    time_elapsed    | 64222    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 286         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 343         |
|    time_elapsed         | 64389       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.014608485 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 46.5        |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=312.63 +/- 199.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.021097805 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 81.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 344      |
|    time_elapsed    | 64589    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=328.14 +/- 145.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 328        |
| time/                   |            |
|    total_timesteps      | 5650000    |
| train/                  |            |
|    approx_kl            | 0.01361154 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0003     |
|    loss                 | 26.2       |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.00631   |
|    value_loss           | 334        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 345      |
|    time_elapsed    | 64787    |
|    total_timesteps | 5652480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 228        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 346        |
|    time_elapsed         | 64954      |
|    total_timesteps      | 5668864    |
| train/                  |            |
|    approx_kl            | 0.07908064 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.3        |
|    n_updates            | 3450       |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 66.2       |
----------------------------------------
Eval num_timesteps=5675000, episode_reward=281.05 +/- 169.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 281        |
| time/                   |            |
|    total_timesteps      | 5675000    |
| train/                  |            |
|    approx_kl            | 0.01838995 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.04      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.07       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.00816   |
|    value_loss           | 40.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 347      |
|    time_elapsed    | 65152    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=373.06 +/- 233.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 373         |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.010286276 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.7        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 1.26e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 348      |
|    time_elapsed    | 65351    |
|    total_timesteps | 5701632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 349        |
|    time_elapsed         | 65518      |
|    total_timesteps      | 5718016    |
| train/                  |            |
|    approx_kl            | 0.01779237 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.39       |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 25.4       |
----------------------------------------
Eval num_timesteps=5725000, episode_reward=231.75 +/- 101.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.016096283 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 300         |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00778    |
|    value_loss           | 580         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 350      |
|    time_elapsed    | 65717    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=293.81 +/- 175.93
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 5750000    |
| train/                  |            |
|    approx_kl            | 0.01823125 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.18      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.00895   |
|    value_loss           | 671        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 351      |
|    time_elapsed    | 65915    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 217         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 352         |
|    time_elapsed         | 66082       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.022915997 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 571         |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 241         |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=236.73 +/- 96.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.020062327 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.49        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 11.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 353      |
|    time_elapsed    | 66281    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 304         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 354         |
|    time_elapsed         | 66448       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.020129174 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.95        |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 12          |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=235.07 +/- 103.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.018600428 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 162         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 258      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 355      |
|    time_elapsed    | 66649    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=227.77 +/- 128.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.018868139 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 3550        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 17          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 356      |
|    time_elapsed    | 66849    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 225         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 357         |
|    time_elapsed         | 67016       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.013848085 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.6        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=220.89 +/- 132.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.023649398 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00901    |
|    value_loss           | 17.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 358      |
|    time_elapsed    | 67215    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=199.22 +/- 88.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.013421893 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 211         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 359      |
|    time_elapsed    | 67414    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 256         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 360         |
|    time_elapsed         | 67581       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.025366303 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 358         |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 452         |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=371.93 +/- 165.65
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 372       |
| time/                   |           |
|    total_timesteps      | 5900000   |
| train/                  |           |
|    approx_kl            | 0.0194375 |
|    clip_fraction        | 0.205     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.38     |
|    explained_variance   | 0.909     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.3      |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.00777  |
|    value_loss           | 114       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 361      |
|    time_elapsed    | 67779    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=301.09 +/- 151.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.022368316 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.47        |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 156         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 362      |
|    time_elapsed    | 67978    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 298         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 363         |
|    time_elapsed         | 68145       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.021745391 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81        |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00703    |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=153.18 +/- 75.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 153        |
| time/                   |            |
|    total_timesteps      | 5950000    |
| train/                  |            |
|    approx_kl            | 0.02405046 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.42      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.49       |
|    n_updates            | 3630       |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 16.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 364      |
|    time_elapsed    | 68344    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=341.63 +/- 153.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.023427747 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.3         |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 49          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 365      |
|    time_elapsed    | 68544    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 193         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 366         |
|    time_elapsed         | 68711       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.017757796 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.4        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 375         |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=192.21 +/- 47.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.021245023 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.02        |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 26.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 367      |
|    time_elapsed    | 68909    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=246.92 +/- 161.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.012972411 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.9        |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 385         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 368      |
|    time_elapsed    | 69108    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 230         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 369         |
|    time_elapsed         | 69275       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.022182273 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 67.9        |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=241.31 +/- 127.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.026811428 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 253         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 82.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 370      |
|    time_elapsed    | 69473    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=210.75 +/- 61.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.018892616 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 166         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 371      |
|    time_elapsed    | 69672    |
|    total_timesteps | 6078464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 372        |
|    time_elapsed         | 69839      |
|    total_timesteps      | 6094848    |
| train/                  |            |
|    approx_kl            | 0.01788518 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.82      |
|    explained_variance   | 0.38       |
|    learning_rate        | 0.0003     |
|    loss                 | 194        |
|    n_updates            | 3710       |
|    policy_gradient_loss | -0.00499   |
|    value_loss           | 695        |
----------------------------------------
Eval num_timesteps=6100000, episode_reward=372.57 +/- 186.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 373         |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.017031044 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 160         |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 375         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 92.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 373      |
|    time_elapsed    | 70038    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=220.45 +/- 26.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.022588866 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07e+03    |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 933         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 374      |
|    time_elapsed    | 70237    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 199         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 375         |
|    time_elapsed         | 70405       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.019750431 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 474         |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=303.15 +/- 109.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 303         |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.022656554 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.3        |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 53.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 224      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 376      |
|    time_elapsed    | 70604    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=322.49 +/- 58.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.018744996 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.35        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 50.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 377      |
|    time_elapsed    | 70802    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 265         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 378         |
|    time_elapsed         | 70970       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.022795124 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.38        |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 39.3        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=340.48 +/- 105.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 340         |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.019490238 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.28        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 443         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 294      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 379      |
|    time_elapsed    | 71168    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=387.62 +/- 219.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 388         |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.019785492 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 51.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 252      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 380      |
|    time_elapsed    | 71367    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 381         |
|    time_elapsed         | 71534       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.011591034 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=369.43 +/- 211.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.015190156 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 174         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 382      |
|    time_elapsed    | 71733    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=576.56 +/- 137.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 577         |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.011422761 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 260         |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 218         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 383      |
|    time_elapsed    | 71931    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 194         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 384         |
|    time_elapsed         | 72098       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.021953879 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 209         |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 363         |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=368.60 +/- 219.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.019114979 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.11        |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 97.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 385      |
|    time_elapsed    | 72296    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 233         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 386         |
|    time_elapsed         | 72462       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.017467486 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=317.37 +/- 37.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.023180872 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.9         |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 11.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 270      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 387      |
|    time_elapsed    | 72660    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=153.22 +/- 86.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.014980314 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 202         |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 56.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 228      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 388      |
|    time_elapsed    | 72858    |
|    total_timesteps | 6356992  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 229        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 389        |
|    time_elapsed         | 73025      |
|    total_timesteps      | 6373376    |
| train/                  |            |
|    approx_kl            | 0.01447271 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.48      |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.0003     |
|    loss                 | 130        |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.00776   |
|    value_loss           | 394        |
----------------------------------------
Eval num_timesteps=6375000, episode_reward=287.70 +/- 143.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.018826585 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00926    |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 390      |
|    time_elapsed    | 73223    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=-3093.53 +/- 6889.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.09e+03   |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.017897766 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.42        |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 391      |
|    time_elapsed    | 73422    |
|    total_timesteps | 6406144  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 238        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 392        |
|    time_elapsed         | 73588      |
|    total_timesteps      | 6422528    |
| train/                  |            |
|    approx_kl            | 0.01002729 |
|    clip_fraction        | 0.0828     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.91      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.1       |
|    n_updates            | 3910       |
|    policy_gradient_loss | -0.00459   |
|    value_loss           | 651        |
----------------------------------------
Eval num_timesteps=6425000, episode_reward=485.96 +/- 217.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 486         |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.021844398 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.34        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 130         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 240      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 393      |
|    time_elapsed    | 73786    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=253.32 +/- 101.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.019225065 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 28.8        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 73.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 394      |
|    time_elapsed    | 73983    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 285         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 395         |
|    time_elapsed         | 74150       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.023624565 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00966    |
|    value_loss           | 10.6        |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=364.70 +/- 217.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.017007533 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.96        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 22          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 269      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 396      |
|    time_elapsed    | 74350    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=253.68 +/- 153.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.015238847 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.01        |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 473         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 397      |
|    time_elapsed    | 74549    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 398         |
|    time_elapsed         | 74716       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.026010541 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58        |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=414.44 +/- 197.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 414         |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.014063657 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.03        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 41.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 275      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 399      |
|    time_elapsed    | 74914    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=359.87 +/- 229.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 360         |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.017255336 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.67        |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 84.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 400      |
|    time_elapsed    | 75113    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 295         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 401         |
|    time_elapsed         | 75280       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.017616946 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.2        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 130         |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=345.14 +/- 94.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 345         |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.022932459 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 55.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 402      |
|    time_elapsed    | 75479    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=338.24 +/- 128.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 338         |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.015859883 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.9        |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 78.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 403      |
|    time_elapsed    | 75677    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 283         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 404         |
|    time_elapsed         | 75844       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.019998241 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.29        |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 69.4        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=355.00 +/- 91.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 355         |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.015460471 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.5        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 1.11e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 221      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 405      |
|    time_elapsed    | 76043    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=406.00 +/- 97.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 406         |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.013085936 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 1.16e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 406      |
|    time_elapsed    | 76242    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 104         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 407         |
|    time_elapsed         | 76408       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.017558713 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 339         |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 752         |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=241.91 +/- 178.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.015131093 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 373         |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 315         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 408      |
|    time_elapsed    | 76606    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=-2073.69 +/- 4489.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.07e+03   |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.016060226 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.1        |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00915    |
|    value_loss           | 155         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 409      |
|    time_elapsed    | 76804    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 410         |
|    time_elapsed         | 76970       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.012319898 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 58          |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 564         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=-852.55 +/- 2401.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -853        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.020935915 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 488         |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 458         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 411      |
|    time_elapsed    | 77168    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=74.14 +/- 354.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 74.1        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.024392582 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 24.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 412      |
|    time_elapsed    | 77366    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 260        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 413        |
|    time_elapsed         | 77532      |
|    total_timesteps      | 6766592    |
| train/                  |            |
|    approx_kl            | 0.03152154 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 8.59       |
----------------------------------------
Eval num_timesteps=6775000, episode_reward=206.43 +/- 242.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.013571282 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.95        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 299         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 414      |
|    time_elapsed    | 77730    |
|    total_timesteps | 6782976  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 58.4         |
| time/                   |              |
|    fps                  | 87           |
|    iterations           | 415          |
|    time_elapsed         | 77896        |
|    total_timesteps      | 6799360      |
| train/                  |              |
|    approx_kl            | 0.0134662185 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.512        |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.000307    |
|    value_loss           | 348          |
------------------------------------------
Eval num_timesteps=6800000, episode_reward=297.85 +/- 79.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.011652785 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.0773     |
|    learning_rate        | 0.0003      |
|    loss                 | 189         |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 891         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 41.6     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 416      |
|    time_elapsed    | 78094    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=429.41 +/- 139.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 429         |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.021201393 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.23        |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 45.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 40.3     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 417      |
|    time_elapsed    | 78293    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 276         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 418         |
|    time_elapsed         | 78460       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.014124136 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 451         |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 767         |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=311.54 +/- 81.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.019069694 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.26        |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 33.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 419      |
|    time_elapsed    | 78659    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=324.21 +/- 49.59
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 324        |
| time/                   |            |
|    total_timesteps      | 6875000    |
| train/                  |            |
|    approx_kl            | 0.01093547 |
|    clip_fraction        | 0.0721     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.497      |
|    learning_rate        | 0.0003     |
|    loss                 | 349        |
|    n_updates            | 4190       |
|    policy_gradient_loss | -0.00397   |
|    value_loss           | 1.09e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 420      |
|    time_elapsed    | 78857    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 217         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 421         |
|    time_elapsed         | 79023       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.026349604 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 58.6        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=213.83 +/- 192.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.019783683 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.2         |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 126         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 422      |
|    time_elapsed    | 79221    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=266.02 +/- 115.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 266        |
| time/                   |            |
|    total_timesteps      | 6925000    |
| train/                  |            |
|    approx_kl            | 0.01943323 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2         |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.4       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.00985   |
|    value_loss           | 139        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 423      |
|    time_elapsed    | 79419    |
|    total_timesteps | 6930432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 280        |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 424        |
|    time_elapsed         | 79586      |
|    total_timesteps      | 6946816    |
| train/                  |            |
|    approx_kl            | 0.02472162 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.36      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.87       |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.00869   |
|    value_loss           | 17.3       |
----------------------------------------
Eval num_timesteps=6950000, episode_reward=145.27 +/- 78.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.010839418 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.4        |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 279         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 425      |
|    time_elapsed    | 79784    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=174.80 +/- 178.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.013555262 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.38        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.52e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 426      |
|    time_elapsed    | 79982    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.3       |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 427         |
|    time_elapsed         | 80149       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.029717455 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.79        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=-409.35 +/- 1790.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.013065828 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.0914      |
|    learning_rate        | 0.0003      |
|    loss                 | 298         |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 1.95e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.7    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 428      |
|    time_elapsed    | 80347    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=300.53 +/- 91.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.012004051 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33e+03    |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 2.03e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 52.3     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 429      |
|    time_elapsed    | 80545    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 49.8        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 430         |
|    time_elapsed         | 80711       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.015556256 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00915    |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=135.09 +/- 187.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.020964377 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.1        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 36.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 431      |
|    time_elapsed    | 80909    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=210.19 +/- 129.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 210        |
| time/                   |            |
|    total_timesteps      | 7075000    |
| train/                  |            |
|    approx_kl            | 0.01620623 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.97      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 215        |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.00865   |
|    value_loss           | 520        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 432      |
|    time_elapsed    | 81108    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 252         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 433         |
|    time_elapsed         | 81274       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.015549529 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 248         |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=232.35 +/- 150.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.017628897 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 69.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 434      |
|    time_elapsed    | 81472    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=278.13 +/- 175.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.013502031 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | 176         |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 509         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 435      |
|    time_elapsed    | 81670    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 226         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 436         |
|    time_elapsed         | 81836       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.019185498 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.77        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 18.1        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=327.33 +/- 149.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 327        |
| time/                   |            |
|    total_timesteps      | 7150000    |
| train/                  |            |
|    approx_kl            | 0.01681748 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.4       |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.00995   |
|    value_loss           | 54.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 437      |
|    time_elapsed    | 82035    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=172.72 +/- 44.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.018022371 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.26        |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 30.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 438      |
|    time_elapsed    | 82233    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 202         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 439         |
|    time_elapsed         | 82399       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.015551673 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.2        |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0066     |
|    value_loss           | 354         |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=279.31 +/- 153.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.010017905 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0003      |
|    loss                 | 46.6        |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 845         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 440      |
|    time_elapsed    | 82597    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=293.52 +/- 125.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.024346998 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.5        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 123         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 227      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 441      |
|    time_elapsed    | 82795    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 238         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 442         |
|    time_elapsed         | 82961       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.023931503 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.43        |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 27.7        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=378.43 +/- 167.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 378         |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.014883218 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 26.8        |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 82          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 443      |
|    time_elapsed    | 83158    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 290         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 444         |
|    time_elapsed         | 83325       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.016419474 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 86.7        |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=-2896.99 +/- 6410.45
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -2.9e+03   |
| time/                   |            |
|    total_timesteps      | 7275000    |
| train/                  |            |
|    approx_kl            | 0.02256969 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.57      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 71.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 445      |
|    time_elapsed    | 83522    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=324.99 +/- 202.76
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 325        |
| time/                   |            |
|    total_timesteps      | 7300000    |
| train/                  |            |
|    approx_kl            | 0.01070724 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 101        |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.00928   |
|    value_loss           | 375        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 446      |
|    time_elapsed    | 83720    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 174         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 447         |
|    time_elapsed         | 83887       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.019484697 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 112         |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 666         |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=-3079.38 +/- 6596.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -3.08e+03   |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.018337537 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.63        |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00866    |
|    value_loss           | 37.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 213      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 448      |
|    time_elapsed    | 84085    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=178.18 +/- 76.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 7350000     |
| train/                  |             |
|    approx_kl            | 0.018759731 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.76        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 33.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 449      |
|    time_elapsed    | 84283    |
|    total_timesteps | 7356416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 156         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 450         |
|    time_elapsed         | 84449       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.008618636 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 157         |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 428         |
-----------------------------------------
Eval num_timesteps=7375000, episode_reward=331.45 +/- 191.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.018307868 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.81        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 34          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 451      |
|    time_elapsed    | 84647    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=137.47 +/- 104.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.013138846 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.1        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 96.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 452      |
|    time_elapsed    | 84845    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 453         |
|    time_elapsed         | 85012       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.023798324 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.19        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 76.2        |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=-886.41 +/- 2224.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -886       |
| time/                   |            |
|    total_timesteps      | 7425000    |
| train/                  |            |
|    approx_kl            | 0.01686881 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.29      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | 212        |
|    n_updates            | 4530       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 240        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 454      |
|    time_elapsed    | 85210    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=158.45 +/- 159.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.017656272 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.88        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 41.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 455      |
|    time_elapsed    | 85410    |
|    total_timesteps | 7454720  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 202       |
| time/                   |           |
|    fps                  | 87        |
|    iterations           | 456       |
|    time_elapsed         | 85576     |
|    total_timesteps      | 7471104   |
| train/                  |           |
|    approx_kl            | 0.0166316 |
|    clip_fraction        | 0.171     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.51     |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.0003    |
|    loss                 | 176       |
|    n_updates            | 4550      |
|    policy_gradient_loss | -0.00311  |
|    value_loss           | 445       |
---------------------------------------
Eval num_timesteps=7475000, episode_reward=172.48 +/- 103.86
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 172        |
| time/                   |            |
|    total_timesteps      | 7475000    |
| train/                  |            |
|    approx_kl            | 0.01858117 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.37      |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.0003     |
|    loss                 | 24.9       |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 527        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 457      |
|    time_elapsed    | 85775    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=229.65 +/- 58.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 230        |
| time/                   |            |
|    total_timesteps      | 7500000    |
| train/                  |            |
|    approx_kl            | 0.01711342 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.7       |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 108        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 458      |
|    time_elapsed    | 85973    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 195         |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 459         |
|    time_elapsed         | 86141       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.016121916 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=417.08 +/- 185.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 417         |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.020639217 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.9        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.00863    |
|    value_loss           | 50.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 460      |
|    time_elapsed    | 86340    |
|    total_timesteps | 7536640  |
---------------------------------
slurmstepd: error: *** JOB 26565267 ON m3i031 CANCELLED AT 2022-06-28T01:06:58 DUE TO TIME LIMIT ***
