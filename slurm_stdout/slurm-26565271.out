========== uav-v6 ==========
Seed: 237361102
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v6_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -6.33e+03 |
| time/              |           |
|    fps             | 402       |
|    iterations      | 1         |
|    time_elapsed    | 40        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-2733.74 +/- 2488.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -2.73e+03    |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0076284567 |
|    clip_fraction        | 0.0602       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | -0.0002      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.83e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00794     |
|    value_loss           | 4.39e+04     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.72e+03 |
| time/              |           |
|    fps             | 321       |
|    iterations      | 2         |
|    time_elapsed    | 101       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.66e+03   |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 3           |
|    time_elapsed         | 151         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008853712 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -3.05e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 9.16e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 2.79e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-1437.49 +/- 294.04
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.44e+03  |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.02279666 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.8       |
|    explained_variance   | -1.79e-06  |
|    learning_rate        | 0.0003     |
|    loss                 | 5.61e+03   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00623   |
|    value_loss           | 1.64e+04   |
----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.85e+03 |
| time/              |           |
|    fps             | 310       |
|    iterations      | 4         |
|    time_elapsed    | 211       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-1083.55 +/- 448.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.010265386 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.93e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 6.3e+03     |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.77e+03 |
| time/              |           |
|    fps             | 302       |
|    iterations      | 5         |
|    time_elapsed    | 270       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.24e+03   |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 6           |
|    time_elapsed         | 320         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.005351365 |
|    clip_fraction        | 0.00722     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.6e+03     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 1.23e+04    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1438.05 +/- 293.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.44e+03    |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0070605893 |
|    clip_fraction        | 0.0541       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.375        |
|    learning_rate        | 0.0003       |
|    loss                 | 725          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00771     |
|    value_loss           | 2.76e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    fps             | 301       |
|    iterations      | 7         |
|    time_elapsed    | 379       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1797.44 +/- 0.25
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -1.8e+03  |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0094991 |
|    clip_fraction        | 0.108     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.76     |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.0003    |
|    loss                 | 152       |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0116   |
|    value_loss           | 385       |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.13e+03 |
| time/              |           |
|    fps             | 298       |
|    iterations      | 8         |
|    time_elapsed    | 439       |
|    total_timesteps | 131072    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -624        |
| time/                   |             |
|    fps                  | 301         |
|    iterations           | 9           |
|    time_elapsed         | 488         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.013269699 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 68.3        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-1558.04 +/- 480.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.017211972 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.94        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 22.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -361     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 10       |
|    time_elapsed    | 548      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-358.07 +/- 293.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -358        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.017711576 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 29.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -318     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 11       |
|    time_elapsed    | 607      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -304        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 12          |
|    time_elapsed         | 657         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.016307551 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.1         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 44.1        |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-715.24 +/- 445.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -715        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.010101139 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00835    |
|    value_loss           | 690         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -352     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 13       |
|    time_elapsed    | 716      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-477.48 +/- 449.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -477        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.014471458 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 151         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 128         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -330     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 14       |
|    time_elapsed    | 775      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -329        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 15          |
|    time_elapsed         | 825         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.018376917 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 88.7        |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1.83 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.015016649 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.9        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 155         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -258     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 16       |
|    time_elapsed    | 884      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=1.76 +/- 1.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.76       |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.01809755 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.74      |
|    explained_variance   | 0.21       |
|    learning_rate        | 0.0003     |
|    loss                 | 353        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.00925   |
|    value_loss           | 317        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 17       |
|    time_elapsed    | 943      |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 18          |
|    time_elapsed         | 993         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.020523876 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.51        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 61.5        |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=1.53 +/- 1.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.53       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.01990154 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.75       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 8.35       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -261     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 19       |
|    time_elapsed    | 1052     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=1.34 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.009255983 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.19e+03    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 2.15e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -350     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 20       |
|    time_elapsed    | 1111     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -555        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 21          |
|    time_elapsed         | 1161        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.012225166 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 4.84e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=5.07 +/- 4.86
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.07         |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0076104733 |
|    clip_fraction        | 0.0648       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.72        |
|    explained_variance   | 0.525        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 2.52e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -441     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 22       |
|    time_elapsed    | 1220     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-114.58 +/- 231.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.017841171 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.9        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 56.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -432     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 23       |
|    time_elapsed    | 1279     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -141        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 24          |
|    time_elapsed         | 1329        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.019899126 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.922       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 7.94        |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1.84 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.017154101 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 368         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 80          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -214     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 25       |
|    time_elapsed    | 1388     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=1.20 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.010429766 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 267         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 3.43e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 26       |
|    time_elapsed    | 1447     |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -233      |
| time/                   |           |
|    fps                  | 295       |
|    iterations           | 27        |
|    time_elapsed         | 1497      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0210915 |
|    clip_fraction        | 0.321     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.68     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.803     |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0117   |
|    value_loss           | 8.35      |
---------------------------------------
Eval num_timesteps=450000, episode_reward=0.86 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.86        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.017301597 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0003      |
|    loss                 | 292         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 282         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 28       |
|    time_elapsed    | 1556     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.49 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.016625106 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16e+03    |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.000396    |
|    value_loss           | 980         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 29       |
|    time_elapsed    | 1614     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -128        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 30          |
|    time_elapsed         | 1663        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.016977318 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.82        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 13.6        |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=1.75 +/- 1.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.021390483 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 46.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 31       |
|    time_elapsed    | 1721     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -97.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 32          |
|    time_elapsed         | 1770        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.012594588 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 320         |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=2.01 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.022046223 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.0644      |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 171         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 33       |
|    time_elapsed    | 1828     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=0.69 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.69        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.021575904 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.32        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 119         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -98.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 34       |
|    time_elapsed    | 1887     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 35          |
|    time_elapsed         | 1936        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.028050637 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.8        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.000195   |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=3.10 +/- 3.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.013123004 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.3        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 178         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 36       |
|    time_elapsed    | 1994     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=1.26 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.018017724 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.87        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 257         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 37       |
|    time_elapsed    | 2052     |
|    total_timesteps | 606208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -86.8      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 38         |
|    time_elapsed         | 2101       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.01864588 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.79       |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00909   |
|    value_loss           | 35.5       |
----------------------------------------
Eval num_timesteps=625000, episode_reward=1.61 +/- 0.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.019109793 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00792    |
|    value_loss           | 21.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 39       |
|    time_elapsed    | 2159     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=3.31 +/- 5.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.31        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.019752713 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.314       |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 2.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 40       |
|    time_elapsed    | 2218     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -180        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 41          |
|    time_elapsed         | 2267        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.012284137 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.7        |
|    n_updates            | 400         |
|    policy_gradient_loss | 1.93e-05    |
|    value_loss           | 274         |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=-2224.02 +/- 4454.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.22e+03   |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.024117395 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.3        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000874   |
|    value_loss           | 185         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 42       |
|    time_elapsed    | 2325     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=1.07 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.012106487 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41e+03    |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00083    |
|    value_loss           | 3.27e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -352     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 43       |
|    time_elapsed    | 2383     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 44          |
|    time_elapsed         | 2432        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.013074107 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.44e+03    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00463    |
|    value_loss           | 1.01e+03    |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=2.76 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.76        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.019351639 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 28.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -255     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 45       |
|    time_elapsed    | 2490     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2.13 +/- 1.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.13       |
| time/                   |            |
|    total_timesteps      | 750000     |
| train/                  |            |
|    approx_kl            | 0.02214947 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.3       |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00492   |
|    value_loss           | 18.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 46       |
|    time_elapsed    | 2549     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -74         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 47          |
|    time_elapsed         | 2598        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.012544969 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 468         |
|    n_updates            | 460         |
|    policy_gradient_loss | 8.68e-06    |
|    value_loss           | 265         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=3.39 +/- 0.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.39       |
| time/                   |            |
|    total_timesteps      | 775000     |
| train/                  |            |
|    approx_kl            | 0.01835217 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.37       |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00658   |
|    value_loss           | 17.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 48       |
|    time_elapsed    | 2656     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2.17 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.018096007 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.1        |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 74.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.8    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 49       |
|    time_elapsed    | 2714     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 50          |
|    time_elapsed         | 2763        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.019980647 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 3.98        |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=2.17 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.17        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.017646892 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.95        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 189         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 51       |
|    time_elapsed    | 2821     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=1.41 +/- 1.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.41       |
| time/                   |            |
|    total_timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.01868325 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.2       |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 790        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 52       |
|    time_elapsed    | 2879     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -375        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 53          |
|    time_elapsed         | 2928        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.032222074 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 520         |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 791         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=2.85 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.85        |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.007513389 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 3.15e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -335     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 54       |
|    time_elapsed    | 2986     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=0.79 +/- 0.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.786      |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.02014229 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.62      |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.52       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00779   |
|    value_loss           | 63.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -344     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 55       |
|    time_elapsed    | 3045     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -219        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 56          |
|    time_elapsed         | 3094        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.015342507 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 313         |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=2.37 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.017367454 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.889       |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 4.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 57       |
|    time_elapsed    | 3152     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1.87 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.016116917 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00364    |
|    value_loss           | 90.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 58       |
|    time_elapsed    | 3210     |
|    total_timesteps | 950272   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 59         |
|    time_elapsed         | 3259       |
|    total_timesteps      | 966656     |
| train/                  |            |
|    approx_kl            | 0.02005232 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 98.1       |
|    n_updates            | 580        |
|    policy_gradient_loss | 0.000857   |
|    value_loss           | 357        |
----------------------------------------
Eval num_timesteps=975000, episode_reward=1.37 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.019237328 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.24        |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 802         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 60       |
|    time_elapsed    | 3317     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -291        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 61          |
|    time_elapsed         | 3366        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.021614198 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.3         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 15.8        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=1.72 +/- 0.95
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.007778237 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 70.6        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 636         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -363     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 62       |
|    time_elapsed    | 3424     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=1.43 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.009234713 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.2        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 655         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 63       |
|    time_elapsed    | 3483     |
|    total_timesteps | 1032192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -396        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 64          |
|    time_elapsed         | 3531        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.012577362 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 196         |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=1.86 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.019781329 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 38.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -283     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 65       |
|    time_elapsed    | 3590     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=-1053.73 +/- 2109.38
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.05e+03  |
| time/                   |            |
|    total_timesteps      | 1075000    |
| train/                  |            |
|    approx_kl            | 0.01739566 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.59      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.762      |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.00387   |
|    value_loss           | 1.97       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 66       |
|    time_elapsed    | 3648     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 67          |
|    time_elapsed         | 3697        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.005727991 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 26          |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=2.29 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.018820904 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.9         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 22.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 68       |
|    time_elapsed    | 3755     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=2.50 +/- 1.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.019294515 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.0362      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.82        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 66.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 69       |
|    time_elapsed    | 3814     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 70          |
|    time_elapsed         | 3865        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.018951934 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.591       |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 2.63        |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=1.46 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.017357975 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.874       |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 15.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 71       |
|    time_elapsed    | 3923     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=3.47 +/- 1.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.47        |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.016957056 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.715       |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 7.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 72       |
|    time_elapsed    | 3981     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -115        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 73          |
|    time_elapsed         | 4030        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.012215735 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 212         |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 416         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=-1.78 +/- 8.66
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | -1.78     |
| time/                   |           |
|    total_timesteps      | 1200000   |
| train/                  |           |
|    approx_kl            | 0.0388399 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.56     |
|    explained_variance   | 0.655     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.68      |
|    n_updates            | 730       |
|    policy_gradient_loss | -0.00262  |
|    value_loss           | 177       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 74       |
|    time_elapsed    | 4089     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=1.32 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.018122341 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.1        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 90.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 75       |
|    time_elapsed    | 4147     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -49.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 76          |
|    time_elapsed         | 4196        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.014918468 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 29          |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=2.18 +/- 2.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.018087678 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 77       |
|    time_elapsed    | 4254     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=2.72 +/- 1.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 1275000     |
| train/                  |             |
|    approx_kl            | 0.023502858 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.000867   |
|    value_loss           | 88.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -55.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 78       |
|    time_elapsed    | 4313     |
|    total_timesteps | 1277952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 79         |
|    time_elapsed         | 4362       |
|    total_timesteps      | 1294336    |
| train/                  |            |
|    approx_kl            | 0.01948861 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.9       |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.00208   |
|    value_loss           | 48         |
----------------------------------------
Eval num_timesteps=1300000, episode_reward=1.16 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.017438736 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.791       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00463    |
|    value_loss           | 31.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 80       |
|    time_elapsed    | 4420     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=3.42 +/- 1.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.42       |
| time/                   |            |
|    total_timesteps      | 1325000    |
| train/                  |            |
|    approx_kl            | 0.02394614 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.53      |
|    explained_variance   | 0.0467     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.95       |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.000575   |
|    value_loss           | 123        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 81       |
|    time_elapsed    | 4478     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 82          |
|    time_elapsed         | 4527        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.017822228 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 306         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 202         |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=2.43 +/- 1.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.021330684 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 17.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 83       |
|    time_elapsed    | 4586     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=3.46 +/- 5.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.46        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.013119149 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00342    |
|    value_loss           | 38.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 84       |
|    time_elapsed    | 4644     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 85          |
|    time_elapsed         | 4693        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.022940535 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.5        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 53.3        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=1.72 +/- 1.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 1400000    |
| train/                  |            |
|    approx_kl            | 0.02181818 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.52      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.904      |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.000606  |
|    value_loss           | 26.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 86       |
|    time_elapsed    | 4751     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=2.82 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.006706057 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.1        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 463         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 87       |
|    time_elapsed    | 4810     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -216        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 88          |
|    time_elapsed         | 4859        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.009990497 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=2.11 +/- 1.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.11       |
| time/                   |            |
|    total_timesteps      | 1450000    |
| train/                  |            |
|    approx_kl            | 0.01761832 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.00543   |
|    value_loss           | 7.72       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 89       |
|    time_elapsed    | 4917     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -141        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 90          |
|    time_elapsed         | 4966        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.017205417 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0909      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 7.41        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=2.80 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.005215833 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 347         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -83.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 91       |
|    time_elapsed    | 5024     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1.05 +/- 1.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.05       |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.01728277 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 910        |
|    policy_gradient_loss | 0.00105    |
|    value_loss           | 11.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 92       |
|    time_elapsed    | 5083     |
|    total_timesteps | 1507328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 93         |
|    time_elapsed         | 5132       |
|    total_timesteps      | 1523712    |
| train/                  |            |
|    approx_kl            | 0.01215896 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 158        |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00478   |
|    value_loss           | 126        |
----------------------------------------
Eval num_timesteps=1525000, episode_reward=2.64 +/- 1.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.018460806 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.933       |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 23.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 94       |
|    time_elapsed    | 5190     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=1.66 +/- 1.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 1550000      |
| train/                  |              |
|    approx_kl            | 0.0116391415 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.2         |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00284     |
|    value_loss           | 61.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 95       |
|    time_elapsed    | 5248     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -54.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 96          |
|    time_elapsed         | 5298        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.025237784 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 38.8        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=1.91 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.013443724 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 49.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 97       |
|    time_elapsed    | 5356     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1.18 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.022751257 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.5        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 2.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 98       |
|    time_elapsed    | 5414     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -98.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 99          |
|    time_elapsed         | 5463        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.008286531 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 16          |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 462         |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=1.78 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.020676505 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 3.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 100      |
|    time_elapsed    | 5521     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2.16 +/- 1.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.16       |
| time/                   |            |
|    total_timesteps      | 1650000    |
| train/                  |            |
|    approx_kl            | 0.01753618 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.54      |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.1       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.00597   |
|    value_loss           | 47.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 101      |
|    time_elapsed    | 5580     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 102         |
|    time_elapsed         | 5629        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.016668374 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.973       |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 38.5        |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=3.68 +/- 1.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.68       |
| time/                   |            |
|    total_timesteps      | 1675000    |
| train/                  |            |
|    approx_kl            | 0.01172544 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.000295  |
|    value_loss           | 339        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 103      |
|    time_elapsed    | 5687     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1.60 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.020832641 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.095       |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00866    |
|    value_loss           | 1.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 104      |
|    time_elapsed    | 5745     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -115        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 105         |
|    time_elapsed         | 5795        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.019418826 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00707    |
|    value_loss           | 3.24        |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=2.19 +/- 2.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.013998867 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 59.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 106      |
|    time_elapsed    | 5853     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=1.55 +/- 1.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.021793919 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00707    |
|    value_loss           | 9.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 107      |
|    time_elapsed    | 5911     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -225        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 108         |
|    time_elapsed         | 5960        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.005140584 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.99        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=6.23 +/- 3.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6.23         |
| time/                   |              |
|    total_timesteps      | 1775000      |
| train/                  |              |
|    approx_kl            | 0.0071841525 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.6         |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 271          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -381     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 109      |
|    time_elapsed    | 6018     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=1.86 +/- 1.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.018495087 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 545         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 786         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 110      |
|    time_elapsed    | 6077     |
|    total_timesteps | 1802240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -319        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 111         |
|    time_elapsed         | 6126        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.020975772 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.8        |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=1825000, episode_reward=3.23 +/- 2.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.23        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.021592623 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.085       |
|    n_updates            | 1110        |
|    policy_gradient_loss | 1.78e-06    |
|    value_loss           | 0.727       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 112      |
|    time_elapsed    | 6184     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=1.76 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.010806595 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 46          |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 131         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 113      |
|    time_elapsed    | 6243     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -50.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 114         |
|    time_elapsed         | 6292        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.020147935 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 50.2        |
|    n_updates            | 1130        |
|    policy_gradient_loss | 0.000478    |
|    value_loss           | 60.1        |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=2.33 +/- 1.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.018518323 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 4.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 115      |
|    time_elapsed    | 6350     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=3.46 +/- 5.03
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.46       |
| time/                   |            |
|    total_timesteps      | 1900000    |
| train/                  |            |
|    approx_kl            | 0.01901836 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 1150       |
|    policy_gradient_loss | 0.00139    |
|    value_loss           | 10.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 116      |
|    time_elapsed    | 6408     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 117         |
|    time_elapsed         | 6457        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.020628415 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.523       |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 9.98        |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=3.21 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.21        |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.009871567 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.19        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 104         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 118      |
|    time_elapsed    | 6516     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -77.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 119         |
|    time_elapsed         | 6565        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.026176352 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.91        |
|    n_updates            | 1180        |
|    policy_gradient_loss | 0.00317     |
|    value_loss           | 42.2        |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=0.38 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 0.378        |
| time/                   |              |
|    total_timesteps      | 1950000      |
| train/                  |              |
|    approx_kl            | 0.0059748944 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.0003       |
|    loss                 | 38.6         |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 212          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -76.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 120      |
|    time_elapsed    | 6623     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=1.40 +/- 1.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.020462938 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00483    |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 0.677       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 121      |
|    time_elapsed    | 6681     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 122         |
|    time_elapsed         | 6731        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.014508067 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0629      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 19.1        |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=1.33 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.020886518 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 6.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.87    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 123      |
|    time_elapsed    | 6789     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=1.07 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.014548246 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 1230        |
|    policy_gradient_loss | 0.00042     |
|    value_loss           | 30.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 124      |
|    time_elapsed    | 6847     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -47.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 125         |
|    time_elapsed         | 6896        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.022593278 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.34        |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.00294     |
|    value_loss           | 20          |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=2.32 +/- 1.64
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.32         |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 0.0049120374 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | 0.671        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.8         |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00243     |
|    value_loss           | 370          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 126      |
|    time_elapsed    | 6955     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=1.52 +/- 1.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0063867974 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.39        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.0003       |
|    loss                 | 227          |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00621     |
|    value_loss           | 355          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 127      |
|    time_elapsed    | 7013     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -259        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 128         |
|    time_elapsed         | 7062        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.008998978 |
|    clip_fraction        | 0.0631      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 245         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 371         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=1.81 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.028475847 |
|    clip_fraction        | 0.401       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.394       |
|    n_updates            | 1280        |
|    policy_gradient_loss | 0.00589     |
|    value_loss           | 16.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 129      |
|    time_elapsed    | 7120     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=2.53 +/- 2.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.021829095 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.34        |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 130      |
|    time_elapsed    | 7179     |
|    total_timesteps | 2129920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -54        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 131        |
|    time_elapsed         | 7228       |
|    total_timesteps      | 2146304    |
| train/                  |            |
|    approx_kl            | 0.03726346 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.8       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.00169   |
|    value_loss           | 149        |
----------------------------------------
Eval num_timesteps=2150000, episode_reward=1.95 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.033271074 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.06        |
|    n_updates            | 1310        |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 41.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 132      |
|    time_elapsed    | 7286     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=1.83 +/- 1.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.016950509 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 5.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -51.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 133      |
|    time_elapsed    | 7344     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 134         |
|    time_elapsed         | 7393        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.018059071 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.38        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 6.86        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=2.10 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.019834414 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.156       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0284      |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 4.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.19    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 135      |
|    time_elapsed    | 7452     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=1.67 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.015389768 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 93.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 136      |
|    time_elapsed    | 7510     |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -76.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 137         |
|    time_elapsed         | 7559        |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.015603457 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0319      |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 84.7        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=4.85 +/- 2.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.85        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.027225723 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.79        |
|    n_updates            | 1370        |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 31.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 138      |
|    time_elapsed    | 7617     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=1.67 +/- 1.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.67         |
| time/                   |              |
|    total_timesteps      | 2275000      |
| train/                  |              |
|    approx_kl            | 0.0032358584 |
|    clip_fraction        | 0.00949      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.46        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 492          |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 922          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 139      |
|    time_elapsed    | 7676     |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -225        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 140         |
|    time_elapsed         | 7725        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.023799164 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.618       |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.00117     |
|    value_loss           | 322         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=2.45 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.030719623 |
|    clip_fraction        | 0.452       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.3        |
|    n_updates            | 1400        |
|    policy_gradient_loss | 0.00725     |
|    value_loss           | 89.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 141      |
|    time_elapsed    | 7784     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=1.40 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.022500448 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -337     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 142      |
|    time_elapsed    | 7842     |
|    total_timesteps | 2326528  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -311         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 143          |
|    time_elapsed         | 7891         |
|    total_timesteps      | 2342912      |
| train/                  |              |
|    approx_kl            | 0.0011994813 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.39        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.0003       |
|    loss                 | 167          |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 862          |
------------------------------------------
Eval num_timesteps=2350000, episode_reward=4.36 +/- 2.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.36        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.012647585 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 26          |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.000298   |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -313     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 144      |
|    time_elapsed    | 7950     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=2.02 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.030199386 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.331       |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.000981    |
|    value_loss           | 54.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 145      |
|    time_elapsed    | 8008     |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -64.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 146         |
|    time_elapsed         | 8057        |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.044329364 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.836       |
|    n_updates            | 1450        |
|    policy_gradient_loss | 0.00384     |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=2.24 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.023132052 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 7.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 147      |
|    time_elapsed    | 8115     |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -191        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 148         |
|    time_elapsed         | 8165        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.010491984 |
|    clip_fraction        | 0.0416      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 439         |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=2.77 +/- 2.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.018134259 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.6        |
|    n_updates            | 1480        |
|    policy_gradient_loss | 0.00135     |
|    value_loss           | 99.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 149      |
|    time_elapsed    | 8223     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=0.96 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.961       |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.021219086 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.342       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00922    |
|    value_loss           | 8.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 150      |
|    time_elapsed    | 8281     |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -199        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 151         |
|    time_elapsed         | 8330        |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.010443036 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.000533   |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=7.42 +/- 6.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 7.42        |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.007946858 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.7        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00432    |
|    value_loss           | 594         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 152      |
|    time_elapsed    | 8389     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2.84 +/- 2.59
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.84       |
| time/                   |            |
|    total_timesteps      | 2500000    |
| train/                  |            |
|    approx_kl            | 0.01996841 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.47      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.907      |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.00198   |
|    value_loss           | 16.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 153      |
|    time_elapsed    | 8447     |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 154         |
|    time_elapsed         | 8496        |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.019812467 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.407       |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 1.14        |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=1.84 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.012539623 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.37        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 11.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.17    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 155      |
|    time_elapsed    | 8554     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=3.20 +/- 2.37
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3.2       |
| time/                   |           |
|    total_timesteps      | 2550000   |
| train/                  |           |
|    approx_kl            | 0.0173921 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.47     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.241     |
|    n_updates            | 1550      |
|    policy_gradient_loss | 0.00118   |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.69    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 156      |
|    time_elapsed    | 8613     |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.31       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 157         |
|    time_elapsed         | 8662        |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.017314296 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.806       |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 2.96        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=1.26 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.016554534 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0957      |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.000889   |
|    value_loss           | 0.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 158      |
|    time_elapsed    | 8720     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=3.88 +/- 3.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.88         |
| time/                   |              |
|    total_timesteps      | 2600000      |
| train/                  |              |
|    approx_kl            | 0.0071962783 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.42         |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 72           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 159      |
|    time_elapsed    | 8778     |
|    total_timesteps | 2605056  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -53.7     |
| time/                   |           |
|    fps                  | 296       |
|    iterations           | 160       |
|    time_elapsed         | 8828      |
|    total_timesteps      | 2621440   |
| train/                  |           |
|    approx_kl            | 0.0300677 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.4      |
|    explained_variance   | 0.961     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.27      |
|    n_updates            | 1590      |
|    policy_gradient_loss | 0.00575   |
|    value_loss           | 125       |
---------------------------------------
Eval num_timesteps=2625000, episode_reward=2.10 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 2625000     |
| train/                  |             |
|    approx_kl            | 0.022246458 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00101     |
|    value_loss           | 25          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 161      |
|    time_elapsed    | 8886     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=4.54 +/- 3.79
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.54       |
| time/                   |            |
|    total_timesteps      | 2650000    |
| train/                  |            |
|    approx_kl            | 0.02551258 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 30.1       |
|    n_updates            | 1610       |
|    policy_gradient_loss | 0.00396    |
|    value_loss           | 138        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -84.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 162      |
|    time_elapsed    | 8944     |
|    total_timesteps | 2654208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -73        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 163        |
|    time_elapsed         | 8993       |
|    total_timesteps      | 2670592    |
| train/                  |            |
|    approx_kl            | 0.01436797 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.41      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.28       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.00711   |
|    value_loss           | 93.8       |
----------------------------------------
Eval num_timesteps=2675000, episode_reward=3.71 +/- 4.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.71       |
| time/                   |            |
|    total_timesteps      | 2675000    |
| train/                  |            |
|    approx_kl            | 0.01872899 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.658      |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.00715   |
|    value_loss           | 23.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 164      |
|    time_elapsed    | 9052     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2.61 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.61        |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.020624222 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 221         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 165      |
|    time_elapsed    | 9110     |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -96.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 166         |
|    time_elapsed         | 9159        |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.017819505 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | 73.3        |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 216         |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=4.55 +/- 3.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.55        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.017020643 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.3        |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 45.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 167      |
|    time_elapsed    | 9217     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2.20 +/- 2.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.018714957 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 59.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 168      |
|    time_elapsed    | 9276     |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 169         |
|    time_elapsed         | 9325        |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.018918552 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.1        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=2.92 +/- 1.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.92       |
| time/                   |            |
|    total_timesteps      | 2775000    |
| train/                  |            |
|    approx_kl            | 0.02001245 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.00217   |
|    value_loss           | 24.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 170      |
|    time_elapsed    | 9383     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=3.63 +/- 1.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.63        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.025538806 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.3        |
|    n_updates            | 1700        |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 81.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -119     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 171      |
|    time_elapsed    | 9441     |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 172         |
|    time_elapsed         | 9491        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.012359532 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=6.30 +/- 8.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.3         |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.019448806 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0772      |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 0.996       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -99.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 173      |
|    time_elapsed    | 9549     |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2.06 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.06        |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.010364546 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37        |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 174      |
|    time_elapsed    | 9607     |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 175         |
|    time_elapsed         | 9656        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.023505788 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.387       |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 40.6        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=5.38 +/- 5.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 2875000     |
| train/                  |             |
|    approx_kl            | 0.019525204 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0965      |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 3.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 176      |
|    time_elapsed    | 9715     |
|    total_timesteps | 2883584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -25.7        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 177          |
|    time_elapsed         | 9764         |
|    total_timesteps      | 2899968      |
| train/                  |              |
|    approx_kl            | 0.0114934165 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.43        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.505        |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00733     |
|    value_loss           | 10.7         |
------------------------------------------
Eval num_timesteps=2900000, episode_reward=3.06 +/- 1.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.06        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.008874124 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.9        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 52          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 178      |
|    time_elapsed    | 9822     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=1.92 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.018287834 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 3.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 179      |
|    time_elapsed    | 9881     |
|    total_timesteps | 2932736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -66.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 180         |
|    time_elapsed         | 9930        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.018868975 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.366       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.000721   |
|    value_loss           | 2.4         |
-----------------------------------------
Eval num_timesteps=2950000, episode_reward=2.11 +/- 1.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.11         |
| time/                   |              |
|    total_timesteps      | 2950000      |
| train/                  |              |
|    approx_kl            | 0.0032684808 |
|    clip_fraction        | 0.00883      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.58         |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 223          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -55.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 181      |
|    time_elapsed    | 9988     |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=-21.12 +/- 47.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -21.1       |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.018536678 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.287       |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 3.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 182      |
|    time_elapsed    | 10046    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -133        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 183         |
|    time_elapsed         | 10096       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.004477131 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.7        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=2.56 +/- 1.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.007929999 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.34        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 68.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 184      |
|    time_elapsed    | 10154    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=2.54 +/- 1.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.028907932 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.00198     |
|    value_loss           | 110         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 185      |
|    time_elapsed    | 10212    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 186         |
|    time_elapsed         | 10261       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.026789987 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.7        |
|    n_updates            | 1850        |
|    policy_gradient_loss | 0.00487     |
|    value_loss           | 72.1        |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=1.81 +/- 1.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 3050000    |
| train/                  |            |
|    approx_kl            | 0.04274559 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.44       |
|    n_updates            | 1860       |
|    policy_gradient_loss | 0.00333    |
|    value_loss           | 169        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 187      |
|    time_elapsed    | 10320    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=3.88 +/- 1.80
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3.88      |
| time/                   |           |
|    total_timesteps      | 3075000   |
| train/                  |           |
|    approx_kl            | 0.0137039 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.2      |
|    explained_variance   | 0.948     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.8      |
|    n_updates            | 1870      |
|    policy_gradient_loss | -0.0054   |
|    value_loss           | 92        |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 188      |
|    time_elapsed    | 10378    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 189         |
|    time_elapsed         | 10427       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.027518801 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.00271     |
|    value_loss           | 14.9        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=3.27 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.27        |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.019720547 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 53.4        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 20.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -66.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 190      |
|    time_elapsed    | 10486    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=3.68 +/- 2.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.68        |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.016570315 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 55          |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.000984   |
|    value_loss           | 167         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 191      |
|    time_elapsed    | 10544    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -57.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 192         |
|    time_elapsed         | 10593       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.023312598 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.389       |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 3.98        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=4.32 +/- 3.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.32        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.018440159 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 16.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 193      |
|    time_elapsed    | 10651    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=2.75 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75        |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.017590526 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00451    |
|    value_loss           | 14          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 194      |
|    time_elapsed    | 10710    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -91.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 195         |
|    time_elapsed         | 10759       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.011016608 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=3.68 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.68        |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.023982089 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.67        |
|    n_updates            | 1950        |
|    policy_gradient_loss | 0.000999    |
|    value_loss           | 90.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 196      |
|    time_elapsed    | 10817    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=2.72 +/- 2.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.019938678 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.02        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 30.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 197      |
|    time_elapsed    | 10875    |
|    total_timesteps | 3227648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -56.6      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 198        |
|    time_elapsed         | 10924      |
|    total_timesteps      | 3244032    |
| train/                  |            |
|    approx_kl            | 0.01421166 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 44         |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.005     |
|    value_loss           | 50.8       |
----------------------------------------
Eval num_timesteps=3250000, episode_reward=1.14 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.022431295 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 19.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 199      |
|    time_elapsed    | 10983    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=1.75 +/- 0.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.75       |
| time/                   |            |
|    total_timesteps      | 3275000    |
| train/                  |            |
|    approx_kl            | 0.02218915 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.6       |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.00572   |
|    value_loss           | 12         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 200      |
|    time_elapsed    | 11041    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 201         |
|    time_elapsed         | 11090       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.021450914 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.03        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=3.55 +/- 1.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.55       |
| time/                   |            |
|    total_timesteps      | 3300000    |
| train/                  |            |
|    approx_kl            | 0.01664718 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.335      |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 1.57       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 202      |
|    time_elapsed    | 11149    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=2.01 +/- 1.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.019297589 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 5.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.52    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 203      |
|    time_elapsed    | 11207    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 204         |
|    time_elapsed         | 11256       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.018834822 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.871       |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 34.5        |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=0.89 +/- 5.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.895       |
| time/                   |             |
|    total_timesteps      | 3350000     |
| train/                  |             |
|    approx_kl            | 0.019517094 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.15        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 8.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 205      |
|    time_elapsed    | 11314    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=1.32 +/- 0.65
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.32       |
| time/                   |            |
|    total_timesteps      | 3375000    |
| train/                  |            |
|    approx_kl            | 0.02051603 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0449     |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.00397   |
|    value_loss           | 1.33       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 206      |
|    time_elapsed    | 11373    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 207         |
|    time_elapsed         | 11422       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.022720387 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 14.7        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=0.92 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.919      |
| time/                   |            |
|    total_timesteps      | 3400000    |
| train/                  |            |
|    approx_kl            | 0.02246715 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.73       |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.00403   |
|    value_loss           | 9.71       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 208      |
|    time_elapsed    | 11480    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -89.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 209         |
|    time_elapsed         | 11529       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.014669957 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 27.8        |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=4.78 +/- 6.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.78        |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.020255402 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.000436   |
|    value_loss           | 186         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 210      |
|    time_elapsed    | 11588    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=2.43 +/- 2.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.025163442 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.63        |
|    n_updates            | 2100        |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 5.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -95.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 211      |
|    time_elapsed    | 11646    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 212         |
|    time_elapsed         | 11695       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.011618406 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.465       |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 87.2        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=1.78 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.017212281 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.912       |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 5.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 213      |
|    time_elapsed    | 11754    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3.15 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.15        |
| time/                   |             |
|    total_timesteps      | 3500000     |
| train/                  |             |
|    approx_kl            | 0.016929746 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.66        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00342    |
|    value_loss           | 88.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 214      |
|    time_elapsed    | 11812    |
|    total_timesteps | 3506176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -16.8      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 215        |
|    time_elapsed         | 11861      |
|    total_timesteps      | 3522560    |
| train/                  |            |
|    approx_kl            | 0.01906022 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.635      |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.00365   |
|    value_loss           | 18         |
----------------------------------------
Eval num_timesteps=3525000, episode_reward=2.27 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.27        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.020173335 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.3        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 4.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 216      |
|    time_elapsed    | 11920    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=1.94 +/- 0.97
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.94       |
| time/                   |            |
|    total_timesteps      | 3550000    |
| train/                  |            |
|    approx_kl            | 0.01996236 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.62       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.00758   |
|    value_loss           | 2.09       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.36    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 217      |
|    time_elapsed    | 11978    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.41       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 218         |
|    time_elapsed         | 12027       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.016648013 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.855       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 5.85        |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=3.32 +/- 4.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.32        |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.018296879 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0154      |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 0.533       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.65    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 219      |
|    time_elapsed    | 12085    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=2.33 +/- 2.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.017975155 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.337       |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 1.25        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 220      |
|    time_elapsed    | 12144    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -107        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 221         |
|    time_elapsed         | 12193       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.013844369 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 33.1        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 337         |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=2.43 +/- 2.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.021389734 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.01        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 7.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 222      |
|    time_elapsed    | 12251    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=2.19 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.089882694 |
|    clip_fraction        | 0.421       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 2220        |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 489         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -241     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 223      |
|    time_elapsed    | 12309    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -140        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 224         |
|    time_elapsed         | 12359       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.038817476 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.04        |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.00502     |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=1.38 +/- 1.18
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.38      |
| time/                   |           |
|    total_timesteps      | 3675000   |
| train/                  |           |
|    approx_kl            | 0.0446309 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4        |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.769     |
|    n_updates            | 2240      |
|    policy_gradient_loss | 0.00522   |
|    value_loss           | 23.5      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -75.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 225      |
|    time_elapsed    | 12417    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=1.50 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.023182549 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.000914   |
|    value_loss           | 4.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.3     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 226      |
|    time_elapsed    | 12475    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 227         |
|    time_elapsed         | 12524       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.019144136 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.438       |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00432    |
|    value_loss           | 0.768       |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=2.11 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.011453857 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.78        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 46.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 228      |
|    time_elapsed    | 12583    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=2.57 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.57        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.018642845 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 20.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 229      |
|    time_elapsed    | 12641    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 230         |
|    time_elapsed         | 12690       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.014995301 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | -0.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=2.96 +/- 2.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.018850954 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 1.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 231      |
|    time_elapsed    | 12749    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=2.68 +/- 1.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.018038662 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 32.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 232      |
|    time_elapsed    | 12807    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 233         |
|    time_elapsed         | 12856       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.020030726 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.269       |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 9.34        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=2.96 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.013205471 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.72        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00651    |
|    value_loss           | 80.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 234      |
|    time_elapsed    | 12914    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2.83 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.017434385 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.433       |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 8.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 235      |
|    time_elapsed    | 12973    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -167        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 236         |
|    time_elapsed         | 13022       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.012530535 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 337         |
|    n_updates            | 2350        |
|    policy_gradient_loss | 0.000917    |
|    value_loss           | 674         |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=1.77 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.018546589 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 23.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 237      |
|    time_elapsed    | 13080    |
|    total_timesteps | 3883008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -207        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 238         |
|    time_elapsed         | 13129       |
|    total_timesteps      | 3899392     |
| train/                  |             |
|    approx_kl            | 0.009259503 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 525         |
-----------------------------------------
Eval num_timesteps=3900000, episode_reward=1.74 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.017437076 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 2.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 239      |
|    time_elapsed    | 13187    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=1.82 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.018448763 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.214       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 2.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 240      |
|    time_elapsed    | 13246    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.95       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 241         |
|    time_elapsed         | 13295       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.015042108 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.192       |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 2.93        |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=2.42 +/- 1.98
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.42       |
| time/                   |            |
|    total_timesteps      | 3950000    |
| train/                  |            |
|    approx_kl            | 0.01842529 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.89      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.00658   |
|    value_loss           | 12.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.31    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 242      |
|    time_elapsed    | 13353    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=2.35 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35        |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.018560307 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.495       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.32    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 243      |
|    time_elapsed    | 13412    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 244         |
|    time_elapsed         | 13461       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.021613497 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.401       |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.48        |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=2.96 +/- 2.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.013204473 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 2440        |
|    policy_gradient_loss | -7.34e-05   |
|    value_loss           | 241         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 245      |
|    time_elapsed    | 13519    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=1.24 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.015254547 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.94        |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 82.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 246      |
|    time_elapsed    | 13577    |
|    total_timesteps | 4030464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -25.9      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 247        |
|    time_elapsed         | 13626      |
|    total_timesteps      | 4046848    |
| train/                  |            |
|    approx_kl            | 0.02469214 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.08      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.48       |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.00784   |
|    value_loss           | 25.7       |
----------------------------------------
Eval num_timesteps=4050000, episode_reward=6.07 +/- 4.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6.07        |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.016189374 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 51.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 248      |
|    time_elapsed    | 13685    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=1.88 +/- 1.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 4075000     |
| train/                  |             |
|    approx_kl            | 0.017761856 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00047    |
|    value_loss           | 1.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 249      |
|    time_elapsed    | 13743    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 250         |
|    time_elapsed         | 13792       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.019947415 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.9        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 9.31        |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=1.59 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.59        |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.018647453 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 2.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.91    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 251      |
|    time_elapsed    | 13850    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=2.60 +/- 1.76
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.6        |
| time/                   |            |
|    total_timesteps      | 4125000    |
| train/                  |            |
|    approx_kl            | 0.01844123 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 2510       |
|    policy_gradient_loss | -0.00493   |
|    value_loss           | 5.49       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 252      |
|    time_elapsed    | 13909    |
|    total_timesteps | 4128768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -14.2      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 253        |
|    time_elapsed         | 13958      |
|    total_timesteps      | 4145152    |
| train/                  |            |
|    approx_kl            | 0.02285356 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.0003     |
|    loss                 | 608        |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.00144   |
|    value_loss           | 139        |
----------------------------------------
Eval num_timesteps=4150000, episode_reward=1.78 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.017434172 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 2.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 254      |
|    time_elapsed    | 14016    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2.74 +/- 1.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.022403453 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.2        |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 19.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 255      |
|    time_elapsed    | 14074    |
|    total_timesteps | 4177920  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 600      |
|    ep_rew_mean          | -81.5    |
| time/                   |          |
|    fps                  | 296      |
|    iterations           | 256      |
|    time_elapsed         | 14123    |
|    total_timesteps      | 4194304  |
| train/                  |          |
|    approx_kl            | 0.04981  |
|    clip_fraction        | 0.26     |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.22    |
|    explained_variance   | 0.00131  |
|    learning_rate        | 0.0003   |
|    loss                 | 13.3     |
|    n_updates            | 2550     |
|    policy_gradient_loss | 0.000391 |
|    value_loss           | 197      |
--------------------------------------
Eval num_timesteps=4200000, episode_reward=1.66 +/- 1.01
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.66       |
| time/                   |            |
|    total_timesteps      | 4200000    |
| train/                  |            |
|    approx_kl            | 0.01794539 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 39.6       |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.000528  |
|    value_loss           | 604        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -94      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 257      |
|    time_elapsed    | 14182    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2.36 +/- 1.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.020818198 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.99        |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 38.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 258      |
|    time_elapsed    | 14240    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -49.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 259         |
|    time_elapsed         | 14289       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.019562861 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 12.3        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=1.76 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.019852348 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.5        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 13.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 260      |
|    time_elapsed    | 14348    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=0.56 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.563       |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.013359256 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 142         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 261      |
|    time_elapsed    | 14406    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 262         |
|    time_elapsed         | 14455       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.019289738 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.451       |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 3.91        |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=1.19 +/- 1.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.19       |
| time/                   |            |
|    total_timesteps      | 4300000    |
| train/                  |            |
|    approx_kl            | 0.01856118 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.234      |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.00808   |
|    value_loss           | 1.03       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 263      |
|    time_elapsed    | 14513    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=0.74 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.744       |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.017007807 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.547       |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 0.482       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.06    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 264      |
|    time_elapsed    | 14572    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.75       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 265         |
|    time_elapsed         | 14621       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.016203996 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 11          |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=3.75 +/- 2.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.75        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.017111057 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.889       |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 19.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 266      |
|    time_elapsed    | 14679    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.6       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 267         |
|    time_elapsed         | 14728       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.020914217 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 86          |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 42.8        |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=1.87 +/- 1.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.013555856 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.1        |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 77.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 268      |
|    time_elapsed    | 14787    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=0.53 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.53        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.018809462 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 0.388       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 269      |
|    time_elapsed    | 14845    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 270         |
|    time_elapsed         | 14894       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.016356356 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0665      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=1.14 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.018496737 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0687      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 19          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.84    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 271      |
|    time_elapsed    | 14952    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=0.51 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.51        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.020153081 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 2.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.13    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 272      |
|    time_elapsed    | 15011    |
|    total_timesteps | 4456448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -49.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 273         |
|    time_elapsed         | 15060       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.016006494 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 11.2        |
-----------------------------------------
Eval num_timesteps=4475000, episode_reward=1.15 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.15        |
| time/                   |             |
|    total_timesteps      | 4475000     |
| train/                  |             |
|    approx_kl            | 0.020403601 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64        |
|    n_updates            | 2730        |
|    policy_gradient_loss | -5.04e-05   |
|    value_loss           | 129         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 274      |
|    time_elapsed    | 15118    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=3.37 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.37        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.020385487 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 0.248       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 275      |
|    time_elapsed    | 15177    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 276         |
|    time_elapsed         | 15226       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.016885884 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 1.09        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=1.65 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.017891966 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0386      |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 0.617       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.17    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 277      |
|    time_elapsed    | 15284    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=1.10 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.018800892 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0638      |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 0.824       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 278      |
|    time_elapsed    | 15342    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -78.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 279         |
|    time_elapsed         | 15392       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.027420789 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.7        |
|    n_updates            | 2780        |
|    policy_gradient_loss | 0.00394     |
|    value_loss           | 311         |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=2.60 +/- 2.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.018310081 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.47        |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00772    |
|    value_loss           | 0.556       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 280      |
|    time_elapsed    | 15450    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=0.85 +/- 0.87
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 0.847     |
| time/                   |           |
|    total_timesteps      | 4600000   |
| train/                  |           |
|    approx_kl            | 0.0085895 |
|    clip_fraction        | 0.0699    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.87     |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.0003    |
|    loss                 | 314       |
|    n_updates            | 2800      |
|    policy_gradient_loss | -0.00461  |
|    value_loss           | 195       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 281      |
|    time_elapsed    | 15508    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 282         |
|    time_elapsed         | 15557       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.028669117 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.613       |
|    n_updates            | 2810        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 100         |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=2.15 +/- 1.58
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.15       |
| time/                   |            |
|    total_timesteps      | 4625000    |
| train/                  |            |
|    approx_kl            | 0.01777691 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.622      |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.00692   |
|    value_loss           | 0.941      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 283      |
|    time_elapsed    | 15616    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=1.25 +/- 0.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.018776864 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 2.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.39    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 284      |
|    time_elapsed    | 15674    |
|    total_timesteps | 4653056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.38       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 285         |
|    time_elapsed         | 15723       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.010584585 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.2        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 32.5        |
-----------------------------------------
Eval num_timesteps=4675000, episode_reward=1.66 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.022034774 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0376      |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 286      |
|    time_elapsed    | 15782    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=3.39 +/- 2.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.39        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.017806083 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.14        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 2.39        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.44    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 287      |
|    time_elapsed    | 15840    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.37       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 288         |
|    time_elapsed         | 15889       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.017491134 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.028       |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 0.19        |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=1.56 +/- 1.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.56       |
| time/                   |            |
|    total_timesteps      | 4725000    |
| train/                  |            |
|    approx_kl            | 0.01880083 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0794     |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.00645   |
|    value_loss           | 0.537      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 289      |
|    time_elapsed    | 15947    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=1.04 +/- 0.54
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.04       |
| time/                   |            |
|    total_timesteps      | 4750000    |
| train/                  |            |
|    approx_kl            | 0.01663962 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0136     |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.00794   |
|    value_loss           | 0.0985     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.52    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 290      |
|    time_elapsed    | 16006    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.26       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 291         |
|    time_elapsed         | 16055       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.014774462 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 2.07        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=2.52 +/- 1.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.016964816 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 7.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 292      |
|    time_elapsed    | 16113    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=0.88 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.883       |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.022256237 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0755      |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 13.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.01    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 293      |
|    time_elapsed    | 16172    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -85.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 294         |
|    time_elapsed         | 16221       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.019138087 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 8.75        |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=1.87 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.010394169 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | 150         |
|    n_updates            | 2940        |
|    policy_gradient_loss | 0.000142    |
|    value_loss           | 721         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 295      |
|    time_elapsed    | 16279    |
|    total_timesteps | 4833280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -305         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 296          |
|    time_elapsed         | 16328        |
|    total_timesteps      | 4849664      |
| train/                  |              |
|    approx_kl            | 0.0071101612 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.709        |
|    learning_rate        | 0.0003       |
|    loss                 | 21.2         |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 524          |
------------------------------------------
Eval num_timesteps=4850000, episode_reward=3.74 +/- 2.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.74       |
| time/                   |            |
|    total_timesteps      | 4850000    |
| train/                  |            |
|    approx_kl            | 0.02551587 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.79      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.00882   |
|    value_loss           | 69.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -310     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 297      |
|    time_elapsed    | 16387    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=3.43 +/- 4.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.021944024 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 59.5        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.000669   |
|    value_loss           | 96.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 298      |
|    time_elapsed    | 16445    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -81.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 299         |
|    time_elapsed         | 16494       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.020669807 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 82.5        |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=3.44 +/- 1.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.44        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.024985017 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 214         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 300      |
|    time_elapsed    | 16553    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=1.51 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.019087348 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.35        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 8.39        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 301      |
|    time_elapsed    | 16611    |
|    total_timesteps | 4931584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -43.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 302         |
|    time_elapsed         | 16660       |
|    total_timesteps      | 4947968     |
| train/                  |             |
|    approx_kl            | 0.020525781 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 3010        |
|    policy_gradient_loss | -5.62e-05   |
|    value_loss           | 22.1        |
-----------------------------------------
Eval num_timesteps=4950000, episode_reward=3.47 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.47        |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.014638195 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 81.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 303      |
|    time_elapsed    | 16718    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=1.52 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.009594629 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 502         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 304      |
|    time_elapsed    | 16777    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -167        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 305         |
|    time_elapsed         | 16826       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.020671565 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.43        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 56.3        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=2.53 +/- 2.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.019144557 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 3050        |
|    policy_gradient_loss | 0.000944    |
|    value_loss           | 207         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 306      |
|    time_elapsed    | 16884    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=1.97 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.015214802 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.7        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 117         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 307      |
|    time_elapsed    | 16942    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -67         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 308         |
|    time_elapsed         | 16992       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.018011563 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 310         |
|    n_updates            | 3070        |
|    policy_gradient_loss | 0.000378    |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=2.67 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.67        |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.022285461 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.457       |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 5.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 309      |
|    time_elapsed    | 17050    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=3.70 +/- 3.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.7         |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.018240178 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.324       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 2.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 310      |
|    time_elapsed    | 17108    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.25       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 311         |
|    time_elapsed         | 17157       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.012635905 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.504       |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 4.52        |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=3.22 +/- 1.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.019126307 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 3.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.155    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 312      |
|    time_elapsed    | 17216    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=1.45 +/- 0.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.016709909 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 3.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 313      |
|    time_elapsed    | 17274    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 314         |
|    time_elapsed         | 17323       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.011194741 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 559         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=1.40 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.018374253 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00913    |
|    value_loss           | 0.758       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 315      |
|    time_elapsed    | 17382    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=1.89 +/- 0.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 5175000     |
| train/                  |             |
|    approx_kl            | 0.018497178 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 3150        |
|    policy_gradient_loss | 3.5e-05     |
|    value_loss           | 3.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 316      |
|    time_elapsed    | 17440    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 317         |
|    time_elapsed         | 17489       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.016760882 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 27.9        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=2.89 +/- 1.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.012976687 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.6        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.00062    |
|    value_loss           | 40.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 318      |
|    time_elapsed    | 17547    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=0.93 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.929       |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.020136787 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.513       |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 4.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 319      |
|    time_elapsed    | 17606    |
|    total_timesteps | 5226496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.12       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 320         |
|    time_elapsed         | 17655       |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.020503761 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.523       |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 8.03        |
-----------------------------------------
Eval num_timesteps=5250000, episode_reward=3.88 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.88        |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.023129538 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 12.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 321      |
|    time_elapsed    | 17713    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=3.47 +/- 2.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.47        |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.014375521 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.12        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 50.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 322      |
|    time_elapsed    | 17772    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 323         |
|    time_elapsed         | 17821       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.021948539 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0544      |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 0.725       |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=3.13 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.13        |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.012959035 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.312       |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 9.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.21    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 324      |
|    time_elapsed    | 17879    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.66       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 325         |
|    time_elapsed         | 17928       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.030093128 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.21        |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.000496   |
|    value_loss           | 5.14        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=1.83 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.022799747 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 3250        |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 6.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.78    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 326      |
|    time_elapsed    | 17986    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=1.47 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.021021744 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.242       |
|    n_updates            | 3260        |
|    policy_gradient_loss | 0.000297    |
|    value_loss           | 1.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.95    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 327      |
|    time_elapsed    | 18045    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.48       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 328         |
|    time_elapsed         | 18094       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.020872151 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0685      |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 0.411       |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=1.90 +/- 1.13
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.9        |
| time/                   |            |
|    total_timesteps      | 5375000    |
| train/                  |            |
|    approx_kl            | 0.01819844 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4         |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.00192   |
|    value_loss           | 7.09       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.39    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 329      |
|    time_elapsed    | 18152    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=2.37 +/- 1.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37        |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.024547298 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.27        |
|    n_updates            | 3290        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 7.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.25    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 330      |
|    time_elapsed    | 18211    |
|    total_timesteps | 5406720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.5      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 331        |
|    time_elapsed         | 18260      |
|    total_timesteps      | 5423104    |
| train/                  |            |
|    approx_kl            | 0.02914771 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.08      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 3300       |
|    policy_gradient_loss | 0.000244   |
|    value_loss           | 4.79       |
----------------------------------------
Eval num_timesteps=5425000, episode_reward=1.46 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.015349807 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 28.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 332      |
|    time_elapsed    | 18318    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2.04 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.020954154 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 1.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 333      |
|    time_elapsed    | 18376    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 334         |
|    time_elapsed         | 18426       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.020620432 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0252      |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 1.04        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=2.35 +/- 2.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35        |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.004153526 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.25        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 18.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 335      |
|    time_elapsed    | 18484    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=0.97 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.97        |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.021692004 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0854      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00987    |
|    value_loss           | 10.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 336      |
|    time_elapsed    | 18542    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.61       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 337         |
|    time_elapsed         | 18592       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.025891513 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0613      |
|    n_updates            | 3360        |
|    policy_gradient_loss | 0.00526     |
|    value_loss           | 4.94        |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=2.56 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 5525000     |
| train/                  |             |
|    approx_kl            | 0.022851676 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0892      |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 0.915       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.73    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 338      |
|    time_elapsed    | 18650    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=4.14 +/- 3.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.14       |
| time/                   |            |
|    total_timesteps      | 5550000    |
| train/                  |            |
|    approx_kl            | 0.01094474 |
|    clip_fraction        | 0.0612     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.73      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.362      |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.00167   |
|    value_loss           | 9.78       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.68    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 339      |
|    time_elapsed    | 18708    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.05       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 340         |
|    time_elapsed         | 18757       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.024351142 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.553       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 1.37        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=4.56 +/- 4.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.56        |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.026114453 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 1.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.94    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 341      |
|    time_elapsed    | 18816    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=3.85 +/- 2.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.85        |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.025974097 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.558       |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 1.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.81    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 342      |
|    time_elapsed    | 18874    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.61       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 343         |
|    time_elapsed         | 18923       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.010924172 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 23.4        |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=3.77 +/- 3.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.77        |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.027312042 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 3430        |
|    policy_gradient_loss | 0.00564     |
|    value_loss           | 0.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 344      |
|    time_elapsed    | 18981    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=2.31 +/- 1.25
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.31       |
| time/                   |            |
|    total_timesteps      | 5650000    |
| train/                  |            |
|    approx_kl            | 0.01986777 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0474     |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.00773   |
|    value_loss           | 21         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 345      |
|    time_elapsed    | 19040    |
|    total_timesteps | 5652480  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -36.3        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 346          |
|    time_elapsed         | 19089        |
|    total_timesteps      | 5668864      |
| train/                  |              |
|    approx_kl            | 0.0135988295 |
|    clip_fraction        | 0.161        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.691        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.74         |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00371     |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=5675000, episode_reward=0.55 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.551       |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.012851428 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 90.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -69.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 347      |
|    time_elapsed    | 19147    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=2.03 +/- 1.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.014045941 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.79        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 44.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 348      |
|    time_elapsed    | 19206    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -128        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 349         |
|    time_elapsed         | 19255       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.022322427 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.2        |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 110         |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=2.75 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.75        |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.014588568 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 310         |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 436         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -117     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 350      |
|    time_elapsed    | 19313    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2.03 +/- 2.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.022106096 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.802       |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 3.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 351      |
|    time_elapsed    | 19371    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -96.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 352         |
|    time_elapsed         | 19421       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.017083999 |
|    clip_fraction        | 0.0566      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.41        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=3.74 +/- 2.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.74        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.020126248 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.624       |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 9.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 353      |
|    time_elapsed    | 19479    |
|    total_timesteps | 5783552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -90.3      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 354        |
|    time_elapsed         | 19528      |
|    total_timesteps      | 5799936    |
| train/                  |            |
|    approx_kl            | 0.02954898 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.256      |
|    n_updates            | 3530       |
|    policy_gradient_loss | -0.00379   |
|    value_loss           | 2.24       |
----------------------------------------
Eval num_timesteps=5800000, episode_reward=1.75 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.033000555 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 3540        |
|    policy_gradient_loss | 0.00632     |
|    value_loss           | 42.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 355      |
|    time_elapsed    | 19587    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=1.64 +/- 0.64
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.64      |
| time/                   |           |
|    total_timesteps      | 5825000   |
| train/                  |           |
|    approx_kl            | 0.0115653 |
|    clip_fraction        | 0.0745    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.67     |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.11      |
|    n_updates            | 3550      |
|    policy_gradient_loss | -0.00411  |
|    value_loss           | 72.2      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 356      |
|    time_elapsed    | 19645    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -64.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 357         |
|    time_elapsed         | 19694       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.017224804 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.268       |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 1.92        |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=2.38 +/- 2.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.017118365 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.786       |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00917    |
|    value_loss           | 5.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -57.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 358      |
|    time_elapsed    | 19752    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=1.41 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.019176627 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.000297   |
|    value_loss           | 50.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 359      |
|    time_elapsed    | 19811    |
|    total_timesteps | 5881856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 360         |
|    time_elapsed         | 19860       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.023399107 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.03        |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00829    |
|    value_loss           | 169         |
-----------------------------------------
Eval num_timesteps=5900000, episode_reward=2.94 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.024742909 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.343       |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 23.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -35.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 361      |
|    time_elapsed    | 19918    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=1.57 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.021799278 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.288       |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 7.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -37.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 362      |
|    time_elapsed    | 19976    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 363         |
|    time_elapsed         | 20026       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.020946544 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 4.48        |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=1.03 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.03        |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.019255122 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.388       |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 17.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 364      |
|    time_elapsed    | 20084    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=1.69 +/- 1.23
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 5975000    |
| train/                  |            |
|    approx_kl            | 0.01641361 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.87      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.135      |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.00208   |
|    value_loss           | 3.31       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.16    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 365      |
|    time_elapsed    | 20143    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 366         |
|    time_elapsed         | 20192       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.032962933 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 4.09        |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=1.40 +/- 0.82
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 6000000    |
| train/                  |            |
|    approx_kl            | 0.01707168 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.96      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.19       |
|    n_updates            | 3660       |
|    policy_gradient_loss | -3.65e-05  |
|    value_loss           | 287        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 367      |
|    time_elapsed    | 20250    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=1.81 +/- 1.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.015959784 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.217       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 9.42        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 368      |
|    time_elapsed    | 20309    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -193        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 369         |
|    time_elapsed         | 20358       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.012077168 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.29        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 198         |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=3.45 +/- 4.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.45        |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.022040155 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 327         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 370      |
|    time_elapsed    | 20416    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=3.23 +/- 2.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.23        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.018325105 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.37        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 371      |
|    time_elapsed    | 20474    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -473        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 372         |
|    time_elapsed         | 20523       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.007685528 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 365         |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 927         |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=1.95 +/- 2.02
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.95       |
| time/                   |            |
|    total_timesteps      | 6100000    |
| train/                  |            |
|    approx_kl            | 0.02519326 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 127        |
|    n_updates            | 3720       |
|    policy_gradient_loss | 0.000659   |
|    value_loss           | 469        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -459     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 373      |
|    time_elapsed    | 20582    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=1.99 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.016340649 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 422         |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 314         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -312     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 374      |
|    time_elapsed    | 20640    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -117        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 375         |
|    time_elapsed         | 20689       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.018640745 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.65        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 24.1        |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=3.07 +/- 1.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.023523416 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.2        |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 21.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 376      |
|    time_elapsed    | 20748    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=2.04 +/- 1.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.020204242 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.75        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 12.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 377      |
|    time_elapsed    | 20806    |
|    total_timesteps | 6176768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -15        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 378        |
|    time_elapsed         | 20855      |
|    total_timesteps      | 6193152    |
| train/                  |            |
|    approx_kl            | 0.01742977 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.51       |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.00294   |
|    value_loss           | 6.36       |
----------------------------------------
Eval num_timesteps=6200000, episode_reward=2.09 +/- 1.42
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.09       |
| time/                   |            |
|    total_timesteps      | 6200000    |
| train/                  |            |
|    approx_kl            | 0.01953519 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.92      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.78       |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.00257   |
|    value_loss           | 15         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 379      |
|    time_elapsed    | 20914    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=0.95 +/- 0.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.951       |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.020007761 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.06        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 28.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 380      |
|    time_elapsed    | 20972    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 381         |
|    time_elapsed         | 21021       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.009336226 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 53.3        |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=2.39 +/- 1.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.39       |
| time/                   |            |
|    total_timesteps      | 6250000    |
| train/                  |            |
|    approx_kl            | 0.01707909 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.83      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.569      |
|    n_updates            | 3810       |
|    policy_gradient_loss | -0.00646   |
|    value_loss           | 6.26       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 382      |
|    time_elapsed    | 21079    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=1.83 +/- 1.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.017081305 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.486       |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 8.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 383      |
|    time_elapsed    | 21138    |
|    total_timesteps | 6275072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 384         |
|    time_elapsed         | 21187       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.017912205 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.675       |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=6300000, episode_reward=2.19 +/- 1.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.015818996 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.612       |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 9.87        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 385      |
|    time_elapsed    | 21245    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 386         |
|    time_elapsed         | 21295       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.022505172 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.74        |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 1.51        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=2.43 +/- 1.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.014863707 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 4.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.45    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 387      |
|    time_elapsed    | 21353    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=3.13 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.13        |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.018007003 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.168       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 0.893       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.89    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 388      |
|    time_elapsed    | 21411    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.01       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 389         |
|    time_elapsed         | 21460       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.026631609 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.000424   |
|    value_loss           | 31.2        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=1.73 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.015511772 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.363       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 0.949       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.52    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 390      |
|    time_elapsed    | 21519    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=1.82 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.016646445 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 5.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.87    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 391      |
|    time_elapsed    | 21577    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.99       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 392         |
|    time_elapsed         | 21626       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.021139946 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.268       |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 0.576       |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=1.91 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.015319023 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.958       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.81    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 393      |
|    time_elapsed    | 21685    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=1.03 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.03        |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.019203158 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.38        |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 3.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.2     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 394      |
|    time_elapsed    | 21743    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5          |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 395         |
|    time_elapsed         | 21792       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.018594496 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.05        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 3.31        |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=1.08 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.08        |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.015544525 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.449       |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 1.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.86    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 396      |
|    time_elapsed    | 21851    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2.72 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.014949683 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 0.581       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.53    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 397      |
|    time_elapsed    | 21909    |
|    total_timesteps | 6504448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.29       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 398         |
|    time_elapsed         | 21958       |
|    total_timesteps      | 6520832     |
| train/                  |             |
|    approx_kl            | 0.018007137 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.459       |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 10.1        |
-----------------------------------------
Eval num_timesteps=6525000, episode_reward=0.89 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.892       |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.017305579 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.91        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 0.882       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.24    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 399      |
|    time_elapsed    | 22017    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=2.29 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.021091219 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0408      |
|    n_updates            | 3990        |
|    policy_gradient_loss | 0.000847    |
|    value_loss           | 0.332       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.79    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 400      |
|    time_elapsed    | 22075    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.97       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 401         |
|    time_elapsed         | 22124       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.017905368 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.555       |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 0.479       |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=1.28 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.018083207 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 7.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 402      |
|    time_elapsed    | 22182    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=1.00 +/- 0.92
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.010083952 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.67        |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 62.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 403      |
|    time_elapsed    | 22241    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 404         |
|    time_elapsed         | 22291       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.019753616 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 6.45        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=1.71 +/- 1.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.016059449 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.445       |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 1.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 405      |
|    time_elapsed    | 22349    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=1.46 +/- 0.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.46       |
| time/                   |            |
|    total_timesteps      | 6650000    |
| train/                  |            |
|    approx_kl            | 0.02275209 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.144      |
|    n_updates            | 4050       |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 11.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.64    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 406      |
|    time_elapsed    | 22407    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 407         |
|    time_elapsed         | 22457       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.015750617 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.333       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 7.15        |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=2.21 +/- 0.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.21       |
| time/                   |            |
|    total_timesteps      | 6675000    |
| train/                  |            |
|    approx_kl            | 0.01960092 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.05       |
|    n_updates            | 4070       |
|    policy_gradient_loss | -0.00459   |
|    value_loss           | 27.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 408      |
|    time_elapsed    | 22515    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=1.76 +/- 1.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.76       |
| time/                   |            |
|    total_timesteps      | 6700000    |
| train/                  |            |
|    approx_kl            | 0.01623676 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.00483   |
|    value_loss           | 5.82       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -31.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 409      |
|    time_elapsed    | 22573    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -35.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 410         |
|    time_elapsed         | 22622       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.022360966 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 102         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=0.87 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.871       |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.013990912 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.1         |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 411      |
|    time_elapsed    | 22681    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2.11 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.027983919 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.543       |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00835    |
|    value_loss           | 2.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 412      |
|    time_elapsed    | 22739    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -136        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 413         |
|    time_elapsed         | 22788       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.025622115 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 4120        |
|    policy_gradient_loss | 0.00143     |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=2.91 +/- 0.55
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.91       |
| time/                   |            |
|    total_timesteps      | 6775000    |
| train/                  |            |
|    approx_kl            | 0.01884348 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.809      |
|    n_updates            | 4130       |
|    policy_gradient_loss | 0.00199    |
|    value_loss           | 251        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 414      |
|    time_elapsed    | 22847    |
|    total_timesteps | 6782976  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -78.3      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 415        |
|    time_elapsed         | 22896      |
|    total_timesteps      | 6799360    |
| train/                  |            |
|    approx_kl            | 0.01861327 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.172      |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.00501   |
|    value_loss           | 1.52       |
----------------------------------------
Eval num_timesteps=6800000, episode_reward=2.94 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.016949605 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.291       |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 0.884       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 416      |
|    time_elapsed    | 22954    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=3.22 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.013492014 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.676       |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 2.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 417      |
|    time_elapsed    | 23012    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -128        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 418         |
|    time_elapsed         | 23062       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.026362645 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.9        |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 860         |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=2.10 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.020029142 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.426       |
|    n_updates            | 4180        |
|    policy_gradient_loss | 0.00085     |
|    value_loss           | 30.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 419      |
|    time_elapsed    | 23120    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=1.01 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.01        |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.021706376 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.399       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 8.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 420      |
|    time_elapsed    | 23178    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.97       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 421         |
|    time_elapsed         | 23227       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.016351212 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 10.4        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=2.74 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.019219818 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 0.924       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.46    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 422      |
|    time_elapsed    | 23286    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=0.90 +/- 0.77
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.898      |
| time/                   |            |
|    total_timesteps      | 6925000    |
| train/                  |            |
|    approx_kl            | 0.01973699 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0741     |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.00914   |
|    value_loss           | 0.448      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.18    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 423      |
|    time_elapsed    | 23344    |
|    total_timesteps | 6930432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.879      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 424         |
|    time_elapsed         | 23394       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.016061272 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.533       |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 1.45        |
-----------------------------------------
Eval num_timesteps=6950000, episode_reward=1.69 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.69        |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.019707208 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.407       |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.000413   |
|    value_loss           | 0.521       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.42    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 425      |
|    time_elapsed    | 23452    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=0.46 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.462       |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.007594227 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.381       |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 19          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 426      |
|    time_elapsed    | 23510    |
|    total_timesteps | 6979584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.5       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 427        |
|    time_elapsed         | 23560      |
|    total_timesteps      | 6995968    |
| train/                  |            |
|    approx_kl            | 0.02265855 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.00404   |
|    value_loss           | 9.16       |
----------------------------------------
Eval num_timesteps=7000000, episode_reward=3.54 +/- 2.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.54        |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.019658234 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.969       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 22.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 428      |
|    time_elapsed    | 23618    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=2.94 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.019376587 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.27        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00513    |
|    value_loss           | 16.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 429      |
|    time_elapsed    | 23676    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 430         |
|    time_elapsed         | 23726       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.017014042 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 4.83        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=1.78 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.019215012 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.779       |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 3.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 431      |
|    time_elapsed    | 23784    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=2.49 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.024579737 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 152         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 432      |
|    time_elapsed    | 23842    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -25.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 433         |
|    time_elapsed         | 23891       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.016654287 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 30.8        |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=1.97 +/- 1.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.017517995 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.282       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 4.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 434      |
|    time_elapsed    | 23950    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=2.85 +/- 0.46
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.85       |
| time/                   |            |
|    total_timesteps      | 7125000    |
| train/                  |            |
|    approx_kl            | 0.01080171 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.7       |
|    explained_variance   | 0.0857     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.85       |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.00364   |
|    value_loss           | 383        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 435      |
|    time_elapsed    | 24008    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -132        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 436         |
|    time_elapsed         | 24057       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.023734376 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 69.6        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=1.65 +/- 1.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.017207641 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.8        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 35.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 437      |
|    time_elapsed    | 24116    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=-1177.94 +/- 2360.98
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.18e+03  |
| time/                   |            |
|    total_timesteps      | 7175000    |
| train/                  |            |
|    approx_kl            | 0.01827507 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.8       |
|    n_updates            | 4370       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 10.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 438      |
|    time_elapsed    | 24174    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.41       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 439         |
|    time_elapsed         | 24223       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.015168582 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.643       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 3.69        |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=2.53 +/- 1.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.53       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.01858221 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.03       |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 12.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 440      |
|    time_elapsed    | 24281    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=1.43 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.016441222 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 44.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 441      |
|    time_elapsed    | 24340    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 442         |
|    time_elapsed         | 24389       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.016546443 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.623       |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0076     |
|    value_loss           | 2.95        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=3.04 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.04        |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.019210383 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 30.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 443      |
|    time_elapsed    | 24447    |
|    total_timesteps | 7258112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.17       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 444         |
|    time_elapsed         | 24497       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.016693741 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 8.81        |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 19.6        |
-----------------------------------------
Eval num_timesteps=7275000, episode_reward=2.81 +/- 0.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.81       |
| time/                   |            |
|    total_timesteps      | 7275000    |
| train/                  |            |
|    approx_kl            | 0.01776791 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.482      |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.00312   |
|    value_loss           | 3.49       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 445      |
|    time_elapsed    | 24555    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=2.18 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.017753722 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.5         |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 6.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 446      |
|    time_elapsed    | 24613    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 447         |
|    time_elapsed         | 24662       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.021560214 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 48.3        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=2.90 +/- 1.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 7325000     |
| train/                  |             |
|    approx_kl            | 0.019030081 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.642       |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 41          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 448      |
|    time_elapsed    | 24721    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=2.33 +/- 0.91
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.33       |
| time/                   |            |
|    total_timesteps      | 7350000    |
| train/                  |            |
|    approx_kl            | 0.02630267 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.115      |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.000915  |
|    value_loss           | 7.66       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 449      |
|    time_elapsed    | 24779    |
|    total_timesteps | 7356416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -9.34      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 450        |
|    time_elapsed         | 24828      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.01707827 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.32       |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.00505   |
|    value_loss           | 5.06       |
----------------------------------------
Eval num_timesteps=7375000, episode_reward=2.23 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.016184604 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.86        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 4.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 451      |
|    time_elapsed    | 24887    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=1.74 +/- 1.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 7400000    |
| train/                  |            |
|    approx_kl            | 0.01683497 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.189      |
|    n_updates            | 4510       |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 0.954      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.94    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 452      |
|    time_elapsed    | 24945    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.33       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 453         |
|    time_elapsed         | 24994       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.017881516 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00953    |
|    value_loss           | 1.42        |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=2.95 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.95        |
| time/                   |             |
|    total_timesteps      | 7425000     |
| train/                  |             |
|    approx_kl            | 0.016089786 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.68        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 6.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 454      |
|    time_elapsed    | 25053    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=3.22 +/- 0.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.019309739 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 1.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.01    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 455      |
|    time_elapsed    | 25111    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.59       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 456         |
|    time_elapsed         | 25160       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.018405918 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 1.69        |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=2.09 +/- 0.70
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.09       |
| time/                   |            |
|    total_timesteps      | 7475000    |
| train/                  |            |
|    approx_kl            | 0.01639483 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.861      |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.00304   |
|    value_loss           | 14.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.97    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 457      |
|    time_elapsed    | 25219    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=1.75 +/- 1.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.75       |
| time/                   |            |
|    total_timesteps      | 7500000    |
| train/                  |            |
|    approx_kl            | 0.02239599 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4         |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.55       |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.00474   |
|    value_loss           | 17.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 458      |
|    time_elapsed    | 25277    |
|    total_timesteps | 7503872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -14.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 459         |
|    time_elapsed         | 25326       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.016086578 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3           |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 27.3        |
-----------------------------------------
Eval num_timesteps=7525000, episode_reward=1.89 +/- 1.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 7525000     |
| train/                  |             |
|    approx_kl            | 0.017376177 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 14.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 460      |
|    time_elapsed    | 25385    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=2.39 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.39        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.015936691 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 21.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 461      |
|    time_elapsed    | 25443    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.94       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 462         |
|    time_elapsed         | 25492       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.022501478 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 7.09        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=2.94 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.94       |
| time/                   |            |
|    total_timesteps      | 7575000    |
| train/                  |            |
|    approx_kl            | 0.01934881 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0509     |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.00976   |
|    value_loss           | 0.364      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.81    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 463      |
|    time_elapsed    | 25551    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=3.04 +/- 0.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.04       |
| time/                   |            |
|    total_timesteps      | 7600000    |
| train/                  |            |
|    approx_kl            | 0.01432306 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.366      |
|    n_updates            | 4630       |
|    policy_gradient_loss | -0.00366   |
|    value_loss           | 1.61       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 464      |
|    time_elapsed    | 25609    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.95       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 465         |
|    time_elapsed         | 25658       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.017886542 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.525       |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 21.9        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=2.18 +/- 1.13
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.18      |
| time/                   |           |
|    total_timesteps      | 7625000   |
| train/                  |           |
|    approx_kl            | 0.0189226 |
|    clip_fraction        | 0.271     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.31     |
|    explained_variance   | 0.885     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.14      |
|    n_updates            | 4650      |
|    policy_gradient_loss | -0.00621  |
|    value_loss           | 2.09      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 466      |
|    time_elapsed    | 25716    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=2.33 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.013715791 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.46        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 32          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 467      |
|    time_elapsed    | 25775    |
|    total_timesteps | 7651328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -12.7      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 468        |
|    time_elapsed         | 25824      |
|    total_timesteps      | 7667712    |
| train/                  |            |
|    approx_kl            | 0.01993687 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.97      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.923      |
|    n_updates            | 4670       |
|    policy_gradient_loss | -0.00121   |
|    value_loss           | 2.14       |
----------------------------------------
Eval num_timesteps=7675000, episode_reward=2.78 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 7675000     |
| train/                  |             |
|    approx_kl            | 0.017020762 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.259       |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 26.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 469      |
|    time_elapsed    | 25882    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=1.39 +/- 1.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.39        |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.018524878 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.53        |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 35.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.37    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 470      |
|    time_elapsed    | 25941    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.65       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 471         |
|    time_elapsed         | 25990       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.020283997 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0176      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 1.19        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=2.91 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91        |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.012250364 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.286       |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 3.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 472      |
|    time_elapsed    | 26048    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.35       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 473         |
|    time_elapsed         | 26097       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.023864431 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0765      |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 9.93        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=2.04 +/- 1.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 7750000     |
| train/                  |             |
|    approx_kl            | 0.018506505 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.231       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 4.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.86    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 474      |
|    time_elapsed    | 26156    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=2.65 +/- 1.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.65       |
| time/                   |            |
|    total_timesteps      | 7775000    |
| train/                  |            |
|    approx_kl            | 0.01095075 |
|    clip_fraction        | 0.0967     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.61       |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.00357   |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 475      |
|    time_elapsed    | 26214    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 476         |
|    time_elapsed         | 26263       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.013252603 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.677       |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 40.5        |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=0.76 +/- 5.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.755      |
| time/                   |            |
|    total_timesteps      | 7800000    |
| train/                  |            |
|    approx_kl            | 0.02106286 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.1       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.00545   |
|    value_loss           | 6.51       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 477      |
|    time_elapsed    | 26322    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=2.84 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.84        |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.021427767 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 4.75        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.16    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 478      |
|    time_elapsed    | 26380    |
|    total_timesteps | 7831552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.58      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 479        |
|    time_elapsed         | 26429      |
|    total_timesteps      | 7847936    |
| train/                  |            |
|    approx_kl            | 0.01213591 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.72      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.059      |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.00358   |
|    value_loss           | 1.99       |
----------------------------------------
Eval num_timesteps=7850000, episode_reward=3.32 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.32        |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.022673424 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.318       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 19          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 480      |
|    time_elapsed    | 26487    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=3.29 +/- 0.66
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 3.29      |
| time/                   |           |
|    total_timesteps      | 7875000   |
| train/                  |           |
|    approx_kl            | 0.0190796 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.02     |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.162     |
|    n_updates            | 4800      |
|    policy_gradient_loss | -0.00357  |
|    value_loss           | 20.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.41    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 481      |
|    time_elapsed    | 26546    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.77       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 482         |
|    time_elapsed         | 26595       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.021658193 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 4.76        |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=2.56 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.019829135 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.452       |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 1.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.24    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 483      |
|    time_elapsed    | 26653    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=1.11 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.11        |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.016842838 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.886       |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.46    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 484      |
|    time_elapsed    | 26712    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.9        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 485         |
|    time_elapsed         | 26761       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.021008462 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 3.81        |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=2.42 +/- 1.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.020307573 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 3.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.44    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 486      |
|    time_elapsed    | 26819    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=2.10 +/- 1.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 7975000     |
| train/                  |             |
|    approx_kl            | 0.018223144 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0604      |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 0.438       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 487      |
|    time_elapsed    | 26877    |
|    total_timesteps | 7979008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 488         |
|    time_elapsed         | 26927       |
|    total_timesteps      | 7995392     |
| train/                  |             |
|    approx_kl            | 0.011617459 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 63.9        |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2.00 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.017791297 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0932      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 18.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 489      |
|    time_elapsed    | 26985    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=2.86 +/- 0.32
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.86       |
| time/                   |            |
|    total_timesteps      | 8025000    |
| train/                  |            |
|    approx_kl            | 0.01788962 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.92      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.14       |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.00529   |
|    value_loss           | 11.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 490      |
|    time_elapsed    | 27043    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 491         |
|    time_elapsed         | 27092       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.019287553 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.489       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 4.51        |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=1.36 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.017641714 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.332       |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 4.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.98    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 492      |
|    time_elapsed    | 27151    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=3.01 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.01        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.017026063 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.278       |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 9.39        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 493      |
|    time_elapsed    | 27209    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -50.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 494         |
|    time_elapsed         | 27258       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.013482452 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.0003      |
|    loss                 | 223         |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=3.16 +/- 0.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.16       |
| time/                   |            |
|    total_timesteps      | 8100000    |
| train/                  |            |
|    approx_kl            | 0.01835554 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 39.1       |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.00552   |
|    value_loss           | 16.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 495      |
|    time_elapsed    | 27317    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=3.05 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.017422292 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 30.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 496      |
|    time_elapsed    | 27375    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -21.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 497         |
|    time_elapsed         | 27424       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.018765146 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 5.18        |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=3.24 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.24        |
| time/                   |             |
|    total_timesteps      | 8150000     |
| train/                  |             |
|    approx_kl            | 0.016128227 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.55        |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 22.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 498      |
|    time_elapsed    | 27483    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=1.75 +/- 1.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.015539011 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 2.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 499      |
|    time_elapsed    | 27541    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 500         |
|    time_elapsed         | 27590       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.023569727 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.822       |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 3.45        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=2.22 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.015071759 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 10          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 501      |
|    time_elapsed    | 27649    |
|    total_timesteps | 8208384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 502         |
|    time_elapsed         | 27698       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.016968353 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.745       |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 18.5        |
-----------------------------------------
Eval num_timesteps=8225000, episode_reward=2.05 +/- 1.71
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.05       |
| time/                   |            |
|    total_timesteps      | 8225000    |
| train/                  |            |
|    approx_kl            | 0.01750897 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.89      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.711      |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.000675  |
|    value_loss           | 4.82       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.91    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 503      |
|    time_elapsed    | 27756    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=1.67 +/- 1.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.67       |
| time/                   |            |
|    total_timesteps      | 8250000    |
| train/                  |            |
|    approx_kl            | 0.01795729 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.138      |
|    n_updates            | 5030       |
|    policy_gradient_loss | -0.00337   |
|    value_loss           | 1.73       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 504      |
|    time_elapsed    | 27814    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.55       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 505         |
|    time_elapsed         | 27864       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.016957339 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.642       |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 7.75        |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=2.91 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91        |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.016365418 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.311       |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 1.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.37    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 506      |
|    time_elapsed    | 27922    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=2.08 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.019356914 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.513       |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 3.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.97    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 507      |
|    time_elapsed    | 27980    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.24       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 508         |
|    time_elapsed         | 28029       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.017896846 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=2.53 +/- 1.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.53       |
| time/                   |            |
|    total_timesteps      | 8325000    |
| train/                  |            |
|    approx_kl            | 0.01792665 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.96      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.3        |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.00216   |
|    value_loss           | 42.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.29    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 509      |
|    time_elapsed    | 28088    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=2.66 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.019797402 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.964       |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 1.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.97    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 510      |
|    time_elapsed    | 28146    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.66       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 511         |
|    time_elapsed         | 28195       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.014913565 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.252       |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 4.14        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=2.59 +/- 1.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59        |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.019828191 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.85        |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 7.15        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.76    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 512      |
|    time_elapsed    | 28254    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=3.82 +/- 3.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.82        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.017493729 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.428       |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 1.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.45    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 513      |
|    time_elapsed    | 28312    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.89       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 514         |
|    time_elapsed         | 28361       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.018069133 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.74        |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 21.6        |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=2.02 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.019686412 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.229       |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 8.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.07    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 515      |
|    time_elapsed    | 28419    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=2.03 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.019360472 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0699      |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 0.307       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 516      |
|    time_elapsed    | 28478    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.91       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 517         |
|    time_elapsed         | 28527       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.013481123 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00742    |
|    value_loss           | 5.79        |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=2.62 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.018611126 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.44        |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 16.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 518      |
|    time_elapsed    | 28585    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=2.28 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.28        |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.017682446 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.391       |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 4.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 519      |
|    time_elapsed    | 28644    |
|    total_timesteps | 8503296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 520         |
|    time_elapsed         | 28693       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.016236816 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16        |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 17.4        |
-----------------------------------------
Eval num_timesteps=8525000, episode_reward=2.19 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.19        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.017988939 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 3.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.22    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 521      |
|    time_elapsed    | 28751    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=2.78 +/- 1.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.017660476 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.06        |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 1.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 522      |
|    time_elapsed    | 28810    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 523         |
|    time_elapsed         | 28859       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.020677287 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.185       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 44.5        |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=2.39 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.39       |
| time/                   |            |
|    total_timesteps      | 8575000    |
| train/                  |            |
|    approx_kl            | 0.01762681 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.373      |
|    n_updates            | 5230       |
|    policy_gradient_loss | -0.00695   |
|    value_loss           | 5.86       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 524      |
|    time_elapsed    | 28917    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=2.53 +/- 0.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.53       |
| time/                   |            |
|    total_timesteps      | 8600000    |
| train/                  |            |
|    approx_kl            | 0.01884017 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.43       |
|    n_updates            | 5240       |
|    policy_gradient_loss | 0.00412    |
|    value_loss           | 344        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 525      |
|    time_elapsed    | 28975    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -82         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 526         |
|    time_elapsed         | 29024       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.021621678 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0139      |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 0.757       |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=2.72 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.021491056 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.828       |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00809    |
|    value_loss           | 2.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 527      |
|    time_elapsed    | 29083    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=2.62 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.007256871 |
|    clip_fraction        | 0.0476      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 121         |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 169         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 528      |
|    time_elapsed    | 29141    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -87.4      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 529        |
|    time_elapsed         | 29190      |
|    total_timesteps      | 8667136    |
| train/                  |            |
|    approx_kl            | 0.02142529 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.92      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.112      |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0086    |
|    value_loss           | 1.04       |
----------------------------------------
Eval num_timesteps=8675000, episode_reward=2.12 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.017545596 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.206       |
|    n_updates            | 5290        |
|    policy_gradient_loss | 0.000207    |
|    value_loss           | 3.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -87.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 530      |
|    time_elapsed    | 29248    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.16       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 531         |
|    time_elapsed         | 29298       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.013584676 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 4.13        |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=5.68 +/- 6.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.68        |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.019063964 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.35        |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 1.85        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.46    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 532      |
|    time_elapsed    | 29356    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=2.46 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.46       |
| time/                   |            |
|    total_timesteps      | 8725000    |
| train/                  |            |
|    approx_kl            | 0.01721517 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.00697   |
|    value_loss           | 0.856      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 533      |
|    time_elapsed    | 29414    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.31       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 534         |
|    time_elapsed         | 29463       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.016863964 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0483      |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 2.95        |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=2.32 +/- 1.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.021765675 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0968      |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 0.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.19    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 535      |
|    time_elapsed    | 29522    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=2.58 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.011732469 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 2.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.68    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 536      |
|    time_elapsed    | 29580    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.71       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 537         |
|    time_elapsed         | 29629       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.016621204 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 1.81        |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=2.51 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.51        |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.012086457 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.632       |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 8.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.17    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 538      |
|    time_elapsed    | 29687    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=2.74 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 8825000     |
| train/                  |             |
|    approx_kl            | 0.018514879 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 0.459       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.393   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 539      |
|    time_elapsed    | 29746    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.75       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 540         |
|    time_elapsed         | 29795       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.014430206 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 1.56        |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=2.54 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.020177718 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.53        |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 4.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 541      |
|    time_elapsed    | 29853    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=2.44 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.018788878 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 7.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.07    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 542      |
|    time_elapsed    | 29912    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.48       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 543         |
|    time_elapsed         | 29961       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.016816542 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000147    |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 0.113       |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=2.11 +/- 1.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 8900000     |
| train/                  |             |
|    approx_kl            | 0.012190048 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.178       |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 2.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 544      |
|    time_elapsed    | 30019    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=2.87 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.87        |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.019350909 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0764      |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.374       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.1     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 545      |
|    time_elapsed    | 30077    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.78       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 546         |
|    time_elapsed         | 30127       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.016425364 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 0.42        |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=2.29 +/- 1.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.017243877 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 6.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.38    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 547      |
|    time_elapsed    | 30185    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=2.44 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.018372107 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.06        |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 0.881       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.748   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 548      |
|    time_elapsed    | 30243    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.24       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 549         |
|    time_elapsed         | 30292       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.017743759 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.397       |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.585       |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2.66 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.014982277 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0521      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 6.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 550      |
|    time_elapsed    | 30351    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=2.64 +/- 2.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.017858421 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.245       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 25          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.07    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 551      |
|    time_elapsed    | 30409    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.91       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 552         |
|    time_elapsed         | 30458       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.020943867 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0689      |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 10.8        |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=2.33 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.019483158 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0495      |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 0.695       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.22    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 553      |
|    time_elapsed    | 30517    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=2.70 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.017865751 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0634      |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 0.952       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.1     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 554      |
|    time_elapsed    | 30575    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 555         |
|    time_elapsed         | 30624       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.016751768 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0347      |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 0.54        |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=2.61 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.61         |
| time/                   |              |
|    total_timesteps      | 9100000      |
| train/                  |              |
|    approx_kl            | 0.0056443606 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.67        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.3         |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 270          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 556      |
|    time_elapsed    | 30682    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=2.62 +/- 1.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.025832256 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 1.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 557      |
|    time_elapsed    | 30741    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 558         |
|    time_elapsed         | 30790       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.014299949 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 62.1        |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=3.23 +/- 2.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.23        |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.010244334 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.2        |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.000734   |
|    value_loss           | 269         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 559      |
|    time_elapsed    | 30848    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=3.05 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.018622436 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 1.93        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 560      |
|    time_elapsed    | 30906    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -32.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 561         |
|    time_elapsed         | 30956       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.016800703 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.597       |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 1.31        |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=2.89 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.014453409 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 966         |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 270         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 562      |
|    time_elapsed    | 31014    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 563         |
|    time_elapsed         | 31063       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.019861873 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.07        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.000852   |
|    value_loss           | 5.05        |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=2.86 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.018013176 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03        |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 3.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.73    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 564      |
|    time_elapsed    | 31121    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=2.76 +/- 0.38
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.76      |
| time/                   |           |
|    total_timesteps      | 9250000   |
| train/                  |           |
|    approx_kl            | 0.0162134 |
|    clip_fraction        | 0.189     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.72     |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.357     |
|    n_updates            | 5640      |
|    policy_gradient_loss | -0.00459  |
|    value_loss           | 6         |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.18    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 565      |
|    time_elapsed    | 31180    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.78       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 566         |
|    time_elapsed         | 31229       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.019364495 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.278       |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.000292   |
|    value_loss           | 8.06        |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=3.28 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.28        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.020124093 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00049    |
|    value_loss           | 11.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.33    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 567      |
|    time_elapsed    | 31287    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=2.13 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.014701694 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.36        |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 20          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.66    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 568      |
|    time_elapsed    | 31346    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 569         |
|    time_elapsed         | 31395       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.016945992 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 44.1        |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=2.42 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.016762001 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.398       |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 8.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 570      |
|    time_elapsed    | 31453    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=1.49 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.016684437 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.73        |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 4.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 571      |
|    time_elapsed    | 31511    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 572         |
|    time_elapsed         | 31561       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.019017627 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.89        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 42          |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=1.60 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.018963931 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.89        |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 1.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 573      |
|    time_elapsed    | 31619    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=2.36 +/- 0.31
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.36       |
| time/                   |            |
|    total_timesteps      | 9400000    |
| train/                  |            |
|    approx_kl            | 0.01669243 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.85       |
|    n_updates            | 5730       |
|    policy_gradient_loss | -0.00347   |
|    value_loss           | 13         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 574      |
|    time_elapsed    | 31677    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.31       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 575         |
|    time_elapsed         | 31726       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.021498308 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 2.95        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=2.71 +/- 0.78
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 9425000    |
| train/                  |            |
|    approx_kl            | 0.01207871 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.82      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.792      |
|    n_updates            | 5750       |
|    policy_gradient_loss | -0.00526   |
|    value_loss           | 3.63       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 576      |
|    time_elapsed    | 31785    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=2.41 +/- 1.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.41        |
| time/                   |             |
|    total_timesteps      | 9450000     |
| train/                  |             |
|    approx_kl            | 0.021695081 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.2         |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 24.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 577      |
|    time_elapsed    | 31843    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.64       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 578         |
|    time_elapsed         | 31892       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.015844801 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 1.2         |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=2.49 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.016841166 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.611       |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 1.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.79    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 579      |
|    time_elapsed    | 31951    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=2.73 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.73       |
| time/                   |            |
|    total_timesteps      | 9500000    |
| train/                  |            |
|    approx_kl            | 0.01785766 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 5790       |
|    policy_gradient_loss | -0.00802   |
|    value_loss           | 2.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.88    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 580      |
|    time_elapsed    | 32009    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.95       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 581         |
|    time_elapsed         | 32058       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.012998788 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 5.64        |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=2.02 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.016286803 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 16.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.73    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 582      |
|    time_elapsed    | 32117    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=2.43 +/- 1.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.017291654 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 1.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.07    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 583      |
|    time_elapsed    | 32175    |
|    total_timesteps | 9551872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -9.31      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 584        |
|    time_elapsed         | 32224      |
|    total_timesteps      | 9568256    |
| train/                  |            |
|    approx_kl            | 0.01745883 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.486      |
|    n_updates            | 5830       |
|    policy_gradient_loss | -0.00721   |
|    value_loss           | 2.49       |
----------------------------------------
Eval num_timesteps=9575000, episode_reward=2.53 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.017331824 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 0.975       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.44    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 585      |
|    time_elapsed    | 32282    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=1.71 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.017970879 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0984      |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 1.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.6     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 586      |
|    time_elapsed    | 32341    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.71       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 587         |
|    time_elapsed         | 32390       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.017073408 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.922       |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 2.79        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=2.49 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.026345225 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.239       |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 2.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.36    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 588      |
|    time_elapsed    | 32449    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=2.60 +/- 0.66
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.6          |
| time/                   |              |
|    total_timesteps      | 9650000      |
| train/                  |              |
|    approx_kl            | 0.0141366795 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.108        |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00459     |
|    value_loss           | 0.957        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.92    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 589      |
|    time_elapsed    | 32507    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.38       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 590         |
|    time_elapsed         | 32556       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.015908822 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.024       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00588    |
|    value_loss           | 0.273       |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=2.85 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.85        |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.018127806 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0514      |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 0.202       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.62    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 591      |
|    time_elapsed    | 32615    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.91       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 592         |
|    time_elapsed         | 32664       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.014158405 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.732       |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 6.75        |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=2.60 +/- 0.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.6        |
| time/                   |            |
|    total_timesteps      | 9700000    |
| train/                  |            |
|    approx_kl            | 0.01290114 |
|    clip_fraction        | 0.0673     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.74       |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.00463   |
|    value_loss           | 6.59       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.28    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 593      |
|    time_elapsed    | 32722    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=2.44 +/- 1.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.017733462 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00253     |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 0.981       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 594      |
|    time_elapsed    | 32781    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.01       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 595         |
|    time_elapsed         | 32830       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.018064855 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 1.17        |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=2.12 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.013945093 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.33        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 37.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 596      |
|    time_elapsed    | 32888    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=5.96 +/- 6.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.96        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.017569812 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.681       |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 3.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 597      |
|    time_elapsed    | 32946    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 598         |
|    time_elapsed         | 32996       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.017883625 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0622      |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 3.22        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=0.91 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.915       |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.017269712 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 2.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.49    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 599      |
|    time_elapsed    | 33054    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=2.42 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.015402174 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 2.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 600      |
|    time_elapsed    | 33112    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 601         |
|    time_elapsed         | 33161       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.015419001 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    n_updates            | 6000        |
|    policy_gradient_loss | 0.00305     |
|    value_loss           | 30.3        |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=3.11 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.11        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.017568279 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.731       |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 1.25        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 602      |
|    time_elapsed    | 33220    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=3.51 +/- 2.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.51        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.020073855 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.314       |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 1.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 603      |
|    time_elapsed    | 33278    |
|    total_timesteps | 9879552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.29      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 604        |
|    time_elapsed         | 33327      |
|    total_timesteps      | 9895936    |
| train/                  |            |
|    approx_kl            | 0.01578588 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.4       |
|    n_updates            | 6030       |
|    policy_gradient_loss | 7.67e-05   |
|    value_loss           | 8.11       |
----------------------------------------
Eval num_timesteps=9900000, episode_reward=1.27 +/- 1.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 9900000     |
| train/                  |             |
|    approx_kl            | 0.017454099 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 16.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.01    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 605      |
|    time_elapsed    | 33386    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=1.85 +/- 1.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.85       |
| time/                   |            |
|    total_timesteps      | 9925000    |
| train/                  |            |
|    approx_kl            | 0.01776703 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 6050       |
|    policy_gradient_loss | -0.00581   |
|    value_loss           | 9.92       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.88    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 606      |
|    time_elapsed    | 33444    |
|    total_timesteps | 9928704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.39      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 607        |
|    time_elapsed         | 33493      |
|    total_timesteps      | 9945088    |
| train/                  |            |
|    approx_kl            | 0.01792683 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0187     |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.00779   |
|    value_loss           | 0.965      |
----------------------------------------
Eval num_timesteps=9950000, episode_reward=0.82 +/- 1.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.816       |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.012420189 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 4.36        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 608      |
|    time_elapsed    | 33553    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=2.48 +/- 0.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.48        |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.019166421 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | -0.166      |
|    learning_rate        | 0.0003      |
|    loss                 | 30.4        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -7.06e-05   |
|    value_loss           | 345         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 609      |
|    time_elapsed    | 33611    |
|    total_timesteps | 9977856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -85.4      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 610        |
|    time_elapsed         | 33660      |
|    total_timesteps      | 9994240    |
| train/                  |            |
|    approx_kl            | 0.01941936 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.469      |
|    n_updates            | 6090       |
|    policy_gradient_loss | -0.0069    |
|    value_loss           | 8.53       |
----------------------------------------
Eval num_timesteps=10000000, episode_reward=8.14 +/- 11.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 8.14        |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.010865426 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 111         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 611      |
|    time_elapsed    | 33719    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v6_2
