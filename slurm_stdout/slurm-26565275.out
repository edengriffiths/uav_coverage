========== uav-v10 ==========
Seed: 1281936421
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v10_1
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.93e+03 |
| time/              |           |
|    fps             | 290       |
|    iterations      | 1         |
|    time_elapsed    | 56        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=4682.55 +/- 240.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.68e+03     |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0077851224 |
|    clip_fraction        | 0.0778       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | -0.00156     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0084      |
|    value_loss           | 2.4e+04      |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.33e+03 |
| time/              |           |
|    fps             | 244       |
|    iterations      | 2         |
|    time_elapsed    | 134       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.3e+03    |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 3           |
|    time_elapsed         | 199         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008891666 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.23e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.46e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=4202.67 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.2e+03     |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.007972809 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 3.36e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 4        |
|    time_elapsed    | 277      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=75000, episode_reward=4322.56 +/- 240.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.32e+03    |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.009539111 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0003      |
|    loss                 | 815         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.7e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 586      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 5        |
|    time_elapsed    | 355      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.06e+03    |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 6           |
|    time_elapsed         | 420         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.008649456 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=4442.25 +/- 293.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.44e+03    |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.008518446 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | 739         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 1.81e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 230      |
|    iterations      | 7        |
|    time_elapsed    | 498      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=125000, episode_reward=4442.51 +/- 293.77
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.44e+03     |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0092470255 |
|    clip_fraction        | 0.0905       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.0003       |
|    loss                 | 807          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0094      |
|    value_loss           | 1.68e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 8        |
|    time_elapsed    | 576      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 9           |
|    time_elapsed         | 641         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.007515558 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | 651         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 1.89e+03    |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=4202.64 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.2e+03     |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.006694032 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 796         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 2.52e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 10       |
|    time_elapsed    | 719      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=4682.36 +/- 239.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.68e+03     |
| time/                   |              |
|    total_timesteps      | 175000       |
| train/                  |              |
|    approx_kl            | 0.0094555225 |
|    clip_fraction        | 0.0912       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.0003       |
|    loss                 | 701          |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00804     |
|    value_loss           | 1.88e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 11       |
|    time_elapsed    | 796      |
|    total_timesteps | 180224   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.94e+03     |
| time/                   |              |
|    fps                  | 228          |
|    iterations           | 12           |
|    time_elapsed         | 862          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0065915966 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.66        |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.0003       |
|    loss                 | 637          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00669     |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=200000, episode_reward=3283.02 +/- 2151.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.28e+03    |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.006646592 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 285         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 1.58e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 13       |
|    time_elapsed    | 940      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=4682.81 +/- 240.04
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.68e+03     |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0063371044 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0003       |
|    loss                 | 876          |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00525     |
|    value_loss           | 1.85e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 14       |
|    time_elapsed    | 1017     |
|    total_timesteps | 229376   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.89e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 15           |
|    time_elapsed         | 1083         |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0059812265 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.0003       |
|    loss                 | 455          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0053      |
|    value_loss           | 1.47e+03     |
------------------------------------------
Eval num_timesteps=250000, episode_reward=4802.38 +/- 536.67
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.8e+03      |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0061051697 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.0003       |
|    loss                 | 252          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 1.26e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 16       |
|    time_elapsed    | 1160     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=4562.86 +/- 294.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.56e+03    |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.005892979 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 483         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 1.11e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 224      |
|    iterations      | 17       |
|    time_elapsed    | 1238     |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.17e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 18          |
|    time_elapsed         | 1304        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.005819327 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 590         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=4682.45 +/- 699.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.68e+03    |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.005822651 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 309         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 864         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 19       |
|    time_elapsed    | 1382     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=5042.68 +/- 479.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.04e+03     |
| time/                   |              |
|    total_timesteps      | 325000       |
| train/                  |              |
|    approx_kl            | 0.0067215906 |
|    clip_fraction        | 0.049        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.61        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 480          |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00598     |
|    value_loss           | 958          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 20       |
|    time_elapsed    | 1459     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.17e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 21          |
|    time_elapsed         | 1525        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.005857268 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 406         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00502    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=4682.59 +/- 449.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.68e+03   |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.00535112 |
|    clip_fraction        | 0.029      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.6       |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 723        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00452   |
|    value_loss           | 1.4e+03    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 22       |
|    time_elapsed    | 1602     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=4802.66 +/- 536.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.8e+03      |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0063250354 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0003       |
|    loss                 | 71.2         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 540          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 23       |
|    time_elapsed    | 1680     |
|    total_timesteps | 376832   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.2e+03      |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 24           |
|    time_elapsed         | 1745         |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0054183435 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.57        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 517          |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=400000, episode_reward=5162.66 +/- 611.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.16e+03     |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0047263536 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 526          |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 1.32e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 25       |
|    time_elapsed    | 1823     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=5042.66 +/- 480.02
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.04e+03     |
| time/                   |              |
|    total_timesteps      | 425000       |
| train/                  |              |
|    approx_kl            | 0.0054949643 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 525          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 1.06e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 26       |
|    time_elapsed    | 1901     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.7e+03     |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 27          |
|    time_elapsed         | 1966        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.005854737 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 539         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=4682.70 +/- 448.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.68e+03    |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.004255818 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 631         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 28       |
|    time_elapsed    | 2044     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=4242.91 +/- 2350.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.24e+03    |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.007319169 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 645         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 1.12e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    fps             | 223      |
|    iterations      | 29       |
|    time_elapsed    | 2121     |
|    total_timesteps | 475136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.69e+03   |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 30         |
|    time_elapsed         | 2186       |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.00543387 |
|    clip_fraction        | 0.0244     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.5       |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 207        |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0039    |
|    value_loss           | 804        |
----------------------------------------
Eval num_timesteps=500000, episode_reward=5642.22 +/- 293.94
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.64e+03     |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0059784646 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 603          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 1.35e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 31       |
|    time_elapsed    | 2266     |
|    total_timesteps | 507904   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.7e+03      |
| time/                   |              |
|    fps                  | 224          |
|    iterations           | 32           |
|    time_elapsed         | 2330         |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0049590687 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.42        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 366          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 1.43e+03     |
------------------------------------------
Eval num_timesteps=525000, episode_reward=5642.24 +/- 293.84
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.64e+03     |
| time/                   |              |
|    total_timesteps      | 525000       |
| train/                  |              |
|    approx_kl            | 0.0054091597 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.39        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 232          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00364     |
|    value_loss           | 478          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 33       |
|    time_elapsed    | 2406     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=4242.99 +/- 2319.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.24e+03    |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.005449887 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 152         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 34       |
|    time_elapsed    | 2482     |
|    total_timesteps | 557056   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.2e+03      |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 35           |
|    time_elapsed         | 2546         |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0061625033 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 202          |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00557     |
|    value_loss           | 462          |
------------------------------------------
Eval num_timesteps=575000, episode_reward=5522.54 +/- 239.74
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.52e+03     |
| time/                   |              |
|    total_timesteps      | 575000       |
| train/                  |              |
|    approx_kl            | 0.0047700303 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 565          |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 1.15e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 36       |
|    time_elapsed    | 2622     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=5522.27 +/- 239.96
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.52e+03     |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0056604436 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 137          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 711          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 37       |
|    time_elapsed    | 2698     |
|    total_timesteps | 606208   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.84e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 38           |
|    time_elapsed         | 2762         |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 0.0046969475 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 684          |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 1.11e+03     |
------------------------------------------
Eval num_timesteps=625000, episode_reward=5882.19 +/- 240.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.005449501 |
|    clip_fraction        | 0.0214      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 267         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 39       |
|    time_elapsed    | 2838     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=5882.08 +/- 240.10
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.88e+03     |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0051333355 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 132          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 872          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 40       |
|    time_elapsed    | 2914     |
|    total_timesteps | 655360   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.88e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 41           |
|    time_elapsed         | 2977         |
|    total_timesteps      | 671744       |
| train/                  |              |
|    approx_kl            | 0.0052345935 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0003       |
|    loss                 | 505          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=675000, episode_reward=5882.61 +/- 239.88
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.88e+03     |
| time/                   |              |
|    total_timesteps      | 675000       |
| train/                  |              |
|    approx_kl            | 0.0054925596 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.3         |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00492     |
|    value_loss           | 421          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 42       |
|    time_elapsed    | 3053     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=5882.44 +/- 239.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.007596532 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 459         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 799         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.34e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 43       |
|    time_elapsed    | 3129     |
|    total_timesteps | 704512   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.25e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 44           |
|    time_elapsed         | 3193         |
|    total_timesteps      | 720896       |
| train/                  |              |
|    approx_kl            | 0.0048573795 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 144          |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=725000, episode_reward=6002.39 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.004668735 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 203         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 532         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 45       |
|    time_elapsed    | 3269     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=6002.50 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 750000     |
| train/                  |            |
|    approx_kl            | 0.00614353 |
|    clip_fraction        | 0.0254     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.1       |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 304        |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00306   |
|    value_loss           | 1.24e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 46       |
|    time_elapsed    | 3345     |
|    total_timesteps | 753664   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 2.79e+03   |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 47         |
|    time_elapsed         | 3409       |
|    total_timesteps      | 770048     |
| train/                  |            |
|    approx_kl            | 0.00405275 |
|    clip_fraction        | 0.0151     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 540        |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00272   |
|    value_loss           | 1e+03      |
----------------------------------------
Eval num_timesteps=775000, episode_reward=6002.38 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.006051168 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 331         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 974         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 48       |
|    time_elapsed    | 3485     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=5882.43 +/- 239.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.005091776 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 907         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 49       |
|    time_elapsed    | 3561     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.62e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 50          |
|    time_elapsed         | 3625        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.007619745 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 733         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=6002.44 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 825000       |
| train/                  |              |
|    approx_kl            | 0.0052859266 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 240          |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 870          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 51       |
|    time_elapsed    | 3701     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=5882.57 +/- 240.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.005697514 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 516         |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 1.11e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 52       |
|    time_elapsed    | 3777     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.15e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 53          |
|    time_elapsed         | 3841        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.006497978 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.1        |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00438    |
|    value_loss           | 631         |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=5882.61 +/- 239.97
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.88e+03     |
| time/                   |              |
|    total_timesteps      | 875000       |
| train/                  |              |
|    approx_kl            | 0.0042041056 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.7         |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 112          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 501          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.46e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 54       |
|    time_elapsed    | 3917     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=6002.66 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0064298334 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.7         |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 443          |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00385     |
|    value_loss           | 698          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.59e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 55       |
|    time_elapsed    | 3993     |
|    total_timesteps | 901120   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.58e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 56           |
|    time_elapsed         | 4057         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0050307596 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 267          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.0036      |
|    value_loss           | 700          |
------------------------------------------
Eval num_timesteps=925000, episode_reward=6002.42 +/- 0.42
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 925000     |
| train/                  |            |
|    approx_kl            | 0.00817154 |
|    clip_fraction        | 0.054      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 179        |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00279   |
|    value_loss           | 452        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.55e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 57       |
|    time_elapsed    | 4133     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=6002.47 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.004713961 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 569         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 880         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.66e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 58       |
|    time_elapsed    | 4209     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.43e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 59          |
|    time_elapsed         | 4273        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.004425845 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=4643.96 +/- 2428.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.64e+03     |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0053420486 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 187          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00376     |
|    value_loss           | 661          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.27e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 60       |
|    time_elapsed    | 4349     |
|    total_timesteps | 983040   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.59e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 61          |
|    time_elapsed         | 4413        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.005334035 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.1        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 624         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=6002.69 +/- 0.62
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0042573926 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.4         |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 381          |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 580          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.78e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 62       |
|    time_elapsed    | 4488     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=5882.72 +/- 239.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.004294488 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 196         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00382    |
|    value_loss           | 1.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.05e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 63       |
|    time_elapsed    | 4564     |
|    total_timesteps | 1032192  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.07e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 64           |
|    time_elapsed         | 4628         |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0051965765 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 711          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 892          |
------------------------------------------
Eval num_timesteps=1050000, episode_reward=6002.68 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0051967897 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.34        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 216          |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 1.06e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.99e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 65       |
|    time_elapsed    | 4704     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=6002.28 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.006118961 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.2        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 441         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.01e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 66       |
|    time_elapsed    | 4780     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.03e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 67          |
|    time_elapsed         | 4844        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.006210792 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 271         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 950         |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=6002.33 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0056991475 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.18        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 51.7         |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 333          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.93e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 68       |
|    time_elapsed    | 4920     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=6002.30 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.006572676 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 222         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 725         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.84e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 69       |
|    time_elapsed    | 4995     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.7e+03     |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 70          |
|    time_elapsed         | 5059        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.006201084 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 229         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 595         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=6002.42 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1150000      |
| train/                  |              |
|    approx_kl            | 0.0037291294 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.85        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 207          |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 882          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.92e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 71       |
|    time_elapsed    | 5135     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=4883.10 +/- 2238.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.88e+03    |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.008868957 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0036     |
|    value_loss           | 703         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.07e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 72       |
|    time_elapsed    | 5211     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.29e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 73          |
|    time_elapsed         | 5275        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.007162772 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 66.9        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=6002.57 +/- 0.11
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0059800283 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 433          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.46e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 74       |
|    time_elapsed    | 5351     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=6002.43 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1225000      |
| train/                  |              |
|    approx_kl            | 0.0053082295 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 97.6         |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 470          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.63e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 75       |
|    time_elapsed    | 5426     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.76e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 76          |
|    time_elapsed         | 5490        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.007020334 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.22        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=6002.21 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.006400012 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 449         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.86e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 77       |
|    time_elapsed    | 5566     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=6002.64 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1275000      |
| train/                  |              |
|    approx_kl            | 0.0044726254 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.23        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 295          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00223     |
|    value_loss           | 551          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.58e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 78       |
|    time_elapsed    | 5642     |
|    total_timesteps | 1277952  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.46e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 79           |
|    time_elapsed         | 5706         |
|    total_timesteps      | 1294336      |
| train/                  |              |
|    approx_kl            | 0.0067091985 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 160          |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.0041      |
|    value_loss           | 797          |
------------------------------------------
Eval num_timesteps=1300000, episode_reward=6002.09 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.008140633 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 131         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.19e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 80       |
|    time_elapsed    | 5782     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=4843.03 +/- 2319.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.004778158 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 467         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 778         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.31e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 81       |
|    time_elapsed    | 5858     |
|    total_timesteps | 1327104  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.63e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 82           |
|    time_elapsed         | 5922         |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0025154897 |
|    clip_fraction        | 0.00734      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 321          |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 648          |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=6002.35 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1350000      |
| train/                  |              |
|    approx_kl            | 0.0027918268 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.2         |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 468          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.74e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 83       |
|    time_elapsed    | 5997     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=6002.56 +/- 0.55
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1375000      |
| train/                  |              |
|    approx_kl            | 0.0040820437 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 163          |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 568          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.97e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 84       |
|    time_elapsed    | 6073     |
|    total_timesteps | 1376256  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.05e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 85           |
|    time_elapsed         | 6137         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0028179288 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.9         |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 232          |
------------------------------------------
Eval num_timesteps=1400000, episode_reward=6002.47 +/- 0.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1400000      |
| train/                  |              |
|    approx_kl            | 0.0024022842 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 81.1         |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 353          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 86       |
|    time_elapsed    | 6213     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=4843.14 +/- 2318.85
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 1425000      |
| train/                  |              |
|    approx_kl            | 0.0037718029 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.4         |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 172          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.95e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 87       |
|    time_elapsed    | 6289     |
|    total_timesteps | 1425408  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.05e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 88           |
|    time_elapsed         | 6353         |
|    total_timesteps      | 1441792      |
| train/                  |              |
|    approx_kl            | 0.0033501876 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.47        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 299          |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 446          |
------------------------------------------
Eval num_timesteps=1450000, episode_reward=6002.64 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1450000      |
| train/                  |              |
|    approx_kl            | 0.0067702024 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.9         |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00391     |
|    value_loss           | 334          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.13e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 89       |
|    time_elapsed    | 6429     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.02e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 90          |
|    time_elapsed         | 6493        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.004733541 |
|    clip_fraction        | 0.0567      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.000829   |
|    value_loss           | 87.1        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=4843.04 +/- 2319.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.84e+03   |
| time/                   |            |
|    total_timesteps      | 1475000    |
| train/                  |            |
|    approx_kl            | 0.00505523 |
|    clip_fraction        | 0.0361     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 32.2       |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.000818  |
|    value_loss           | 323        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.29e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 91       |
|    time_elapsed    | 6569     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=6002.12 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0025428683 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.34         |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000996    |
|    value_loss           | 93           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 92       |
|    time_elapsed    | 6645     |
|    total_timesteps | 1507328  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.36e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 93           |
|    time_elapsed         | 6709         |
|    total_timesteps      | 1523712      |
| train/                  |              |
|    approx_kl            | 0.0032678675 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.912       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 33.8         |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=1525000, episode_reward=6002.70 +/- 0.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1525000      |
| train/                  |              |
|    approx_kl            | 0.0033651998 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.911       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.8         |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 175          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.41e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 94       |
|    time_elapsed    | 6784     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=6002.31 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.004020194 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.2         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 71.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.34e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 95       |
|    time_elapsed    | 6860     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.37e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 96          |
|    time_elapsed         | 6924        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.006578219 |
|    clip_fraction        | 0.0401      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 235         |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=4843.03 +/- 2318.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 1575000     |
| train/                  |             |
|    approx_kl            | 0.003369238 |
|    clip_fraction        | 0.0222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 169         |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 489         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.23e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 97       |
|    time_elapsed    | 7000     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=6002.59 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0023511902 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.834       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 178          |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00199     |
|    value_loss           | 499          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.11e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 98       |
|    time_elapsed    | 7076     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.02e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 99          |
|    time_elapsed         | 7140        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.002134917 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=6002.35 +/- 0.13
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1625000      |
| train/                  |              |
|    approx_kl            | 0.0015339132 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.709       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 286          |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 142          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.1e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 100      |
|    time_elapsed    | 7216     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=6002.30 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1650000      |
| train/                  |              |
|    approx_kl            | 0.0021792173 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.697       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 393          |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00082     |
|    value_loss           | 519          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.21e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 101      |
|    time_elapsed    | 7292     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.41e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 102         |
|    time_elapsed         | 7356        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.004446544 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 53.7        |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=6002.34 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1675000      |
| train/                  |              |
|    approx_kl            | 0.0073226383 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.0003       |
|    loss                 | 69.8         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000707    |
|    value_loss           | 318          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.32e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 103      |
|    time_elapsed    | 7432     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=4842.73 +/- 2319.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0045355614 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.901        |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00077     |
|    value_loss           | 160          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.3e+03  |
| time/              |          |
|    fps             | 226      |
|    iterations      | 104      |
|    time_elapsed    | 7508     |
|    total_timesteps | 1703936  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.32e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 105          |
|    time_elapsed         | 7572         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0023247087 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.9         |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.000387    |
|    value_loss           | 61.5         |
------------------------------------------
Eval num_timesteps=1725000, episode_reward=6002.53 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.004986896 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.11        |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 64.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 106      |
|    time_elapsed    | 7647     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=6002.55 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.010468203 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00193    |
|    value_loss           | 125         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.47e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 107      |
|    time_elapsed    | 7723     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.4e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 108         |
|    time_elapsed         | 7787        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.006558067 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 435         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 334         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=6002.69 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1775000      |
| train/                  |              |
|    approx_kl            | 0.0036928877 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.718       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 59.5         |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 318          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.94e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 109      |
|    time_elapsed    | 7863     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=4842.53 +/- 2319.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.004392742 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 477         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.75e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 110      |
|    time_elapsed    | 7939     |
|    total_timesteps | 1802240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.93e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 111          |
|    time_elapsed         | 8003         |
|    total_timesteps      | 1818624      |
| train/                  |              |
|    approx_kl            | 0.0054047513 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.573       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 55.7         |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 119          |
------------------------------------------
Eval num_timesteps=1825000, episode_reward=6002.19 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1825000      |
| train/                  |              |
|    approx_kl            | 0.0077880565 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.704       |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.2          |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00062     |
|    value_loss           | 145          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.95e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 112      |
|    time_elapsed    | 8079     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=6002.72 +/- 0.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1850000      |
| train/                  |              |
|    approx_kl            | 0.0042120814 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.646       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.32         |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 86.3         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.41e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 113      |
|    time_elapsed    | 8155     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.49e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 114         |
|    time_elapsed         | 8219        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.004446298 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 87.8        |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=6002.38 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1875000      |
| train/                  |              |
|    approx_kl            | 0.0042006187 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.3         |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 168          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.58e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 115      |
|    time_elapsed    | 8294     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=6002.24 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.015393667 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.3         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 36.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.44e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 116      |
|    time_elapsed    | 8370     |
|    total_timesteps | 1900544  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.4e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 117          |
|    time_elapsed         | 8434         |
|    total_timesteps      | 1916928      |
| train/                  |              |
|    approx_kl            | 0.0055990447 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | 34.7         |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00427     |
|    value_loss           | 256          |
------------------------------------------
Eval num_timesteps=1925000, episode_reward=6002.58 +/- 0.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1925000      |
| train/                  |              |
|    approx_kl            | 0.0033912756 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 49.2         |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 218          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 118      |
|    time_elapsed    | 8510     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 119         |
|    time_elapsed         | 8574        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.004256363 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00022    |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=6002.39 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1950000      |
| train/                  |              |
|    approx_kl            | 0.0022914698 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 11           |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 232          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.19e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 120      |
|    time_elapsed    | 8650     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=6002.49 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 1975000      |
| train/                  |              |
|    approx_kl            | 0.0018742314 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.376       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 227          |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 164          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.21e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 121      |
|    time_elapsed    | 8726     |
|    total_timesteps | 1982464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.55e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 122          |
|    time_elapsed         | 8790         |
|    total_timesteps      | 1998848      |
| train/                  |              |
|    approx_kl            | 0.0015683089 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.1         |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.000767    |
|    value_loss           | 220          |
------------------------------------------
Eval num_timesteps=2000000, episode_reward=6002.61 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.012446217 |
|    clip_fraction        | 0.038       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.716       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 37.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.42e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 123      |
|    time_elapsed    | 8866     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=6002.72 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2025000      |
| train/                  |              |
|    approx_kl            | 0.0031139976 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.457       |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.0003       |
|    loss                 | 106          |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 545          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.01e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 124      |
|    time_elapsed    | 8941     |
|    total_timesteps | 2031616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.7e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 125          |
|    time_elapsed         | 9005         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0067782593 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.831        |
|    learning_rate        | 0.0003       |
|    loss                 | 88.8         |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=2050000, episode_reward=4842.84 +/- 2319.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 0.0026673302 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 139          |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 428          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 126      |
|    time_elapsed    | 9081     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=6002.72 +/- 0.59
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0045409645 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.2         |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 35.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.19e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 127      |
|    time_elapsed    | 9157     |
|    total_timesteps | 2080768  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.42e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 128          |
|    time_elapsed         | 9221         |
|    total_timesteps      | 2097152      |
| train/                  |              |
|    approx_kl            | 0.0038619332 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.409       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.5         |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00322     |
|    value_loss           | 129          |
------------------------------------------
Eval num_timesteps=2100000, episode_reward=6002.63 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.005637378 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.75        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 43          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.67e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 129      |
|    time_elapsed    | 9297     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=6002.81 +/- 0.57
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 6e+03         |
| time/                   |               |
|    total_timesteps      | 2125000       |
| train/                  |               |
|    approx_kl            | 0.00066054903 |
|    clip_fraction        | 0.00824       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.166        |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.02          |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.000296     |
|    value_loss           | 132           |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.61e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 130      |
|    time_elapsed    | 9373     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.56e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 131         |
|    time_elapsed         | 9437        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.007440058 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.000421   |
|    value_loss           | 4.98        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=6002.49 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.008220028 |
|    clip_fraction        | 0.0343      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.277      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.509       |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.64e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 132      |
|    time_elapsed    | 9512     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=4842.81 +/- 2319.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.001448008 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 1320        |
|    policy_gradient_loss | 0.000639    |
|    value_loss           | 100         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 133      |
|    time_elapsed    | 9588     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.65e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 134         |
|    time_elapsed         | 9652        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.003232691 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.267      |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.3        |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.000165    |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=6002.83 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2200000      |
| train/                  |              |
|    approx_kl            | 0.0031932034 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.262       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.932        |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000444    |
|    value_loss           | 57.4         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.78e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 135      |
|    time_elapsed    | 9728     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=6002.46 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2225000      |
| train/                  |              |
|    approx_kl            | 0.0012383582 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 174          |
|    n_updates            | 1350         |
|    policy_gradient_loss | 0.000331     |
|    value_loss           | 88.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.78e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 136      |
|    time_elapsed    | 9804     |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.7e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 137         |
|    time_elapsed         | 9867        |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.032594588 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.515       |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 37.8        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=6002.66 +/- 0.45
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0001440651 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0723      |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.6         |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00019     |
|    value_loss           | 68.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.72e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 138      |
|    time_elapsed    | 9943     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=6002.69 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2275000      |
| train/                  |              |
|    approx_kl            | 0.0006214201 |
|    clip_fraction        | 0.00621      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0647      |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.409        |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.000263    |
|    value_loss           | 45.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.78e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 139      |
|    time_elapsed    | 10019    |
|    total_timesteps | 2277376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.82e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 140        |
|    time_elapsed         | 10083      |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.04184844 |
|    clip_fraction        | 0.0355     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.00354   |
|    value_loss           | 1.94       |
----------------------------------------
Eval num_timesteps=2300000, episode_reward=3410.92 +/- 3174.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41e+03    |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.002054818 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 581         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 1.67e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 141      |
|    time_elapsed    | 10158    |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=6002.36 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2325000      |
| train/                  |              |
|    approx_kl            | 0.0033761184 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.679       |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72e+03     |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 1.82e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 142      |
|    time_elapsed    | 10234    |
|    total_timesteps | 2326528  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 115          |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 143          |
|    time_elapsed         | 10298        |
|    total_timesteps      | 2342912      |
| train/                  |              |
|    approx_kl            | 0.0071171406 |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.783       |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 603          |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 926          |
------------------------------------------
Eval num_timesteps=2350000, episode_reward=6002.80 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.004659273 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 867         |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 492      |
| time/              |          |
|    fps             | 227      |
|    iterations      | 144      |
|    time_elapsed    | 10374    |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=6002.61 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.004942581 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 1.01e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.59e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 145      |
|    time_elapsed    | 10450    |
|    total_timesteps | 2375680  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.95e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 146          |
|    time_elapsed         | 10514        |
|    total_timesteps      | 2392064      |
| train/                  |              |
|    approx_kl            | 0.0069399443 |
|    clip_fraction        | 0.0619       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 159          |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 514          |
------------------------------------------
Eval num_timesteps=2400000, episode_reward=4842.90 +/- 2319.61
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2400000      |
| train/                  |              |
|    approx_kl            | 0.0023415382 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 250          |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00335     |
|    value_loss           | 555          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.93e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 147      |
|    time_elapsed    | 10590    |
|    total_timesteps | 2408448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.05e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 148          |
|    time_elapsed         | 10653        |
|    total_timesteps      | 2424832      |
| train/                  |              |
|    approx_kl            | 0.0027589453 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.2         |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 298          |
------------------------------------------
Eval num_timesteps=2425000, episode_reward=4842.83 +/- 2319.59
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2425000      |
| train/                  |              |
|    approx_kl            | 0.0017580588 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 115          |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 294          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 149      |
|    time_elapsed    | 10729    |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=6002.53 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.006359054 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.9        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00099    |
|    value_loss           | 133         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.81e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 150      |
|    time_elapsed    | 10805    |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.68e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 151         |
|    time_elapsed         | 10869       |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.032716542 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.96        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 18.4        |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=6002.65 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.008401605 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 225         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.39e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 152      |
|    time_elapsed    | 10944    |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=6002.44 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.005324589 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.6        |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000943   |
|    value_loss           | 201         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.3e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 153      |
|    time_elapsed    | 11020    |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 154         |
|    time_elapsed         | 11084       |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.019313961 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.3        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 96.8        |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=6002.21 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2525000      |
| train/                  |              |
|    approx_kl            | 0.0076580704 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 1            |
|    n_updates            | 1540         |
|    policy_gradient_loss | 0.00396      |
|    value_loss           | 84.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.46e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 155      |
|    time_elapsed    | 11160    |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=6002.72 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.004365706 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.97        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 56.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.59e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 156      |
|    time_elapsed    | 11235    |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.64e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 157         |
|    time_elapsed         | 11299       |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.013042163 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.85        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 35.5        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=6002.81 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.011262847 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.31        |
|    n_updates            | 1570        |
|    policy_gradient_loss | 0.000262    |
|    value_loss           | 35.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 158      |
|    time_elapsed    | 11375    |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=6002.65 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 2600000      |
| train/                  |              |
|    approx_kl            | 0.0072298213 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.349       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.8          |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 57.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.8e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 159      |
|    time_elapsed    | 11451    |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.3e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 160         |
|    time_elapsed         | 11515       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.026107509 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.424      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 34.8        |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=4842.89 +/- 2319.53
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2625000      |
| train/                  |              |
|    approx_kl            | 0.0010483088 |
|    clip_fraction        | 0.00245      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.0003       |
|    loss                 | 580          |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.000695    |
|    value_loss           | 954          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.1e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 161      |
|    time_elapsed    | 11590    |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=4842.83 +/- 2319.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2650000      |
| train/                  |              |
|    approx_kl            | 0.0071073966 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.397       |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.0003       |
|    loss                 | 85.9         |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 279          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.99e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 162      |
|    time_elapsed    | 11666    |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 163         |
|    time_elapsed         | 11731       |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.023828695 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 46          |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=6002.40 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.007810473 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 246         |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 389         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.18e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 164      |
|    time_elapsed    | 11806    |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=6002.40 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.011374146 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.374      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.5         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 226         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.44e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 165      |
|    time_elapsed    | 11882    |
|    total_timesteps | 2703360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.53e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 166         |
|    time_elapsed         | 11946       |
|    total_timesteps      | 2719744     |
| train/                  |             |
|    approx_kl            | 0.007164554 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.675       |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 15.5        |
-----------------------------------------
Eval num_timesteps=2725000, episode_reward=6002.16 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.013318488 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.08        |
|    n_updates            | 1660        |
|    policy_gradient_loss | 0.00326     |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 167      |
|    time_elapsed    | 12022    |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=6002.48 +/- 0.60
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 2750000    |
| train/                  |            |
|    approx_kl            | 0.01723002 |
|    clip_fraction        | 0.0404     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.9       |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.000852  |
|    value_loss           | 223        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.66e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 168      |
|    time_elapsed    | 12098    |
|    total_timesteps | 2752512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.53e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 169         |
|    time_elapsed         | 12162       |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.011207234 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 73.1        |
-----------------------------------------
Eval num_timesteps=2775000, episode_reward=6002.77 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.011092523 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.2        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 191         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.46e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 170      |
|    time_elapsed    | 12238    |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=6002.40 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.004367752 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0629     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.55e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 171      |
|    time_elapsed    | 12314    |
|    total_timesteps | 2801664  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.69e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 172           |
|    time_elapsed         | 12378         |
|    total_timesteps      | 2818048       |
| train/                  |               |
|    approx_kl            | 0.00018962118 |
|    clip_fraction        | 0.00175       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0499       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.25          |
|    n_updates            | 1710          |
|    policy_gradient_loss | -4.92e-05     |
|    value_loss           | 2.59          |
-------------------------------------------
Eval num_timesteps=2825000, episode_reward=6002.45 +/- 0.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.005838034 |
|    clip_fraction        | 0.00996     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0488     |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.32        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 2.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.88e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 173      |
|    time_elapsed    | 12454    |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=4842.93 +/- 2319.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.000606026 |
|    clip_fraction        | 0.00406     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0292     |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.343       |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.000533   |
|    value_loss           | 0.543       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 6e+03    |
| time/              |          |
|    fps             | 227      |
|    iterations      | 174      |
|    time_elapsed    | 12530    |
|    total_timesteps | 2850816  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | 3.93e+03  |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 175       |
|    time_elapsed         | 12594     |
|    total_timesteps      | 2867200   |
| train/                  |           |
|    approx_kl            | 0.1412588 |
|    clip_fraction        | 0.151     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.135    |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.664     |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0159   |
|    value_loss           | 37.6      |
---------------------------------------
Eval num_timesteps=2875000, episode_reward=4843.26 +/- 2319.86
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2875000      |
| train/                  |              |
|    approx_kl            | 0.0031918806 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.0003       |
|    loss                 | 162          |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 1.41e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 176      |
|    time_elapsed    | 12670    |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 361         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 177         |
|    time_elapsed         | 12734       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.008184047 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 763         |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=3546.91 +/- 3015.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.55e+03    |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.004817697 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 730         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -704     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 178      |
|    time_elapsed    | 12810    |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=3546.72 +/- 3015.49
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.55e+03     |
| time/                   |              |
|    total_timesteps      | 2925000      |
| train/                  |              |
|    approx_kl            | 0.0023276052 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.403       |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.0003       |
|    loss                 | 214          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.000815    |
|    value_loss           | 793          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 227      |
|    iterations      | 179      |
|    time_elapsed    | 12886    |
|    total_timesteps | 2932736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 903          |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 180          |
|    time_elapsed         | 12950        |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0015887516 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 467          |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 567          |
------------------------------------------
Eval num_timesteps=2950000, episode_reward=4842.73 +/- 2319.69
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 2950000      |
| train/                  |              |
|    approx_kl            | 0.0039514257 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 120          |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 410          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 181      |
|    time_elapsed    | 13026    |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=4706.36 +/- 2592.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.71e+03    |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.002587285 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 153         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.000942   |
|    value_loss           | 350         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 182      |
|    time_elapsed    | 13102    |
|    total_timesteps | 2981888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.32e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 183          |
|    time_elapsed         | 13165        |
|    total_timesteps      | 2998272      |
| train/                  |              |
|    approx_kl            | 0.0022189042 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 286          |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 340          |
------------------------------------------
Eval num_timesteps=3000000, episode_reward=6002.74 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0013141665 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 87.6         |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 394          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.85e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 184      |
|    time_elapsed    | 13241    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=6002.76 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.005666962 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 63.4        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.28e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 185      |
|    time_elapsed    | 13317    |
|    total_timesteps | 3031040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.08e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 186          |
|    time_elapsed         | 13381        |
|    total_timesteps      | 3047424      |
| train/                  |              |
|    approx_kl            | 0.0023513637 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 71.3         |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.000965    |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=3050000, episode_reward=6002.57 +/- 0.17
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3050000      |
| train/                  |              |
|    approx_kl            | 0.0014474634 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 62           |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 228          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.13e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 187      |
|    time_elapsed    | 13457    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=6002.35 +/- 0.05
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3075000      |
| train/                  |              |
|    approx_kl            | 0.0022895504 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0978      |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 231          |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.000711    |
|    value_loss           | 136          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.35e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 188      |
|    time_elapsed    | 13532    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.56e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 189         |
|    time_elapsed         | 13596       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.001757368 |
|    clip_fraction        | 0.00868     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0921     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 76          |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=6002.12 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.010509694 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0801     |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.7        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.96e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 190      |
|    time_elapsed    | 13672    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=6002.57 +/- 0.48
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3125000      |
| train/                  |              |
|    approx_kl            | 0.0025337685 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.051       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 12.8         |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 166          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.28e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 191      |
|    time_elapsed    | 13748    |
|    total_timesteps | 3129344  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.43e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 192           |
|    time_elapsed         | 13812         |
|    total_timesteps      | 3145728       |
| train/                  |               |
|    approx_kl            | 3.1920245e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00891      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.46          |
|    n_updates            | 1910          |
|    policy_gradient_loss | -0.000138     |
|    value_loss           | 29.8          |
-------------------------------------------
Eval num_timesteps=3150000, episode_reward=6002.46 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3150000      |
| train/                  |              |
|    approx_kl            | 0.0018284093 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0843      |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.04         |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.000727    |
|    value_loss           | 85.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.51e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 193      |
|    time_elapsed    | 13888    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=4842.76 +/- 2319.75
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 3175000      |
| train/                  |              |
|    approx_kl            | 0.0032336207 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.3          |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.000565    |
|    value_loss           | 54.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.5e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 194      |
|    time_elapsed    | 13964    |
|    total_timesteps | 3178496  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.44e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 195           |
|    time_elapsed         | 14028         |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 0.00035048288 |
|    clip_fraction        | 0.00242       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.07          |
|    n_updates            | 1940          |
|    policy_gradient_loss | -0.000437     |
|    value_loss           | 98.8          |
-------------------------------------------
Eval num_timesteps=3200000, episode_reward=6002.70 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3200000      |
| train/                  |              |
|    approx_kl            | 5.637349e-05 |
|    clip_fraction        | 0.000714     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00921     |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.1          |
|    n_updates            | 1950         |
|    policy_gradient_loss | -9.49e-05    |
|    value_loss           | 4.3          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.68e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 196      |
|    time_elapsed    | 14103    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=6002.26 +/- 0.17
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3225000      |
| train/                  |              |
|    approx_kl            | 0.0019052451 |
|    clip_fraction        | 0.00565      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0261      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.04         |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.000578    |
|    value_loss           | 52           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.8e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 197      |
|    time_elapsed    | 14179    |
|    total_timesteps | 3227648  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.92e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 198           |
|    time_elapsed         | 14243         |
|    total_timesteps      | 3244032       |
| train/                  |               |
|    approx_kl            | 4.4841312e-05 |
|    clip_fraction        | 0.000397      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.35          |
|    n_updates            | 1970          |
|    policy_gradient_loss | -4.6e-06      |
|    value_loss           | 6.51          |
-------------------------------------------
Eval num_timesteps=3250000, episode_reward=6002.57 +/- 0.25
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 6e+03         |
| time/                   |               |
|    total_timesteps      | 3250000       |
| train/                  |               |
|    approx_kl            | 4.7620546e-05 |
|    clip_fraction        | 0.000739      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00807      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.925         |
|    n_updates            | 1980          |
|    policy_gradient_loss | -4.67e-05     |
|    value_loss           | 3.01          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.89e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 199      |
|    time_elapsed    | 14319    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=3683.65 +/- 2840.44
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.68e+03     |
| time/                   |              |
|    total_timesteps      | 3275000      |
| train/                  |              |
|    approx_kl            | 8.467445e-05 |
|    clip_fraction        | 0.000708     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.79         |
|    n_updates            | 1990         |
|    policy_gradient_loss | 3.02e-05     |
|    value_loss           | 61.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.89e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 200      |
|    time_elapsed    | 14395    |
|    total_timesteps | 3276800  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.83e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 201           |
|    time_elapsed         | 14459         |
|    total_timesteps      | 3293184       |
| train/                  |               |
|    approx_kl            | 5.0956598e-05 |
|    clip_fraction        | 0.000793      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.23          |
|    n_updates            | 2000          |
|    policy_gradient_loss | -7.02e-05     |
|    value_loss           | 47.5          |
-------------------------------------------
Eval num_timesteps=3300000, episode_reward=6002.66 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3300000     |
| train/                  |             |
|    approx_kl            | 0.029482858 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0434      |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 1.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.75e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 202      |
|    time_elapsed    | 14535    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=3739.47 +/- 4525.76
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.74e+03   |
| time/                   |            |
|    total_timesteps      | 3325000    |
| train/                  |            |
|    approx_kl            | 0.16637444 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.617      |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 73.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 203      |
|    time_elapsed    | 14611    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.66e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 204         |
|    time_elapsed         | 14674       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.002970751 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 348         |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.000509   |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=4842.47 +/- 2319.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 3350000      |
| train/                  |              |
|    approx_kl            | 0.0026160027 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.405       |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.0003       |
|    loss                 | 119          |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 983          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 205      |
|    time_elapsed    | 14750    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=6002.51 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.002136097 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.7        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 633         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 206      |
|    time_elapsed    | 14826    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 207         |
|    time_elapsed         | 14890       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.005433918 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 304         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 863         |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=6002.61 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3400000      |
| train/                  |              |
|    approx_kl            | 0.0036677602 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.359       |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 96.8         |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 417          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 208      |
|    time_elapsed    | 14966    |
|    total_timesteps | 3407872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.47e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 209          |
|    time_elapsed         | 15030        |
|    total_timesteps      | 3424256      |
| train/                  |              |
|    approx_kl            | 0.0018825319 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.321       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 143          |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 588          |
------------------------------------------
Eval num_timesteps=3425000, episode_reward=6002.56 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3425000      |
| train/                  |              |
|    approx_kl            | 0.0064789993 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 137          |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00345     |
|    value_loss           | 381          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.02e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 210      |
|    time_elapsed    | 15106    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=6002.38 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3450000      |
| train/                  |              |
|    approx_kl            | 0.0044341143 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 99.9         |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 486          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.73e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 211      |
|    time_elapsed    | 15181    |
|    total_timesteps | 3457024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.93e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 212          |
|    time_elapsed         | 15245        |
|    total_timesteps      | 3473408      |
| train/                  |              |
|    approx_kl            | 0.0038113818 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 127          |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 521          |
------------------------------------------
Eval num_timesteps=3475000, episode_reward=6002.39 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3475000      |
| train/                  |              |
|    approx_kl            | 0.0058152257 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.544       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 813          |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 731          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 213      |
|    time_elapsed    | 15321    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=6002.97 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3500000      |
| train/                  |              |
|    approx_kl            | 0.0031491923 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.333       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 58.8         |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 117          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.8e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 214      |
|    time_elapsed    | 15397    |
|    total_timesteps | 3506176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.9e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 215         |
|    time_elapsed         | 15461       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.004374762 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 2140        |
|    policy_gradient_loss | 8.31e-05    |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=3525000, episode_reward=6002.75 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.009340882 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 119         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.51e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 216      |
|    time_elapsed    | 15537    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=4842.96 +/- 2318.81
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.006098171 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.59        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 25.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.49e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 217      |
|    time_elapsed    | 15613    |
|    total_timesteps | 3555328  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.33e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 218          |
|    time_elapsed         | 15677        |
|    total_timesteps      | 3571712      |
| train/                  |              |
|    approx_kl            | 0.0027371035 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.4          |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 151          |
------------------------------------------
Eval num_timesteps=3575000, episode_reward=6002.57 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.001974846 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 2180        |
|    policy_gradient_loss | -3.73e-05   |
|    value_loss           | 257         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 219      |
|    time_elapsed    | 15752    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=6002.71 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.002444149 |
|    clip_fraction        | 0.0098      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0723     |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.000158   |
|    value_loss           | 29.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 220      |
|    time_elapsed    | 15828    |
|    total_timesteps | 3604480  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.8e+03       |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 221           |
|    time_elapsed         | 15892         |
|    total_timesteps      | 3620864       |
| train/                  |               |
|    approx_kl            | 0.00066435273 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.055        |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 50.6          |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000253     |
|    value_loss           | 30.4          |
-------------------------------------------
Eval num_timesteps=3625000, episode_reward=6002.47 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.006170679 |
|    clip_fraction        | 0.00853     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0474     |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.69        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.000453   |
|    value_loss           | 16.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.94e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 222      |
|    time_elapsed    | 15969    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=6002.26 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0010343583 |
|    clip_fraction        | 0.00378      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.03        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.9          |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000419    |
|    value_loss           | 38.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.83e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 223      |
|    time_elapsed    | 16045    |
|    total_timesteps | 3653632  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.83e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 224          |
|    time_elapsed         | 16109        |
|    total_timesteps      | 3670016      |
| train/                  |              |
|    approx_kl            | 0.0018870749 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0651      |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5          |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 25.2         |
------------------------------------------
Eval num_timesteps=3675000, episode_reward=6002.24 +/- 0.40
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 3675000    |
| train/                  |            |
|    approx_kl            | 0.03655658 |
|    clip_fraction        | 0.0222     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.36       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.00404   |
|    value_loss           | 14.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.67e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 225      |
|    time_elapsed    | 16184    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=6002.43 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3700000      |
| train/                  |              |
|    approx_kl            | 0.0024330579 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 34.5         |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.000547    |
|    value_loss           | 76.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 226      |
|    time_elapsed    | 16260    |
|    total_timesteps | 3702784  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.68e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 227           |
|    time_elapsed         | 16324         |
|    total_timesteps      | 3719168       |
| train/                  |               |
|    approx_kl            | 0.00058265147 |
|    clip_fraction        | 0.00632       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.198        |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | 33.7          |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.00047      |
|    value_loss           | 139           |
-------------------------------------------
Eval num_timesteps=3725000, episode_reward=6002.40 +/- 0.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3725000      |
| train/                  |              |
|    approx_kl            | 0.0058653327 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.345        |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 4.38         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.76e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 228      |
|    time_elapsed    | 16400    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=6002.74 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.012797918 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.591       |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 14.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.68e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 229      |
|    time_elapsed    | 16476    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.7e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 230         |
|    time_elapsed         | 16540       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.010985608 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.239      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.4        |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 91.6        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=6002.36 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 3775000    |
| train/                  |            |
|    approx_kl            | 0.00364973 |
|    clip_fraction        | 0.0309     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 2300       |
|    policy_gradient_loss | 0.00223    |
|    value_loss           | 68.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.5e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 231      |
|    time_elapsed    | 16616    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=6002.44 +/- 0.28
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 6e+03         |
| time/                   |               |
|    total_timesteps      | 3800000       |
| train/                  |               |
|    approx_kl            | 0.00052017224 |
|    clip_fraction        | 0.00257       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.107        |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.0003        |
|    loss                 | 88.7          |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000157     |
|    value_loss           | 176           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.5e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 232      |
|    time_elapsed    | 16691    |
|    total_timesteps | 3801088  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.58e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 233        |
|    time_elapsed         | 16755      |
|    total_timesteps      | 3817472    |
| train/                  |            |
|    approx_kl            | 0.10233111 |
|    clip_fraction        | 0.0751     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.251     |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.9       |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.00785   |
|    value_loss           | 43         |
----------------------------------------
Eval num_timesteps=3825000, episode_reward=5882.19 +/- 240.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.021244997 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.31        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 181         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.48e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 234      |
|    time_elapsed    | 16831    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=5882.44 +/- 239.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.014135627 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 93.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.58e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 235      |
|    time_elapsed    | 16907    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.52e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 236         |
|    time_elapsed         | 16971       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.013926138 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 38.6        |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=6002.45 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.006688008 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.35        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -7.44e-05   |
|    value_loss           | 39.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 237      |
|    time_elapsed    | 17047    |
|    total_timesteps | 3883008  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.67e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 238          |
|    time_elapsed         | 17111        |
|    total_timesteps      | 3899392      |
| train/                  |              |
|    approx_kl            | 0.0042505446 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.382       |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.27         |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 6.89         |
------------------------------------------
Eval num_timesteps=3900000, episode_reward=6002.21 +/- 0.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 3900000    |
| train/                  |            |
|    approx_kl            | 0.04471073 |
|    clip_fraction        | 0.0843     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.3        |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.00333   |
|    value_loss           | 82.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 239      |
|    time_elapsed    | 17187    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=6002.43 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.001878425 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 438         |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.000533    |
|    value_loss           | 903         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.81e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 240      |
|    time_elapsed    | 17263    |
|    total_timesteps | 3932160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.31e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 241          |
|    time_elapsed         | 17327        |
|    total_timesteps      | 3948544      |
| train/                  |              |
|    approx_kl            | 0.0029135144 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.881       |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.0003       |
|    loss                 | 85.4         |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 689          |
------------------------------------------
Eval num_timesteps=3950000, episode_reward=6002.50 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.046425655 |
|    clip_fraction        | 0.0661      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 270         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 676         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.57e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 242      |
|    time_elapsed    | 17402    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=6002.70 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 3975000      |
| train/                  |              |
|    approx_kl            | 0.0039537763 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.765       |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0003       |
|    loss                 | 175          |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00395     |
|    value_loss           | 330          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.48e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 243      |
|    time_elapsed    | 17478    |
|    total_timesteps | 3981312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.17e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 244          |
|    time_elapsed         | 17542        |
|    total_timesteps      | 3997696      |
| train/                  |              |
|    approx_kl            | 0.0075389575 |
|    clip_fraction        | 0.0753       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.777       |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.0003       |
|    loss                 | 55.9         |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.00234     |
|    value_loss           | 294          |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=6002.51 +/- 0.10
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4000000      |
| train/                  |              |
|    approx_kl            | 0.0055989344 |
|    clip_fraction        | 0.0733       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.762       |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.0003       |
|    loss                 | 108          |
|    n_updates            | 2440         |
|    policy_gradient_loss | 0.000422     |
|    value_loss           | 139          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.72e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 245      |
|    time_elapsed    | 17618    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=6002.63 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.010451399 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.69        |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.59e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 246      |
|    time_elapsed    | 17694    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.47e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 247         |
|    time_elapsed         | 17758       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.076205164 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00851    |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=6002.51 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.014018822 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.925      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.86        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 47          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 248      |
|    time_elapsed    | 17834    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=6002.42 +/- 0.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 4075000    |
| train/                  |            |
|    approx_kl            | 0.06584592 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.24       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 3.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.21e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 249      |
|    time_elapsed    | 17910    |
|    total_timesteps | 4079616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.92e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 250          |
|    time_elapsed         | 17974        |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0065383464 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.979       |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.0003       |
|    loss                 | 184          |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 471          |
------------------------------------------
Eval num_timesteps=4100000, episode_reward=6002.57 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4100000      |
| train/                  |              |
|    approx_kl            | 0.0076842224 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.648       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | 69.6         |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00277     |
|    value_loss           | 276          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.5e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 251      |
|    time_elapsed    | 18050    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=4842.69 +/- 2319.72
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 4125000      |
| train/                  |              |
|    approx_kl            | 0.0075425776 |
|    clip_fraction        | 0.0572       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.962       |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.0003       |
|    loss                 | 256          |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 772          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.29e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 252      |
|    time_elapsed    | 18126    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.65e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 253         |
|    time_elapsed         | 18190       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.006853427 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 184         |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 484         |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=6002.69 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.022715043 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 120         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 254      |
|    time_elapsed    | 18266    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=4842.69 +/- 2319.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.057351567 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.9        |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 53.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.11e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 255      |
|    time_elapsed    | 18342    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.42e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 256         |
|    time_elapsed         | 18406       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.010346551 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.000772   |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=6002.49 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4200000      |
| train/                  |              |
|    approx_kl            | 0.0055252174 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.417       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 4.9          |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000797    |
|    value_loss           | 73.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 257      |
|    time_elapsed    | 18482    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=6002.44 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.007428319 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 161         |
|    n_updates            | 2570        |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 159         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 258      |
|    time_elapsed    | 18557    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.42e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 259         |
|    time_elapsed         | 18621       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.029473096 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.23        |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 13.4        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=6002.55 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4250000      |
| train/                  |              |
|    approx_kl            | 0.0015228945 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 92.8         |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.000333    |
|    value_loss           | 388          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 260      |
|    time_elapsed    | 18697    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=4842.64 +/- 2319.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.014402276 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.4        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 194         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 261      |
|    time_elapsed    | 18773    |
|    total_timesteps | 4276224  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.34e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 262          |
|    time_elapsed         | 18837        |
|    total_timesteps      | 4292608      |
| train/                  |              |
|    approx_kl            | 0.0030988033 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 262          |
|    n_updates            | 2610         |
|    policy_gradient_loss | 0.000459     |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=4300000, episode_reward=6002.54 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4300000      |
| train/                  |              |
|    approx_kl            | 0.0028282264 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.416       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 295          |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 434          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.59e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 263      |
|    time_elapsed    | 18913    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=6002.53 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4325000      |
| train/                  |              |
|    approx_kl            | 0.0035003447 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | 3.03         |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.00817     |
|    value_loss           | 60.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.27e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 264      |
|    time_elapsed    | 18989    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.38e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 265         |
|    time_elapsed         | 19053       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.002073619 |
|    clip_fraction        | 0.00867     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0926     |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=4842.56 +/- 2319.66
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 4350000      |
| train/                  |              |
|    approx_kl            | 0.0008693021 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0994      |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 37.9         |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.000426    |
|    value_loss           | 77.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.83e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 266      |
|    time_elapsed    | 19129    |
|    total_timesteps | 4358144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.71e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 267          |
|    time_elapsed         | 19193        |
|    total_timesteps      | 4374528      |
| train/                  |              |
|    approx_kl            | 0.0006866149 |
|    clip_fraction        | 0.0052       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.25         |
|    n_updates            | 2660         |
|    policy_gradient_loss | -8.47e-05    |
|    value_loss           | 79.9         |
------------------------------------------
Eval num_timesteps=4375000, episode_reward=4842.63 +/- 2319.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.030723723 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0834     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.452       |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 29.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.72e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 268      |
|    time_elapsed    | 19269    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=6002.92 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.005346587 |
|    clip_fraction        | 0.00701     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0609     |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.4         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.000575   |
|    value_loss           | 92.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.66e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 269      |
|    time_elapsed    | 19345    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.72e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 270         |
|    time_elapsed         | 19409       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.012450528 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0888     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 144         |
|    n_updates            | 2690        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 88.3        |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=4842.56 +/- 2319.55
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 4.84e+03  |
| time/                   |           |
|    total_timesteps      | 4425000   |
| train/                  |           |
|    approx_kl            | 0.1115309 |
|    clip_fraction        | 0.132     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.169    |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.945     |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.00927  |
|    value_loss           | 1.81      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.37e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 271      |
|    time_elapsed    | 19484    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=6002.50 +/- 0.47
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4450000      |
| train/                  |              |
|    approx_kl            | 0.0031915333 |
|    clip_fraction        | 0.00805      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.0003       |
|    loss                 | 385          |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.000792    |
|    value_loss           | 979          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.35e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 272      |
|    time_elapsed    | 19560    |
|    total_timesteps | 4456448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.21e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 273          |
|    time_elapsed         | 19624        |
|    total_timesteps      | 4472832      |
| train/                  |              |
|    approx_kl            | 0.0023644255 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.0003       |
|    loss                 | 231          |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 893          |
------------------------------------------
Eval num_timesteps=4475000, episode_reward=4706.34 +/- 2591.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 4475000      |
| train/                  |              |
|    approx_kl            | 0.0030440823 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.445       |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0003       |
|    loss                 | 195          |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 775          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 274      |
|    time_elapsed    | 19700    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=4843.14 +/- 2319.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 0.0041043335 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.428       |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.0003       |
|    loss                 | 78.1         |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 573          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 275      |
|    time_elapsed    | 19776    |
|    total_timesteps | 4505600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.62e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 276          |
|    time_elapsed         | 19840        |
|    total_timesteps      | 4521984      |
| train/                  |              |
|    approx_kl            | 0.0034443638 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 137          |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 359          |
------------------------------------------
Eval num_timesteps=4525000, episode_reward=4842.86 +/- 2319.67
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 4525000      |
| train/                  |              |
|    approx_kl            | 0.0029324125 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.374       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 98.6         |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 416          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 277      |
|    time_elapsed    | 19916    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=6002.62 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4550000      |
| train/                  |              |
|    approx_kl            | 0.0051317317 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 176          |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.00337     |
|    value_loss           | 610          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 278      |
|    time_elapsed    | 19992    |
|    total_timesteps | 4554752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.49e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 279          |
|    time_elapsed         | 20056        |
|    total_timesteps      | 4571136      |
| train/                  |              |
|    approx_kl            | 0.0026201345 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0003       |
|    loss                 | 57.8         |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 391          |
------------------------------------------
Eval num_timesteps=4575000, episode_reward=6002.61 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.003786346 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.262      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.5        |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 287         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 280      |
|    time_elapsed    | 20132    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=6002.57 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4600000      |
| train/                  |              |
|    approx_kl            | 0.0012537895 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 452          |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.000688    |
|    value_loss           | 337          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.77e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 281      |
|    time_elapsed    | 20208    |
|    total_timesteps | 4603904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.06e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 282          |
|    time_elapsed         | 20271        |
|    total_timesteps      | 4620288      |
| train/                  |              |
|    approx_kl            | 0.0018262124 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 161          |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.000647    |
|    value_loss           | 369          |
------------------------------------------
Eval num_timesteps=4625000, episode_reward=6002.49 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4625000      |
| train/                  |              |
|    approx_kl            | 0.0018733467 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.294       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 191          |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 624          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.07e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 283      |
|    time_elapsed    | 20347    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=6002.52 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.002793174 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 79.5        |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 216         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.64e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 284      |
|    time_elapsed    | 20423    |
|    total_timesteps | 4653056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.89e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 285          |
|    time_elapsed         | 20487        |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0051806457 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.6         |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=4675000, episode_reward=6002.44 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4675000      |
| train/                  |              |
|    approx_kl            | 0.0059836917 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.8         |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.00447     |
|    value_loss           | 271          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 286      |
|    time_elapsed    | 20563    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=6002.80 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.010127502 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 54.4        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 164         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 287      |
|    time_elapsed    | 20639    |
|    total_timesteps | 4702208  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.63e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 288          |
|    time_elapsed         | 20703        |
|    total_timesteps      | 4718592      |
| train/                  |              |
|    approx_kl            | 0.0052384036 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.39         |
|    n_updates            | 2870         |
|    policy_gradient_loss | 0.000154     |
|    value_loss           | 60.6         |
------------------------------------------
Eval num_timesteps=4725000, episode_reward=6002.60 +/- 0.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4725000      |
| train/                  |              |
|    approx_kl            | 0.0010423788 |
|    clip_fraction        | 0.00994      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.05         |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.000285    |
|    value_loss           | 22.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.77e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 289      |
|    time_elapsed    | 20779    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=6002.49 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.006000326 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.000999   |
|    value_loss           | 71          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.82e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 290      |
|    time_elapsed    | 20854    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.87e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 291         |
|    time_elapsed         | 20919       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.004400399 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.000967   |
|    value_loss           | 13.1        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=6002.50 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.050636467 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.18        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 88.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.09e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 292      |
|    time_elapsed    | 20995    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=6002.35 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4800000      |
| train/                  |              |
|    approx_kl            | 0.0031742423 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.345       |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.0003       |
|    loss                 | 378          |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 615          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.73e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 293      |
|    time_elapsed    | 21071    |
|    total_timesteps | 4800512  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.91e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 294          |
|    time_elapsed         | 21135        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 0.0026871697 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.311       |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0003       |
|    loss                 | 194          |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 882          |
------------------------------------------
Eval num_timesteps=4825000, episode_reward=4843.08 +/- 2319.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.004287359 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 201         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 412         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 295      |
|    time_elapsed    | 21211    |
|    total_timesteps | 4833280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.2e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 296          |
|    time_elapsed         | 21275        |
|    total_timesteps      | 4849664      |
| train/                  |              |
|    approx_kl            | 0.0016238784 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 221          |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00172     |
|    value_loss           | 565          |
------------------------------------------
Eval num_timesteps=4850000, episode_reward=6002.54 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.022616457 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.3        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 234         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.04e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 297      |
|    time_elapsed    | 21350    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=6002.73 +/- 0.67
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4875000      |
| train/                  |              |
|    approx_kl            | 0.0034101414 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 68.9         |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 194          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.4e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 298      |
|    time_elapsed    | 21426    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.71e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 299         |
|    time_elapsed         | 21490       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.001535227 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.4        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.000716   |
|    value_loss           | 274         |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=6002.82 +/- 0.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 4900000      |
| train/                  |              |
|    approx_kl            | 0.0037713218 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 82.1         |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 240          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 300      |
|    time_elapsed    | 21566    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=6002.24 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.002820489 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.3        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.000476   |
|    value_loss           | 263         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.94e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 301      |
|    time_elapsed    | 21642    |
|    total_timesteps | 4931584  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.01e+03      |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 302           |
|    time_elapsed         | 21706         |
|    total_timesteps      | 4947968       |
| train/                  |               |
|    approx_kl            | 0.00038442295 |
|    clip_fraction        | 0.00372       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.075        |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0003        |
|    loss                 | 45.5          |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.000734     |
|    value_loss           | 185           |
-------------------------------------------
Eval num_timesteps=4950000, episode_reward=6002.51 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.001876552 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.3        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.000949   |
|    value_loss           | 210         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.15e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 303      |
|    time_elapsed    | 21782    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=6002.28 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.002270962 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 235         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.46e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 304      |
|    time_elapsed    | 21858    |
|    total_timesteps | 4980736  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.44e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 305          |
|    time_elapsed         | 21922        |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0057250634 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0992      |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.965        |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.00655     |
|    value_loss           | 23.8         |
------------------------------------------
Eval num_timesteps=5000000, episode_reward=6002.54 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.010060555 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0747     |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.424       |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 306      |
|    time_elapsed    | 21997    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=6002.18 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5025000      |
| train/                  |              |
|    approx_kl            | 0.0048062787 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.101       |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 23.9         |
|    n_updates            | 3060         |
|    policy_gradient_loss | 4.37e-05     |
|    value_loss           | 99.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.68e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 307      |
|    time_elapsed    | 22073    |
|    total_timesteps | 5029888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.74e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 308          |
|    time_elapsed         | 22138        |
|    total_timesteps      | 5046272      |
| train/                  |              |
|    approx_kl            | 0.0032587512 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0433      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.268        |
|    n_updates            | 3070         |
|    policy_gradient_loss | -0.00922     |
|    value_loss           | 22.8         |
------------------------------------------
Eval num_timesteps=5050000, episode_reward=6002.60 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5050000      |
| train/                  |              |
|    approx_kl            | 0.0043281866 |
|    clip_fraction        | 0.00245      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0151      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.44         |
|    n_updates            | 3080         |
|    policy_gradient_loss | -8.57e-05    |
|    value_loss           | 63.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.58e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 309      |
|    time_elapsed    | 22214    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=6002.46 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5075000      |
| train/                  |              |
|    approx_kl            | 0.0012654724 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0541      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.23         |
|    n_updates            | 3090         |
|    policy_gradient_loss | 0.00209      |
|    value_loss           | 114          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.66e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 310      |
|    time_elapsed    | 22289    |
|    total_timesteps | 5079040  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.6e+03       |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 311           |
|    time_elapsed         | 22353         |
|    total_timesteps      | 5095424       |
| train/                  |               |
|    approx_kl            | 0.00027411486 |
|    clip_fraction        | 0.00375       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0332       |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0003        |
|    loss                 | 6.01          |
|    n_updates            | 3100          |
|    policy_gradient_loss | -5.97e-05     |
|    value_loss           | 8.75          |
-------------------------------------------
Eval num_timesteps=5100000, episode_reward=2265.93 +/- 4921.89
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.27e+03   |
| time/                   |            |
|    total_timesteps      | 5100000    |
| train/                  |            |
|    approx_kl            | 0.18581674 |
|    clip_fraction        | 0.0723     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0801    |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.313      |
|    n_updates            | 3110       |
|    policy_gradient_loss | -0.0088    |
|    value_loss           | 63.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 312      |
|    time_elapsed    | 22429    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=3571.86 +/- 4861.75
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.57e+03   |
| time/                   |            |
|    total_timesteps      | 5125000    |
| train/                  |            |
|    approx_kl            | 0.00935244 |
|    clip_fraction        | 0.0381     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 185        |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.0022    |
|    value_loss           | 847        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 313      |
|    time_elapsed    | 22505    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.84e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 314         |
|    time_elapsed         | 22569       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.005780332 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 604         |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=6002.49 +/- 0.40
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5150000      |
| train/                  |              |
|    approx_kl            | 0.0071044913 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.388       |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.0003       |
|    loss                 | 90.7         |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 686          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 531      |
| time/              |          |
|    fps             | 227      |
|    iterations      | 315      |
|    time_elapsed    | 22645    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=4842.78 +/- 2319.15
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.84e+03   |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.00551288 |
|    clip_fraction        | 0.0331     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 240        |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.00175   |
|    value_loss           | 625        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 969      |
| time/              |          |
|    fps             | 227      |
|    iterations      | 316      |
|    time_elapsed    | 22721    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.56e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 317         |
|    time_elapsed         | 22785       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.004821172 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.000311   |
|    value_loss           | 378         |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=6002.50 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.001838938 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.000669   |
|    value_loss           | 390         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 318      |
|    time_elapsed    | 22861    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=4706.35 +/- 2591.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.71e+03    |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.008459741 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 186         |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 495         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 319      |
|    time_elapsed    | 22937    |
|    total_timesteps | 5226496  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.61e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 320          |
|    time_elapsed         | 23001        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0014475584 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.164       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.2         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00199     |
|    value_loss           | 394          |
------------------------------------------
Eval num_timesteps=5250000, episode_reward=6002.59 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5250000      |
| train/                  |              |
|    approx_kl            | 0.0034256286 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 138          |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 317          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.52e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 321      |
|    time_elapsed    | 23077    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=4842.86 +/- 2319.62
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 5275000      |
| train/                  |              |
|    approx_kl            | 0.0033069388 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.24        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 241          |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 364          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.51e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 322      |
|    time_elapsed    | 23153    |
|    total_timesteps | 5275648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.43e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 323        |
|    time_elapsed         | 23217      |
|    total_timesteps      | 5292032    |
| train/                  |            |
|    approx_kl            | 0.00104933 |
|    clip_fraction        | 0.0109     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 229        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.00259   |
|    value_loss           | 256        |
----------------------------------------
Eval num_timesteps=5300000, episode_reward=6002.69 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.004102315 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.8        |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 218         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.45e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 324      |
|    time_elapsed    | 23292    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.11e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 325         |
|    time_elapsed         | 23357       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.008702194 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 75.2        |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 94.1        |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=4706.21 +/- 2591.98
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 5325000      |
| train/                  |              |
|    approx_kl            | 0.0020083794 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0427      |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 211          |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 229          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.25e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 326      |
|    time_elapsed    | 23432    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=6002.44 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5350000      |
| train/                  |              |
|    approx_kl            | 0.0035994807 |
|    clip_fraction        | 0.00582      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0249      |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 8.07         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.000676    |
|    value_loss           | 43           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.75e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 327      |
|    time_elapsed    | 23508    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.92e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 328         |
|    time_elapsed         | 23572       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.003446951 |
|    clip_fraction        | 0.00546     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0304     |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.7        |
|    n_updates            | 3270        |
|    policy_gradient_loss | 0.000498    |
|    value_loss           | 59.5        |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=6002.53 +/- 0.37
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5375000      |
| train/                  |              |
|    approx_kl            | 0.0013841907 |
|    clip_fraction        | 0.00946      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0238      |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0003       |
|    loss                 | 7.45         |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.00744     |
|    value_loss           | 19.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.82e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 329      |
|    time_elapsed    | 23648    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=4842.57 +/- 2319.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.016418248 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0703     |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.93        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 28.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.81e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 330      |
|    time_elapsed    | 23724    |
|    total_timesteps | 5406720  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.8e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 331          |
|    time_elapsed         | 23788        |
|    total_timesteps      | 5423104      |
| train/                  |              |
|    approx_kl            | 0.0014371772 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 15           |
|    n_updates            | 3300         |
|    policy_gradient_loss | -8.03e-05    |
|    value_loss           | 48.2         |
------------------------------------------
Eval num_timesteps=5425000, episode_reward=6002.37 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5425000      |
| train/                  |              |
|    approx_kl            | 0.0026278156 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.637        |
|    n_updates            | 3310         |
|    policy_gradient_loss | 0.000819     |
|    value_loss           | 19.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.67e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 332      |
|    time_elapsed    | 23864    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=6002.63 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5450000     |
| train/                  |             |
|    approx_kl            | 0.021793341 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.618       |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 69.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.74e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 333      |
|    time_elapsed    | 23940    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.75e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 334         |
|    time_elapsed         | 24005       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.005395716 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.249       |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.000986   |
|    value_loss           | 4.17        |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=6002.48 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.019365171 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 52.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.74e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 335      |
|    time_elapsed    | 24081    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=6002.34 +/- 0.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 5500000    |
| train/                  |            |
|    approx_kl            | 0.01383556 |
|    clip_fraction        | 0.0475     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 3350       |
|    policy_gradient_loss | 0.000294   |
|    value_loss           | 3.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.84e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 336      |
|    time_elapsed    | 24157    |
|    total_timesteps | 5505024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.55e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 337          |
|    time_elapsed         | 24221        |
|    total_timesteps      | 5521408      |
| train/                  |              |
|    approx_kl            | 0.0054130205 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 2.86         |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00049     |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=5525000, episode_reward=6002.10 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5525000      |
| train/                  |              |
|    approx_kl            | 0.0058010584 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00021     |
|    value_loss           | 115          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.58e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 338      |
|    time_elapsed    | 24297    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=6003.05 +/- 0.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5550000      |
| train/                  |              |
|    approx_kl            | 0.0004620111 |
|    clip_fraction        | 0.00411      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0811      |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0003       |
|    loss                 | 3            |
|    n_updates            | 3380         |
|    policy_gradient_loss | -3.13e-05    |
|    value_loss           | 58.4         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.61e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 339      |
|    time_elapsed    | 24373    |
|    total_timesteps | 5554176  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.73e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 340          |
|    time_elapsed         | 24437        |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 0.0139395995 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.113       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.986        |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000619    |
|    value_loss           | 27.7         |
------------------------------------------
Eval num_timesteps=5575000, episode_reward=6002.50 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.018871184 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0914     |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.0367     |
|    value_loss           | 52.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.79e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 341      |
|    time_elapsed    | 24513    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=5762.70 +/- 293.34
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.76e+03   |
| time/                   |            |
|    total_timesteps      | 5600000    |
| train/                  |            |
|    approx_kl            | 0.07097851 |
|    clip_fraction        | 0.0511     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0817    |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.603      |
|    n_updates            | 3410       |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 2.29       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 342      |
|    time_elapsed    | 24589    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.37e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 343         |
|    time_elapsed         | 24653       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.021750554 |
|    clip_fraction        | 0.0658      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=6002.52 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5625000      |
| train/                  |              |
|    approx_kl            | 0.0028317545 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 48.1         |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 602          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 344      |
|    time_elapsed    | 24729    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=4842.34 +/- 2319.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.008087279 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 891         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.76e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 345      |
|    time_elapsed    | 24805    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.56e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 346         |
|    time_elapsed         | 24869       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.009412086 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.436      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 371         |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=4722.94 +/- 2558.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.72e+03    |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.007013722 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.3        |
|    n_updates            | 3460        |
|    policy_gradient_loss | 0.000491    |
|    value_loss           | 182         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.87e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 347      |
|    time_elapsed    | 24945    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=6002.45 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5700000     |
| train/                  |             |
|    approx_kl            | 0.002387693 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 155         |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 370         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.41e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 348      |
|    time_elapsed    | 25021    |
|    total_timesteps | 5701632  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.06e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 349          |
|    time_elapsed         | 25085        |
|    total_timesteps      | 5718016      |
| train/                  |              |
|    approx_kl            | 0.0046461606 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.316       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 50.7         |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 267          |
------------------------------------------
Eval num_timesteps=5725000, episode_reward=6002.54 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.010328192 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 98.2        |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 217         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.33e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 350      |
|    time_elapsed    | 25161    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=6002.55 +/- 0.07
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 5750000      |
| train/                  |              |
|    approx_kl            | 0.0013650716 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.8          |
|    n_updates            | 3500         |
|    policy_gradient_loss | 0.000684     |
|    value_loss           | 33.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.65e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 351      |
|    time_elapsed    | 25237    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.78e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 352         |
|    time_elapsed         | 25301       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.016502721 |
|    clip_fraction        | 0.0476      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 72.4        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=4883.29 +/- 2237.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.88e+03    |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.013303518 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0942     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.000929   |
|    value_loss           | 9.39        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.75e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 353      |
|    time_elapsed    | 25377    |
|    total_timesteps | 5783552  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.75e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 354          |
|    time_elapsed         | 25441        |
|    total_timesteps      | 5799936      |
| train/                  |              |
|    approx_kl            | 0.0068994327 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.25         |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.000263    |
|    value_loss           | 53.3         |
------------------------------------------
Eval num_timesteps=5800000, episode_reward=6002.49 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.002646533 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0958     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.000589   |
|    value_loss           | 13.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.87e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 355      |
|    time_elapsed    | 25517    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=6002.42 +/- 0.18
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 5825000    |
| train/                  |            |
|    approx_kl            | 0.26156476 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0954     |
|    n_updates            | 3550       |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 8.75       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.89e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 356      |
|    time_elapsed    | 25593    |
|    total_timesteps | 5832704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.23e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 357        |
|    time_elapsed         | 25657      |
|    total_timesteps      | 5849088    |
| train/                  |            |
|    approx_kl            | 0.18156776 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.467     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0047    |
|    value_loss           | 51.2       |
----------------------------------------
Eval num_timesteps=5850000, episode_reward=6002.55 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.001889001 |
|    clip_fraction        | 0.00961     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.484      |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 222         |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.000416   |
|    value_loss           | 1.03e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.48e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 358      |
|    time_elapsed    | 25733    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=5882.40 +/- 240.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.88e+03     |
| time/                   |              |
|    total_timesteps      | 5875000      |
| train/                  |              |
|    approx_kl            | 0.0032479428 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.609       |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0003       |
|    loss                 | 185          |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 518          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.68e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 359      |
|    time_elapsed    | 25809    |
|    total_timesteps | 5881856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.28e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 360          |
|    time_elapsed         | 25873        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0051002735 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.0003       |
|    loss                 | 130          |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.000983    |
|    value_loss           | 398          |
------------------------------------------
Eval num_timesteps=5900000, episode_reward=6002.45 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.014733986 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.5        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 476         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.78e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 361      |
|    time_elapsed    | 25949    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=5882.74 +/- 240.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 5925000     |
| train/                  |             |
|    approx_kl            | 0.028706644 |
|    clip_fraction        | 0.0552      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 364         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.58e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 362      |
|    time_elapsed    | 26025    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.02e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 363         |
|    time_elapsed         | 26089       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.006208103 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.36       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 3620        |
|    policy_gradient_loss | 0.000671    |
|    value_loss           | 56.9        |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=6002.94 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.009514481 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.2        |
|    n_updates            | 3630        |
|    policy_gradient_loss | 0.000193    |
|    value_loss           | 69.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.36e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 364      |
|    time_elapsed    | 26165    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=5882.34 +/- 240.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.005088246 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.8        |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 92.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 365      |
|    time_elapsed    | 26241    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.74e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 366         |
|    time_elapsed         | 26305       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.020008245 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 24          |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=5882.52 +/- 240.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.006441594 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 39.2        |
|    n_updates            | 3660        |
|    policy_gradient_loss | 0.000419    |
|    value_loss           | 176         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 367      |
|    time_elapsed    | 26381    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=5882.48 +/- 240.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.88e+03    |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.010718314 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 32.3        |
|    n_updates            | 3670        |
|    policy_gradient_loss | 0.000888    |
|    value_loss           | 156         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.74e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 368      |
|    time_elapsed    | 26456    |
|    total_timesteps | 6029312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.19e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 369        |
|    time_elapsed         | 26520      |
|    total_timesteps      | 6045696    |
| train/                  |            |
|    approx_kl            | 0.15213472 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 5.43       |
----------------------------------------
Eval num_timesteps=6050000, episode_reward=4706.59 +/- 2591.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 6050000      |
| train/                  |              |
|    approx_kl            | 0.0086557865 |
|    clip_fraction        | 0.0677       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.0003       |
|    loss                 | 833          |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 1.43e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 370      |
|    time_elapsed    | 26596    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=3410.69 +/- 3174.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41e+03    |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.005088227 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 91.3        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00156    |
|    value_loss           | 839         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 371      |
|    time_elapsed    | 26672    |
|    total_timesteps | 6078464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 289          |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 372          |
|    time_elapsed         | 26736        |
|    total_timesteps      | 6094848      |
| train/                  |              |
|    approx_kl            | 0.0061861025 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0003       |
|    loss                 | 884          |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.00415     |
|    value_loss           | 941          |
------------------------------------------
Eval num_timesteps=6100000, episode_reward=4706.53 +/- 2592.03
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 6100000      |
| train/                  |              |
|    approx_kl            | 0.0030339055 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0003       |
|    loss                 | 379          |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 1.19e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 373      |
|    time_elapsed    | 26812    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=1074.61 +/- 6035.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.07e+03     |
| time/                   |              |
|    total_timesteps      | 6125000      |
| train/                  |              |
|    approx_kl            | 0.0051334174 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.97        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 129          |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 979          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 374      |
|    time_elapsed    | 26888    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 375         |
|    time_elapsed         | 26952       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.004695282 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 748         |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=1282.87 +/- 5781.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.006407347 |
|    clip_fraction        | 0.0571      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 172         |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 603         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 376      |
|    time_elapsed    | 27028    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=6002.43 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.005327332 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 562         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 377      |
|    time_elapsed    | 27104    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 2.38e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 378         |
|    time_elapsed         | 27168       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.007140725 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 548         |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=6002.41 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6200000      |
| train/                  |              |
|    approx_kl            | 0.0054170806 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.851       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 516          |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 606          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 379      |
|    time_elapsed    | 27243    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=6002.29 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6225000      |
| train/                  |              |
|    approx_kl            | 0.0035324083 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 249          |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 439          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.5e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 380      |
|    time_elapsed    | 27319    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.19e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 381         |
|    time_elapsed         | 27383       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.008874841 |
|    clip_fraction        | 0.0546      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 600         |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=3683.46 +/- 2840.05
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.68e+03     |
| time/                   |              |
|    total_timesteps      | 6250000      |
| train/                  |              |
|    approx_kl            | 0.0022097654 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 70.9         |
|    n_updates            | 3810         |
|    policy_gradient_loss | -0.000924    |
|    value_loss           | 193          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.32e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 382      |
|    time_elapsed    | 27459    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=6002.83 +/- 0.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6275000      |
| train/                  |              |
|    approx_kl            | 0.0024108407 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.856       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 270          |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 681          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.33e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 383      |
|    time_elapsed    | 27535    |
|    total_timesteps | 6275072  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.6e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 384          |
|    time_elapsed         | 27599        |
|    total_timesteps      | 6291456      |
| train/                  |              |
|    approx_kl            | 0.0022010917 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.725       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 75.8         |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 300          |
------------------------------------------
Eval num_timesteps=6300000, episode_reward=6002.32 +/- 0.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6300000      |
| train/                  |              |
|    approx_kl            | 0.0028385906 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 522          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.54e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 385      |
|    time_elapsed    | 27675    |
|    total_timesteps | 6307840  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.14e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 386          |
|    time_elapsed         | 27739        |
|    total_timesteps      | 6324224      |
| train/                  |              |
|    approx_kl            | 0.0040800483 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.3         |
|    n_updates            | 3850         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 272          |
------------------------------------------
Eval num_timesteps=6325000, episode_reward=6002.43 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6325000      |
| train/                  |              |
|    approx_kl            | 0.0036229263 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 23.4         |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.00339     |
|    value_loss           | 119          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.15e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 387      |
|    time_elapsed    | 27814    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=6002.39 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6350000      |
| train/                  |              |
|    approx_kl            | 0.0020231516 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 40.4         |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.0009      |
|    value_loss           | 170          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.44e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 388      |
|    time_elapsed    | 27890    |
|    total_timesteps | 6356992  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.79e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 389          |
|    time_elapsed         | 27954        |
|    total_timesteps      | 6373376      |
| train/                  |              |
|    approx_kl            | 0.0117501635 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 16.8         |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 86.7         |
------------------------------------------
Eval num_timesteps=6375000, episode_reward=4842.79 +/- 2319.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 6375000     |
| train/                  |             |
|    approx_kl            | 0.015529723 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.22        |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 23.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.74e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 390      |
|    time_elapsed    | 28030    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=4842.61 +/- 2319.55
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 6400000      |
| train/                  |              |
|    approx_kl            | 0.0026396057 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0003       |
|    loss                 | 85.9         |
|    n_updates            | 3900         |
|    policy_gradient_loss | -2.11e-05    |
|    value_loss           | 144          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.69e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 391      |
|    time_elapsed    | 28106    |
|    total_timesteps | 6406144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.7e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 392         |
|    time_elapsed         | 28170       |
|    total_timesteps      | 6422528     |
| train/                  |             |
|    approx_kl            | 0.011545196 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.12        |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 89.5        |
-----------------------------------------
Eval num_timesteps=6425000, episode_reward=4842.98 +/- 2319.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 6425000      |
| train/                  |              |
|    approx_kl            | 0.0020775914 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 314          |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.000884    |
|    value_loss           | 107          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.54e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 393      |
|    time_elapsed    | 28246    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=4843.26 +/- 2318.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.010338455 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 226         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 394      |
|    time_elapsed    | 28322    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.64e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 395         |
|    time_elapsed         | 28386       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.004298728 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.1        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 88.4        |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=6002.48 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6475000      |
| train/                  |              |
|    approx_kl            | 0.0045422367 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.000386    |
|    value_loss           | 103          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.62e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 396      |
|    time_elapsed    | 28462    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=6002.46 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6500000      |
| train/                  |              |
|    approx_kl            | 0.0065723443 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.198       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.04         |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.000643    |
|    value_loss           | 10.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.7e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 397      |
|    time_elapsed    | 28538    |
|    total_timesteps | 6504448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.71e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 398          |
|    time_elapsed         | 28601        |
|    total_timesteps      | 6520832      |
| train/                  |              |
|    approx_kl            | 0.0042209355 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.375       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.98         |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.00071     |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=6525000, episode_reward=6002.20 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.010551564 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 107         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.65e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 399      |
|    time_elapsed    | 28677    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=6002.60 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.009092136 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.874       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00066    |
|    value_loss           | 68.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 400      |
|    time_elapsed    | 28753    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 401         |
|    time_elapsed         | 28818       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.019110817 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.58        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 95.1        |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=4842.72 +/- 2319.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 6575000     |
| train/                  |             |
|    approx_kl            | 0.008421783 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 644         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.52e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 402      |
|    time_elapsed    | 28894    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=6002.76 +/- 0.70
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6600000      |
| train/                  |              |
|    approx_kl            | 0.0090210615 |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.421       |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 538          |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00599     |
|    value_loss           | 730          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.25e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 403      |
|    time_elapsed    | 28970    |
|    total_timesteps | 6602752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.42e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 404        |
|    time_elapsed         | 29034      |
|    total_timesteps      | 6619136    |
| train/                  |            |
|    approx_kl            | 0.00371301 |
|    clip_fraction        | 0.0287     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 130        |
|    n_updates            | 4030       |
|    policy_gradient_loss | -0.00136   |
|    value_loss           | 338        |
----------------------------------------
Eval num_timesteps=6625000, episode_reward=6002.83 +/- 0.64
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6625000      |
| train/                  |              |
|    approx_kl            | 0.0050031254 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.9         |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 43.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.87e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 405      |
|    time_elapsed    | 29110    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=2973.21 +/- 3875.93
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.97e+03     |
| time/                   |              |
|    total_timesteps      | 6650000      |
| train/                  |              |
|    approx_kl            | 0.0123598175 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 183          |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.00691     |
|    value_loss           | 302          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.28e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 406      |
|    time_elapsed    | 29186    |
|    total_timesteps | 6651904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.39e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 407         |
|    time_elapsed         | 29250       |
|    total_timesteps      | 6668288     |
| train/                  |             |
|    approx_kl            | 0.003127039 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 331         |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 367         |
-----------------------------------------
Eval num_timesteps=6675000, episode_reward=6002.50 +/- 0.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6675000      |
| train/                  |              |
|    approx_kl            | 0.0120629445 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.32        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 70.2         |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 103          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.54e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 408      |
|    time_elapsed    | 29327    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=6002.43 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 6700000    |
| train/                  |            |
|    approx_kl            | 0.04336715 |
|    clip_fraction        | 0.0912     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.466     |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 24.8       |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.00439   |
|    value_loss           | 13.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 409      |
|    time_elapsed    | 29403    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.5e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 410         |
|    time_elapsed         | 29467       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.008573981 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 176         |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=5882.33 +/- 240.22
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.88e+03     |
| time/                   |              |
|    total_timesteps      | 6725000      |
| train/                  |              |
|    approx_kl            | 0.0042829094 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 126          |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 270          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 411      |
|    time_elapsed    | 29543    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=6002.21 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.018075872 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.28        |
|    n_updates            | 4110        |
|    policy_gradient_loss | 0.000365    |
|    value_loss           | 67.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 412      |
|    time_elapsed    | 29619    |
|    total_timesteps | 6750208  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.12e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 413          |
|    time_elapsed         | 29683        |
|    total_timesteps      | 6766592      |
| train/                  |              |
|    approx_kl            | 0.0036400415 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.411       |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.0003       |
|    loss                 | 98.3         |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 404          |
------------------------------------------
Eval num_timesteps=6775000, episode_reward=6002.57 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.008099032 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 144         |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 354         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.08e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 414      |
|    time_elapsed    | 29760    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 415         |
|    time_elapsed         | 29825       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.005823682 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.0049     |
|    value_loss           | 331         |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=6002.50 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6800000      |
| train/                  |              |
|    approx_kl            | 0.0069120014 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.31        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.6          |
|    n_updates            | 4150         |
|    policy_gradient_loss | -0.00044     |
|    value_loss           | 8.86         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.35e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 416      |
|    time_elapsed    | 29903    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=6002.46 +/- 0.40
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.004533061 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.2        |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 289         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.77e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 417      |
|    time_elapsed    | 29981    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.81e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 418         |
|    time_elapsed         | 30046       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.012936226 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.71        |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 36.9        |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=6002.40 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 6850000    |
| train/                  |            |
|    approx_kl            | 0.01928813 |
|    clip_fraction        | 0.035      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 55.1       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.7e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 419      |
|    time_elapsed    | 30123    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=6002.32 +/- 0.12
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6875000      |
| train/                  |              |
|    approx_kl            | 0.0063269846 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.647        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.6         |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 147          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.51e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 420      |
|    time_elapsed    | 30201    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.31e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 421         |
|    time_elapsed         | 30266       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.015975837 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 37.3        |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=6002.71 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.003990454 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.58        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 198         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.28e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 422      |
|    time_elapsed    | 30343    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=6002.55 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 6925000      |
| train/                  |              |
|    approx_kl            | 0.0031145592 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.48         |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.000692    |
|    value_loss           | 75.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.38e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 423      |
|    time_elapsed    | 30421    |
|    total_timesteps | 6930432  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.54e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 424          |
|    time_elapsed         | 30486        |
|    total_timesteps      | 6946816      |
| train/                  |              |
|    approx_kl            | 0.0021421895 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.4          |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.000411    |
|    value_loss           | 47           |
------------------------------------------
Eval num_timesteps=6950000, episode_reward=6002.68 +/- 0.53
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6950000     |
| train/                  |             |
|    approx_kl            | 0.005626204 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.79        |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 169         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.62e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 425      |
|    time_elapsed    | 30563    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=6002.16 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 6975000     |
| train/                  |             |
|    approx_kl            | 0.003335092 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 207         |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 266         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.7e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 426      |
|    time_elapsed    | 30641    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.76e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 427         |
|    time_elapsed         | 30706       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.010287888 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.77        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 42          |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=4842.64 +/- 2319.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.008872854 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.41        |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.000638   |
|    value_loss           | 13.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.81e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 428      |
|    time_elapsed    | 30783    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=6002.33 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.012699911 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 19.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.85e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 429      |
|    time_elapsed    | 30860    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.79e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 430         |
|    time_elapsed         | 30926       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.026582334 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.966       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 49.7        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=4924.81 +/- 2155.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.92e+03    |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.008688705 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.386      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 4300        |
|    policy_gradient_loss | 4.46e-06    |
|    value_loss           | 32.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.62e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 431      |
|    time_elapsed    | 31003    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=4706.48 +/- 2592.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.71e+03    |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.041981556 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.748       |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 20.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.76e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 432      |
|    time_elapsed    | 31080    |
|    total_timesteps | 7077888  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.02e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 433          |
|    time_elapsed         | 31145        |
|    total_timesteps      | 7094272      |
| train/                  |              |
|    approx_kl            | 0.0023815508 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.408       |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.0003       |
|    loss                 | 348          |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 1.03e+03     |
------------------------------------------
Eval num_timesteps=7100000, episode_reward=6002.46 +/- 0.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7100000      |
| train/                  |              |
|    approx_kl            | 0.0026407405 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.0003       |
|    loss                 | 401          |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 732          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.56e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 434      |
|    time_elapsed    | 31222    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=6002.31 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7125000      |
| train/                  |              |
|    approx_kl            | 0.0010848137 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 181          |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 407          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 435      |
|    time_elapsed    | 31300    |
|    total_timesteps | 7127040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 3.71e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 436        |
|    time_elapsed         | 31366      |
|    total_timesteps      | 7143424    |
| train/                  |            |
|    approx_kl            | 0.01954203 |
|    clip_fraction        | 0.0535     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 219        |
|    n_updates            | 4350       |
|    policy_gradient_loss | -0.00809   |
|    value_loss           | 582        |
----------------------------------------
Eval num_timesteps=7150000, episode_reward=6002.25 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.010946404 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 518         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.23e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 437      |
|    time_elapsed    | 31443    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=4843.24 +/- 2319.69
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 7175000      |
| train/                  |              |
|    approx_kl            | 0.0065456517 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.415       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.8         |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.000327    |
|    value_loss           | 325          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.71e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 438      |
|    time_elapsed    | 31522    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 439         |
|    time_elapsed         | 31587       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.003957198 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 72.8        |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 78          |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=6002.59 +/- 0.15
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7200000      |
| train/                  |              |
|    approx_kl            | 0.0034433305 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.268       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 8.97         |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.000128    |
|    value_loss           | 87.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.49e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 440      |
|    time_elapsed    | 31664    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=6002.53 +/- 0.50
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7225000      |
| train/                  |              |
|    approx_kl            | 0.0009474198 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 35.3         |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.000366    |
|    value_loss           | 170          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.49e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 441      |
|    time_elapsed    | 31740    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.6e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 442         |
|    time_elapsed         | 31804       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.011320224 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.83        |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.000791   |
|    value_loss           | 24.5        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=6002.28 +/- 0.39
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7250000      |
| train/                  |              |
|    approx_kl            | 0.0067063486 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 23.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.6e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 443      |
|    time_elapsed    | 31880    |
|    total_timesteps | 7258112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.58e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 444          |
|    time_elapsed         | 31945        |
|    total_timesteps      | 7274496      |
| train/                  |              |
|    approx_kl            | 0.0071166223 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.236       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 22.8         |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.000373    |
|    value_loss           | 61.5         |
------------------------------------------
Eval num_timesteps=7275000, episode_reward=5882.64 +/- 240.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.88e+03   |
| time/                   |            |
|    total_timesteps      | 7275000    |
| train/                  |            |
|    approx_kl            | 0.07992299 |
|    clip_fraction        | 0.0769     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.874      |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 61         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.62e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 445      |
|    time_elapsed    | 32021    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=6002.33 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7300000     |
| train/                  |             |
|    approx_kl            | 0.009203429 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.32        |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 28          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.73e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 446      |
|    time_elapsed    | 32097    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.62e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 447         |
|    time_elapsed         | 32162       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.024028312 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.718       |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 68.8        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=6002.31 +/- 0.16
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7325000      |
| train/                  |              |
|    approx_kl            | 0.0020359869 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.000319    |
|    value_loss           | 107          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 448      |
|    time_elapsed    | 32238    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=6002.27 +/- 0.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 7350000    |
| train/                  |            |
|    approx_kl            | 0.11385849 |
|    clip_fraction        | 0.078      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.287     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.00973   |
|    value_loss           | 7.78       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.67e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 449      |
|    time_elapsed    | 32314    |
|    total_timesteps | 7356416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.51e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 450        |
|    time_elapsed         | 32378      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.11341048 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.436     |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.766      |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.007     |
|    value_loss           | 4.24       |
----------------------------------------
Eval num_timesteps=7375000, episode_reward=6002.42 +/- 0.17
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 7375000    |
| train/                  |            |
|    approx_kl            | 0.09302381 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.0379    |
|    value_loss           | 95.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.38e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 451      |
|    time_elapsed    | 32455    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=5522.10 +/- 449.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.52e+03     |
| time/                   |              |
|    total_timesteps      | 7400000      |
| train/                  |              |
|    approx_kl            | 0.0016784865 |
|    clip_fraction        | 0.00995      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.445       |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.0003       |
|    loss                 | 186          |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000423    |
|    value_loss           | 417          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.2e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 452      |
|    time_elapsed    | 32531    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.11e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 453         |
|    time_elapsed         | 32595       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.015325204 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.8        |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 331         |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=6002.72 +/- 0.41
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7425000      |
| train/                  |              |
|    approx_kl            | 0.0077466858 |
|    clip_fraction        | 0.0646       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 237          |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.00181     |
|    value_loss           | 182          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 454      |
|    time_elapsed    | 32672    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=6002.42 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 7450000    |
| train/                  |            |
|    approx_kl            | 0.00986719 |
|    clip_fraction        | 0.0576     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.47      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 189        |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.00441   |
|    value_loss           | 217        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.08e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 455      |
|    time_elapsed    | 32748    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.21e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 456         |
|    time_elapsed         | 32812       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.022351734 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 13.1        |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 193         |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=5762.59 +/- 480.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 5.76e+03     |
| time/                   |              |
|    total_timesteps      | 7475000      |
| train/                  |              |
|    approx_kl            | 0.0080532245 |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.715       |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 125          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.32e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 457      |
|    time_elapsed    | 32888    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=6002.32 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.008920024 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 427         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.64e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 458      |
|    time_elapsed    | 32964    |
|    total_timesteps | 7503872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.59e+03   |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 459        |
|    time_elapsed         | 33028      |
|    total_timesteps      | 7520256    |
| train/                  |            |
|    approx_kl            | 0.02833233 |
|    clip_fraction        | 0.0898     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.69       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.00357   |
|    value_loss           | 39.4       |
----------------------------------------
Eval num_timesteps=7525000, episode_reward=6002.41 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7525000      |
| train/                  |              |
|    approx_kl            | 0.0020461802 |
|    clip_fraction        | 0.0351       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.424       |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0003       |
|    loss                 | 110          |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 227          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.65e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 460      |
|    time_elapsed    | 33104    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=6002.54 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.008345516 |
|    clip_fraction        | 0.0571      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 141         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.47e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 461      |
|    time_elapsed    | 33181    |
|    total_timesteps | 7553024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.25e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 462          |
|    time_elapsed         | 33245        |
|    total_timesteps      | 7569408      |
| train/                  |              |
|    approx_kl            | 0.0072928034 |
|    clip_fraction        | 0.0559       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.83         |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00416     |
|    value_loss           | 319          |
------------------------------------------
Eval num_timesteps=7575000, episode_reward=6002.67 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7575000      |
| train/                  |              |
|    approx_kl            | 0.0047300085 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | 127          |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.00454     |
|    value_loss           | 587          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.12e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 463      |
|    time_elapsed    | 33321    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=6002.21 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.008656345 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.1         |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 198         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.23e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 464      |
|    time_elapsed    | 33397    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.53e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 465         |
|    time_elapsed         | 33461       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.006932875 |
|    clip_fraction        | 0.061       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.28        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 52.1        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=6002.60 +/- 0.68
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7625000      |
| train/                  |              |
|    approx_kl            | 0.0133474395 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.94         |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00398     |
|    value_loss           | 44.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.48e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 466      |
|    time_elapsed    | 33537    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=6002.64 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7650000     |
| train/                  |             |
|    approx_kl            | 0.008064941 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 356         |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 190         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 467      |
|    time_elapsed    | 33613    |
|    total_timesteps | 7651328  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.28e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 468          |
|    time_elapsed         | 33676        |
|    total_timesteps      | 7667712      |
| train/                  |              |
|    approx_kl            | 0.0056016697 |
|    clip_fraction        | 0.0579       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 154          |
|    n_updates            | 4670         |
|    policy_gradient_loss | 0.00342      |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=7675000, episode_reward=6002.55 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7675000      |
| train/                  |              |
|    approx_kl            | 0.0060701426 |
|    clip_fraction        | 0.0517       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.94         |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.000958    |
|    value_loss           | 66.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.35e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 469      |
|    time_elapsed    | 33752    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=6001.96 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7700000     |
| train/                  |             |
|    approx_kl            | 0.022888847 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 4690        |
|    policy_gradient_loss | 0.000502    |
|    value_loss           | 46.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.49e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 470      |
|    time_elapsed    | 33828    |
|    total_timesteps | 7700480  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.63e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 471          |
|    time_elapsed         | 33892        |
|    total_timesteps      | 7716864      |
| train/                  |              |
|    approx_kl            | 0.0069062547 |
|    clip_fraction        | 0.0559       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.92         |
|    n_updates            | 4700         |
|    policy_gradient_loss | 0.00208      |
|    value_loss           | 68           |
------------------------------------------
Eval num_timesteps=7725000, episode_reward=6002.30 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.013733371 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.58        |
|    n_updates            | 4710        |
|    policy_gradient_loss | 0.00051     |
|    value_loss           | 25.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.73e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 472      |
|    time_elapsed    | 33968    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.91e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 473         |
|    time_elapsed         | 34033       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.048989035 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.452      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.656       |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 2.72        |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=6002.41 +/- 0.23
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7750000      |
| train/                  |              |
|    approx_kl            | 0.0040585604 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.0003       |
|    loss                 | 502          |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 1.12e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.25e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 474      |
|    time_elapsed    | 34109    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=6002.86 +/- 0.49
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7775000      |
| train/                  |              |
|    approx_kl            | 0.0026165443 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.674       |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0003       |
|    loss                 | 705          |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 935          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.52e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 475      |
|    time_elapsed    | 34184    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.33e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 476         |
|    time_elapsed         | 34248       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.006098958 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 331         |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 695         |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=6002.64 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7800000      |
| train/                  |              |
|    approx_kl            | 0.0040035564 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.696       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 681          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.82e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 477      |
|    time_elapsed    | 34325    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=6002.49 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7825000      |
| train/                  |              |
|    approx_kl            | 0.0030756393 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 215          |
|    n_updates            | 4770         |
|    policy_gradient_loss | -0.00267     |
|    value_loss           | 448          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.52e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 478      |
|    time_elapsed    | 34401    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 479         |
|    time_elapsed         | 34466       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.008120608 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 186         |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 77          |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=4842.89 +/- 2319.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 7850000      |
| train/                  |              |
|    approx_kl            | 0.0052331467 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.7         |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 218          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.54e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 480      |
|    time_elapsed    | 34544    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=6002.31 +/- 0.19
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7875000      |
| train/                  |              |
|    approx_kl            | 0.0086274985 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.1          |
|    n_updates            | 4800         |
|    policy_gradient_loss | -0.000338    |
|    value_loss           | 30.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.7e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 481      |
|    time_elapsed    | 34622    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.54e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 482         |
|    time_elapsed         | 34690       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.022131348 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.36        |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 67.3        |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=6002.92 +/- 0.80
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 7900000      |
| train/                  |              |
|    approx_kl            | 0.0029364494 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.419       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 41.1         |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.000718    |
|    value_loss           | 197          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.6e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 483      |
|    time_elapsed    | 34771    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=6002.41 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.027600527 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.3        |
|    n_updates            | 4830        |
|    policy_gradient_loss | 0.000673    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 484      |
|    time_elapsed    | 34851    |
|    total_timesteps | 7929856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.69e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 485          |
|    time_elapsed         | 34919        |
|    total_timesteps      | 7946240      |
| train/                  |              |
|    approx_kl            | 0.0028616264 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.0003       |
|    loss                 | 199          |
|    n_updates            | 4840         |
|    policy_gradient_loss | -0.000637    |
|    value_loss           | 225          |
------------------------------------------
Eval num_timesteps=7950000, episode_reward=6002.59 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.009542473 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.23        |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 85.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.86e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 486      |
|    time_elapsed    | 35000    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=4842.90 +/- 2319.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.84e+03   |
| time/                   |            |
|    total_timesteps      | 7975000    |
| train/                  |            |
|    approx_kl            | 0.14105211 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.444     |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.4        |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 1.83       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.68e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 487      |
|    time_elapsed    | 35081    |
|    total_timesteps | 7979008  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 3.7e+03      |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 488          |
|    time_elapsed         | 35149        |
|    total_timesteps      | 7995392      |
| train/                  |              |
|    approx_kl            | 0.0062955567 |
|    clip_fraction        | 0.0572       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.0003       |
|    loss                 | 558          |
|    n_updates            | 4870         |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=8000000, episode_reward=3410.35 +/- 3174.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.41e+03    |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.006027896 |
|    clip_fraction        | 0.0388      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 511         |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 723         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 489      |
|    time_elapsed    | 35231    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=4706.27 +/- 2592.09
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 8025000      |
| train/                  |              |
|    approx_kl            | 0.0067768814 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.0003       |
|    loss                 | 121          |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 591          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 490      |
|    time_elapsed    | 35311    |
|    total_timesteps | 8028160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 2.34e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 491          |
|    time_elapsed         | 35378        |
|    total_timesteps      | 8044544      |
| train/                  |              |
|    approx_kl            | 0.0077848937 |
|    clip_fraction        | 0.0461       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.754       |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0003       |
|    loss                 | 443          |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00388     |
|    value_loss           | 988          |
------------------------------------------
Eval num_timesteps=8050000, episode_reward=6002.32 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.005176083 |
|    clip_fraction        | 0.0528      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 285         |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 656         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 492      |
|    time_elapsed    | 35457    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=6002.55 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.017047778 |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 414         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 493      |
|    time_elapsed    | 35539    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.25e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 494         |
|    time_elapsed         | 35606       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.009963814 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 260         |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 592         |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=6002.88 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8100000     |
| train/                  |             |
|    approx_kl            | 0.011918197 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 713         |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 481         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.65e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 495      |
|    time_elapsed    | 35687    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=6002.89 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.004516262 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 1.19e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.2e+03  |
| time/              |          |
|    fps             | 227      |
|    iterations      | 496      |
|    time_elapsed    | 35768    |
|    total_timesteps | 8126464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.52e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 497          |
|    time_elapsed         | 35836        |
|    total_timesteps      | 8142848      |
| train/                  |              |
|    approx_kl            | 0.0035212855 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 975          |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 860          |
------------------------------------------
Eval num_timesteps=8150000, episode_reward=6002.33 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8150000      |
| train/                  |              |
|    approx_kl            | 0.0036350796 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.431       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 139          |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.000692    |
|    value_loss           | 196          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.63e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 498      |
|    time_elapsed    | 35917    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=4706.39 +/- 2592.09
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.71e+03     |
| time/                   |              |
|    total_timesteps      | 8175000      |
| train/                  |              |
|    approx_kl            | 0.0061692577 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.277       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 147          |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.000297    |
|    value_loss           | 103          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.16e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 499      |
|    time_elapsed    | 35998    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.43e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 500         |
|    time_elapsed         | 36065       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.010418954 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.29        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.000503   |
|    value_loss           | 93.2        |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=6002.44 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8200000     |
| train/                  |             |
|    approx_kl            | 0.004336496 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 83.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.54e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 501      |
|    time_elapsed    | 36146    |
|    total_timesteps | 8208384  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.31e+03     |
| time/                   |              |
|    fps                  | 227          |
|    iterations           | 502          |
|    time_elapsed         | 36213        |
|    total_timesteps      | 8224768      |
| train/                  |              |
|    approx_kl            | 0.0029399488 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 24.1         |
|    n_updates            | 5010         |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 295          |
------------------------------------------
Eval num_timesteps=8225000, episode_reward=6002.75 +/- 0.51
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 8225000    |
| train/                  |            |
|    approx_kl            | 0.01267859 |
|    clip_fraction        | 0.0678     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.41      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.00234   |
|    value_loss           | 241        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.33e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 503      |
|    time_elapsed    | 36293    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=6002.55 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8250000     |
| train/                  |             |
|    approx_kl            | 0.008237278 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.79        |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 20.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 504      |
|    time_elapsed    | 36374    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.64e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 505         |
|    time_elapsed         | 36442       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.010275635 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 42.9        |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=6002.56 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8275000      |
| train/                  |              |
|    approx_kl            | 0.0073660426 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.488        |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.000802    |
|    value_loss           | 8.19         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.93e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 506      |
|    time_elapsed    | 36524    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=6002.55 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.011256593 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0804     |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 5.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.87e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 507      |
|    time_elapsed    | 36605    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.81e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 508         |
|    time_elapsed         | 36673       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.014312545 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.983       |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 66.3        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=6002.61 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.009512046 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.984       |
|    n_updates            | 5080        |
|    policy_gradient_loss | 0.000223    |
|    value_loss           | 24.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.82e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 509      |
|    time_elapsed    | 36754    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=4842.46 +/- 2319.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.010802008 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 54.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.53e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 510      |
|    time_elapsed    | 36835    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.4e+03     |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 511         |
|    time_elapsed         | 36904       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.046647426 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 100         |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=6002.60 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8375000     |
| train/                  |             |
|    approx_kl            | 0.002834909 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 5110        |
|    policy_gradient_loss | 0.000348    |
|    value_loss           | 186         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.26e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 512      |
|    time_elapsed    | 36985    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=4842.96 +/- 2319.29
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 8400000      |
| train/                  |              |
|    approx_kl            | 0.0041270517 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.4         |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.00389     |
|    value_loss           | 179          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.77e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 513      |
|    time_elapsed    | 37066    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 514         |
|    time_elapsed         | 37134       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.021876357 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=6002.76 +/- 0.79
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8425000      |
| train/                  |              |
|    approx_kl            | 0.0061080144 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0972      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.51         |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.000968    |
|    value_loss           | 27.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 515      |
|    time_elapsed    | 37215    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=6002.26 +/- 0.36
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8450000      |
| train/                  |              |
|    approx_kl            | 0.0026888573 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0919      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.25         |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.000816    |
|    value_loss           | 3.47         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.44e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 516      |
|    time_elapsed    | 37296    |
|    total_timesteps | 8454144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.83e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 517          |
|    time_elapsed         | 37364        |
|    total_timesteps      | 8470528      |
| train/                  |              |
|    approx_kl            | 0.0036596318 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0782      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.96         |
|    n_updates            | 5160         |
|    policy_gradient_loss | 0.000265     |
|    value_loss           | 45.6         |
------------------------------------------
Eval num_timesteps=8475000, episode_reward=3583.06 +/- 4838.47
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.58e+03   |
| time/                   |            |
|    total_timesteps      | 8475000    |
| train/                  |            |
|    approx_kl            | 0.36015368 |
|    clip_fraction        | 0.0445     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0897    |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.357      |
|    n_updates            | 5170       |
|    policy_gradient_loss | -0.00143   |
|    value_loss           | 1.36       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 518      |
|    time_elapsed    | 37445    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=6002.54 +/- 0.25
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8500000      |
| train/                  |              |
|    approx_kl            | 0.0012444627 |
|    clip_fraction        | 0.00554      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.24        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0003       |
|    loss                 | 157          |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.000565    |
|    value_loss           | 556          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.78e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 519      |
|    time_elapsed    | 37526    |
|    total_timesteps | 8503296  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.31e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 520          |
|    time_elapsed         | 37595        |
|    total_timesteps      | 8519680      |
| train/                  |              |
|    approx_kl            | 0.0023496686 |
|    clip_fraction        | 0.00994      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0003       |
|    loss                 | 97.5         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 425          |
------------------------------------------
Eval num_timesteps=8525000, episode_reward=6002.54 +/- 0.30
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8525000      |
| train/                  |              |
|    approx_kl            | 0.0029531564 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 5200         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 543          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.33e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 521      |
|    time_elapsed    | 37676    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=6002.76 +/- 0.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8550000      |
| train/                  |              |
|    approx_kl            | 0.0011895124 |
|    clip_fraction        | 0.00656      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0777      |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 35.6         |
|    n_updates            | 5210         |
|    policy_gradient_loss | -7.35e-05    |
|    value_loss           | 120          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.81e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 522      |
|    time_elapsed    | 37757    |
|    total_timesteps | 8552448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.02e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 523          |
|    time_elapsed         | 37825        |
|    total_timesteps      | 8568832      |
| train/                  |              |
|    approx_kl            | 0.0061454726 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.8         |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 343          |
------------------------------------------
Eval num_timesteps=8575000, episode_reward=6002.34 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.007812664 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.23        |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 204         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 524      |
|    time_elapsed    | 37905    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=6002.78 +/- 0.60
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8600000      |
| train/                  |              |
|    approx_kl            | 0.0002642082 |
|    clip_fraction        | 0.00253      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0821      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 25.1         |
|    n_updates            | 5240         |
|    policy_gradient_loss | 7.02e-05     |
|    value_loss           | 21           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.5e+03  |
| time/              |          |
|    fps             | 226      |
|    iterations      | 525      |
|    time_elapsed    | 37986    |
|    total_timesteps | 8601600  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.83e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 526          |
|    time_elapsed         | 38054        |
|    total_timesteps      | 8617984      |
| train/                  |              |
|    approx_kl            | 0.0010115702 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.82         |
|    n_updates            | 5250         |
|    policy_gradient_loss | -5.27e-05    |
|    value_loss           | 86.1         |
------------------------------------------
Eval num_timesteps=8625000, episode_reward=6002.71 +/- 0.33
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.008702282 |
|    clip_fraction        | 0.00892     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0764     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.13        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.000971   |
|    value_loss           | 15.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.64e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 527      |
|    time_elapsed    | 38135    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=4843.16 +/- 2319.51
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 8650000      |
| train/                  |              |
|    approx_kl            | 0.0017560686 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 64.4         |
|    n_updates            | 5270         |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 492          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5e+03    |
| time/              |          |
|    fps             | 226      |
|    iterations      | 528      |
|    time_elapsed    | 38216    |
|    total_timesteps | 8650752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.74e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 529          |
|    time_elapsed         | 38284        |
|    total_timesteps      | 8667136      |
| train/                  |              |
|    approx_kl            | 0.0057383343 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 607          |
|    n_updates            | 5280         |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 687          |
------------------------------------------
Eval num_timesteps=8675000, episode_reward=6002.50 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8675000      |
| train/                  |              |
|    approx_kl            | 0.0024927915 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | 321          |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00326     |
|    value_loss           | 729          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.58e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 530      |
|    time_elapsed    | 38365    |
|    total_timesteps | 8683520  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.86e+03     |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 531          |
|    time_elapsed         | 38432        |
|    total_timesteps      | 8699904      |
| train/                  |              |
|    approx_kl            | 0.0015051733 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | 46.5         |
|    n_updates            | 5300         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 338          |
------------------------------------------
Eval num_timesteps=8700000, episode_reward=4842.64 +/- 2319.03
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 8700000      |
| train/                  |              |
|    approx_kl            | 0.0062797894 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 38           |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.00597     |
|    value_loss           | 152          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.29e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 532      |
|    time_elapsed    | 38513    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=4842.94 +/- 2319.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.009105336 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 28.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.52e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 533      |
|    time_elapsed    | 38593    |
|    total_timesteps | 8732672  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.69e+03      |
| time/                   |               |
|    fps                  | 226           |
|    iterations           | 534           |
|    time_elapsed         | 38661         |
|    total_timesteps      | 8749056       |
| train/                  |               |
|    approx_kl            | 0.00038585078 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0743       |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.0003        |
|    loss                 | 33.4          |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.000304     |
|    value_loss           | 119           |
-------------------------------------------
Eval num_timesteps=8750000, episode_reward=6002.39 +/- 0.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 8750000    |
| train/                  |            |
|    approx_kl            | 0.01736769 |
|    clip_fraction        | 0.0329     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.36       |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.00351   |
|    value_loss           | 84.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.72e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 535      |
|    time_elapsed    | 38740    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=6002.52 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.004786933 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0976     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.01        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.000602   |
|    value_loss           | 59.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.68e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 536      |
|    time_elapsed    | 38820    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.57e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 537         |
|    time_elapsed         | 38887       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.009805632 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0756     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.62e+03    |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 85.3        |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=4842.95 +/- 2319.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.001830683 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0458     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.387       |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 28.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.46e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 538      |
|    time_elapsed    | 38967    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=6002.77 +/- 0.21
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8825000      |
| train/                  |              |
|    approx_kl            | 0.0010005482 |
|    clip_fraction        | 0.0058       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0811      |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 83.8         |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.00073     |
|    value_loss           | 398          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.25e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 539      |
|    time_elapsed    | 39046    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.16e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 540         |
|    time_elapsed         | 39115       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.006975567 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.9        |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 254         |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=6002.59 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.008576822 |
|    clip_fraction        | 0.0168      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0958     |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 5400        |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 224         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.12e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 541      |
|    time_elapsed    | 39194    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=6002.49 +/- 0.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.004978804 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.000453   |
|    value_loss           | 257         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.16e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 542      |
|    time_elapsed    | 39273    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.34e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 543         |
|    time_elapsed         | 39340       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.011409627 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.43        |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=6002.56 +/- 0.28
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8900000      |
| train/                  |              |
|    approx_kl            | 0.0067918585 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.39         |
|    n_updates            | 5430         |
|    policy_gradient_loss | 0.00127      |
|    value_loss           | 92           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.54e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 544      |
|    time_elapsed    | 39420    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=6002.38 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.002774247 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.7        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.000391   |
|    value_loss           | 27.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.71e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 545      |
|    time_elapsed    | 39500    |
|    total_timesteps | 8929280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.79e+03    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 546         |
|    time_elapsed         | 39566       |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.003593781 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.631       |
|    n_updates            | 5450        |
|    policy_gradient_loss | -2.46e-05   |
|    value_loss           | 101         |
-----------------------------------------
Eval num_timesteps=8950000, episode_reward=6002.41 +/- 0.52
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 8950000      |
| train/                  |              |
|    approx_kl            | 0.0018986482 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0906      |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0003       |
|    loss                 | 80.7         |
|    n_updates            | 5460         |
|    policy_gradient_loss | 0.000634     |
|    value_loss           | 44.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.79e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 547      |
|    time_elapsed    | 39646    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=6002.82 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.006836245 |
|    clip_fraction        | 0.00846     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0572     |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 33.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.91e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 548      |
|    time_elapsed    | 39725    |
|    total_timesteps | 8978432  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.94e+03      |
| time/                   |               |
|    fps                  | 226           |
|    iterations           | 549           |
|    time_elapsed         | 39792         |
|    total_timesteps      | 8994816       |
| train/                  |               |
|    approx_kl            | 0.00012214809 |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0214       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.237         |
|    n_updates            | 5480          |
|    policy_gradient_loss | -9.75e-05     |
|    value_loss           | 1.41          |
-------------------------------------------
Eval num_timesteps=9000000, episode_reward=6002.29 +/- 0.29
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 6e+03         |
| time/                   |               |
|    total_timesteps      | 9000000       |
| train/                  |               |
|    approx_kl            | 0.00042045373 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.197         |
|    n_updates            | 5490          |
|    policy_gradient_loss | -0.000248     |
|    value_loss           | 0.464         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.88e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 550      |
|    time_elapsed    | 39872    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=6002.71 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.004479702 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0536     |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.799       |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 35.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.7e+03  |
| time/              |          |
|    fps             | 225      |
|    iterations      | 551      |
|    time_elapsed    | 39951    |
|    total_timesteps | 9027584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.37e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 552          |
|    time_elapsed         | 40018        |
|    total_timesteps      | 9043968      |
| train/                  |              |
|    approx_kl            | 0.0018827693 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0552      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 114          |
|    n_updates            | 5510         |
|    policy_gradient_loss | 0.000317     |
|    value_loss           | 352          |
------------------------------------------
Eval num_timesteps=9050000, episode_reward=6002.57 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9050000     |
| train/                  |             |
|    approx_kl            | 0.014771627 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0591     |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.67        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 86.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 553      |
|    time_elapsed    | 40097    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=4842.57 +/- 2319.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.015074134 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.67        |
|    n_updates            | 5530        |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 65          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.25e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 554      |
|    time_elapsed    | 40177    |
|    total_timesteps | 9076736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.71e+03   |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 555        |
|    time_elapsed         | 40244      |
|    total_timesteps      | 9093120    |
| train/                  |            |
|    approx_kl            | 0.00150378 |
|    clip_fraction        | 0.00964    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0799    |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.55       |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.00041   |
|    value_loss           | 66.9       |
----------------------------------------
Eval num_timesteps=9100000, episode_reward=4842.53 +/- 2319.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.017346576 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0731      |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 51.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.71e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 556      |
|    time_elapsed    | 40323    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=6002.57 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9125000      |
| train/                  |              |
|    approx_kl            | 0.0077889594 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.29         |
|    n_updates            | 5560         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 114          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.83e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 557      |
|    time_elapsed    | 40403    |
|    total_timesteps | 9125888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.41e+03   |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 558        |
|    time_elapsed         | 40469      |
|    total_timesteps      | 9142272    |
| train/                  |            |
|    approx_kl            | 0.06161472 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.296     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.497      |
|    n_updates            | 5570       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 1.31       |
----------------------------------------
Eval num_timesteps=9150000, episode_reward=4706.60 +/- 2592.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.71e+03    |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.004670957 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 345         |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 1.21e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 559      |
|    time_elapsed    | 40549    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=6002.33 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9175000      |
| train/                  |              |
|    approx_kl            | 0.0034692213 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0003       |
|    loss                 | 194          |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 1.11e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 560      |
|    time_elapsed    | 40628    |
|    total_timesteps | 9175040  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 431          |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 561          |
|    time_elapsed         | 40695        |
|    total_timesteps      | 9191424      |
| train/                  |              |
|    approx_kl            | 0.0047116084 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.677       |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | 365          |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00244     |
|    value_loss           | 848          |
------------------------------------------
Eval num_timesteps=9200000, episode_reward=4842.60 +/- 2319.43
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 9200000      |
| train/                  |              |
|    approx_kl            | 0.0035700225 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.73        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 163          |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 613          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 669      |
| time/              |          |
|    fps             | 225      |
|    iterations      | 562      |
|    time_elapsed    | 40775    |
|    total_timesteps | 9207808  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 1.43e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 563          |
|    time_elapsed         | 40841        |
|    total_timesteps      | 9224192      |
| train/                  |              |
|    approx_kl            | 0.0037122075 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.584       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 239          |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 764          |
------------------------------------------
Eval num_timesteps=9225000, episode_reward=6002.57 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.008912168 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 293         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 564      |
|    time_elapsed    | 40921    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=6002.64 +/- 0.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9250000      |
| train/                  |              |
|    approx_kl            | 0.0035535244 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | 125          |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 302          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.44e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 565      |
|    time_elapsed    | 41000    |
|    total_timesteps | 9256960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.89e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 566         |
|    time_elapsed         | 41067       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.002711901 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.9        |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 511         |
-----------------------------------------
Eval num_timesteps=9275000, episode_reward=6002.42 +/- 0.24
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9275000      |
| train/                  |              |
|    approx_kl            | 0.0014627983 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.33        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 65.8         |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.000611    |
|    value_loss           | 563          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.04e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 567      |
|    time_elapsed    | 41146    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=4883.07 +/- 2238.34
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.88e+03     |
| time/                   |              |
|    total_timesteps      | 9300000      |
| train/                  |              |
|    approx_kl            | 0.0026414627 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.667       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 212          |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 633          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.25e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 568      |
|    time_elapsed    | 41226    |
|    total_timesteps | 9306112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 4.39e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 569          |
|    time_elapsed         | 41292        |
|    total_timesteps      | 9322496      |
| train/                  |              |
|    approx_kl            | 0.0037216896 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.378       |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 62.8         |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=9325000, episode_reward=6002.64 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9325000     |
| train/                  |             |
|    approx_kl            | 0.005636099 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.316      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 56.8        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 274         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.42e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 570      |
|    time_elapsed    | 41372    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=6002.84 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.004854996 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 308         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.86e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 571      |
|    time_elapsed    | 41452    |
|    total_timesteps | 9355264  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.36e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 572          |
|    time_elapsed         | 41518        |
|    total_timesteps      | 9371648      |
| train/                  |              |
|    approx_kl            | 0.0016704877 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.65         |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.000362    |
|    value_loss           | 90.8         |
------------------------------------------
Eval num_timesteps=9375000, episode_reward=4843.18 +/- 2319.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.84e+03    |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.003039333 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.6        |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.000437   |
|    value_loss           | 113         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.43e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 573      |
|    time_elapsed    | 41598    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=6002.27 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.005148459 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0676     |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.000583   |
|    value_loss           | 67.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.79e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 574      |
|    time_elapsed    | 41677    |
|    total_timesteps | 9404416  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.79e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 575          |
|    time_elapsed         | 41744        |
|    total_timesteps      | 9420800      |
| train/                  |              |
|    approx_kl            | 0.0037620794 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.97         |
|    n_updates            | 5740         |
|    policy_gradient_loss | -0.000425    |
|    value_loss           | 105          |
------------------------------------------
Eval num_timesteps=9425000, episode_reward=6002.76 +/- 0.62
Episode length: 600.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 600           |
|    mean_reward          | 6e+03         |
| time/                   |               |
|    total_timesteps      | 9425000       |
| train/                  |               |
|    approx_kl            | 0.00031426773 |
|    clip_fraction        | 0.00329       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0667       |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.22          |
|    n_updates            | 5750          |
|    policy_gradient_loss | -1.11e-05     |
|    value_loss           | 48.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.76e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 576      |
|    time_elapsed    | 41824    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=6002.64 +/- 0.31
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9450000      |
| train/                  |              |
|    approx_kl            | 0.0007108782 |
|    clip_fraction        | 0.00807      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0785      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.29         |
|    n_updates            | 5760         |
|    policy_gradient_loss | 0.000912     |
|    value_loss           | 69.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.82e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 577      |
|    time_elapsed    | 41903    |
|    total_timesteps | 9453568  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.88e+03      |
| time/                   |               |
|    fps                  | 225           |
|    iterations           | 578           |
|    time_elapsed         | 41970         |
|    total_timesteps      | 9469952       |
| train/                  |               |
|    approx_kl            | 0.00092005543 |
|    clip_fraction        | 0.00458       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0624       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.88          |
|    n_updates            | 5770          |
|    policy_gradient_loss | -0.000478     |
|    value_loss           | 14            |
-------------------------------------------
Eval num_timesteps=9475000, episode_reward=6002.53 +/- 0.42
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9475000      |
| train/                  |              |
|    approx_kl            | 0.0002446216 |
|    clip_fraction        | 0.00275      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0493      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.655        |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000169    |
|    value_loss           | 7.72         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.88e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 579      |
|    time_elapsed    | 42049    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=6002.41 +/- 0.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9500000      |
| train/                  |              |
|    approx_kl            | 0.0002806901 |
|    clip_fraction        | 0.0031       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0545      |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.769        |
|    n_updates            | 5790         |
|    policy_gradient_loss | 3.99e-06     |
|    value_loss           | 43.8         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.94e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 580      |
|    time_elapsed    | 42129    |
|    total_timesteps | 9502720  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 600           |
|    ep_rew_mean          | 5.94e+03      |
| time/                   |               |
|    fps                  | 225           |
|    iterations           | 581           |
|    time_elapsed         | 42196         |
|    total_timesteps      | 9519104       |
| train/                  |               |
|    approx_kl            | 0.00026734077 |
|    clip_fraction        | 0.00339       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0381       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.709         |
|    n_updates            | 5800          |
|    policy_gradient_loss | -0.000288     |
|    value_loss           | 6.04          |
-------------------------------------------
Eval num_timesteps=9525000, episode_reward=4842.57 +/- 2319.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.84e+03     |
| time/                   |              |
|    total_timesteps      | 9525000      |
| train/                  |              |
|    approx_kl            | 0.0005459032 |
|    clip_fraction        | 0.00289      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0492      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.307        |
|    n_updates            | 5810         |
|    policy_gradient_loss | -0.000235    |
|    value_loss           | 2.71         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.94e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 582      |
|    time_elapsed    | 42275    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=6002.40 +/- 0.20
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9550000      |
| train/                  |              |
|    approx_kl            | 0.0015624114 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0745      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.461        |
|    n_updates            | 5820         |
|    policy_gradient_loss | -0.000604    |
|    value_loss           | 2.62         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.94e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 583      |
|    time_elapsed    | 42354    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.94e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 584         |
|    time_elapsed         | 42421       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.014332313 |
|    clip_fraction        | 0.00905     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0853     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 49.8        |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=6002.76 +/- 0.33
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9575000      |
| train/                  |              |
|    approx_kl            | 0.0076562986 |
|    clip_fraction        | 0.00763      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0728      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.09         |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.000839    |
|    value_loss           | 2.1          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.94e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 585      |
|    time_elapsed    | 42501    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=5882.26 +/- 240.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.88e+03   |
| time/                   |            |
|    total_timesteps      | 9600000    |
| train/                  |            |
|    approx_kl            | 0.12559772 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.839      |
|    n_updates            | 5850       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 2.32       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.89e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 586      |
|    time_elapsed    | 42580    |
|    total_timesteps | 9601024  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 5.4e+03    |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 587        |
|    time_elapsed         | 42648      |
|    total_timesteps      | 9617408    |
| train/                  |            |
|    approx_kl            | 0.13760546 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.599     |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.05       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 49.2       |
----------------------------------------
Eval num_timesteps=9625000, episode_reward=6002.75 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.003160236 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 398         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.24e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 588      |
|    time_elapsed    | 42727    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=6002.51 +/- 0.54
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9650000      |
| train/                  |              |
|    approx_kl            | 0.0049353475 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0003       |
|    loss                 | 450          |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 595          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.74e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 589      |
|    time_elapsed    | 42807    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.6e+03     |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 590         |
|    time_elapsed         | 42874       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.016938228 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.45        |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 284         |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=6003.04 +/- 0.80
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9675000      |
| train/                  |              |
|    approx_kl            | 0.0026899758 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.359       |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.9         |
|    n_updates            | 5900         |
|    policy_gradient_loss | -0.000856    |
|    value_loss           | 208          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.86e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 591      |
|    time_elapsed    | 42954    |
|    total_timesteps | 9682944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 4.2e+03    |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 592        |
|    time_elapsed         | 43021      |
|    total_timesteps      | 9699328    |
| train/                  |            |
|    approx_kl            | 0.00455813 |
|    clip_fraction        | 0.0403     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.472     |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 159        |
|    n_updates            | 5910       |
|    policy_gradient_loss | -0.00188   |
|    value_loss           | 392        |
----------------------------------------
Eval num_timesteps=9700000, episode_reward=6003.06 +/- 0.68
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9700000      |
| train/                  |              |
|    approx_kl            | 0.0041769342 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 74.4         |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 439          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.98e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 593      |
|    time_elapsed    | 43101    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=6002.52 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.005416818 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 214         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 3.55e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 594      |
|    time_elapsed    | 43181    |
|    total_timesteps | 9732096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 3.91e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 595         |
|    time_elapsed         | 43249       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.008149134 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.6        |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 266         |
-----------------------------------------
Eval num_timesteps=9750000, episode_reward=6002.65 +/- 0.27
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9750000      |
| train/                  |              |
|    approx_kl            | 0.0035075943 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 14           |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000483    |
|    value_loss           | 106          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.27e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 596      |
|    time_elapsed    | 43330    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=3547.06 +/- 3015.16
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.55e+03   |
| time/                   |            |
|    total_timesteps      | 9775000    |
| train/                  |            |
|    approx_kl            | 0.01090011 |
|    clip_fraction        | 0.0323     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 350        |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.00378   |
|    value_loss           | 188        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 4.71e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 597      |
|    time_elapsed    | 43411    |
|    total_timesteps | 9781248  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.04e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 598          |
|    time_elapsed         | 43480        |
|    total_timesteps      | 9797632      |
| train/                  |              |
|    approx_kl            | 0.0029856646 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.279       |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.07         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.000921    |
|    value_loss           | 59.9         |
------------------------------------------
Eval num_timesteps=9800000, episode_reward=6002.74 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9800000     |
| train/                  |             |
|    approx_kl            | 0.005300463 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 367         |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 165         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.17e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 599      |
|    time_elapsed    | 43561    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=4726.81 +/- 2552.09
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 4.73e+03     |
| time/                   |              |
|    total_timesteps      | 9825000      |
| train/                  |              |
|    approx_kl            | 0.0035543905 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 20.1         |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.000872    |
|    value_loss           | 109          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 600      |
|    time_elapsed    | 43642    |
|    total_timesteps | 9830400  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.52e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 601          |
|    time_elapsed         | 43710        |
|    total_timesteps      | 9846784      |
| train/                  |              |
|    approx_kl            | 0.0048420317 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.78         |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 195          |
------------------------------------------
Eval num_timesteps=9850000, episode_reward=6002.48 +/- 0.46
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9850000      |
| train/                  |              |
|    approx_kl            | 0.0030533473 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.24        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 73.5         |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.000343    |
|    value_loss           | 164          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.61e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 602      |
|    time_elapsed    | 43791    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=6002.34 +/- 0.35
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9875000      |
| train/                  |              |
|    approx_kl            | 0.0016308841 |
|    clip_fraction        | 0.00736      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0499      |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.24         |
|    n_updates            | 6020         |
|    policy_gradient_loss | 0.000262     |
|    value_loss           | 33.6         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.77e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 603      |
|    time_elapsed    | 43872    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 5.77e+03    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 604         |
|    time_elapsed         | 43940       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.008625053 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.21        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 24.7        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=6002.39 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 6e+03        |
| time/                   |              |
|    total_timesteps      | 9900000      |
| train/                  |              |
|    approx_kl            | 0.0056249527 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0821      |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.81         |
|    n_updates            | 6040         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 29.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.87e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 605      |
|    time_elapsed    | 44021    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=6002.75 +/- 0.58
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 9925000    |
| train/                  |            |
|    approx_kl            | 0.06373224 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.16       |
|    n_updates            | 6050       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 4.55       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.92e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 606      |
|    time_elapsed    | 44102    |
|    total_timesteps | 9928704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.91e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 607          |
|    time_elapsed         | 44170        |
|    total_timesteps      | 9945088      |
| train/                  |              |
|    approx_kl            | 0.0031728677 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.06         |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.000498    |
|    value_loss           | 6.21         |
------------------------------------------
Eval num_timesteps=9950000, episode_reward=6002.49 +/- 0.22
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 6e+03     |
| time/                   |           |
|    total_timesteps      | 9950000   |
| train/                  |           |
|    approx_kl            | 0.0161413 |
|    clip_fraction        | 0.0527    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.288    |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.468     |
|    n_updates            | 6070      |
|    policy_gradient_loss | -0.00195  |
|    value_loss           | 5.7       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.83e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 608      |
|    time_elapsed    | 44251    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=6002.45 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 6e+03       |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.027920054 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.318      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 41          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.77e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 609      |
|    time_elapsed    | 44332    |
|    total_timesteps | 9977856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | 5.71e+03     |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 610          |
|    time_elapsed         | 44400        |
|    total_timesteps      | 9994240      |
| train/                  |              |
|    approx_kl            | 0.0071872426 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.14         |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 38.6         |
------------------------------------------
Eval num_timesteps=10000000, episode_reward=6002.45 +/- 0.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 6e+03      |
| time/                   |            |
|    total_timesteps      | 10000000   |
| train/                  |            |
|    approx_kl            | 0.05709835 |
|    clip_fraction        | 0.0639     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.00424   |
|    value_loss           | 24         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 5.51e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 611      |
|    time_elapsed    | 44481    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v10_1
