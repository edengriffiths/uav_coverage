========== uav-v4 ==========
Seed: 3908848727
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v4_2
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -5.81e+03 |
| time/              |           |
|    fps             | 402       |
|    iterations      | 1         |
|    time_elapsed    | 40        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-3196.02 +/- 2512.03
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -3.2e+03     |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0073658284 |
|    clip_fraction        | 0.062        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | 0.000551     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.65e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00854     |
|    value_loss           | 3.93e+04     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.74e+03 |
| time/              |           |
|    fps             | 321       |
|    iterations      | 2         |
|    time_elapsed    | 101       |
|    total_timesteps | 32768     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.13e+03   |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 3           |
|    time_elapsed         | 152         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008670168 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -2.06e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 2.45e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-1559.13 +/- 293.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.56e+03   |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.020557553 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -1.31e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6e+03     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 1.29e+04    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.7e+03 |
| time/              |          |
|    fps             | 309      |
|    iterations      | 4        |
|    time_elapsed    | 211      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-1199.14 +/- 379.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.2e+03    |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.012862353 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.53e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 1.07e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -2.66e+03 |
| time/              |           |
|    fps             | 302       |
|    iterations      | 5         |
|    time_elapsed    | 271       |
|    total_timesteps | 81920     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -2.1e+03     |
| time/                   |              |
|    fps                  | 306          |
|    iterations           | 6            |
|    time_elapsed         | 321          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0124661755 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1e+03        |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 4.41e+03     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=-1679.13 +/- 240.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.68e+03   |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.006269234 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.71e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00697    |
|    value_loss           | 5.89e+03    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.67e+03 |
| time/              |           |
|    fps             | 301       |
|    iterations      | 7         |
|    time_elapsed    | 380       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1559.27 +/- 293.84
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.56e+03    |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0049901167 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 2.25e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -1.16e+03 |
| time/              |           |
|    fps             | 297       |
|    iterations      | 8         |
|    time_elapsed    | 439       |
|    total_timesteps | 131072    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -948        |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 9           |
|    time_elapsed         | 489         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.007827396 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.0003      |
|    loss                 | 85.4        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 369         |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=-959.47 +/- 611.45
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -959         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0077810907 |
|    clip_fraction        | 0.0728       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.76        |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0003       |
|    loss                 | 122          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00977     |
|    value_loss           | 1.25e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -681     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 10       |
|    time_elapsed    | 549      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-1139.91 +/- 610.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.14e+03   |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.009957582 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.1        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00946    |
|    value_loss           | 437         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -602     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 11       |
|    time_elapsed    | 608      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -610        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 12          |
|    time_elapsed         | 658         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.011046428 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.23        |
|    learning_rate        | 0.0003      |
|    loss                 | 249         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 599         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=-838.84 +/- 479.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -839        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.010196595 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 305         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 13       |
|    time_elapsed    | 717      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-959.22 +/- 479.90
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -959         |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0142328115 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.76        |
|    explained_variance   | 0.689        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.2         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0138      |
|    value_loss           | 117          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -468     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 14       |
|    time_elapsed    | 777      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -411        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 15          |
|    time_elapsed         | 827         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.009439995 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.2        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 562         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=-119.13 +/- 240.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.013465583 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.0003      |
|    loss                 | 65.2        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 109         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 16       |
|    time_elapsed    | 886      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-239.21 +/- 293.92
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -239       |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.01123926 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.76      |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.0003     |
|    loss                 | 225        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 290        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -359     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 17       |
|    time_elapsed    | 946      |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -520        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 18          |
|    time_elapsed         | 995         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.014694748 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-119.16 +/- 240.02
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -119         |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0087257605 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.75        |
|    explained_variance   | -0.15        |
|    learning_rate        | 0.0003       |
|    loss                 | 155          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00549     |
|    value_loss           | 2.02e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -584     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 19       |
|    time_elapsed    | 1055     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-239.21 +/- 293.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -239        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.008552335 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.44        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 3.71e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -732     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 20       |
|    time_elapsed    | 1114     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -749        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 21          |
|    time_elapsed         | 1164        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.009941533 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00646    |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=0.96 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.961       |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.011893243 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0003      |
|    loss                 | 583         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 391         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -587     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 22       |
|    time_elapsed    | 1223     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=375000, episode_reward=1.14 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.010637617 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 779         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 2.29e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -469     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 23       |
|    time_elapsed    | 1283     |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -521        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 24          |
|    time_elapsed         | 1333        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.014699031 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.9        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 248         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-118.97 +/- 240.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.009767887 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 804         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0051     |
|    value_loss           | 1.76e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -471     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 25       |
|    time_elapsed    | 1392     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=425000, episode_reward=0.91 +/- 0.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.914      |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.01492836 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.73      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.0003     |
|    loss                 | 105        |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 173        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 26       |
|    time_elapsed    | 1451     |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -237        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 27          |
|    time_elapsed         | 1501        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.012675745 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 49          |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=1.26 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.017831791 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 160         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 28       |
|    time_elapsed    | 1560     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.00 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.012877243 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 99.9        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 2.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 29       |
|    time_elapsed    | 1620     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -259        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 30          |
|    time_elapsed         | 1669        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.015978461 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 244         |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=0.88 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.884       |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.018474104 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.1        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00899    |
|    value_loss           | 131         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -256     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 31       |
|    time_elapsed    | 1728     |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -150        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 32          |
|    time_elapsed         | 1777        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.024183448 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 109         |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=1.07 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.07        |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.018248186 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 106         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 33       |
|    time_elapsed    | 1835     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=550000, episode_reward=1.27 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.017631121 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | -2.08       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.6        |
|    n_updates            | 330         |
|    policy_gradient_loss | 0.00153     |
|    value_loss           | 414         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 34       |
|    time_elapsed    | 1893     |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -230        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 35          |
|    time_elapsed         | 1942        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.018427208 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 121         |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=1.74 +/- 1.61
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.01269234 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.67      |
|    explained_variance   | 0.0955     |
|    learning_rate        | 0.0003     |
|    loss                 | 36.3       |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.00549   |
|    value_loss           | 278        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 36       |
|    time_elapsed    | 2000     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=600000, episode_reward=0.87 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.872       |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.022796063 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | 98.6        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 49.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 37       |
|    time_elapsed    | 2059     |
|    total_timesteps | 606208   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -143         |
| time/                   |              |
|    fps                  | 295          |
|    iterations           | 38           |
|    time_elapsed         | 2108         |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 0.0111298645 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0003       |
|    loss                 | 52.7         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00658     |
|    value_loss           | 218          |
------------------------------------------
Eval num_timesteps=625000, episode_reward=1.16 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.026886452 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | -0.0103     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.985       |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.00344     |
|    value_loss           | 149         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 39       |
|    time_elapsed    | 2166     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-2356.61 +/- 2888.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.36e+03   |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.016771818 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0003      |
|    loss                 | 434         |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.000607   |
|    value_loss           | 355         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 40       |
|    time_elapsed    | 2224     |
|    total_timesteps | 655360   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -218      |
| time/                   |           |
|    fps                  | 295       |
|    iterations           | 41        |
|    time_elapsed         | 2273      |
|    total_timesteps      | 671744    |
| train/                  |           |
|    approx_kl            | 0.0181141 |
|    clip_fraction        | 0.23      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.65     |
|    explained_variance   | 0.204     |
|    learning_rate        | 0.0003    |
|    loss                 | 30        |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.00788  |
|    value_loss           | 124       |
---------------------------------------
Eval num_timesteps=675000, episode_reward=1.65 +/- 0.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.039577283 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | -0.714      |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000701   |
|    value_loss           | 222         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 42       |
|    time_elapsed    | 2331     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-588.45 +/- 1180.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -588        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.008665774 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.0003      |
|    loss                 | 305         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 467         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -283     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 43       |
|    time_elapsed    | 2390     |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -289        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 44          |
|    time_elapsed         | 2439        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.019542258 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.2        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000428   |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=2.34 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.015036627 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.3        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 143         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -316     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 45       |
|    time_elapsed    | 2497     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2.77 +/- 2.04
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.77       |
| time/                   |            |
|    total_timesteps      | 750000     |
| train/                  |            |
|    approx_kl            | 0.01445741 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.64      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 84.5       |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.000759  |
|    value_loss           | 323        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 46       |
|    time_elapsed    | 2555     |
|    total_timesteps | 753664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -218        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 47          |
|    time_elapsed         | 2604        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.012980243 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.8        |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 718         |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=0.11 +/- 4.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 775000      |
| train/                  |             |
|    approx_kl            | 0.024768572 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.7         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -291     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 48       |
|    time_elapsed    | 2662     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2.09 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.009425746 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 832         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 49       |
|    time_elapsed    | 2721     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -179        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 50          |
|    time_elapsed         | 2770        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.015168202 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.1        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 249         |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=1.54 +/- 0.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.021866526 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 102         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 51       |
|    time_elapsed    | 2828     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-900.61 +/- 1806.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -901        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.028250642 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42        |
|    n_updates            | 510         |
|    policy_gradient_loss | 0.00325     |
|    value_loss           | 306         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 52       |
|    time_elapsed    | 2886     |
|    total_timesteps | 851968   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -184       |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 53         |
|    time_elapsed         | 2935       |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.01772794 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.00433   |
|    value_loss           | 38.5       |
----------------------------------------
Eval num_timesteps=875000, episode_reward=1.41 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 875000      |
| train/                  |             |
|    approx_kl            | 0.035262026 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 89.3        |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 252         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 54       |
|    time_elapsed    | 2994     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=900000, episode_reward=1.84 +/- 1.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.011360381 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 421         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 709         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 55       |
|    time_elapsed    | 3052     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -225        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 56          |
|    time_elapsed         | 3101        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.018305626 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.7        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 51.4        |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=4.19 +/- 5.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.19        |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.014994308 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 42.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 57       |
|    time_elapsed    | 3159     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-10.73 +/- 23.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -10.7       |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.016440589 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 58       |
|    time_elapsed    | 3217     |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -179        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 59          |
|    time_elapsed         | 3266        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.013627777 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.7        |
|    n_updates            | 580         |
|    policy_gradient_loss | 0.000279    |
|    value_loss           | 652         |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=1.61 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.019654293 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.14        |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.000704   |
|    value_loss           | 77.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 60       |
|    time_elapsed    | 3325     |
|    total_timesteps | 983040   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 600      |
|    ep_rew_mean          | -232     |
| time/                   |          |
|    fps                  | 296      |
|    iterations           | 61       |
|    time_elapsed         | 3374     |
|    total_timesteps      | 999424   |
| train/                  |          |
|    approx_kl            | 0.016532 |
|    clip_fraction        | 0.184    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.59    |
|    explained_variance   | 0.923    |
|    learning_rate        | 0.0003   |
|    loss                 | 386      |
|    n_updates            | 600      |
|    policy_gradient_loss | -0.00229 |
|    value_loss           | 625      |
--------------------------------------
Eval num_timesteps=1000000, episode_reward=1.51 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.019985948 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 217         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 62       |
|    time_elapsed    | 3432     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=0.97 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.966       |
| time/                   |             |
|    total_timesteps      | 1025000     |
| train/                  |             |
|    approx_kl            | 0.011914059 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.55        |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 921         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -237     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 63       |
|    time_elapsed    | 3490     |
|    total_timesteps | 1032192  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -218         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 64           |
|    time_elapsed         | 3539         |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0144102005 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.58        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.2         |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00721     |
|    value_loss           | 199          |
------------------------------------------
Eval num_timesteps=1050000, episode_reward=2.23 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.23        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.017107328 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.06        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 67          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 65       |
|    time_elapsed    | 3597     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=1.16 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.020027142 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93.6    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 66       |
|    time_elapsed    | 3655     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -79.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 67          |
|    time_elapsed         | 3704        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.025184104 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.09        |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 36.2        |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=2.87 +/- 1.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.87        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.014833285 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 77.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 68       |
|    time_elapsed    | 3763     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=1.95 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.014256089 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.5        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 385         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 69       |
|    time_elapsed    | 3821     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -237        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 70          |
|    time_elapsed         | 3872        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.010869486 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=1150000, episode_reward=1.86 +/- 1.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.015560044 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 660         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 71       |
|    time_elapsed    | 3930     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=1.48 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 1175000     |
| train/                  |             |
|    approx_kl            | 0.017636849 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.95        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 72       |
|    time_elapsed    | 3989     |
|    total_timesteps | 1179648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -70.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 73          |
|    time_elapsed         | 4038        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.021411883 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.22        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=1.49 +/- 0.51
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.016712688 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37        |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 66.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 74       |
|    time_elapsed    | 4096     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=1.18 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.012941273 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 266         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -93.4    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 75       |
|    time_elapsed    | 4154     |
|    total_timesteps | 1228800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -87.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 76          |
|    time_elapsed         | 4204        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.022875346 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 62          |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=0.86 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.863      |
| time/                   |            |
|    total_timesteps      | 1250000    |
| train/                  |            |
|    approx_kl            | 0.02069807 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.783      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00656   |
|    value_loss           | 19.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 77       |
|    time_elapsed    | 4262     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=0.92 +/- 0.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.923      |
| time/                   |            |
|    total_timesteps      | 1275000    |
| train/                  |            |
|    approx_kl            | 0.01404752 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.48      |
|    explained_variance   | 0.0419     |
|    learning_rate        | 0.0003     |
|    loss                 | 262        |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.00176   |
|    value_loss           | 241        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.1    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 78       |
|    time_elapsed    | 4320     |
|    total_timesteps | 1277952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -73.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 79          |
|    time_elapsed         | 4369        |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.017952628 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.48       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.944       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 59.7        |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=1.02 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.02        |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.017856093 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.98        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 79.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 80       |
|    time_elapsed    | 4428     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=1.16 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.020517958 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.987       |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 14          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -41.5    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 81       |
|    time_elapsed    | 4486     |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -36.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 82          |
|    time_elapsed         | 4535        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.014214566 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00801    |
|    value_loss           | 25.3        |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=0.94 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.942       |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.019196015 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 32.1        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 47.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -40.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 83       |
|    time_elapsed    | 4593     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=1.04 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.04        |
| time/                   |             |
|    total_timesteps      | 1375000     |
| train/                  |             |
|    approx_kl            | 0.013788782 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.3        |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 46.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.3    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 84       |
|    time_elapsed    | 4652     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -34.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 85          |
|    time_elapsed         | 4701        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.020436129 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.368       |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 9.29        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=1.00 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.999       |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.018844612 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.349       |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 12.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 86       |
|    time_elapsed    | 4759     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=1.46 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1425000     |
| train/                  |             |
|    approx_kl            | 0.017639533 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 51.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.9    |
| time/              |          |
|    fps             | 295      |
|    iterations      | 87       |
|    time_elapsed    | 4817     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -25.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 88          |
|    time_elapsed         | 4866        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.020199353 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 7.85        |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=0.94 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.937       |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.020010915 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | 42.5        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 8.9         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 89       |
|    time_elapsed    | 4925     |
|    total_timesteps | 1458176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 90          |
|    time_elapsed         | 4974        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.019953152 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.41        |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 57.1        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=1.20 +/- 0.34
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 1475000     |
| train/                  |             |
|    approx_kl            | 0.017759776 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.43        |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 116         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 91       |
|    time_elapsed    | 5032     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1.28 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.018832946 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 33          |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 88.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 92       |
|    time_elapsed    | 5090     |
|    total_timesteps | 1507328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -39.7      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 93         |
|    time_elapsed         | 5139       |
|    total_timesteps      | 1523712    |
| train/                  |            |
|    approx_kl            | 0.02224185 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.43      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.35       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00299   |
|    value_loss           | 15.9       |
----------------------------------------
Eval num_timesteps=1525000, episode_reward=1.25 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.020775095 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.51        |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 4.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 94       |
|    time_elapsed    | 5198     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=1.18 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.019069888 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 4.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 95       |
|    time_elapsed    | 5256     |
|    total_timesteps | 1556480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 96          |
|    time_elapsed         | 5305        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.018757174 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.397       |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 17.2        |
-----------------------------------------
Eval num_timesteps=1575000, episode_reward=1.45 +/- 0.45
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.45       |
| time/                   |            |
|    total_timesteps      | 1575000    |
| train/                  |            |
|    approx_kl            | 0.01927777 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.36      |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.213      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 43.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 97       |
|    time_elapsed    | 5363     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=-2390.51 +/- 2864.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.39e+03   |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.015027867 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 10          |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 54.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 98       |
|    time_elapsed    | 5422     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -59.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 99          |
|    time_elapsed         | 5471        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.011406468 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.8        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 76.7        |
-----------------------------------------
Eval num_timesteps=1625000, episode_reward=1.20 +/- 0.27
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 1625000    |
| train/                  |            |
|    approx_kl            | 0.01586162 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.2       |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 148        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 100      |
|    time_elapsed    | 5529     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=1.20 +/- 0.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.016239364 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.33        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 101      |
|    time_elapsed    | 5587     |
|    total_timesteps | 1654784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -62.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 102         |
|    time_elapsed         | 5636        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.023189485 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.5        |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 36.8        |
-----------------------------------------
Eval num_timesteps=1675000, episode_reward=1.49 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.018761689 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 55.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 103      |
|    time_elapsed    | 5695     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=-2.74 +/- 10.77
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.74       |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.018809006 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.37        |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 34.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -53.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 104      |
|    time_elapsed    | 5753     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 105         |
|    time_elapsed         | 5802        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.008743579 |
|    clip_fraction        | 0.0566      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.0999      |
|    learning_rate        | 0.0003      |
|    loss                 | 246         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=1725000, episode_reward=1.81 +/- 0.90
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 1725000    |
| train/                  |            |
|    approx_kl            | 0.01964948 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.4       |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.00943   |
|    value_loss           | 2.82       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 106      |
|    time_elapsed    | 5861     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=1.27 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.012512198 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.93        |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 25.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 107      |
|    time_elapsed    | 5919     |
|    total_timesteps | 1753088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 108         |
|    time_elapsed         | 5968        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.009969368 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.8        |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=1775000, episode_reward=0.95 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.954       |
| time/                   |             |
|    total_timesteps      | 1775000     |
| train/                  |             |
|    approx_kl            | 0.018783972 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.9        |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.0046      |
|    value_loss           | 90.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 109      |
|    time_elapsed    | 6026     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=1.14 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.14        |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.020190557 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 7.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 110      |
|    time_elapsed    | 6084     |
|    total_timesteps | 1802240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -18.3        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 111          |
|    time_elapsed         | 6134         |
|    total_timesteps      | 1818624      |
| train/                  |              |
|    approx_kl            | 0.0134791685 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.33        |
|    explained_variance   | 0.646        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.7          |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00731     |
|    value_loss           | 46.3         |
------------------------------------------
Eval num_timesteps=1825000, episode_reward=1.09 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.09        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.018600248 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.379       |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 5.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 112      |
|    time_elapsed    | 6192     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=1.19 +/- 0.48
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.19       |
| time/                   |            |
|    total_timesteps      | 1850000    |
| train/                  |            |
|    approx_kl            | 0.01587163 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.678      |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.00538   |
|    value_loss           | 25.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 113      |
|    time_elapsed    | 6250     |
|    total_timesteps | 1851392  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -21.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 114         |
|    time_elapsed         | 6299        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.016879512 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 6.34        |
-----------------------------------------
Eval num_timesteps=1875000, episode_reward=0.88 +/- 0.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.882       |
| time/                   |             |
|    total_timesteps      | 1875000     |
| train/                  |             |
|    approx_kl            | 0.017665494 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 28.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 115      |
|    time_elapsed    | 6357     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=0.95 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.954       |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.018745482 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.248       |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 1.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 116      |
|    time_elapsed    | 6416     |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.27       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 117         |
|    time_elapsed         | 6465        |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.017638186 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.196       |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 1.2         |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=0.96 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.961       |
| time/                   |             |
|    total_timesteps      | 1925000     |
| train/                  |             |
|    approx_kl            | 0.018157858 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 0.433       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 118      |
|    time_elapsed    | 6523     |
|    total_timesteps | 1933312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -85.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 119         |
|    time_elapsed         | 6572        |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.005489705 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.000802    |
|    learning_rate        | 0.0003      |
|    loss                 | 286         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00192    |
|    value_loss           | 300         |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=1.02 +/- 0.19
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.02       |
| time/                   |            |
|    total_timesteps      | 1950000    |
| train/                  |            |
|    approx_kl            | 0.01915953 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.3       |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.00166   |
|    value_loss           | 1.12       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 120      |
|    time_elapsed    | 6630     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=1.22 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.22        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.017144624 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.59        |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 21          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 121      |
|    time_elapsed    | 6689     |
|    total_timesteps | 1982464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 122         |
|    time_elapsed         | 6738        |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.012369029 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.01        |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 63          |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=1.27 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.018872917 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 11.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 123      |
|    time_elapsed    | 6796     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=1.31 +/- 0.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.31        |
| time/                   |             |
|    total_timesteps      | 2025000     |
| train/                  |             |
|    approx_kl            | 0.014317113 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.1        |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 138         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -254     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 124      |
|    time_elapsed    | 6855     |
|    total_timesteps | 2031616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -241        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 125         |
|    time_elapsed         | 6904        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.008182561 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 356         |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=2050000, episode_reward=1.25 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 2050000     |
| train/                  |             |
|    approx_kl            | 0.019705102 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.41        |
|    n_updates            | 1250        |
|    policy_gradient_loss | 0.000636    |
|    value_loss           | 10.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -237     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 126      |
|    time_elapsed    | 6962     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=1.56 +/- 0.26
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.56       |
| time/                   |            |
|    total_timesteps      | 2075000    |
| train/                  |            |
|    approx_kl            | 0.01942547 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.167      |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.00566   |
|    value_loss           | 5.78       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -232     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 127      |
|    time_elapsed    | 7020     |
|    total_timesteps | 2080768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 128         |
|    time_elapsed         | 7069        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.019505024 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.38        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 68.2        |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=1.46 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.020595448 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.14        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00672    |
|    value_loss           | 18.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 129      |
|    time_elapsed    | 7128     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=1.13 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.014447815 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.56        |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.000459    |
|    value_loss           | 187         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 130      |
|    time_elapsed    | 7186     |
|    total_timesteps | 2129920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -171        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 131         |
|    time_elapsed         | 7235        |
|    total_timesteps      | 2146304     |
| train/                  |             |
|    approx_kl            | 0.014613546 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.000802   |
|    value_loss           | 91.1        |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=1.46 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.025130566 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 1310        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 122         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 132      |
|    time_elapsed    | 7293     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=1.03 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.03        |
| time/                   |             |
|    total_timesteps      | 2175000     |
| train/                  |             |
|    approx_kl            | 0.021785095 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.9        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 56          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 133      |
|    time_elapsed    | 7351     |
|    total_timesteps | 2179072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 134         |
|    time_elapsed         | 7401        |
|    total_timesteps      | 2195456     |
| train/                  |             |
|    approx_kl            | 0.020965897 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.532       |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 1.44        |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=1.16 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.015643042 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.582       |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 99.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 135      |
|    time_elapsed    | 7459     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=1.53 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.015397469 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.59        |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.000566    |
|    value_loss           | 3.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 136      |
|    time_elapsed    | 7517     |
|    total_timesteps | 2228224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -20.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 137         |
|    time_elapsed         | 7566        |
|    total_timesteps      | 2244608     |
| train/                  |             |
|    approx_kl            | 0.019185834 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.335       |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 12.7        |
-----------------------------------------
Eval num_timesteps=2250000, episode_reward=-586.66 +/- 1175.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -587        |
| time/                   |             |
|    total_timesteps      | 2250000     |
| train/                  |             |
|    approx_kl            | 0.017630175 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.554       |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 22.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -68.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 138      |
|    time_elapsed    | 7625     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=1.34 +/- 0.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.011304856 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 88.5        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 346         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -88.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 139      |
|    time_elapsed    | 7683     |
|    total_timesteps | 2277376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -86.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 140         |
|    time_elapsed         | 7732        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.017977804 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 354         |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.000333    |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=1.36 +/- 0.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.019279309 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.51        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 10.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 141      |
|    time_elapsed    | 7791     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=-1746.83 +/- 2321.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.75e+03   |
| time/                   |             |
|    total_timesteps      | 2325000     |
| train/                  |             |
|    approx_kl            | 0.012857089 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.33        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 63.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -34      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 142      |
|    time_elapsed    | 7849     |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 143         |
|    time_elapsed         | 7898        |
|    total_timesteps      | 2342912     |
| train/                  |             |
|    approx_kl            | 0.010237459 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.6        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=2350000, episode_reward=1.15 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.15        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.020641664 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 82.2        |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 148         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 144      |
|    time_elapsed    | 7957     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=1.23 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.23        |
| time/                   |             |
|    total_timesteps      | 2375000     |
| train/                  |             |
|    approx_kl            | 0.023353122 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | -0.259      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.92        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 259         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 145      |
|    time_elapsed    | 8015     |
|    total_timesteps | 2375680  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 146         |
|    time_elapsed         | 8064        |
|    total_timesteps      | 2392064     |
| train/                  |             |
|    approx_kl            | 0.016772956 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 27.2        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.000909   |
|    value_loss           | 21.8        |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=1.45 +/- 0.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.016440786 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.83        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00432    |
|    value_loss           | 36.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 147      |
|    time_elapsed    | 8123     |
|    total_timesteps | 2408448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 148         |
|    time_elapsed         | 8172        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.017062888 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00989    |
|    value_loss           | 35          |
-----------------------------------------
Eval num_timesteps=2425000, episode_reward=1.40 +/- 0.22
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 2425000    |
| train/                  |            |
|    approx_kl            | 0.01957186 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.58       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.00333   |
|    value_loss           | 29.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 149      |
|    time_elapsed    | 8230     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=1.16 +/- 0.10
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.16       |
| time/                   |            |
|    total_timesteps      | 2450000    |
| train/                  |            |
|    approx_kl            | 0.01766714 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.00699   |
|    value_loss           | 11.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 150      |
|    time_elapsed    | 8288     |
|    total_timesteps | 2457600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 151         |
|    time_elapsed         | 8338        |
|    total_timesteps      | 2473984     |
| train/                  |             |
|    approx_kl            | 0.014059641 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.06        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 26          |
-----------------------------------------
Eval num_timesteps=2475000, episode_reward=-1182.43 +/- 1449.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.029944528 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 128         |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.00611     |
|    value_loss           | 209         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 152      |
|    time_elapsed    | 8396     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=-1724.79 +/- 2283.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.72e+03  |
| time/                   |            |
|    total_timesteps      | 2500000    |
| train/                  |            |
|    approx_kl            | 0.01666892 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.4       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 129        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -77.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 153      |
|    time_elapsed    | 8454     |
|    total_timesteps | 2506752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -83.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 154         |
|    time_elapsed         | 8504        |
|    total_timesteps      | 2523136     |
| train/                  |             |
|    approx_kl            | 0.023982722 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.96        |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=2525000, episode_reward=-588.80 +/- 1180.04
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -589        |
| time/                   |             |
|    total_timesteps      | 2525000     |
| train/                  |             |
|    approx_kl            | 0.018135572 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.68        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.000886   |
|    value_loss           | 26.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 155      |
|    time_elapsed    | 8562     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=-2.57 +/- 7.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.57       |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.021625657 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.252       |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 23.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 156      |
|    time_elapsed    | 8620     |
|    total_timesteps | 2555904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 157         |
|    time_elapsed         | 8669        |
|    total_timesteps      | 2572288     |
| train/                  |             |
|    approx_kl            | 0.017127246 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 23.8        |
-----------------------------------------
Eval num_timesteps=2575000, episode_reward=1.38 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 2575000     |
| train/                  |             |
|    approx_kl            | 0.020730175 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 1570        |
|    policy_gradient_loss | 0.000284    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 158      |
|    time_elapsed    | 8727     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=1.41 +/- 0.24
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.41       |
| time/                   |            |
|    total_timesteps      | 2600000    |
| train/                  |            |
|    approx_kl            | 0.02189795 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.217      |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.00683   |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 159      |
|    time_elapsed    | 8786     |
|    total_timesteps | 2605056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -19         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 160         |
|    time_elapsed         | 8835        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.014558718 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.39        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 74.3        |
-----------------------------------------
Eval num_timesteps=2625000, episode_reward=1.21 +/- 0.09
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.21       |
| time/                   |            |
|    total_timesteps      | 2625000    |
| train/                  |            |
|    approx_kl            | 0.01724368 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.14      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.12       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.000324  |
|    value_loss           | 9.68       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 161      |
|    time_elapsed    | 8893     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=1.26 +/- 0.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.018709691 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 29.1        |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.00284     |
|    value_loss           | 184         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 162      |
|    time_elapsed    | 8951     |
|    total_timesteps | 2654208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -54.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 163         |
|    time_elapsed         | 9001        |
|    total_timesteps      | 2670592     |
| train/                  |             |
|    approx_kl            | 0.023204606 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 4.43        |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=1.55 +/- 0.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.55       |
| time/                   |            |
|    total_timesteps      | 2675000    |
| train/                  |            |
|    approx_kl            | 0.02077521 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.327      |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.00412   |
|    value_loss           | 0.757      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -56.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 164      |
|    time_elapsed    | 9059     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=-1178.66 +/- 2359.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 2700000     |
| train/                  |             |
|    approx_kl            | 0.024570458 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.1        |
|    n_updates            | 1640        |
|    policy_gradient_loss | 0.00313     |
|    value_loss           | 9.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.26    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 165      |
|    time_elapsed    | 9117     |
|    total_timesteps | 2703360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.07      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 166        |
|    time_elapsed         | 9166       |
|    total_timesteps      | 2719744    |
| train/                  |            |
|    approx_kl            | 0.01872623 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0939     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.00421   |
|    value_loss           | 0.822      |
----------------------------------------
Eval num_timesteps=2725000, episode_reward=1.66 +/- 0.55
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 2725000     |
| train/                  |             |
|    approx_kl            | 0.018191684 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0293      |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 0.581       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.28    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 167      |
|    time_elapsed    | 9225     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=-588.50 +/- 1180.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -588        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.018727686 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0565      |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.000452   |
|    value_loss           | 0.124       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.554   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 168      |
|    time_elapsed    | 9283     |
|    total_timesteps | 2752512  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -64.9      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 169        |
|    time_elapsed         | 9332       |
|    total_timesteps      | 2768896    |
| train/                  |            |
|    approx_kl            | 0.01871689 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.11      |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00288    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.00737   |
|    value_loss           | 1.41       |
----------------------------------------
Eval num_timesteps=2775000, episode_reward=-588.87 +/- 1179.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -589        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.008748253 |
|    clip_fraction        | 0.0685      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | -0.961      |
|    learning_rate        | 0.0003      |
|    loss                 | 87.5        |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 166         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 170      |
|    time_elapsed    | 9390     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=1.29 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.018512435 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.698       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 4.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -67.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 171      |
|    time_elapsed    | 9449     |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -132        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 172         |
|    time_elapsed         | 9498        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.024715154 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.035       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0617      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 7           |
-----------------------------------------
Eval num_timesteps=2825000, episode_reward=-1178.73 +/- 2360.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.009623965 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 1720        |
|    policy_gradient_loss | 0.000876    |
|    value_loss           | 215         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 173      |
|    time_elapsed    | 9556     |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=-1180.47 +/- 2358.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.048378527 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.6        |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.00351     |
|    value_loss           | 170         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 174      |
|    time_elapsed    | 9614     |
|    total_timesteps | 2850816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -221        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 175         |
|    time_elapsed         | 9664        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.031183261 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.309       |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 13.3        |
-----------------------------------------
Eval num_timesteps=2875000, episode_reward=1.87 +/- 0.58
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 2875000    |
| train/                  |            |
|    approx_kl            | 0.01290484 |
|    clip_fraction        | 0.0654     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.6       |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.00164   |
|    value_loss           | 675        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 176      |
|    time_elapsed    | 9722     |
|    total_timesteps | 2883584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -271        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 177         |
|    time_elapsed         | 9771        |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.023065303 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.167       |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.00387     |
|    value_loss           | 3.17        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=2.24 +/- 0.83
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 2900000      |
| train/                  |              |
|    approx_kl            | 0.0065051382 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.0003       |
|    loss                 | 292          |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 365          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -289     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 178      |
|    time_elapsed    | 9829     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=-14.09 +/- 32.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -14.1       |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.016733097 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 299         |
|    n_updates            | 1780        |
|    policy_gradient_loss | 0.000195    |
|    value_loss           | 794         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -287     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 179      |
|    time_elapsed    | 9888     |
|    total_timesteps | 2932736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -294       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 180        |
|    time_elapsed         | 9937       |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.03534787 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.21       |
|    n_updates            | 1790       |
|    policy_gradient_loss | 0.00823    |
|    value_loss           | 36.9       |
----------------------------------------
Eval num_timesteps=2950000, episode_reward=1.83 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.018496066 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.33        |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 7.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 181      |
|    time_elapsed    | 9995     |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=1.61 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.019945398 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.88        |
|    n_updates            | 1810        |
|    policy_gradient_loss | 0.000626    |
|    value_loss           | 28.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 182      |
|    time_elapsed    | 10053    |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 183         |
|    time_elapsed         | 10102       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.024188522 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.246       |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.0035      |
|    value_loss           | 47          |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=-1176.57 +/- 2355.89
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.021097256 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.82        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 8.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 184      |
|    time_elapsed    | 10161    |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=1.90 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.017738085 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 13.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 185      |
|    time_elapsed    | 10219    |
|    total_timesteps | 3031040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 186         |
|    time_elapsed         | 10268       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.017718246 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 8.47        |
-----------------------------------------
Eval num_timesteps=3050000, episode_reward=1.91 +/- 0.95
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.91         |
| time/                   |              |
|    total_timesteps      | 3050000      |
| train/                  |              |
|    approx_kl            | 0.0066681853 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.23         |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00496     |
|    value_loss           | 106          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -22.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 187      |
|    time_elapsed    | 10327    |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=0.19 +/- 3.99
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 0.191      |
| time/                   |            |
|    total_timesteps      | 3075000    |
| train/                  |            |
|    approx_kl            | 0.01784594 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.266      |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.000239  |
|    value_loss           | 4.52       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 188      |
|    time_elapsed    | 10385    |
|    total_timesteps | 3080192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -28.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 189         |
|    time_elapsed         | 10434       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.026355203 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.1        |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.000582    |
|    value_loss           | 20.3        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=2.20 +/- 1.06
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.2        |
| time/                   |            |
|    total_timesteps      | 3100000    |
| train/                  |            |
|    approx_kl            | 0.01760424 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 21.8       |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.000859  |
|    value_loss           | 36.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 190      |
|    time_elapsed    | 10493    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=2.40 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 3125000     |
| train/                  |             |
|    approx_kl            | 0.017506227 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00566    |
|    value_loss           | 35.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 191      |
|    time_elapsed    | 10551    |
|    total_timesteps | 3129344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -34.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 192         |
|    time_elapsed         | 10600       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.014624875 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 56.7        |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.000922   |
|    value_loss           | 48.8        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=1.58 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.015399659 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.416       |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 1.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 193      |
|    time_elapsed    | 10659    |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=2.10 +/- 0.97
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.1        |
| time/                   |            |
|    total_timesteps      | 3175000    |
| train/                  |            |
|    approx_kl            | 0.01190045 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.96       |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.000126  |
|    value_loss           | 151        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -117     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 194      |
|    time_elapsed    | 10717    |
|    total_timesteps | 3178496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -116        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 195         |
|    time_elapsed         | 10766       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.019373551 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 18.5        |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=2.04 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.016841978 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.5         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.000501   |
|    value_loss           | 2.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 196      |
|    time_elapsed    | 10825    |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=2.47 +/- 0.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.47        |
| time/                   |             |
|    total_timesteps      | 3225000     |
| train/                  |             |
|    approx_kl            | 0.013959603 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.386       |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 15.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 197      |
|    time_elapsed    | 10883    |
|    total_timesteps | 3227648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 198         |
|    time_elapsed         | 10932       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.017754111 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.95        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 5.72        |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=1.96 +/- 0.42
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.016570501 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.443       |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 20.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 199      |
|    time_elapsed    | 10991    |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=1.84 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 3275000     |
| train/                  |             |
|    approx_kl            | 0.015811084 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 2.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 200      |
|    time_elapsed    | 11049    |
|    total_timesteps | 3276800  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 201         |
|    time_elapsed         | 11098       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.017980233 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.55        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 9.27        |
-----------------------------------------
Eval num_timesteps=3300000, episode_reward=1.88 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.88       |
| time/                   |            |
|    total_timesteps      | 3300000    |
| train/                  |            |
|    approx_kl            | 0.01776576 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.34      |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0589     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.00133   |
|    value_loss           | 59.8       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 202      |
|    time_elapsed    | 11156    |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=1.96 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 3325000     |
| train/                  |             |
|    approx_kl            | 0.008206306 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 28.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -65.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 203      |
|    time_elapsed    | 11215    |
|    total_timesteps | 3325952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -80.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 204         |
|    time_elapsed         | 11264       |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.007748211 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 314         |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 370         |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=2.58 +/- 0.59
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.58       |
| time/                   |            |
|    total_timesteps      | 3350000    |
| train/                  |            |
|    approx_kl            | 0.01901268 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.36      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.4        |
|    n_updates            | 2040       |
|    policy_gradient_loss | 0.00169    |
|    value_loss           | 41.7       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -81.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 205      |
|    time_elapsed    | 11322    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=2.95 +/- 1.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.95        |
| time/                   |             |
|    total_timesteps      | 3375000     |
| train/                  |             |
|    approx_kl            | 0.022447474 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.178       |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.00448     |
|    value_loss           | 3.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 206      |
|    time_elapsed    | 11381    |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 207         |
|    time_elapsed         | 11430       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.013615262 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 31.6        |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=2.42 +/- 1.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.012600944 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.2        |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 92.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 208      |
|    time_elapsed    | 11488    |
|    total_timesteps | 3407872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -39.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 209         |
|    time_elapsed         | 11537       |
|    total_timesteps      | 3424256     |
| train/                  |             |
|    approx_kl            | 0.021135049 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.2        |
|    n_updates            | 2080        |
|    policy_gradient_loss | 0.00227     |
|    value_loss           | 59          |
-----------------------------------------
Eval num_timesteps=3425000, episode_reward=1.96 +/- 0.90
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 3425000     |
| train/                  |             |
|    approx_kl            | 0.021657946 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0718      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 2.46        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 210      |
|    time_elapsed    | 11596    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=2.66 +/- 0.97
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 3450000      |
| train/                  |              |
|    approx_kl            | 0.0060837837 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.33        |
|    explained_variance   | 0.584        |
|    learning_rate        | 0.0003       |
|    loss                 | 183          |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00483     |
|    value_loss           | 298          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -32.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 211      |
|    time_elapsed    | 11654    |
|    total_timesteps | 3457024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -94.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 212         |
|    time_elapsed         | 11703       |
|    total_timesteps      | 3473408     |
| train/                  |             |
|    approx_kl            | 0.015011148 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0003      |
|    loss                 | 164         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 86.6        |
-----------------------------------------
Eval num_timesteps=3475000, episode_reward=2.91 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.91        |
| time/                   |             |
|    total_timesteps      | 3475000     |
| train/                  |             |
|    approx_kl            | 0.020800732 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.4        |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 192         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 213      |
|    time_elapsed    | 11762    |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3.41 +/- 1.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.41       |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.01709896 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.31      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.198      |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.00204   |
|    value_loss           | 2.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 214      |
|    time_elapsed    | 11820    |
|    total_timesteps | 3506176  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -187         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 215          |
|    time_elapsed         | 11869        |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0149632525 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | 27.4         |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 673          |
------------------------------------------
Eval num_timesteps=3525000, episode_reward=3.21 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.21        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.019984908 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.6        |
|    n_updates            | 2150        |
|    policy_gradient_loss | 0.000294    |
|    value_loss           | 134         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 216      |
|    time_elapsed    | 11927    |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=1.85 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.013722276 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.9        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 32.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -79.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 217      |
|    time_elapsed    | 11986    |
|    total_timesteps | 3555328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -66.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 218         |
|    time_elapsed         | 12035       |
|    total_timesteps      | 3571712     |
| train/                  |             |
|    approx_kl            | 0.027441116 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    n_updates            | 2170        |
|    policy_gradient_loss | 0.00201     |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=3575000, episode_reward=4.15 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.15        |
| time/                   |             |
|    total_timesteps      | 3575000     |
| train/                  |             |
|    approx_kl            | 0.016093766 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.43        |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 15          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -89.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 219      |
|    time_elapsed    | 12093    |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=4.95 +/- 1.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.95       |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.01422313 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 144        |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.00656   |
|    value_loss           | 211        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -92.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 220      |
|    time_elapsed    | 12152    |
|    total_timesteps | 3604480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -59         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 221         |
|    time_elapsed         | 12201       |
|    total_timesteps      | 3620864     |
| train/                  |             |
|    approx_kl            | 0.015763082 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.465       |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 8.04        |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=2.66 +/- 2.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 3625000     |
| train/                  |             |
|    approx_kl            | 0.017608726 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 6.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 222      |
|    time_elapsed    | 12259    |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=3.17 +/- 2.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 3650000     |
| train/                  |             |
|    approx_kl            | 0.010992741 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.9        |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -27.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 223      |
|    time_elapsed    | 12318    |
|    total_timesteps | 3653632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -28.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 224         |
|    time_elapsed         | 12367       |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.015775606 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.09        |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 4.51        |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=3.42 +/- 1.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.42        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.012897177 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.44        |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 21          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 225      |
|    time_elapsed    | 12425    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=2.14 +/- 0.57
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.14       |
| time/                   |            |
|    total_timesteps      | 3700000    |
| train/                  |            |
|    approx_kl            | 0.00770181 |
|    clip_fraction        | 0.0629     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 24.3       |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.00535   |
|    value_loss           | 239        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -86.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 226      |
|    time_elapsed    | 12483    |
|    total_timesteps | 3702784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -105        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 227         |
|    time_elapsed         | 12533       |
|    total_timesteps      | 3719168     |
| train/                  |             |
|    approx_kl            | 0.016367193 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.000887   |
|    value_loss           | 82.3        |
-----------------------------------------
Eval num_timesteps=3725000, episode_reward=2.66 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.66        |
| time/                   |             |
|    total_timesteps      | 3725000     |
| train/                  |             |
|    approx_kl            | 0.023626212 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00273     |
|    value_loss           | 135         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 228      |
|    time_elapsed    | 12591    |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=1.93 +/- 0.51
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.93      |
| time/                   |           |
|    total_timesteps      | 3750000   |
| train/                  |           |
|    approx_kl            | 0.0177461 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.35     |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.16      |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.0053   |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 229      |
|    time_elapsed    | 12649    |
|    total_timesteps | 3751936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -36.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 230         |
|    time_elapsed         | 12699       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.016751066 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.994       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 12.7        |
-----------------------------------------
Eval num_timesteps=3775000, episode_reward=3.01 +/- 1.54
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.01        |
| time/                   |             |
|    total_timesteps      | 3775000     |
| train/                  |             |
|    approx_kl            | 0.017391192 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 55.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 231      |
|    time_elapsed    | 12757    |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=2.53 +/- 1.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 3800000     |
| train/                  |             |
|    approx_kl            | 0.018420726 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.225       |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 4.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 232      |
|    time_elapsed    | 12816    |
|    total_timesteps | 3801088  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -37.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 233         |
|    time_elapsed         | 12865       |
|    total_timesteps      | 3817472     |
| train/                  |             |
|    approx_kl            | 0.017496675 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.45        |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.000167   |
|    value_loss           | 58.1        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=2.49 +/- 1.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 3825000     |
| train/                  |             |
|    approx_kl            | 0.018329225 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.527       |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 18.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 234      |
|    time_elapsed    | 12923    |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=-51.09 +/- 107.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -51.1       |
| time/                   |             |
|    total_timesteps      | 3850000     |
| train/                  |             |
|    approx_kl            | 0.015848674 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 5.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 235      |
|    time_elapsed    | 12982    |
|    total_timesteps | 3850240  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.68       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 236         |
|    time_elapsed         | 13031       |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.016509708 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.644       |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 1.9         |
-----------------------------------------
Eval num_timesteps=3875000, episode_reward=2.13 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.017524287 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.8         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 6.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 237      |
|    time_elapsed    | 13089    |
|    total_timesteps | 3883008  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -18.2     |
| time/                   |           |
|    fps                  | 296       |
|    iterations           | 238       |
|    time_elapsed         | 13138     |
|    total_timesteps      | 3899392   |
| train/                  |           |
|    approx_kl            | 0.0249016 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.34     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.14      |
|    n_updates            | 2370      |
|    policy_gradient_loss | 0.00395   |
|    value_loss           | 24.8      |
---------------------------------------
Eval num_timesteps=3900000, episode_reward=2.27 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.27        |
| time/                   |             |
|    total_timesteps      | 3900000     |
| train/                  |             |
|    approx_kl            | 0.020350281 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 239      |
|    time_elapsed    | 13197    |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=2.12 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.015230722 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0918      |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 1.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 240      |
|    time_elapsed    | 13255    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -30.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 241         |
|    time_elapsed         | 13304       |
|    total_timesteps      | 3948544     |
| train/                  |             |
|    approx_kl            | 0.024757195 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.00179     |
|    value_loss           | 80.2        |
-----------------------------------------
Eval num_timesteps=3950000, episode_reward=1.89 +/- 0.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.019477982 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.1         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 55          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -49.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 242      |
|    time_elapsed    | 13363    |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=2.13 +/- 1.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.13        |
| time/                   |             |
|    total_timesteps      | 3975000     |
| train/                  |             |
|    approx_kl            | 0.019372141 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.227       |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 173         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 243      |
|    time_elapsed    | 13421    |
|    total_timesteps | 3981312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -49.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 244         |
|    time_elapsed         | 13470       |
|    total_timesteps      | 3997696     |
| train/                  |             |
|    approx_kl            | 0.017638613 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | 44.6        |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.000606   |
|    value_loss           | 81.1        |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=2.31 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.017913995 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 1.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 245      |
|    time_elapsed    | 13528    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=2.10 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 4025000     |
| train/                  |             |
|    approx_kl            | 0.018062646 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.6         |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 16.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -26.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 246      |
|    time_elapsed    | 13587    |
|    total_timesteps | 4030464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.36       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 247         |
|    time_elapsed         | 13636       |
|    total_timesteps      | 4046848     |
| train/                  |             |
|    approx_kl            | 0.015684674 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 7.25        |
-----------------------------------------
Eval num_timesteps=4050000, episode_reward=3.10 +/- 1.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.018204935 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0882      |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 3.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 248      |
|    time_elapsed    | 13694    |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=1.91 +/- 0.42
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.91       |
| time/                   |            |
|    total_timesteps      | 4075000    |
| train/                  |            |
|    approx_kl            | 0.01665958 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.86       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.00405   |
|    value_loss           | 22.6       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 249      |
|    time_elapsed    | 13753    |
|    total_timesteps | 4079616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 250         |
|    time_elapsed         | 13802       |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.018259298 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.21        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 22.2        |
-----------------------------------------
Eval num_timesteps=4100000, episode_reward=-54.17 +/- 115.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -54.2       |
| time/                   |             |
|    total_timesteps      | 4100000     |
| train/                  |             |
|    approx_kl            | 0.016974686 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 96.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 251      |
|    time_elapsed    | 13860    |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=1.95 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.95        |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.016775241 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 71.7        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 146         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 252      |
|    time_elapsed    | 13919    |
|    total_timesteps | 4128768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 253         |
|    time_elapsed         | 13968       |
|    total_timesteps      | 4145152     |
| train/                  |             |
|    approx_kl            | 0.018155236 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.29        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 6.15        |
-----------------------------------------
Eval num_timesteps=4150000, episode_reward=2.49 +/- 1.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 4150000     |
| train/                  |             |
|    approx_kl            | 0.019182712 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.1        |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 68.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -54.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 254      |
|    time_elapsed    | 14026    |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=2.18 +/- 0.73
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 4175000     |
| train/                  |             |
|    approx_kl            | 0.015842587 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 44.2        |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 53.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 255      |
|    time_elapsed    | 14084    |
|    total_timesteps | 4177920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -48.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 256         |
|    time_elapsed         | 14134       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.019022424 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 99.5        |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 76.3        |
-----------------------------------------
Eval num_timesteps=4200000, episode_reward=-39.51 +/- 78.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -39.5      |
| time/                   |            |
|    total_timesteps      | 4200000    |
| train/                  |            |
|    approx_kl            | 0.01834771 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.45       |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.00805   |
|    value_loss           | 7.34       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -28.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 257      |
|    time_elapsed    | 14192    |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2.40 +/- 1.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 4225000     |
| train/                  |             |
|    approx_kl            | 0.021792725 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.761       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 13.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 258      |
|    time_elapsed    | 14250    |
|    total_timesteps | 4227072  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.83       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 259         |
|    time_elapsed         | 14300       |
|    total_timesteps      | 4243456     |
| train/                  |             |
|    approx_kl            | 0.018037625 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.02        |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 1.36        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=1.65 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 4250000     |
| train/                  |             |
|    approx_kl            | 0.015271561 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 2.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 260      |
|    time_elapsed    | 14358    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=2.22 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 4275000     |
| train/                  |             |
|    approx_kl            | 0.009696953 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.05        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 83.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 261      |
|    time_elapsed    | 14416    |
|    total_timesteps | 4276224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 262         |
|    time_elapsed         | 14466       |
|    total_timesteps      | 4292608     |
| train/                  |             |
|    approx_kl            | 0.019513957 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 1.27        |
-----------------------------------------
Eval num_timesteps=4300000, episode_reward=2.29 +/- 0.85
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.018583912 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 0.474       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 263      |
|    time_elapsed    | 14524    |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=2.49 +/- 0.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 4325000     |
| train/                  |             |
|    approx_kl            | 0.015743231 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.19        |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 14          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.2     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 264      |
|    time_elapsed    | 14582    |
|    total_timesteps | 4325376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.12       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 265         |
|    time_elapsed         | 14631       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.017584369 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.37       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 20.2        |
-----------------------------------------
Eval num_timesteps=4350000, episode_reward=1.79 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.016760344 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.022       |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 1.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.63    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 266      |
|    time_elapsed    | 14690    |
|    total_timesteps | 4358144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 267         |
|    time_elapsed         | 14739       |
|    total_timesteps      | 4374528     |
| train/                  |             |
|    approx_kl            | 0.015816133 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 5.58        |
-----------------------------------------
Eval num_timesteps=4375000, episode_reward=1.52 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.016518502 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.0867      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.543       |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00646    |
|    value_loss           | 28          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.95    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 268      |
|    time_elapsed    | 14797    |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=1.37 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.018935123 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 0.793       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.93    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 269      |
|    time_elapsed    | 14856    |
|    total_timesteps | 4407296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.15       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 270         |
|    time_elapsed         | 14905       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.018883392 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0278      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0727      |
-----------------------------------------
Eval num_timesteps=4425000, episode_reward=1.71 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.015541883 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.597       |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 1.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.169    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 271      |
|    time_elapsed    | 14963    |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=2.03 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 4450000     |
| train/                  |             |
|    approx_kl            | 0.024100605 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0171     |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 272      |
|    time_elapsed    | 15022    |
|    total_timesteps | 4456448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -326         |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 273          |
|    time_elapsed         | 15071        |
|    total_timesteps      | 4472832      |
| train/                  |              |
|    approx_kl            | 0.0070943213 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.35        |
|    explained_variance   | 0.632        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.238        |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 205          |
------------------------------------------
Eval num_timesteps=4475000, episode_reward=1.75 +/- 0.32
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 4475000      |
| train/                  |              |
|    approx_kl            | 0.0026126555 |
|    clip_fraction        | 0.00546      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0003       |
|    loss                 | 206          |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 634          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 274      |
|    time_elapsed    | 15129    |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=1.58 +/- 0.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.58       |
| time/                   |            |
|    total_timesteps      | 4500000    |
| train/                  |            |
|    approx_kl            | 0.03343706 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.6       |
|    n_updates            | 2740       |
|    policy_gradient_loss | 0.0083     |
|    value_loss           | 218        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 275      |
|    time_elapsed    | 15187    |
|    total_timesteps | 4505600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -136        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 276         |
|    time_elapsed         | 15237       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.022263046 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.54        |
|    n_updates            | 2750        |
|    policy_gradient_loss | 0.000417    |
|    value_loss           | 24.6        |
-----------------------------------------
Eval num_timesteps=4525000, episode_reward=1.64 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.039623864 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.302       |
|    n_updates            | 2760        |
|    policy_gradient_loss | 0.00668     |
|    value_loss           | 36.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 277      |
|    time_elapsed    | 15295    |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=2.03 +/- 1.27
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.013848546 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.07        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 19.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 278      |
|    time_elapsed    | 15353    |
|    total_timesteps | 4554752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -34.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 279         |
|    time_elapsed         | 15402       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.020774197 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 4.13        |
-----------------------------------------
Eval num_timesteps=4575000, episode_reward=1.02 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.02        |
| time/                   |             |
|    total_timesteps      | 4575000     |
| train/                  |             |
|    approx_kl            | 0.015515489 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 46.5        |
|    n_updates            | 2790        |
|    policy_gradient_loss | -3.94e-05   |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -33.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 280      |
|    time_elapsed    | 15461    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=1.41 +/- 0.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.016576301 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0788      |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 1.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 281      |
|    time_elapsed    | 15519    |
|    total_timesteps | 4603904  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -121        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 282         |
|    time_elapsed         | 15568       |
|    total_timesteps      | 4620288     |
| train/                  |             |
|    approx_kl            | 0.007934794 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | -0.0144     |
|    learning_rate        | 0.0003      |
|    loss                 | 308         |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=4625000, episode_reward=1.29 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 4625000     |
| train/                  |             |
|    approx_kl            | 0.019303089 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.649       |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 3.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 283      |
|    time_elapsed    | 15627    |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=1.06 +/- 0.07
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.06         |
| time/                   |              |
|    total_timesteps      | 4650000      |
| train/                  |              |
|    approx_kl            | 0.0091146985 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0003       |
|    loss                 | 543          |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00468     |
|    value_loss           | 1.43e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 284      |
|    time_elapsed    | 15685    |
|    total_timesteps | 4653056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 285        |
|    time_elapsed         | 15734      |
|    total_timesteps      | 4669440    |
| train/                  |            |
|    approx_kl            | 0.01743643 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.59       |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.007     |
|    value_loss           | 6.66       |
----------------------------------------
Eval num_timesteps=4675000, episode_reward=-586.65 +/- 1176.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -587        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.015540769 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0888      |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 10.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 286      |
|    time_elapsed    | 15793    |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=1.34 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 4700000     |
| train/                  |             |
|    approx_kl            | 0.017474463 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.507       |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 4.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 287      |
|    time_elapsed    | 15851    |
|    total_timesteps | 4702208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 288         |
|    time_elapsed         | 15900       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.022337658 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.587       |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 38.9        |
-----------------------------------------
Eval num_timesteps=4725000, episode_reward=1.33 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.33        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.017626178 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0307      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 2.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 289      |
|    time_elapsed    | 15959    |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=1.28 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 4750000     |
| train/                  |             |
|    approx_kl            | 0.014017642 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.8         |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 5.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.81    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 290      |
|    time_elapsed    | 16017    |
|    total_timesteps | 4751360  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 291         |
|    time_elapsed         | 16066       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.022668019 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 2900        |
|    policy_gradient_loss | 7.41e-05    |
|    value_loss           | 29.8        |
-----------------------------------------
Eval num_timesteps=4775000, episode_reward=1.19 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 4775000     |
| train/                  |             |
|    approx_kl            | 0.019690007 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.522       |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00707    |
|    value_loss           | 19.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 292      |
|    time_elapsed    | 16125    |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=1.19 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.019221958 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.04        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 3.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 293      |
|    time_elapsed    | 16183    |
|    total_timesteps | 4800512  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 294         |
|    time_elapsed         | 16232       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.019992419 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.3        |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 62.8        |
-----------------------------------------
Eval num_timesteps=4825000, episode_reward=1.82 +/- 1.53
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.82       |
| time/                   |            |
|    total_timesteps      | 4825000    |
| train/                  |            |
|    approx_kl            | 0.01786182 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0147     |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 295      |
|    time_elapsed    | 16290    |
|    total_timesteps | 4833280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -64.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 296         |
|    time_elapsed         | 16340       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.010016603 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.33        |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 57.5        |
-----------------------------------------
Eval num_timesteps=4850000, episode_reward=1.18 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 4850000     |
| train/                  |             |
|    approx_kl            | 0.020690378 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.0081      |
|    learning_rate        | 0.0003      |
|    loss                 | 70.8        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 411         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -63.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 297      |
|    time_elapsed    | 16398    |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=1.11 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.11        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.023438992 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 25          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -64.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 298      |
|    time_elapsed    | 16456    |
|    total_timesteps | 4882432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -53.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 299         |
|    time_elapsed         | 16505       |
|    total_timesteps      | 4898816     |
| train/                  |             |
|    approx_kl            | 0.016602077 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0791      |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 0.821       |
-----------------------------------------
Eval num_timesteps=4900000, episode_reward=1.43 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 4900000     |
| train/                  |             |
|    approx_kl            | 0.019420126 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.597       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 5.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 300      |
|    time_elapsed    | 16564    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=1.37 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.009162156 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 140         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 301      |
|    time_elapsed    | 16622    |
|    total_timesteps | 4931584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -20.8      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 302        |
|    time_elapsed         | 16671      |
|    total_timesteps      | 4947968    |
| train/                  |            |
|    approx_kl            | 0.02102865 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.165      |
|    n_updates            | 3010       |
|    policy_gradient_loss | -0.00588   |
|    value_loss           | 9.48       |
----------------------------------------
Eval num_timesteps=4950000, episode_reward=1.20 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 4950000     |
| train/                  |             |
|    approx_kl            | 0.023697333 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.099       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.677       |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.000159   |
|    value_loss           | 87.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 303      |
|    time_elapsed    | 16730    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=1.25 +/- 0.15
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.016563263 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.19        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 5.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 304      |
|    time_elapsed    | 16788    |
|    total_timesteps | 4980736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 305         |
|    time_elapsed         | 16837       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 0.018354038 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.76        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 6.02        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=1.30 +/- 0.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.021536315 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0435      |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 0.819       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.2     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 306      |
|    time_elapsed    | 16896    |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=1.82 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 5025000     |
| train/                  |             |
|    approx_kl            | 0.020136382 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.4        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.000365   |
|    value_loss           | 22.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 307      |
|    time_elapsed    | 16954    |
|    total_timesteps | 5029888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.24       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 308         |
|    time_elapsed         | 17003       |
|    total_timesteps      | 5046272     |
| train/                  |             |
|    approx_kl            | 0.018158507 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0995      |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 0.233       |
-----------------------------------------
Eval num_timesteps=5050000, episode_reward=2.07 +/- 1.38
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 5050000     |
| train/                  |             |
|    approx_kl            | 0.018667521 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.193       |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 0.164       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 309      |
|    time_elapsed    | 17062    |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5075000, episode_reward=1.82 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 5075000     |
| train/                  |             |
|    approx_kl            | 0.019602064 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 7.42        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 310      |
|    time_elapsed    | 17120    |
|    total_timesteps | 5079040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 311         |
|    time_elapsed         | 17169       |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.010177281 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.8        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 92          |
-----------------------------------------
Eval num_timesteps=5100000, episode_reward=1.37 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 5100000     |
| train/                  |             |
|    approx_kl            | 0.020686308 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00282    |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.0508      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 312      |
|    time_elapsed    | 17228    |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5125000, episode_reward=1.32 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 5125000     |
| train/                  |             |
|    approx_kl            | 0.015858518 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 9.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 313      |
|    time_elapsed    | 17286    |
|    total_timesteps | 5128192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.83       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 314         |
|    time_elapsed         | 17335       |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.014727711 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 2.44        |
-----------------------------------------
Eval num_timesteps=5150000, episode_reward=0.98 +/- 0.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.976       |
| time/                   |             |
|    total_timesteps      | 5150000     |
| train/                  |             |
|    approx_kl            | 0.021614578 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.229       |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 2.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 315      |
|    time_elapsed    | 17393    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=1.38 +/- 0.78
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.38       |
| time/                   |            |
|    total_timesteps      | 5175000    |
| train/                  |            |
|    approx_kl            | 0.02047868 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.19      |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.00845   |
|    value_loss           | 0.614      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.58    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 316      |
|    time_elapsed    | 17452    |
|    total_timesteps | 5177344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.05       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 317         |
|    time_elapsed         | 17501       |
|    total_timesteps      | 5193728     |
| train/                  |             |
|    approx_kl            | 0.013596488 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 4.22        |
-----------------------------------------
Eval num_timesteps=5200000, episode_reward=1.74 +/- 1.44
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 5200000     |
| train/                  |             |
|    approx_kl            | 0.016140345 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.384       |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 8.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.52    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 318      |
|    time_elapsed    | 17559    |
|    total_timesteps | 5210112  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=1.90 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 5225000     |
| train/                  |             |
|    approx_kl            | 0.017206825 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0518      |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 0.512       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.84    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 319      |
|    time_elapsed    | 17618    |
|    total_timesteps | 5226496  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -2.47     |
| time/                   |           |
|    fps                  | 296       |
|    iterations           | 320       |
|    time_elapsed         | 17667     |
|    total_timesteps      | 5242880   |
| train/                  |           |
|    approx_kl            | 0.0207772 |
|    clip_fraction        | 0.244     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.02     |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0708    |
|    n_updates            | 3190      |
|    policy_gradient_loss | -0.00993  |
|    value_loss           | 2.82      |
---------------------------------------
Eval num_timesteps=5250000, episode_reward=0.93 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.935       |
| time/                   |             |
|    total_timesteps      | 5250000     |
| train/                  |             |
|    approx_kl            | 0.022189436 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 1.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.16    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 321      |
|    time_elapsed    | 17725    |
|    total_timesteps | 5259264  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=-1179.19 +/- 2360.01
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.18e+03   |
| time/                   |             |
|    total_timesteps      | 5275000     |
| train/                  |             |
|    approx_kl            | 0.020318426 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 8.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.42    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 322      |
|    time_elapsed    | 17784    |
|    total_timesteps | 5275648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 323         |
|    time_elapsed         | 17833       |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.026134875 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 2.88        |
-----------------------------------------
Eval num_timesteps=5300000, episode_reward=0.99 +/- 0.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 0.987       |
| time/                   |             |
|    total_timesteps      | 5300000     |
| train/                  |             |
|    approx_kl            | 0.014823567 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.288       |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 71.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 324      |
|    time_elapsed    | 17891    |
|    total_timesteps | 5308416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -44         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 325         |
|    time_elapsed         | 17940       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.012652132 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | 456         |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=5325000, episode_reward=1.08 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.08        |
| time/                   |             |
|    total_timesteps      | 5325000     |
| train/                  |             |
|    approx_kl            | 0.019321544 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.6        |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 86          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 326      |
|    time_elapsed    | 17998    |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2.01 +/- 1.86
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 5350000     |
| train/                  |             |
|    approx_kl            | 0.018289324 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.75        |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 5.49        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -30.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 327      |
|    time_elapsed    | 18057    |
|    total_timesteps | 5357568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 328         |
|    time_elapsed         | 18106       |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.020758575 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0973      |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 5.84        |
-----------------------------------------
Eval num_timesteps=5375000, episode_reward=1.30 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 5375000     |
| train/                  |             |
|    approx_kl            | 0.016606858 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.548       |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 3.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.66    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 329      |
|    time_elapsed    | 18164    |
|    total_timesteps | 5390336  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=2.63 +/- 2.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63        |
| time/                   |             |
|    total_timesteps      | 5400000     |
| train/                  |             |
|    approx_kl            | 0.010878589 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.000357   |
|    value_loss           | 344         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 330      |
|    time_elapsed    | 18223    |
|    total_timesteps | 5406720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -137        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 331         |
|    time_elapsed         | 18272       |
|    total_timesteps      | 5423104     |
| train/                  |             |
|    approx_kl            | 0.020685365 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.406       |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 2.82        |
-----------------------------------------
Eval num_timesteps=5425000, episode_reward=1.17 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 5425000     |
| train/                  |             |
|    approx_kl            | 0.017133024 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.268       |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 2.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 332      |
|    time_elapsed    | 18330    |
|    total_timesteps | 5439488  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=1.28 +/- 0.55
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.28         |
| time/                   |              |
|    total_timesteps      | 5450000      |
| train/                  |              |
|    approx_kl            | 0.0136275925 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.135        |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00712     |
|    value_loss           | 5.31         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.64    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 333      |
|    time_elapsed    | 18389    |
|    total_timesteps | 5455872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 334         |
|    time_elapsed         | 18438       |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.019654606 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 0.508       |
-----------------------------------------
Eval num_timesteps=5475000, episode_reward=1.41 +/- 0.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 5475000     |
| train/                  |             |
|    approx_kl            | 0.006713571 |
|    clip_fraction        | 0.0602      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.5        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 278         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -36.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 335      |
|    time_elapsed    | 18496    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=1.13 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.13        |
| time/                   |             |
|    total_timesteps      | 5500000     |
| train/                  |             |
|    approx_kl            | 0.016857255 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.196       |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 0.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -38.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 336      |
|    time_elapsed    | 18555    |
|    total_timesteps | 5505024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 337         |
|    time_elapsed         | 18604       |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.019598288 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 25          |
-----------------------------------------
Eval num_timesteps=5525000, episode_reward=-6.03 +/- 16.36
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -6.03      |
| time/                   |            |
|    total_timesteps      | 5525000    |
| train/                  |            |
|    approx_kl            | 0.01855591 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.82      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.409      |
|    n_updates            | 3370       |
|    policy_gradient_loss | -0.00436   |
|    value_loss           | 84.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 338      |
|    time_elapsed    | 18662    |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=1.32 +/- 0.43
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 5550000     |
| train/                  |             |
|    approx_kl            | 0.017622558 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.667       |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.0073     |
|    value_loss           | 11.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 339      |
|    time_elapsed    | 18721    |
|    total_timesteps | 5554176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -26.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 340         |
|    time_elapsed         | 18770       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.017973593 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 39.7        |
-----------------------------------------
Eval num_timesteps=5575000, episode_reward=2.92 +/- 1.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.92        |
| time/                   |             |
|    total_timesteps      | 5575000     |
| train/                  |             |
|    approx_kl            | 0.022016402 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.464       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 10.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 341      |
|    time_elapsed    | 18828    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=1.24 +/- 0.35
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.24       |
| time/                   |            |
|    total_timesteps      | 5600000    |
| train/                  |            |
|    approx_kl            | 0.02022288 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.89       |
|    n_updates            | 3410       |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 10.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 342      |
|    time_elapsed    | 18886    |
|    total_timesteps | 5603328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.83       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 343         |
|    time_elapsed         | 18936       |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.019849803 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 2.29        |
-----------------------------------------
Eval num_timesteps=5625000, episode_reward=1.92 +/- 0.88
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 5625000     |
| train/                  |             |
|    approx_kl            | 0.018146712 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 344      |
|    time_elapsed    | 18994    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=1.80 +/- 0.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 5650000     |
| train/                  |             |
|    approx_kl            | 0.019455034 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.841       |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 8.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.94    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 345      |
|    time_elapsed    | 19052    |
|    total_timesteps | 5652480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.51       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 346         |
|    time_elapsed         | 19102       |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.019185621 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 7           |
-----------------------------------------
Eval num_timesteps=5675000, episode_reward=1.93 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 5675000     |
| train/                  |             |
|    approx_kl            | 0.020700991 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.34        |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 6.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.5     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 347      |
|    time_elapsed    | 19160    |
|    total_timesteps | 5685248  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=2.17 +/- 1.40
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.17       |
| time/                   |            |
|    total_timesteps      | 5700000    |
| train/                  |            |
|    approx_kl            | 0.01563581 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.79      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.159      |
|    n_updates            | 3470       |
|    policy_gradient_loss | -0.00886   |
|    value_loss           | 4.55       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -50.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 348      |
|    time_elapsed    | 19218    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -47.4       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 349         |
|    time_elapsed         | 19267       |
|    total_timesteps      | 5718016     |
| train/                  |             |
|    approx_kl            | 0.037598625 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 3480        |
|    policy_gradient_loss | 0.00248     |
|    value_loss           | 57.1        |
-----------------------------------------
Eval num_timesteps=5725000, episode_reward=1.79 +/- 1.05
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 5725000     |
| train/                  |             |
|    approx_kl            | 0.023018261 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0787      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.587       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -47.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 350      |
|    time_elapsed    | 19326    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=1.00 +/- 0.19
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 5750000     |
| train/                  |             |
|    approx_kl            | 0.021204289 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.6         |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 5.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -46.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 351      |
|    time_elapsed    | 19384    |
|    total_timesteps | 5750784  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.86       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 352         |
|    time_elapsed         | 19433       |
|    total_timesteps      | 5767168     |
| train/                  |             |
|    approx_kl            | 0.019717047 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.634       |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 1.39        |
-----------------------------------------
Eval num_timesteps=5775000, episode_reward=-580.15 +/- 1163.93
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -580        |
| time/                   |             |
|    total_timesteps      | 5775000     |
| train/                  |             |
|    approx_kl            | 0.019843351 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.303       |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 1.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.55    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 353      |
|    time_elapsed    | 19492    |
|    total_timesteps | 5783552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -27.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 354         |
|    time_elapsed         | 19541       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.025883734 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.339       |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.000393   |
|    value_loss           | 43.6        |
-----------------------------------------
Eval num_timesteps=5800000, episode_reward=1.51 +/- 0.37
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 5800000     |
| train/                  |             |
|    approx_kl            | 0.021039248 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | -0.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.09        |
|    n_updates            | 3540        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 89.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 355      |
|    time_elapsed    | 19599    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5825000, episode_reward=1.11 +/- 1.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.11        |
| time/                   |             |
|    total_timesteps      | 5825000     |
| train/                  |             |
|    approx_kl            | 0.028859755 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 3550        |
|    policy_gradient_loss | 0.00556     |
|    value_loss           | 410         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 356      |
|    time_elapsed    | 19658    |
|    total_timesteps | 5832704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -134        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 357         |
|    time_elapsed         | 19707       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.022578122 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0272      |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.836       |
-----------------------------------------
Eval num_timesteps=5850000, episode_reward=1.73 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 5850000     |
| train/                  |             |
|    approx_kl            | 0.012257351 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | -0.376      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 119         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 358      |
|    time_elapsed    | 19765    |
|    total_timesteps | 5865472  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=1.88 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 5875000     |
| train/                  |             |
|    approx_kl            | 0.009633198 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | -0.599      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 156         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -74.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 359      |
|    time_elapsed    | 19824    |
|    total_timesteps | 5881856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -76        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 360        |
|    time_elapsed         | 19873      |
|    total_timesteps      | 5898240    |
| train/                  |            |
|    approx_kl            | 0.02126536 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.486      |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.00877   |
|    value_loss           | 3.11       |
----------------------------------------
Eval num_timesteps=5900000, episode_reward=1.47 +/- 0.47
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 5900000     |
| train/                  |             |
|    approx_kl            | 0.022458788 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00511    |
|    value_loss           | 7.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 361      |
|    time_elapsed    | 19931    |
|    total_timesteps | 5914624  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=-1177.27 +/- 2360.64
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | -1.18e+03  |
| time/                   |            |
|    total_timesteps      | 5925000    |
| train/                  |            |
|    approx_kl            | 0.01921164 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 3610       |
|    policy_gradient_loss | -0.00325   |
|    value_loss           | 14.9       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.5     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 362      |
|    time_elapsed    | 19989    |
|    total_timesteps | 5931008  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.33       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 363         |
|    time_elapsed         | 20039       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.017912941 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0132      |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=5950000, episode_reward=-588.75 +/- 1180.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -589        |
| time/                   |             |
|    total_timesteps      | 5950000     |
| train/                  |             |
|    approx_kl            | 0.019865742 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0326      |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0438      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.05    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 364      |
|    time_elapsed    | 20097    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=1.84 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 5975000     |
| train/                  |             |
|    approx_kl            | 0.018400436 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.754       |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 1.7         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.601   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 365      |
|    time_elapsed    | 20156    |
|    total_timesteps | 5980160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.96       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 366         |
|    time_elapsed         | 20205       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.013028784 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 62.1        |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=2.63 +/- 2.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.63        |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.021401765 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.0343      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0896      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 26.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 367      |
|    time_elapsed    | 20263    |
|    total_timesteps | 6012928  |
---------------------------------
Eval num_timesteps=6025000, episode_reward=1.63 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 6025000     |
| train/                  |             |
|    approx_kl            | 0.010562874 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.915       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.000728   |
|    value_loss           | 13.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 368      |
|    time_elapsed    | 20321    |
|    total_timesteps | 6029312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.62       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 369         |
|    time_elapsed         | 20371       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.031750876 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0687      |
|    n_updates            | 3680        |
|    policy_gradient_loss | 0.000257    |
|    value_loss           | 4.05        |
-----------------------------------------
Eval num_timesteps=6050000, episode_reward=1.70 +/- 0.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 6050000     |
| train/                  |             |
|    approx_kl            | 0.026393887 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | -0.00324    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.79        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -119     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 370      |
|    time_elapsed    | 20429    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6075000, episode_reward=1.28 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 6075000     |
| train/                  |             |
|    approx_kl            | 0.016738398 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 1.6e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 371      |
|    time_elapsed    | 20487    |
|    total_timesteps | 6078464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -115        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 372         |
|    time_elapsed         | 20536       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.018800173 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.423       |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 1.14        |
-----------------------------------------
Eval num_timesteps=6100000, episode_reward=1.35 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.35        |
| time/                   |             |
|    total_timesteps      | 6100000     |
| train/                  |             |
|    approx_kl            | 0.016685493 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 2.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 373      |
|    time_elapsed    | 20595    |
|    total_timesteps | 6111232  |
---------------------------------
Eval num_timesteps=6125000, episode_reward=1.59 +/- 0.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.59        |
| time/                   |             |
|    total_timesteps      | 6125000     |
| train/                  |             |
|    approx_kl            | 0.018173143 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 7.42        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.33    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 374      |
|    time_elapsed    | 20653    |
|    total_timesteps | 6127616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.16       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 375         |
|    time_elapsed         | 20702       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.021939455 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.55        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 6.38        |
-----------------------------------------
Eval num_timesteps=6150000, episode_reward=1.50 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 6150000     |
| train/                  |             |
|    approx_kl            | 0.020352073 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 4.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.988   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 376      |
|    time_elapsed    | 20761    |
|    total_timesteps | 6160384  |
---------------------------------
Eval num_timesteps=6175000, episode_reward=1.42 +/- 0.71
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 6175000     |
| train/                  |             |
|    approx_kl            | 0.019309659 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0964      |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 1.06        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.21    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 377      |
|    time_elapsed    | 20819    |
|    total_timesteps | 6176768  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.32       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 378         |
|    time_elapsed         | 20868       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.014717499 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 1.14        |
-----------------------------------------
Eval num_timesteps=6200000, episode_reward=1.50 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 6200000     |
| train/                  |             |
|    approx_kl            | 0.023227768 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 28          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 379      |
|    time_elapsed    | 20927    |
|    total_timesteps | 6209536  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=2.52 +/- 1.30
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 6225000     |
| train/                  |             |
|    approx_kl            | 0.017797537 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.292       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 86.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 380      |
|    time_elapsed    | 20985    |
|    total_timesteps | 6225920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -17.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 381         |
|    time_elapsed         | 21034       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.018929884 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 5.87        |
-----------------------------------------
Eval num_timesteps=6250000, episode_reward=1.56 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 6250000     |
| train/                  |             |
|    approx_kl            | 0.017790988 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 1.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 382      |
|    time_elapsed    | 21093    |
|    total_timesteps | 6258688  |
---------------------------------
Eval num_timesteps=6275000, episode_reward=-2354.25 +/- 4712.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.35e+03   |
| time/                   |             |
|    total_timesteps      | 6275000     |
| train/                  |             |
|    approx_kl            | 0.018378034 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0261      |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 0.754       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.01    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 383      |
|    time_elapsed    | 21151    |
|    total_timesteps | 6275072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.4       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 384        |
|    time_elapsed         | 21200      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.02303239 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.0476     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0903     |
|    n_updates            | 3830       |
|    policy_gradient_loss | -0.0055    |
|    value_loss           | 9.14       |
----------------------------------------
Eval num_timesteps=6300000, episode_reward=2.28 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.28        |
| time/                   |             |
|    total_timesteps      | 6300000     |
| train/                  |             |
|    approx_kl            | 0.022835877 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.321       |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 14.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.74    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 385      |
|    time_elapsed    | 21259    |
|    total_timesteps | 6307840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -87.2       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 386         |
|    time_elapsed         | 21308       |
|    total_timesteps      | 6324224     |
| train/                  |             |
|    approx_kl            | 0.017835466 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0372      |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 9.64        |
-----------------------------------------
Eval num_timesteps=6325000, episode_reward=2.21 +/- 1.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 6325000     |
| train/                  |             |
|    approx_kl            | 0.009822859 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | -0.728      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.46        |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 453         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -85.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 387      |
|    time_elapsed    | 21366    |
|    total_timesteps | 6340608  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=1.45 +/- 0.45
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 6350000     |
| train/                  |             |
|    approx_kl            | 0.020491548 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.418       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00903    |
|    value_loss           | 2.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -91.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 388      |
|    time_elapsed    | 21424    |
|    total_timesteps | 6356992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -90.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 389         |
|    time_elapsed         | 21474       |
|    total_timesteps      | 6373376     |
| train/                  |             |
|    approx_kl            | 0.008837031 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.31       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35        |
|    n_updates            | 3880        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 35.2        |
-----------------------------------------
Eval num_timesteps=6375000, episode_reward=1.86 +/- 0.56
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.86       |
| time/                   |            |
|    total_timesteps      | 6375000    |
| train/                  |            |
|    approx_kl            | 0.02005544 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0377     |
|    n_updates            | 3890       |
|    policy_gradient_loss | -0.00714   |
|    value_loss           | 1.01       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 390      |
|    time_elapsed    | 21532    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=1.74 +/- 0.26
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.021527909 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 4.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 391      |
|    time_elapsed    | 21590    |
|    total_timesteps | 6406144  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -15.6      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 392        |
|    time_elapsed         | 21640      |
|    total_timesteps      | 6422528    |
| train/                  |            |
|    approx_kl            | 0.02220831 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.41      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.035      |
|    n_updates            | 3910       |
|    policy_gradient_loss | -0.00768   |
|    value_loss           | 0.115      |
----------------------------------------
Eval num_timesteps=6425000, episode_reward=1.19 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 6425000     |
| train/                  |             |
|    approx_kl            | 0.010800569 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 114         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 393      |
|    time_elapsed    | 21698    |
|    total_timesteps | 6438912  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=1.88 +/- 0.75
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 6450000     |
| train/                  |             |
|    approx_kl            | 0.020009013 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0751      |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 394      |
|    time_elapsed    | 21756    |
|    total_timesteps | 6455296  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.68       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 395         |
|    time_elapsed         | 21806       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.017857855 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00511    |
|    value_loss           | 3.11        |
-----------------------------------------
Eval num_timesteps=6475000, episode_reward=1.27 +/- 0.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 6475000     |
| train/                  |             |
|    approx_kl            | 0.019001914 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 0.417       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.04    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 396      |
|    time_elapsed    | 21864    |
|    total_timesteps | 6488064  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=1.44 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 6500000     |
| train/                  |             |
|    approx_kl            | 0.012223801 |
|    clip_fraction        | 0.073       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 12          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.09    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 397      |
|    time_elapsed    | 21922    |
|    total_timesteps | 6504448  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 600       |
|    ep_rew_mean          | -4.24     |
| time/                   |           |
|    fps                  | 296       |
|    iterations           | 398       |
|    time_elapsed         | 21971     |
|    total_timesteps      | 6520832   |
| train/                  |           |
|    approx_kl            | 0.0259788 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.55     |
|    explained_variance   | 0.88      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.608     |
|    n_updates            | 3970      |
|    policy_gradient_loss | -0.00566  |
|    value_loss           | 1.13      |
---------------------------------------
Eval num_timesteps=6525000, episode_reward=1.12 +/- 0.22
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.12        |
| time/                   |             |
|    total_timesteps      | 6525000     |
| train/                  |             |
|    approx_kl            | 0.022482712 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.15        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 0.368       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.23    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 399      |
|    time_elapsed    | 22030    |
|    total_timesteps | 6537216  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=2.12 +/- 1.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 6550000     |
| train/                  |             |
|    approx_kl            | 0.021068059 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.091       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 3.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 400      |
|    time_elapsed    | 22088    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 401         |
|    time_elapsed         | 22137       |
|    total_timesteps      | 6569984     |
| train/                  |             |
|    approx_kl            | 0.022763118 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0717      |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.19        |
-----------------------------------------
Eval num_timesteps=6575000, episode_reward=1.52 +/- 0.63
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 6575000      |
| train/                  |              |
|    approx_kl            | 0.0120618865 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | -0.008       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.64         |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 78.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 402      |
|    time_elapsed    | 22196    |
|    total_timesteps | 6586368  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=1.63 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 6600000     |
| train/                  |             |
|    approx_kl            | 0.020413864 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.56        |
|    n_updates            | 4020        |
|    policy_gradient_loss | 0.000395    |
|    value_loss           | 49.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 403      |
|    time_elapsed    | 22255    |
|    total_timesteps | 6602752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -23.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 404         |
|    time_elapsed         | 22304       |
|    total_timesteps      | 6619136     |
| train/                  |             |
|    approx_kl            | 0.020534702 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.9         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 4.85        |
-----------------------------------------
Eval num_timesteps=6625000, episode_reward=1.51 +/- 0.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 6625000     |
| train/                  |             |
|    approx_kl            | 0.016171915 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.286       |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 15.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.48    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 405      |
|    time_elapsed    | 22362    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=1.37 +/- 0.52
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 6650000     |
| train/                  |             |
|    approx_kl            | 0.031453542 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.000489   |
|    value_loss           | 12.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.34    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 406      |
|    time_elapsed    | 22421    |
|    total_timesteps | 6651904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -6.45      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 407        |
|    time_elapsed         | 22470      |
|    total_timesteps      | 6668288    |
| train/                  |            |
|    approx_kl            | 0.02005061 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.29      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.9       |
|    n_updates            | 4060       |
|    policy_gradient_loss | -0.00657   |
|    value_loss           | 2.08       |
----------------------------------------
Eval num_timesteps=6675000, episode_reward=1.48 +/- 0.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 6675000     |
| train/                  |             |
|    approx_kl            | 0.021421611 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.54        |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 0.903       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.04    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 408      |
|    time_elapsed    | 22528    |
|    total_timesteps | 6684672  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=2.09 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 6700000     |
| train/                  |             |
|    approx_kl            | 0.016031452 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0818      |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 3.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.06    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 409      |
|    time_elapsed    | 22587    |
|    total_timesteps | 6701056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.04       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 410         |
|    time_elapsed         | 22636       |
|    total_timesteps      | 6717440     |
| train/                  |             |
|    approx_kl            | 0.020841619 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0491      |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00707    |
|    value_loss           | 2.3         |
-----------------------------------------
Eval num_timesteps=6725000, episode_reward=2.03 +/- 0.98
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.03        |
| time/                   |             |
|    total_timesteps      | 6725000     |
| train/                  |             |
|    approx_kl            | 0.026295178 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00045    |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00958    |
|    value_loss           | 2.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 411      |
|    time_elapsed    | 22694    |
|    total_timesteps | 6733824  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2.02 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 6750000     |
| train/                  |             |
|    approx_kl            | 0.005005302 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | -0.13       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.3        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 255         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 412      |
|    time_elapsed    | 22752    |
|    total_timesteps | 6750208  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -168        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 413         |
|    time_elapsed         | 22802       |
|    total_timesteps      | 6766592     |
| train/                  |             |
|    approx_kl            | 0.020414505 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.722       |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 16.5        |
-----------------------------------------
Eval num_timesteps=6775000, episode_reward=1.83 +/- 0.69
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 6775000     |
| train/                  |             |
|    approx_kl            | 0.019773196 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.55        |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 29.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 414      |
|    time_elapsed    | 22860    |
|    total_timesteps | 6782976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.4        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 415         |
|    time_elapsed         | 22909       |
|    total_timesteps      | 6799360     |
| train/                  |             |
|    approx_kl            | 0.016874712 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 1.91        |
-----------------------------------------
Eval num_timesteps=6800000, episode_reward=1.57 +/- 0.74
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.57        |
| time/                   |             |
|    total_timesteps      | 6800000     |
| train/                  |             |
|    approx_kl            | 0.022201218 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 0.763       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.03    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 416      |
|    time_elapsed    | 22968    |
|    total_timesteps | 6815744  |
---------------------------------
Eval num_timesteps=6825000, episode_reward=1.37 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 6825000     |
| train/                  |             |
|    approx_kl            | 0.022506384 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0632      |
|    n_updates            | 4160        |
|    policy_gradient_loss | -1.29e-05   |
|    value_loss           | 2.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.126    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 417      |
|    time_elapsed    | 23026    |
|    total_timesteps | 6832128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.3        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 418         |
|    time_elapsed         | 23075       |
|    total_timesteps      | 6848512     |
| train/                  |             |
|    approx_kl            | 0.020404687 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.21        |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 8.72        |
-----------------------------------------
Eval num_timesteps=6850000, episode_reward=1.30 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 6850000     |
| train/                  |             |
|    approx_kl            | 0.018000819 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.696       |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 18.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 419      |
|    time_elapsed    | 23134    |
|    total_timesteps | 6864896  |
---------------------------------
Eval num_timesteps=6875000, episode_reward=1.10 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 6875000     |
| train/                  |             |
|    approx_kl            | 0.019896507 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 37.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.95    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 420      |
|    time_elapsed    | 23192    |
|    total_timesteps | 6881280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 421         |
|    time_elapsed         | 23241       |
|    total_timesteps      | 6897664     |
| train/                  |             |
|    approx_kl            | 0.020144623 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0659      |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 0.82        |
-----------------------------------------
Eval num_timesteps=6900000, episode_reward=1.20 +/- 0.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 6900000     |
| train/                  |             |
|    approx_kl            | 0.021181703 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.0583      |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 4210        |
|    policy_gradient_loss | 0.0027      |
|    value_loss           | 180         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 422      |
|    time_elapsed    | 23299    |
|    total_timesteps | 6914048  |
---------------------------------
Eval num_timesteps=6925000, episode_reward=1.53 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 6925000     |
| train/                  |             |
|    approx_kl            | 0.032873422 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 4220        |
|    policy_gradient_loss | 0.00546     |
|    value_loss           | 41.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -44.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 423      |
|    time_elapsed    | 23358    |
|    total_timesteps | 6930432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -43.2      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 424        |
|    time_elapsed         | 23407      |
|    total_timesteps      | 6946816    |
| train/                  |            |
|    approx_kl            | 0.02112114 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.3       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.00574   |
|    value_loss           | 1.14       |
----------------------------------------
Eval num_timesteps=6950000, episode_reward=1.41 +/- 0.40
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.41       |
| time/                   |            |
|    total_timesteps      | 6950000    |
| train/                  |            |
|    approx_kl            | 0.01624043 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.69      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.181      |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.00544   |
|    value_loss           | 0.53       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.02    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 425      |
|    time_elapsed    | 23466    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6975000, episode_reward=1.69 +/- 0.52
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 6975000    |
| train/                  |            |
|    approx_kl            | 0.02139853 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0798     |
|    n_updates            | 4250       |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 0.0869     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.56    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 426      |
|    time_elapsed    | 23524    |
|    total_timesteps | 6979584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.13       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 427         |
|    time_elapsed         | 23573       |
|    total_timesteps      | 6995968     |
| train/                  |             |
|    approx_kl            | 0.019477017 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.998       |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=1.68 +/- 0.40
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.68      |
| time/                   |           |
|    total_timesteps      | 7000000   |
| train/                  |           |
|    approx_kl            | 0.0205338 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.6      |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0196    |
|    n_updates            | 4270      |
|    policy_gradient_loss | -0.00799  |
|    value_loss           | 1.79      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 428      |
|    time_elapsed    | 23632    |
|    total_timesteps | 7012352  |
---------------------------------
Eval num_timesteps=7025000, episode_reward=1.66 +/- 1.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 7025000     |
| train/                  |             |
|    approx_kl            | 0.016749838 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.92        |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 68.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 429      |
|    time_elapsed    | 23690    |
|    total_timesteps | 7028736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 430         |
|    time_elapsed         | 23739       |
|    total_timesteps      | 7045120     |
| train/                  |             |
|    approx_kl            | 0.019636285 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.397       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 82.4        |
-----------------------------------------
Eval num_timesteps=7050000, episode_reward=1.10 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 7050000     |
| train/                  |             |
|    approx_kl            | 0.017379977 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 0.268       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 431      |
|    time_elapsed    | 23798    |
|    total_timesteps | 7061504  |
---------------------------------
Eval num_timesteps=7075000, episode_reward=1.87 +/- 0.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 7075000     |
| train/                  |             |
|    approx_kl            | 0.021250395 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.3        |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 81.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 432      |
|    time_elapsed    | 23856    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 433         |
|    time_elapsed         | 23905       |
|    total_timesteps      | 7094272     |
| train/                  |             |
|    approx_kl            | 0.018608892 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.77        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 15.1        |
-----------------------------------------
Eval num_timesteps=7100000, episode_reward=1.45 +/- 0.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 7100000     |
| train/                  |             |
|    approx_kl            | 0.019520054 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.236       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 0.405       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.5     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 434      |
|    time_elapsed    | 23964    |
|    total_timesteps | 7110656  |
---------------------------------
Eval num_timesteps=7125000, episode_reward=1.42 +/- 0.21
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 7125000     |
| train/                  |             |
|    approx_kl            | 0.020056862 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 1.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.99    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 435      |
|    time_elapsed    | 24022    |
|    total_timesteps | 7127040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.64       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 436         |
|    time_elapsed         | 24071       |
|    total_timesteps      | 7143424     |
| train/                  |             |
|    approx_kl            | 0.017877318 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 1.42        |
-----------------------------------------
Eval num_timesteps=7150000, episode_reward=1.37 +/- 0.28
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 7150000     |
| train/                  |             |
|    approx_kl            | 0.014979592 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.375       |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 1.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.72    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 437      |
|    time_elapsed    | 24130    |
|    total_timesteps | 7159808  |
---------------------------------
Eval num_timesteps=7175000, episode_reward=1.48 +/- 0.72
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 7175000     |
| train/                  |             |
|    approx_kl            | 0.019778706 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 0.175       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.69    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 438      |
|    time_elapsed    | 24188    |
|    total_timesteps | 7176192  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.449      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 439         |
|    time_elapsed         | 24237       |
|    total_timesteps      | 7192576     |
| train/                  |             |
|    approx_kl            | 0.019109154 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0436      |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00683    |
|    value_loss           | 4.08        |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=1.75 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.017818931 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0122      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 0.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 440      |
|    time_elapsed    | 24295    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7225000, episode_reward=1.44 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 7225000     |
| train/                  |             |
|    approx_kl            | 0.018519279 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.106       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.478       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 441      |
|    time_elapsed    | 24354    |
|    total_timesteps | 7225344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7.89       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 442         |
|    time_elapsed         | 24403       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.016225781 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0997      |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 59.6        |
-----------------------------------------
Eval num_timesteps=7250000, episode_reward=1.81 +/- 0.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 7250000     |
| train/                  |             |
|    approx_kl            | 0.020873144 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.0613      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.000201   |
|    value_loss           | 49.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 443      |
|    time_elapsed    | 24461    |
|    total_timesteps | 7258112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 600          |
|    ep_rew_mean          | -22.7        |
| time/                   |              |
|    fps                  | 296          |
|    iterations           | 444          |
|    time_elapsed         | 24511        |
|    total_timesteps      | 7274496      |
| train/                  |              |
|    approx_kl            | 0.0063757338 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.9          |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 89.6         |
------------------------------------------
Eval num_timesteps=7275000, episode_reward=1.25 +/- 0.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.25        |
| time/                   |             |
|    total_timesteps      | 7275000     |
| train/                  |             |
|    approx_kl            | 0.017984819 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0443      |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 0.561       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 445      |
|    time_elapsed    | 24569    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7300000, episode_reward=1.31 +/- 0.29
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.31       |
| time/                   |            |
|    total_timesteps      | 7300000    |
| train/                  |            |
|    approx_kl            | 0.01834866 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.82      |
|    explained_variance   | 0.4        |
|    learning_rate        | 0.0003     |
|    loss                 | 6.88       |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.00643   |
|    value_loss           | 44         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 446      |
|    time_elapsed    | 24627    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.01       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 447         |
|    time_elapsed         | 24677       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.018413868 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.573       |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 24.7        |
-----------------------------------------
Eval num_timesteps=7325000, episode_reward=1.85 +/- 1.34
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 1.85      |
| time/                   |           |
|    total_timesteps      | 7325000   |
| train/                  |           |
|    approx_kl            | 0.0193676 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.91     |
|    explained_variance   | 0.928     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0976    |
|    n_updates            | 4470      |
|    policy_gradient_loss | -0.00938  |
|    value_loss           | 1         |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.35    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 448      |
|    time_elapsed    | 24735    |
|    total_timesteps | 7340032  |
---------------------------------
Eval num_timesteps=7350000, episode_reward=1.31 +/- 0.41
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.31       |
| time/                   |            |
|    total_timesteps      | 7350000    |
| train/                  |            |
|    approx_kl            | 0.01570406 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.338      |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.00718   |
|    value_loss           | 1.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.58    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 449      |
|    time_elapsed    | 24793    |
|    total_timesteps | 7356416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | 0.00357    |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 450        |
|    time_elapsed         | 24843      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.01781464 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0612     |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.00718   |
|    value_loss           | 1.29       |
----------------------------------------
Eval num_timesteps=7375000, episode_reward=1.12 +/- 0.23
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.12        |
| time/                   |             |
|    total_timesteps      | 7375000     |
| train/                  |             |
|    approx_kl            | 0.019304814 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0499      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 0.335       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.0299   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 451      |
|    time_elapsed    | 24901    |
|    total_timesteps | 7389184  |
---------------------------------
Eval num_timesteps=7400000, episode_reward=1.08 +/- 0.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.08        |
| time/                   |             |
|    total_timesteps      | 7400000     |
| train/                  |             |
|    approx_kl            | 0.018607747 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0467      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 1.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 0.0263   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 452      |
|    time_elapsed    | 24959    |
|    total_timesteps | 7405568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | 0.81        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 453         |
|    time_elapsed         | 25009       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.018030573 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.099       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 0.233       |
-----------------------------------------
Eval num_timesteps=7425000, episode_reward=1.45 +/- 0.61
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.45       |
| time/                   |            |
|    total_timesteps      | 7425000    |
| train/                  |            |
|    approx_kl            | 0.01743286 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0184    |
|    n_updates            | 4530       |
|    policy_gradient_loss | -0.00691   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.02    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 454      |
|    time_elapsed    | 25067    |
|    total_timesteps | 7438336  |
---------------------------------
Eval num_timesteps=7450000, episode_reward=1.47 +/- 0.29
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 7450000     |
| train/                  |             |
|    approx_kl            | 0.015791245 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 18.9        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 455      |
|    time_elapsed    | 25125    |
|    total_timesteps | 7454720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -24.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 456         |
|    time_elapsed         | 25175       |
|    total_timesteps      | 7471104     |
| train/                  |             |
|    approx_kl            | 0.016810678 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.000898   |
|    value_loss           | 157         |
-----------------------------------------
Eval num_timesteps=7475000, episode_reward=1.16 +/- 0.17
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 7475000     |
| train/                  |             |
|    approx_kl            | 0.024865922 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | -0.0973     |
|    learning_rate        | 0.0003      |
|    loss                 | 13.5        |
|    n_updates            | 4560        |
|    policy_gradient_loss | 0.00145     |
|    value_loss           | 132         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 457      |
|    time_elapsed    | 25233    |
|    total_timesteps | 7487488  |
---------------------------------
Eval num_timesteps=7500000, episode_reward=1.19 +/- 0.10
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.19        |
| time/                   |             |
|    total_timesteps      | 7500000     |
| train/                  |             |
|    approx_kl            | 0.019307842 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 0.385       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -25.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 458      |
|    time_elapsed    | 25291    |
|    total_timesteps | 7503872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -15.9      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 459        |
|    time_elapsed         | 25341      |
|    total_timesteps      | 7520256    |
| train/                  |            |
|    approx_kl            | 0.01716135 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.00443   |
|    value_loss           | 22.9       |
----------------------------------------
Eval num_timesteps=7525000, episode_reward=1.22 +/- 0.21
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.22       |
| time/                   |            |
|    total_timesteps      | 7525000    |
| train/                  |            |
|    approx_kl            | 0.02120974 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.73      |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.55       |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.00675   |
|    value_loss           | 6.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 460      |
|    time_elapsed    | 25399    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7550000, episode_reward=1.27 +/- 0.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.27        |
| time/                   |             |
|    total_timesteps      | 7550000     |
| train/                  |             |
|    approx_kl            | 0.017290818 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 7.64        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.65    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 461      |
|    time_elapsed    | 25457    |
|    total_timesteps | 7553024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.68       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 462         |
|    time_elapsed         | 25507       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.020901792 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0292      |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 1.49        |
-----------------------------------------
Eval num_timesteps=7575000, episode_reward=1.37 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 7575000     |
| train/                  |             |
|    approx_kl            | 0.008989036 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 58.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 463      |
|    time_elapsed    | 25565    |
|    total_timesteps | 7585792  |
---------------------------------
Eval num_timesteps=7600000, episode_reward=2.73 +/- 2.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.73        |
| time/                   |             |
|    total_timesteps      | 7600000     |
| train/                  |             |
|    approx_kl            | 0.020678697 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0703      |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 1.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.83    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 464      |
|    time_elapsed    | 25623    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 465         |
|    time_elapsed         | 25673       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.020252254 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 13.2        |
-----------------------------------------
Eval num_timesteps=7625000, episode_reward=1.93 +/- 1.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 7625000     |
| train/                  |             |
|    approx_kl            | 0.020519154 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.424       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.385       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.31    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 466      |
|    time_elapsed    | 25731    |
|    total_timesteps | 7634944  |
---------------------------------
Eval num_timesteps=7650000, episode_reward=1.48 +/- 0.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.48       |
| time/                   |            |
|    total_timesteps      | 7650000    |
| train/                  |            |
|    approx_kl            | 0.01797561 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.73      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0892     |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.00865   |
|    value_loss           | 2.63       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.1     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 467      |
|    time_elapsed    | 25789    |
|    total_timesteps | 7651328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.52       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 468         |
|    time_elapsed         | 25839       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.018045219 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0582      |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 0.708       |
-----------------------------------------
Eval num_timesteps=7675000, episode_reward=1.71 +/- 0.83
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.71       |
| time/                   |            |
|    total_timesteps      | 7675000    |
| train/                  |            |
|    approx_kl            | 0.02020318 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.112      |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.00291   |
|    value_loss           | 14.4       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.28    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 469      |
|    time_elapsed    | 25897    |
|    total_timesteps | 7684096  |
---------------------------------
Eval num_timesteps=7700000, episode_reward=2.31 +/- 1.97
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.31       |
| time/                   |            |
|    total_timesteps      | 7700000    |
| train/                  |            |
|    approx_kl            | 0.01916065 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.84      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.424      |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.00364   |
|    value_loss           | 0.776      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.47    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 470      |
|    time_elapsed    | 25955    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.09       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 471         |
|    time_elapsed         | 26004       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.021520859 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0362      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=7725000, episode_reward=2.16 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 7725000     |
| train/                  |             |
|    approx_kl            | 0.024145951 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.59    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 472      |
|    time_elapsed    | 26063    |
|    total_timesteps | 7733248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 473         |
|    time_elapsed         | 26112       |
|    total_timesteps      | 7749632     |
| train/                  |             |
|    approx_kl            | 0.018442005 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.409       |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 7.2         |
-----------------------------------------
Eval num_timesteps=7750000, episode_reward=2.52 +/- 1.88
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.52       |
| time/                   |            |
|    total_timesteps      | 7750000    |
| train/                  |            |
|    approx_kl            | 0.02796566 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.0115     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0592     |
|    n_updates            | 4730       |
|    policy_gradient_loss | -0.00936   |
|    value_loss           | 150        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 474      |
|    time_elapsed    | 26170    |
|    total_timesteps | 7766016  |
---------------------------------
Eval num_timesteps=7775000, episode_reward=1.64 +/- 0.66
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 7775000     |
| train/                  |             |
|    approx_kl            | 0.019231457 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.409       |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 0.954       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 475      |
|    time_elapsed    | 26229    |
|    total_timesteps | 7782400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 476         |
|    time_elapsed         | 26278       |
|    total_timesteps      | 7798784     |
| train/                  |             |
|    approx_kl            | 0.025504425 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.0786      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0352      |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=7800000, episode_reward=2.10 +/- 1.11
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 7800000     |
| train/                  |             |
|    approx_kl            | 0.020733226 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0788      |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 7.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 477      |
|    time_elapsed    | 26336    |
|    total_timesteps | 7815168  |
---------------------------------
Eval num_timesteps=7825000, episode_reward=1.84 +/- 0.62
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 7825000     |
| train/                  |             |
|    approx_kl            | 0.019244432 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.0998      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0352      |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 43.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 478      |
|    time_elapsed    | 26395    |
|    total_timesteps | 7831552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 479         |
|    time_elapsed         | 26444       |
|    total_timesteps      | 7847936     |
| train/                  |             |
|    approx_kl            | 0.019162893 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.974       |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 20.8        |
-----------------------------------------
Eval num_timesteps=7850000, episode_reward=1.70 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 7850000     |
| train/                  |             |
|    approx_kl            | 0.019436598 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.622       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 9.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 480      |
|    time_elapsed    | 26502    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7875000, episode_reward=2.42 +/- 1.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 7875000     |
| train/                  |             |
|    approx_kl            | 0.019502986 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0257      |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 3.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.24    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 481      |
|    time_elapsed    | 26561    |
|    total_timesteps | 7880704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -15.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 482         |
|    time_elapsed         | 26610       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.008821073 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 214         |
-----------------------------------------
Eval num_timesteps=7900000, episode_reward=1.97 +/- 1.02
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 7900000     |
| train/                  |             |
|    approx_kl            | 0.023421055 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 3.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 483      |
|    time_elapsed    | 26668    |
|    total_timesteps | 7913472  |
---------------------------------
Eval num_timesteps=7925000, episode_reward=2.82 +/- 1.84
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 7925000     |
| train/                  |             |
|    approx_kl            | 0.012760007 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 264         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 484      |
|    time_elapsed    | 26727    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -119        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 485         |
|    time_elapsed         | 26776       |
|    total_timesteps      | 7946240     |
| train/                  |             |
|    approx_kl            | 0.022042591 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.436       |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 7.71        |
-----------------------------------------
Eval num_timesteps=7950000, episode_reward=2.22 +/- 1.50
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 7950000     |
| train/                  |             |
|    approx_kl            | 0.016823638 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.443       |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 23.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 486      |
|    time_elapsed    | 26834    |
|    total_timesteps | 7962624  |
---------------------------------
Eval num_timesteps=7975000, episode_reward=3.05 +/- 2.66
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.05       |
| time/                   |            |
|    total_timesteps      | 7975000    |
| train/                  |            |
|    approx_kl            | 0.02233854 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.45      |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.00507   |
|    value_loss           | 44         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 487      |
|    time_elapsed    | 26892    |
|    total_timesteps | 7979008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11.6      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 488        |
|    time_elapsed         | 26942      |
|    total_timesteps      | 7995392    |
| train/                  |            |
|    approx_kl            | 0.02155066 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.58      |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 4870       |
|    policy_gradient_loss | -0.00546   |
|    value_loss           | 10.8       |
----------------------------------------
Eval num_timesteps=8000000, episode_reward=2.47 +/- 1.43
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.47       |
| time/                   |            |
|    total_timesteps      | 8000000    |
| train/                  |            |
|    approx_kl            | 0.02211956 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.63      |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.76       |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.00545   |
|    value_loss           | 17         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 489      |
|    time_elapsed    | 27000    |
|    total_timesteps | 8011776  |
---------------------------------
Eval num_timesteps=8025000, episode_reward=-1157.16 +/- 2320.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.16e+03   |
| time/                   |             |
|    total_timesteps      | 8025000     |
| train/                  |             |
|    approx_kl            | 0.020292249 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 4.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.82    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 490      |
|    time_elapsed    | 27058    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.63       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 491         |
|    time_elapsed         | 27107       |
|    total_timesteps      | 8044544     |
| train/                  |             |
|    approx_kl            | 0.022232706 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 0.608       |
-----------------------------------------
Eval num_timesteps=8050000, episode_reward=1.71 +/- 1.07
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 8050000     |
| train/                  |             |
|    approx_kl            | 0.017614173 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.277       |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 41.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -7.61    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 492      |
|    time_elapsed    | 27166    |
|    total_timesteps | 8060928  |
---------------------------------
Eval num_timesteps=8075000, episode_reward=4.26 +/- 2.82
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.26        |
| time/                   |             |
|    total_timesteps      | 8075000     |
| train/                  |             |
|    approx_kl            | 0.021870181 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0306      |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 0.339       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 493      |
|    time_elapsed    | 27224    |
|    total_timesteps | 8077312  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -45         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 494         |
|    time_elapsed         | 27274       |
|    total_timesteps      | 8093696     |
| train/                  |             |
|    approx_kl            | 0.023067392 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.59        |
|    n_updates            | 4930        |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 273         |
-----------------------------------------
Eval num_timesteps=8100000, episode_reward=4.01 +/- 2.37
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 4.01       |
| time/                   |            |
|    total_timesteps      | 8100000    |
| train/                  |            |
|    approx_kl            | 0.02676501 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0599     |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.831      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -39.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 495      |
|    time_elapsed    | 27332    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8125000, episode_reward=1.83 +/- 0.97
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 8125000     |
| train/                  |             |
|    approx_kl            | 0.022097373 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0249      |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.000489   |
|    value_loss           | 21.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -95.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 496      |
|    time_elapsed    | 27391    |
|    total_timesteps | 8126464  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 497         |
|    time_elapsed         | 27440       |
|    total_timesteps      | 8142848     |
| train/                  |             |
|    approx_kl            | 0.020967081 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0806      |
|    n_updates            | 4960        |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=8150000, episode_reward=2.94 +/- 1.73
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.94       |
| time/                   |            |
|    total_timesteps      | 8150000    |
| train/                  |            |
|    approx_kl            | 0.02014375 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.58      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0432     |
|    n_updates            | 4970       |
|    policy_gradient_loss | -0.00151   |
|    value_loss           | 14         |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 498      |
|    time_elapsed    | 27498    |
|    total_timesteps | 8159232  |
---------------------------------
Eval num_timesteps=8175000, episode_reward=2.35 +/- 2.14
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.35        |
| time/                   |             |
|    total_timesteps      | 8175000     |
| train/                  |             |
|    approx_kl            | 0.019643053 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 2.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.87    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 499      |
|    time_elapsed    | 27556    |
|    total_timesteps | 8175616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -19.9       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 500         |
|    time_elapsed         | 27606       |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.019733962 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.14       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.45        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 0.893       |
-----------------------------------------
Eval num_timesteps=8200000, episode_reward=3.04 +/- 2.20
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.04       |
| time/                   |            |
|    total_timesteps      | 8200000    |
| train/                  |            |
|    approx_kl            | 0.02466029 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.34      |
|    explained_variance   | 0.0406     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0666     |
|    n_updates            | 5000       |
|    policy_gradient_loss | 0.00343    |
|    value_loss           | 107        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 501      |
|    time_elapsed    | 27664    |
|    total_timesteps | 8208384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -25.5      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 502        |
|    time_elapsed         | 27713      |
|    total_timesteps      | 8224768    |
| train/                  |            |
|    approx_kl            | 0.02214525 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0549     |
|    n_updates            | 5010       |
|    policy_gradient_loss | -0.00594   |
|    value_loss           | 1.34       |
----------------------------------------
Eval num_timesteps=8225000, episode_reward=1.62 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 8225000     |
| train/                  |             |
|    approx_kl            | 0.011356068 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.000769   |
|    value_loss           | 96.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -60.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 503      |
|    time_elapsed    | 27772    |
|    total_timesteps | 8241152  |
---------------------------------
Eval num_timesteps=8250000, episode_reward=3.89 +/- 1.84
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 3.89         |
| time/                   |              |
|    total_timesteps      | 8250000      |
| train/                  |              |
|    approx_kl            | 0.0061758384 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 81           |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 459          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 504      |
|    time_elapsed    | 27830    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -134        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 505         |
|    time_elapsed         | 27879       |
|    total_timesteps      | 8273920     |
| train/                  |             |
|    approx_kl            | 0.022140898 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 65.4        |
-----------------------------------------
Eval num_timesteps=8275000, episode_reward=2.45 +/- 2.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.45        |
| time/                   |             |
|    total_timesteps      | 8275000     |
| train/                  |             |
|    approx_kl            | 0.028101968 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.164       |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 147         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 506      |
|    time_elapsed    | 27938    |
|    total_timesteps | 8290304  |
---------------------------------
Eval num_timesteps=8300000, episode_reward=1.79 +/- 1.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 8300000     |
| train/                  |             |
|    approx_kl            | 0.016536271 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.54        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 247         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -304     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 507      |
|    time_elapsed    | 27996    |
|    total_timesteps | 8306688  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -309        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 508         |
|    time_elapsed         | 28045       |
|    total_timesteps      | 8323072     |
| train/                  |             |
|    approx_kl            | 0.018021788 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.000638   |
|    value_loss           | 81.6        |
-----------------------------------------
Eval num_timesteps=8325000, episode_reward=2.40 +/- 1.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 8325000     |
| train/                  |             |
|    approx_kl            | 0.021246009 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.774       |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 109         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 509      |
|    time_elapsed    | 28104    |
|    total_timesteps | 8339456  |
---------------------------------
Eval num_timesteps=8350000, episode_reward=-1084.26 +/- 2171.13
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -1.08e+03   |
| time/                   |             |
|    total_timesteps      | 8350000     |
| train/                  |             |
|    approx_kl            | 0.020725803 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.629       |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 31.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 510      |
|    time_elapsed    | 28162    |
|    total_timesteps | 8355840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -11.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 511         |
|    time_elapsed         | 28211       |
|    total_timesteps      | 8372224     |
| train/                  |             |
|    approx_kl            | 0.017735315 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.279       |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 5.38        |
-----------------------------------------
Eval num_timesteps=8375000, episode_reward=2.14 +/- 1.30
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.14       |
| time/                   |            |
|    total_timesteps      | 8375000    |
| train/                  |            |
|    approx_kl            | 0.02213876 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.773      |
|    n_updates            | 5110       |
|    policy_gradient_loss | -0.000175  |
|    value_loss           | 7.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.9     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 512      |
|    time_elapsed    | 28269    |
|    total_timesteps | 8388608  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=1.62 +/- 0.91
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.022878677 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.8        |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 5.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3       |
| time/              |          |
|    fps             | 296      |
|    iterations      | 513      |
|    time_elapsed    | 28328    |
|    total_timesteps | 8404992  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.76       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 514         |
|    time_elapsed         | 28377       |
|    total_timesteps      | 8421376     |
| train/                  |             |
|    approx_kl            | 0.019097108 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0123      |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.299       |
-----------------------------------------
Eval num_timesteps=8425000, episode_reward=1.38 +/- 0.25
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 8425000     |
| train/                  |             |
|    approx_kl            | 0.017661585 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 3.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -73.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 515      |
|    time_elapsed    | 28435    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8450000, episode_reward=4.01 +/- 1.94
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.01        |
| time/                   |             |
|    total_timesteps      | 8450000     |
| train/                  |             |
|    approx_kl            | 0.017941015 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.0318      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.17        |
|    n_updates            | 5150        |
|    policy_gradient_loss | 0.00113     |
|    value_loss           | 622         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 516      |
|    time_elapsed    | 28494    |
|    total_timesteps | 8454144  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -71         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 517         |
|    time_elapsed         | 28543       |
|    total_timesteps      | 8470528     |
| train/                  |             |
|    approx_kl            | 0.019902024 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0755      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 0.457       |
-----------------------------------------
Eval num_timesteps=8475000, episode_reward=2.49 +/- 1.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 8475000     |
| train/                  |             |
|    approx_kl            | 0.021623475 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.128       |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 3.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -71.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 518      |
|    time_elapsed    | 28602    |
|    total_timesteps | 8486912  |
---------------------------------
Eval num_timesteps=8500000, episode_reward=2.97 +/- 2.03
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 8500000     |
| train/                  |             |
|    approx_kl            | 0.018638173 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0264      |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 1.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.73    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 519      |
|    time_elapsed    | 28660    |
|    total_timesteps | 8503296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.5       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 520        |
|    time_elapsed         | 28709      |
|    total_timesteps      | 8519680    |
| train/                  |            |
|    approx_kl            | 0.02090596 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.451      |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.00618   |
|    value_loss           | 3.12       |
----------------------------------------
Eval num_timesteps=8525000, episode_reward=1.53 +/- 0.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.53        |
| time/                   |             |
|    total_timesteps      | 8525000     |
| train/                  |             |
|    approx_kl            | 0.020584134 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 0.553       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 521      |
|    time_elapsed    | 28768    |
|    total_timesteps | 8536064  |
---------------------------------
Eval num_timesteps=8550000, episode_reward=3.42 +/- 2.65
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.42        |
| time/                   |             |
|    total_timesteps      | 8550000     |
| train/                  |             |
|    approx_kl            | 0.010990627 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 4.74        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.51    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 522      |
|    time_elapsed    | 28826    |
|    total_timesteps | 8552448  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.39       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 523         |
|    time_elapsed         | 28875       |
|    total_timesteps      | 8568832     |
| train/                  |             |
|    approx_kl            | 0.020498838 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0133      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 0.0292      |
-----------------------------------------
Eval num_timesteps=8575000, episode_reward=3.92 +/- 2.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.92        |
| time/                   |             |
|    total_timesteps      | 8575000     |
| train/                  |             |
|    approx_kl            | 0.019390952 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00722    |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 0.252       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.53    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 524      |
|    time_elapsed    | 28934    |
|    total_timesteps | 8585216  |
---------------------------------
Eval num_timesteps=8600000, episode_reward=4.59 +/- 2.31
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.59        |
| time/                   |             |
|    total_timesteps      | 8600000     |
| train/                  |             |
|    approx_kl            | 0.017163116 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0197      |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 7.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.22    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 525      |
|    time_elapsed    | 28992    |
|    total_timesteps | 8601600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -38.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 526         |
|    time_elapsed         | 29041       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.022315484 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.668       |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 4.5         |
-----------------------------------------
Eval num_timesteps=8625000, episode_reward=3.73 +/- 2.63
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.73        |
| time/                   |             |
|    total_timesteps      | 8625000     |
| train/                  |             |
|    approx_kl            | 0.010706753 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | -0.00424    |
|    learning_rate        | 0.0003      |
|    loss                 | 7.65        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 231         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -43.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 527      |
|    time_elapsed    | 29100    |
|    total_timesteps | 8634368  |
---------------------------------
Eval num_timesteps=8650000, episode_reward=3.18 +/- 2.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.18        |
| time/                   |             |
|    total_timesteps      | 8650000     |
| train/                  |             |
|    approx_kl            | 0.018765463 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 63.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -45.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 528      |
|    time_elapsed    | 29158    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -33.8      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 529        |
|    time_elapsed         | 29207      |
|    total_timesteps      | 8667136    |
| train/                  |            |
|    approx_kl            | 0.02367244 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.62      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.4       |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.00513   |
|    value_loss           | 9.41       |
----------------------------------------
Eval num_timesteps=8675000, episode_reward=2.24 +/- 1.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 8675000     |
| train/                  |             |
|    approx_kl            | 0.007979853 |
|    clip_fraction        | 0.0607      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 530      |
|    time_elapsed    | 29266    |
|    total_timesteps | 8683520  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -22.7       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 531         |
|    time_elapsed         | 29315       |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.020837544 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.216       |
|    n_updates            | 5300        |
|    policy_gradient_loss | -2.82e-06   |
|    value_loss           | 0.768       |
-----------------------------------------
Eval num_timesteps=8700000, episode_reward=1.87 +/- 0.39
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.87        |
| time/                   |             |
|    total_timesteps      | 8700000     |
| train/                  |             |
|    approx_kl            | 0.021488845 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 1.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 532      |
|    time_elapsed    | 29373    |
|    total_timesteps | 8716288  |
---------------------------------
Eval num_timesteps=8725000, episode_reward=3.52 +/- 3.48
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.52        |
| time/                   |             |
|    total_timesteps      | 8725000     |
| train/                  |             |
|    approx_kl            | 0.016831443 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.3        |
|    n_updates            | 5320        |
|    policy_gradient_loss | 0.000728    |
|    value_loss           | 139         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 533      |
|    time_elapsed    | 29431    |
|    total_timesteps | 8732672  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -210        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 534         |
|    time_elapsed         | 29481       |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.013103639 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 68.8        |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=8750000, episode_reward=2.53 +/- 1.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.53        |
| time/                   |             |
|    total_timesteps      | 8750000     |
| train/                  |             |
|    approx_kl            | 0.020491377 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.12        |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.000566   |
|    value_loss           | 21.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 535      |
|    time_elapsed    | 29539    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8775000, episode_reward=1.29 +/- 0.41
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 8775000     |
| train/                  |             |
|    approx_kl            | 0.019886449 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 1.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 536      |
|    time_elapsed    | 29597    |
|    total_timesteps | 8781824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -29.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 537         |
|    time_elapsed         | 29646       |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.021844849 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0909      |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00196    |
|    value_loss           | 4.32        |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=3.03 +/- 2.79
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.03        |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.016139109 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0968      |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 90          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -23.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 538      |
|    time_elapsed    | 29705    |
|    total_timesteps | 8814592  |
---------------------------------
Eval num_timesteps=8825000, episode_reward=2.47 +/- 1.68
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.47       |
| time/                   |            |
|    total_timesteps      | 8825000    |
| train/                  |            |
|    approx_kl            | 0.02617414 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.74      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0788     |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.00683   |
|    value_loss           | 1.96       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -24.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 539      |
|    time_elapsed    | 29763    |
|    total_timesteps | 8830976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.671      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 540         |
|    time_elapsed         | 29812       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.021328885 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.05        |
|    n_updates            | 5390        |
|    policy_gradient_loss | 0.000959    |
|    value_loss           | 20          |
-----------------------------------------
Eval num_timesteps=8850000, episode_reward=2.20 +/- 1.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 8850000     |
| train/                  |             |
|    approx_kl            | 0.026804896 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0189      |
|    n_updates            | 5400        |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 1.72        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.717   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 541      |
|    time_elapsed    | 29871    |
|    total_timesteps | 8863744  |
---------------------------------
Eval num_timesteps=8875000, episode_reward=1.89 +/- 0.80
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 8875000     |
| train/                  |             |
|    approx_kl            | 0.019595813 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.336       |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 3.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.679   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 542      |
|    time_elapsed    | 29929    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -0.772      |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 543         |
|    time_elapsed         | 29978       |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.021670584 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 5420        |
|    policy_gradient_loss | 0.000816    |
|    value_loss           | 3.51        |
-----------------------------------------
Eval num_timesteps=8900000, episode_reward=2.19 +/- 1.82
Episode length: 600.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 600       |
|    mean_reward          | 2.19      |
| time/                   |           |
|    total_timesteps      | 8900000   |
| train/                  |           |
|    approx_kl            | 0.0233794 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.75     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0631    |
|    n_updates            | 5430      |
|    policy_gradient_loss | -0.00902  |
|    value_loss           | 0.759     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.848   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 544      |
|    time_elapsed    | 30037    |
|    total_timesteps | 8912896  |
---------------------------------
Eval num_timesteps=8925000, episode_reward=2.68 +/- 2.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 8925000     |
| train/                  |             |
|    approx_kl            | 0.019577388 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 0.587       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -0.305   |
| time/              |          |
|    fps             | 296      |
|    iterations      | 545      |
|    time_elapsed    | 30095    |
|    total_timesteps | 8929280  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11        |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 546        |
|    time_elapsed         | 30144      |
|    total_timesteps      | 8945664    |
| train/                  |            |
|    approx_kl            | 0.01996725 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.58      |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0145    |
|    n_updates            | 5450       |
|    policy_gradient_loss | -0.0089    |
|    value_loss           | 1.35       |
----------------------------------------
Eval num_timesteps=8950000, episode_reward=2.94 +/- 2.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 8950000     |
| train/                  |             |
|    approx_kl            | 0.026088245 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0839      |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 63.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 547      |
|    time_elapsed    | 30203    |
|    total_timesteps | 8962048  |
---------------------------------
Eval num_timesteps=8975000, episode_reward=1.06 +/- 0.08
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.06        |
| time/                   |             |
|    total_timesteps      | 8975000     |
| train/                  |             |
|    approx_kl            | 0.019039307 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0431      |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 548      |
|    time_elapsed    | 30261    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -12.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 549         |
|    time_elapsed         | 30310       |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.024612097 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.245       |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 4.8         |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2.73 +/- 2.11
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.73       |
| time/                   |            |
|    total_timesteps      | 9000000    |
| train/                  |            |
|    approx_kl            | 0.02762983 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.72      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.000269  |
|    value_loss           | 30.5       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -3.56    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 550      |
|    time_elapsed    | 30368    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9025000, episode_reward=1.65 +/- 0.68
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 9025000     |
| train/                  |             |
|    approx_kl            | 0.017552964 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0735      |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 2.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -2.45    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 551      |
|    time_elapsed    | 30427    |
|    total_timesteps | 9027584  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.35       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 552         |
|    time_elapsed         | 30476       |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.020892264 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.233       |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 0.374       |
-----------------------------------------
Eval num_timesteps=9050000, episode_reward=2.95 +/- 1.96
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 2.95       |
| time/                   |            |
|    total_timesteps      | 9050000    |
| train/                  |            |
|    approx_kl            | 0.02281316 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.524      |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.00178   |
|    value_loss           | 16.3       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 553      |
|    time_elapsed    | 30534    |
|    total_timesteps | 9060352  |
---------------------------------
Eval num_timesteps=9075000, episode_reward=1.64 +/- 0.96
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 9075000     |
| train/                  |             |
|    approx_kl            | 0.027186193 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 208         |
|    n_updates            | 5530        |
|    policy_gradient_loss | 0.00298     |
|    value_loss           | 36.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.35    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 554      |
|    time_elapsed    | 30593    |
|    total_timesteps | 9076736  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -5.66       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 555         |
|    time_elapsed         | 30642       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.015014275 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 4.79        |
-----------------------------------------
Eval num_timesteps=9100000, episode_reward=2.50 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 9100000     |
| train/                  |             |
|    approx_kl            | 0.012290997 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00251    |
|    value_loss           | 17          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.21    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 556      |
|    time_elapsed    | 30700    |
|    total_timesteps | 9109504  |
---------------------------------
Eval num_timesteps=9125000, episode_reward=2.99 +/- 2.32
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.99        |
| time/                   |             |
|    total_timesteps      | 9125000     |
| train/                  |             |
|    approx_kl            | 0.026541779 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 5560        |
|    policy_gradient_loss | 0.00156     |
|    value_loss           | 4.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -1.35    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 557      |
|    time_elapsed    | 30759    |
|    total_timesteps | 9125888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -1.46       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 558         |
|    time_elapsed         | 30808       |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.023876227 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00203     |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.536       |
-----------------------------------------
Eval num_timesteps=9150000, episode_reward=2.15 +/- 1.16
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.15        |
| time/                   |             |
|    total_timesteps      | 9150000     |
| train/                  |             |
|    approx_kl            | 0.011562767 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0646      |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 4.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 559      |
|    time_elapsed    | 30866    |
|    total_timesteps | 9158656  |
---------------------------------
Eval num_timesteps=9175000, episode_reward=-9.61 +/- 27.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -9.61       |
| time/                   |             |
|    total_timesteps      | 9175000     |
| train/                  |             |
|    approx_kl            | 0.017365221 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.39        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 88.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 560      |
|    time_elapsed    | 30925    |
|    total_timesteps | 9175040  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 561         |
|    time_elapsed         | 30974       |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.023425573 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.474       |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=9200000, episode_reward=1.48 +/- 0.78
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.018000089 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0584      |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 1.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.85    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 562      |
|    time_elapsed    | 31032    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.53       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 563         |
|    time_elapsed         | 31081       |
|    total_timesteps      | 9224192     |
| train/                  |             |
|    approx_kl            | 0.009557677 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 18.6        |
-----------------------------------------
Eval num_timesteps=9225000, episode_reward=2.56 +/- 1.24
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.56        |
| time/                   |             |
|    total_timesteps      | 9225000     |
| train/                  |             |
|    approx_kl            | 0.021902414 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.076       |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 0.918       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 564      |
|    time_elapsed    | 31140    |
|    total_timesteps | 9240576  |
---------------------------------
Eval num_timesteps=9250000, episode_reward=2.87 +/- 1.12
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.87        |
| time/                   |             |
|    total_timesteps      | 9250000     |
| train/                  |             |
|    approx_kl            | 0.015507444 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.16        |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 18.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 565      |
|    time_elapsed    | 31198    |
|    total_timesteps | 9256960  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -11.3      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 566        |
|    time_elapsed         | 31248      |
|    total_timesteps      | 9273344    |
| train/                  |            |
|    approx_kl            | 0.02113211 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.78      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.189      |
|    n_updates            | 5650       |
|    policy_gradient_loss | -0.00495   |
|    value_loss           | 1.33       |
----------------------------------------
Eval num_timesteps=9275000, episode_reward=1.62 +/- 1.20
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 9275000     |
| train/                  |             |
|    approx_kl            | 0.013639021 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.643       |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 22.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 567      |
|    time_elapsed    | 31306    |
|    total_timesteps | 9289728  |
---------------------------------
Eval num_timesteps=9300000, episode_reward=4.05 +/- 1.87
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.05        |
| time/                   |             |
|    total_timesteps      | 9300000     |
| train/                  |             |
|    approx_kl            | 0.020640463 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.161       |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 3.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8.79    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 568      |
|    time_elapsed    | 31364    |
|    total_timesteps | 9306112  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -10.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 569         |
|    time_elapsed         | 31414       |
|    total_timesteps      | 9322496     |
| train/                  |             |
|    approx_kl            | 0.019568123 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.000517   |
|    value_loss           | 47          |
-----------------------------------------
Eval num_timesteps=9325000, episode_reward=3.55 +/- 2.33
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.55       |
| time/                   |            |
|    total_timesteps      | 9325000    |
| train/                  |            |
|    approx_kl            | 0.02145663 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.27      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.346      |
|    n_updates            | 5690       |
|    policy_gradient_loss | -0.00657   |
|    value_loss           | 2.01       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.81    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 570      |
|    time_elapsed    | 31472    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9350000, episode_reward=5.56 +/- 3.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 5.56        |
| time/                   |             |
|    total_timesteps      | 9350000     |
| train/                  |             |
|    approx_kl            | 0.020840745 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0832      |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 2.82        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 571      |
|    time_elapsed    | 31530    |
|    total_timesteps | 9355264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -9.4        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 572         |
|    time_elapsed         | 31580       |
|    total_timesteps      | 9371648     |
| train/                  |             |
|    approx_kl            | 0.023482252 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.874       |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=9375000, episode_reward=2.58 +/- 1.06
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 9375000     |
| train/                  |             |
|    approx_kl            | 0.017064493 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.782       |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 19.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 573      |
|    time_elapsed    | 31638    |
|    total_timesteps | 9388032  |
---------------------------------
Eval num_timesteps=9400000, episode_reward=1.96 +/- 1.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 9400000     |
| train/                  |             |
|    approx_kl            | 0.022064904 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0958      |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00861    |
|    value_loss           | 0.314       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.98    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 574      |
|    time_elapsed    | 31696    |
|    total_timesteps | 9404416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.71       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 575         |
|    time_elapsed         | 31745       |
|    total_timesteps      | 9420800     |
| train/                  |             |
|    approx_kl            | 0.010838419 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 13.6        |
-----------------------------------------
Eval num_timesteps=9425000, episode_reward=4.59 +/- 2.56
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 4.59        |
| time/                   |             |
|    total_timesteps      | 9425000     |
| train/                  |             |
|    approx_kl            | 0.033799395 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.07        |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 12.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -58.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 576      |
|    time_elapsed    | 31804    |
|    total_timesteps | 9437184  |
---------------------------------
Eval num_timesteps=9450000, episode_reward=1.38 +/- 0.61
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 9450000      |
| train/                  |              |
|    approx_kl            | 0.0154930055 |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.76        |
|    explained_variance   | -3.89        |
|    learning_rate        | 0.0003       |
|    loss                 | 39.5         |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 229          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -59.5    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 577      |
|    time_elapsed    | 31862    |
|    total_timesteps | 9453568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 578         |
|    time_elapsed         | 31911       |
|    total_timesteps      | 9469952     |
| train/                  |             |
|    approx_kl            | 0.022154592 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.261       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00913    |
|    value_loss           | 8.42        |
-----------------------------------------
Eval num_timesteps=9475000, episode_reward=2.33 +/- 1.00
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 9475000     |
| train/                  |             |
|    approx_kl            | 0.024086729 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.846       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -48.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 579      |
|    time_elapsed    | 31970    |
|    total_timesteps | 9486336  |
---------------------------------
Eval num_timesteps=9500000, episode_reward=1.89 +/- 0.46
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 9500000     |
| train/                  |             |
|    approx_kl            | 0.016947601 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.512       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 35.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -62.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 580      |
|    time_elapsed    | 32028    |
|    total_timesteps | 9502720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -77.6       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 581         |
|    time_elapsed         | 32077       |
|    total_timesteps      | 9519104     |
| train/                  |             |
|    approx_kl            | 0.043961693 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.4        |
|    n_updates            | 5800        |
|    policy_gradient_loss | 0.00632     |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=9525000, episode_reward=1.66 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 9525000     |
| train/                  |             |
|    approx_kl            | 0.025438227 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.08        |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.000449   |
|    value_loss           | 94.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.4    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 582      |
|    time_elapsed    | 32135    |
|    total_timesteps | 9535488  |
---------------------------------
Eval num_timesteps=9550000, episode_reward=2.00 +/- 1.09
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 9550000     |
| train/                  |             |
|    approx_kl            | 0.021913566 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.259       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 4.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -78.6    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 583      |
|    time_elapsed    | 32194    |
|    total_timesteps | 9551872  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -16.1       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 584         |
|    time_elapsed         | 32243       |
|    total_timesteps      | 9568256     |
| train/                  |             |
|    approx_kl            | 0.024490919 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.561       |
-----------------------------------------
Eval num_timesteps=9575000, episode_reward=1.85 +/- 0.57
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 9575000     |
| train/                  |             |
|    approx_kl            | 0.021513857 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0978      |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 585      |
|    time_elapsed    | 32301    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=3.61 +/- 4.61
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.61        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.018709455 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | 232         |
|    n_updates            | 5850        |
|    policy_gradient_loss | 0.000932    |
|    value_loss           | 97.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 586      |
|    time_elapsed    | 32360    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -18.3       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 587         |
|    time_elapsed         | 32409       |
|    total_timesteps      | 9617408     |
| train/                  |             |
|    approx_kl            | 0.023073982 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.416       |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 21.7        |
-----------------------------------------
Eval num_timesteps=9625000, episode_reward=2.01 +/- 1.35
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 9625000     |
| train/                  |             |
|    approx_kl            | 0.020394914 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.2        |
|    n_updates            | 5870        |
|    policy_gradient_loss | 0.00153     |
|    value_loss           | 13.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -20      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 588      |
|    time_elapsed    | 32468    |
|    total_timesteps | 9633792  |
---------------------------------
Eval num_timesteps=9650000, episode_reward=2.42 +/- 1.58
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 9650000     |
| train/                  |             |
|    approx_kl            | 0.022922475 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 8.76        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.89    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 589      |
|    time_elapsed    | 32526    |
|    total_timesteps | 9650176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.4        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 590         |
|    time_elapsed         | 32575       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.026728535 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 3.19        |
-----------------------------------------
Eval num_timesteps=9675000, episode_reward=1.70 +/- 0.70
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 9675000     |
| train/                  |             |
|    approx_kl            | 0.019637482 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.0754      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 21.4        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.54    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 591      |
|    time_elapsed    | 32634    |
|    total_timesteps | 9682944  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -8.23       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 592         |
|    time_elapsed         | 32683       |
|    total_timesteps      | 9699328     |
| train/                  |             |
|    approx_kl            | 0.023286676 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.11        |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 19.4        |
-----------------------------------------
Eval num_timesteps=9700000, episode_reward=1.89 +/- 1.02
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 1.89       |
| time/                   |            |
|    total_timesteps      | 9700000    |
| train/                  |            |
|    approx_kl            | 0.03210684 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.65      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0146     |
|    n_updates            | 5920       |
|    policy_gradient_loss | 0.00126    |
|    value_loss           | 7.89       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -9.01    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 593      |
|    time_elapsed    | 32741    |
|    total_timesteps | 9715712  |
---------------------------------
Eval num_timesteps=9725000, episode_reward=1.67 +/- 0.99
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.67        |
| time/                   |             |
|    total_timesteps      | 9725000     |
| train/                  |             |
|    approx_kl            | 0.023461223 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 1.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.77    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 594      |
|    time_elapsed    | 32800    |
|    total_timesteps | 9732096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -5.95      |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 595        |
|    time_elapsed         | 32849      |
|    total_timesteps      | 9748480    |
| train/                  |            |
|    approx_kl            | 0.02070906 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.00724   |
|    value_loss           | 16.8       |
----------------------------------------
Eval num_timesteps=9750000, episode_reward=2.93 +/- 1.67
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 9750000     |
| train/                  |             |
|    approx_kl            | 0.021216555 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.00606    |
|    value_loss           | 0.334       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.27    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 596      |
|    time_elapsed    | 32907    |
|    total_timesteps | 9764864  |
---------------------------------
Eval num_timesteps=9775000, episode_reward=1.43 +/- 0.59
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.43        |
| time/                   |             |
|    total_timesteps      | 9775000     |
| train/                  |             |
|    approx_kl            | 0.019948933 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0789      |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00872    |
|    value_loss           | 2.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 597      |
|    time_elapsed    | 32965    |
|    total_timesteps | 9781248  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -7          |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 598         |
|    time_elapsed         | 33015       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.022740666 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.185       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 93.8        |
-----------------------------------------
Eval num_timesteps=9800000, episode_reward=3.01 +/- 2.28
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.01       |
| time/                   |            |
|    total_timesteps      | 9800000    |
| train/                  |            |
|    approx_kl            | 0.02873078 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.7       |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.517      |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 0.741      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -18.9    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 599      |
|    time_elapsed    | 33073    |
|    total_timesteps | 9814016  |
---------------------------------
Eval num_timesteps=9825000, episode_reward=2.46 +/- 1.60
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 9825000     |
| train/                  |             |
|    approx_kl            | 0.020038845 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 87          |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 205         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -72.2    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 600      |
|    time_elapsed    | 33131    |
|    total_timesteps | 9830400  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -70.5       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 601         |
|    time_elapsed         | 33180       |
|    total_timesteps      | 9846784     |
| train/                  |             |
|    approx_kl            | 0.012911864 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 369         |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 236         |
-----------------------------------------
Eval num_timesteps=9850000, episode_reward=3.28 +/- 1.76
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 3.28        |
| time/                   |             |
|    total_timesteps      | 9850000     |
| train/                  |             |
|    approx_kl            | 0.025650207 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 0.462       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -70.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 602      |
|    time_elapsed    | 33239    |
|    total_timesteps | 9863168  |
---------------------------------
Eval num_timesteps=9875000, episode_reward=2.37 +/- 1.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.37        |
| time/                   |             |
|    total_timesteps      | 9875000     |
| train/                  |             |
|    approx_kl            | 0.019259054 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 7.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 603      |
|    time_elapsed    | 33297    |
|    total_timesteps | 9879552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -4.73       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 604         |
|    time_elapsed         | 33346       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.025362138 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.993       |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 3.04        |
-----------------------------------------
Eval num_timesteps=9900000, episode_reward=3.19 +/- 3.49
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 3.19       |
| time/                   |            |
|    total_timesteps      | 9900000    |
| train/                  |            |
|    approx_kl            | 0.01986843 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.35      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 46         |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.00537   |
|    value_loss           | 16.2       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.21    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 605      |
|    time_elapsed    | 33405    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9925000, episode_reward=2.59 +/- 2.18
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.59        |
| time/                   |             |
|    total_timesteps      | 9925000     |
| train/                  |             |
|    approx_kl            | 0.021729304 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0393      |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.908       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.65    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 606      |
|    time_elapsed    | 33463    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -3.26       |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 607         |
|    time_elapsed         | 33512       |
|    total_timesteps      | 9945088     |
| train/                  |             |
|    approx_kl            | 0.020907946 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0792      |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 0.972       |
-----------------------------------------
Eval num_timesteps=9950000, episode_reward=2.09 +/- 1.49
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 2.09        |
| time/                   |             |
|    total_timesteps      | 9950000     |
| train/                  |             |
|    approx_kl            | 0.016520858 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 4.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.03    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 608      |
|    time_elapsed    | 33572    |
|    total_timesteps | 9961472  |
---------------------------------
Eval num_timesteps=9975000, episode_reward=1.85 +/- 1.36
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 9975000     |
| train/                  |             |
|    approx_kl            | 0.021740671 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.301       |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 21.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -6.02    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 609      |
|    time_elapsed    | 33630    |
|    total_timesteps | 9977856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -6.1        |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 610         |
|    time_elapsed         | 33679       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.028888416 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 3.81        |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=5.16 +/- 4.07
Episode length: 600.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 600        |
|    mean_reward          | 5.16       |
| time/                   |            |
|    total_timesteps      | 10000000   |
| train/                  |            |
|    approx_kl            | 0.02362994 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0646     |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 0.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -4.22    |
| time/              |          |
|    fps             | 296      |
|    iterations      | 611      |
|    time_elapsed    | 33738    |
|    total_timesteps | 10010624 |
---------------------------------
Saving to logs/ppo/uav-v4_2
