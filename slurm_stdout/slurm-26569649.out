========== uav-v0 ==========
Seed: 1540008232
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('n_envs', 8),
             ('n_timesteps', 10000000.0),
             ('policy', 'MultiInputPolicy')])
Using 8 environments
Creating test environment
Using cpu device
Log path: logs/ppo/uav-v0_3
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -7.03e+03 |
| time/              |           |
|    fps             | 358       |
|    iterations      | 1         |
|    time_elapsed    | 45        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-5783.47 +/- 4864.14
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -5.78e+03    |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0070910775 |
|    clip_fraction        | 0.0564       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | 0.000375     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00748     |
|    value_loss           | 5.03e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -5.6e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 2        |
|    time_elapsed    | 117      |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 600        |
|    ep_rew_mean          | -4.6e+03   |
| time/                   |            |
|    fps                  | 277        |
|    iterations           | 3          |
|    time_elapsed         | 176        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.00873053 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.82      |
|    explained_variance   | -2.25e-05  |
|    learning_rate        | 0.0003     |
|    loss                 | 6.89e+03   |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00985   |
|    value_loss           | 2.06e+04   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-955.05 +/- 298.83
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -955        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.017563954 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.39e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 1.08e+04    |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -4.21e+03 |
| time/              |           |
|    fps             | 263       |
|    iterations      | 4         |
|    time_elapsed    | 248       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-2493.08 +/- 2299.64
Episode length: 600.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | -2.49e+03   |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.015497886 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | -7.15e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 7.2e+03     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 1.85e+04    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 600       |
|    ep_rew_mean     | -3.03e+03 |
| time/              |           |
|    fps             | 255       |
|    iterations      | 5         |
|    time_elapsed    | 320       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 600         |
|    ep_rew_mean          | -2.9e+03    |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 6           |
|    time_elapsed         | 379         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.010205125 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 2.38e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.65e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 8.93e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-1679.13 +/- 239.99
Episode length: 600.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | -1.68e+03    |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0056191464 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.57e+03     |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0047      |
|    value_loss           | 1.34e+04     |
------------------------------------------
slurmstepd: error: *** JOB 26569649 ON m3i020 CANCELLED AT 2022-06-27T13:15:23 ***
